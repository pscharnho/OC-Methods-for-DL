% Otto-von-Guericke-Universit�t Magdeburg
% Fakult�t f�r Mathematik
%
% Vorschlag zur Gestaltung von Abschlussarbeiten mit LaTeX
%
%
\documentclass[a4paper, 12pt]{scrreprt} % KOMA-Script-Report
%
%\documentclass[a4paper, 12pt]{scrartcl} % KOMA-Script-Article 
%                                        (dann keine \chapter m�glich!!!)
%                                        (erfodert Anpassung in FMA-commands.tex) 
%----LaTeX-Pakete-----------------
\usepackage[ngerman, american]{babel}   %andersherum, also "ngerman, american" sind die Ueberschriften in Englisch
%\usepackage[latin1]{inputenc} % Quelltext mit Umlauteingabe
\usepackage[utf8]{inputenc} % alternativ Quelltext von Windows-PC[ansinew]
\usepackage{geometry}
\geometry{twoside, outer=25mm, inner=30mm, top=30mm, bottom=30mm} % zweiseitig
% \geometry{outer=25mm, inner=30mm, top=30mm, bottom=30mm}        % einseitig
\usepackage{latexsym}                                             % spezielle Mathematik-Symbole
\usepackage{graphicx}                                             % zum Einbinden von Bildern
\usepackage{amsmath, amssymb, amsthm}                             % LaTeX-Erweiterungen der AMS
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\sign}{sign}
\DeclareMathOperator*{\diag}{diag}
\usepackage{harvard}                                              % fuer Beispiel-Literaturverzeichnis
\usepackage{tikz}
\usetikzlibrary{patterns,decorations.pathreplacing}
\usetikzlibrary{backgrounds}
\usepackage[]{algorithm2e}

% ----Einbinden von pdf-Dateien oder einzelnen Seiten daraus
\usepackage{pdfpages}
\usepackage{xspace}
\usepackage{setspace}                                             % zum Variieren des Zeilenabstandes
% Anwendung im Text:  \includepdf[pages={7, 9-12}]{Name der pdf-Datei}
%------
\setkomafont{disposition}{\normalfont\bfseries}%rmfamily
% Kopf- und Fu�zeilen mit scrpage2 ---------------*Beginn*------------
\usepackage[automark,headsepline]{scrpage2}
\automark[section]{chapter}
\pagestyle{scrheadings}			
\clearscrheadfoot
\ihead[]{\headmark}
\cfoot[ \thepage{} ]{ \thepage{} }
\setkomafont{pagefoot}{%
% Kopf- und Fu�zeilen mit scrpage2 ---------------*Ende* --------------
\normalfont\rmfamily}
\setlength{\parindent}{0cm}
\setcounter{tocdepth}{4}              % 4 Gliederungsebenen in das Inhaltsverzeichnis
\setcounter{secnumdepth}{3}           % 4 (!) Gliederungsebenen werden nummeriert(0-3) 
\input{FMA-commands}        		      % hilfreiche LaTeX-Befehlsabk�rzungen
\usepackage{acronym}
\usepackage{cite}
\usepackage{subfigure}
\usepackage{multicol}
\usepackage{framed} 
\usepackage{listings}
\usepackage{color}
\usepackage{scrhack}
\usepackage{url}
\usepackage[short]{optidef}
% ------------------------------------------------------------------------------
%
%##################################################################################################
\begin{document}

\definecolor{mylilas}{RGB}{170,55,241}
\definecolor{mygreen}{rgb}{0,0.6,0}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{myExample}[definition]{Example}
\newtheorem{remark}[definition]{Remark}
\theoremstyle{plain}
\newtheorem{myLemma}[definition]{Lemma}
\newtheorem{myTheorem}[definition]{Theorem}
\newtheorem{corollar}[definition]{Corollar}


\lstset{
	% extendedchars=true,      	% funktioniert nicht mit utf-8 �frown�-Emoticon
	basicstyle=\tiny,        % the size of the fonts that are used for the code
	%breaklines=true,
	numbers=left,
	language=Matlab,
	numbersep=10pt,            	% how far the line-numbers are from the code
  numberstyle=\tiny, 					% the style that is used for the line-numbers
	stepnumber=1,
	%frame=single,	                   % adds a frame around the code
	breaklines=true,                 % sets automatic line breaking
	tabsize=2,
	commentstyle=\color{mygreen},    % comment style
	identifierstyle=\color{black},%
  stringstyle=\color{mylilas}}

\pagenumbering{Roman}
\newtheorem{Def}{Definition}
\input{Titelseite}
\clearpage
\cleardoublepage
\tableofcontents
\clearpage

\addcontentsline{toc}{section}{Abbreviations}
\chapter*{Abbreviations}
\begin{acronym}
	\acro{CNN}{Convolutional Neural Network}
	\acro{IVP}{Initial Value Problem}
	\acro{MSA}{Method of Successive Approximations}
	\acro{NN}{Neural Network}
	\acro{OCP}{Optimal Control Problem}
	\acro{ODE}{Ordinary Differential Equation}
	\acro{PMP}{Pontryagin Maximum Principle}
	\acro{RNN}{Residual Neural Network}
	
\end{acronym}

\clearpage

\addcontentsline{toc}{section}{List of Figures}
\listoffigures

\cleardoublepage

\pagenumbering{arabic}
\setcounter{page}{1}      %Beginn der Textseitenzaehlung
% -----------------------------------------------------------------------------------------
% hier beginnen die einzelnen Kapitel der Arbeit
% -----------------------------------------------------------------------------------------


% -----------------------------------------------------------------------------------------
% Einleitung
% -----------------------------------------------------------------------------------------

\chapter{Introduction}
\label{chap:Introduction}

% -----------------------------------------------------------------------------------------
% Mathematische Voraussetzungen
% -----------------------------------------------------------------------------------------

\chapter{Mathematical Prerequisites}
\label{chap:mp}
In this chapter we will summarize the necessary definitions and theorems from numerical analysis and optimal control.

\section{Numerical Analysis}
\label{sec:na}
The definitions of this section are primarily based on \cite{sodei} and \cite{ngd}.
\begin{definition}[Ordinary Differential Equation]
Let $f:\mathbb R \times \mathbb R^n \to \mathbb R^n$ be a function. The equation
\begin{equation}
\dot{x} = f(t,x)
\label{eq:ODE}
\end{equation}
is called \emph{\ac{ODE}}. $x(t):\mathbb R \to \mathbb R^n$ is a \emph{solution} of the \ac{ODE} in the interval $[t_0, T]$ if
\begin{equation}
\dot x(t) = f\left(t,x(t)\right), \forall t \in [t_0, T]
\label{eq:ODE2}
\end{equation}
is satisfied.
\end{definition}

\begin{definition}[Initial Value Problem]
An \ac{ODE} with an additional initial value condition 
\begin{align}
\label{eq:ivp}
\begin{split}
\dot{x}(t) &= f(t, x(t)), \hspace{0.5cm} \forall t \in [t_0, T]\\
x(t_0) &= \bar x_0
\end{split}
\end{align}
is called \emph{\ac{IVP}}.
\end{definition}

\begin{definition}[Stability]
Let $\dot x(t) = f\left(t,x(t)\right)$ be an \ac{ODE} and $x_0, \tilde{x}_0$ initial values with the corresponding solutions $x(t), \tilde{x}(t)$ of the \ac{ODE}. The \ac{ODE} is called \emph{stable} if for every $\epsilon > 0$ there exists a $\delta > 0$ such that
\begin{equation}
\Vert x_0 - \tilde{x}_0 \Vert < \delta \Rightarrow \Vert x(t) - \tilde{x}(t) \Vert < \epsilon \hspace{0.5cm} \forall t > t_0
\label{eq:Stab}
\end{equation}
\end{definition}

Stability describes the property, that solutions of the same \ac{ODE} with different initial values are close to each other if the initial values are close to each other. %A slightly weaker formulation of stability is derived in the following lines, providing a criterion which can be used in chapter \ref{chap:AotPS}.

For two solutions $x(t), \tilde{x}(t)$ of $\dot x(t) = f\left(t,x(t)\right)$ let $y(t) := x(t) - \tilde{x}(t)$ be the difference of the solutions. If $y(t)$ is bounded, the \ac{ODE} is stable.
\begin{align*}
\dot{y}(t) &= \dot{x}(t) - \dot{\tilde{x}}(t) \\
 &= f\left(t,x(t)\right) - f\left(t,\tilde{x}(t)\right)
\end{align*}
with the Taylor expansion of $f$ in $\tilde{x}(t)$ we have 
\begin{equation*}
f(t,x(t)) = f(t,\tilde{x}(t)) + f_x(t,\tilde{x}(t)) y(t) + \dots
\end{equation*}
Inserting this gives
\begin{equation}
\dot{y}(t) = f_x(t,\tilde{x}(t)) y(t) + r(t,x(t),\tilde{x}(t))%\dots
\label{eq:ODEStab}
\end{equation}
with the residual $r(t,x,\tilde{x})$. 
Neglecting the residual term, (\ref{eq:ODEStab}) gives the \ac{ODE}
\begin{equation}
\dot{y}(t) = J(t) y(t)
\label{eq:vareq}
\end{equation}
with $J(t)$ being the Jacobian $f_x(t,\tilde{x}(t))$. This equation is called the variational equation of the \ac{ODE}.

In the constant coefficient case $J(t) = J \forall t \in [t_0,T]$, the stability of \ref{eq:vareq} can be verified through examining the eigenvalues of $J$.

\begin{definition}[Stability]
The \ac{ODE} $\dot{y}(t) = J y(t)$ is called \emph{stable} on the time interval $[t_0,T]$, if the following holds:
\begin{align}
\underset{j=1,\dots,k}{max} \Re (\lambda_j(J)) &\leq  0 \\
\underset{l=k+1,\dots,n}{max} \Re (\lambda_l(J)) &<  0
\label{eq:stab2}
\end{align}
with $\lambda_j(J)$ being a singular eigenvalue of $J$ for $j=1,\dots,k$ and $\lambda_l(J)$ being a nonsingular eigenvalue of $J$ for $l=k+1,\dots,m$.
\end{definition}

In the non-constant coefficient case the stability can not directly be checked via looking at the eigenvalues of $J(t)$, especially when $J(t)$ is changing fast with $t$. A more involved method using the kinematic eigenvalues as in \cite{nsobvpfode}, is required to derive stability properties for this kind of problem. 

Analogous to \cite{safdnn} we define a weaker criterion, using the eigenvalues in the non-constant coefficient case. This criterion will be used in section \ref{sec:SotFP} to stabilize \ac{NN} training.

\begin{definition}[Weak Stability Criterion]
Let $\dot x(t) = f\left(t,x(t)\right)$ be an \ac{ODE} and $\dot{y}(t) = J(t) y(t)$ the corresponding variational equation. The \ac{ODE} fulfills the \emph{weak stability criterion} if $J(t)$ changes sufficiently slow in $t$ and
\begin{equation}
\underset{j=1,\dots,n}{max} \Re (\lambda_j(J(t))) \leq  0, \hspace{0.5cm} \forall t\in[t_0,T]
\label{eq:wstabcrit}
\end{equation}
with $\lambda_j(J(t))$ being the $j-$th eigenvalue of $J(t)$.
\end{definition}

\iffalse
$+++++++++++++++++++++++$

A discretization $t_0 < t_1 < \dots < t_m = T$ of the time horizon and a piecewise constant approximation $J_{t_i}$ of the Jacobian $f_x(t,\tilde{x}(t))$ for $t \in [t_i,t_{i+1})$ yields the linear \acp{ODE} 
\begin{equation*}
\dot y(t) = J_{t_i} y(t), \hspace{0.5cm} \forall t \in [t_i, t_{i+1}), i = 0, \dots, m-1
\end{equation*}
The solutions to these \acp{ODE} can be given explicitly via the eigenvalues of the $J_{t_i}$ as can be seen in \cite{sodei}, with the solutions being bounded if the real part of the eigenvalues is negative. 

\begin{definition}[Weak Stability]
The \ac{ODE} $\dot x(t) = f\left(t,x(t)\right)$ is called \emph{weakly-stable} on the time interval $[t_0,T]$, if for a given discretization $t_0 < t_1 < \dots < t_m = T$ and approximations $J_{t_i}$ of the Jacobi matrix $f_x(t,x(t))$ on $[t_i, t_{i+1})$ the following holds:
\begin{align}
\underset{j=1,\dots,k}{max} \Re (\lambda_j(J_{t_i})) &\leq  0, \hspace{0.5cm} i=0, \dots, m-1 \\
\underset{l=k+1,\dots,n}{max} \Re (\lambda_l(J_{t_i})) &<  0, \hspace{0.5cm} i=0, \dots, m-1
\label{eq:stab2}
\end{align}
with $\lambda_j(J_{t_i})$ being a singular eigenvalue of $J_{t_i}$ for $j=1,\dots,k$ and $\lambda_l(J_{t_i})$ being a nonsingular eigenvalue of $J_{t_i}$ for $l=k+1,\dots,m$.
\end{definition}

$++++++++++++++++$
\fi

Plenty of methods for solving \acp{IVP} numerically exist, with the forward Euler method being one of the simplest. It can be motivated by using finite differences for approximating the derivative in discrete time points $t_0 < t_1 < \dots < t_m = T$:
\begin{equation*}
\dot x(t_k) \approx \frac{x_{k+1}- x_k}{h_k} = f(t_k, x_k) 
\end{equation*}
$x_k$ denotes the approximation of $x(\cdot)$ at time $t_k$ and $h_k := t_{k+1}-t_k$.

\begin{definition}[Forward Euler Method]
The \emph{forward Euler method} for the \ac{IVP} \ref{eq:ivp}, with the discretization $t_0 < t_1 < \dots < t_m = T$ of $[t_0,T]$ is defined as
\begin{equation}
x_{k+1} = x_k + h_k f(t_k,x_k), \hspace{0.5cm} k=0,\dots, m-1
\label{eq:fweul}
\end{equation}
with $h_k := t_{k+1}-t_k, k=0,\dots,m-1$, $x(t_k) \approx x_k, k=1,\dots,m$ and $x_0 = \bar x_0$. 
\end{definition}

In the case of equidistant time points $t_k$, a parameter $h$ exists with $h_k = h \forall k \in \{0,\dots,m-1\}$. 

Given a stable \ac{ODE} the discretization and the method need also to fulfill certain conditions to ensure stability of the solution. Here only the stability of the forward Euler method with equidistant time points is considered.

\begin{definition}[Stability of the Forward Euler method]
Let $J_k := f_x(t_k,x(t_k))$ be the Jacobi-matrix $f_x(t,x(t))$ of a given \ac{ODE}, evaluated at $t_k$ for a given equidistant discretization $t_0 < t_1 < \dots < t_m = T$ of $[t_0,T]$. The forward Euler method is called \emph{stable} if
\begin{equation}
\underset{j=1,\dots,n}{max} \vert 1+h\lambda_j(J_k) \vert \leq 1 ,\hspace{0.5cm} \forall k=0,\dots,m
\label{eq:fweulstab}
\end{equation}
with $\lambda_j(J_k)$ being the $j-$th eigenvalue of $J_k$.
\end{definition}

For a given \ac{ODE}, when using the forward Euler method, the discretization has to be chosen with an $h$ small enough to ensure that (\ref{eq:fweulstab}) holds.

\section{Optimal Control}
\label{sec:oc}
This section is mainly based on \cite{ocatcov} and \cite{os}. In optimal control the aim is to minimize (or maximize) a given cost-function, depending on the state variables $x(t)$, with the choice of the control function $u(t)$.
% The $u(t)$ is chosen such that certain constraints are fulfilled. 
The state variables $x(t): [t_0,T] \to \mathbb{R}^{n_x}$ can be determined via the solution of an \ac{ODE}
\begin{equation}
\dot x(t) = f(x(t),u(t)), \hspace{0.5cm} \forall t \in [t_0, T]
\label{eq:odeocp}
\end{equation} 
with given controls $u(t): [t_0,T] \to \mathbb{R}^{n_u}$ and an initial condition $x(t_0) = \bar x_0$. 


\begin{definition}[Cost-Function]
Let $x(t):[t_0,T]\to \mathbb R^{n_x}$ be state variables, determined by solving the \ac{ODE} (\ref{eq:odeocp}) with fixed controls $u(t): [t_0,T] \to \mathbb{R}^{n_u}$. 
The \emph{cost-function} $J(x,u)$ is written as:
\begin{equation}
J(x,u) = E(x(T)) + \int_{t_0}^T L(t,x(t),u(t)) \mathrm{d}t
\label{eq:costfunc}
\end{equation}
with the \emph{Mayer-term} $E(x(T))$ and the \emph{Lagrange-term} $\int_{t_0}^T L(t,x(t),u(t)) \mathrm{d}t$.
\end{definition}

Through a combination of the cost-function (\ref{eq:costfunc}), the \ac{ODE} (\ref{eq:odeocp}) and other constraints, we derive the formulation of an \ac{OCP}.

\begin{definition}[Optimal Control Problem]
Let $x(t):[t_0,T]\to \mathbb R^{n_x}$ be state variables and $u(t): [t_0,T] \to \mathbb{R}^{n_u}$ be sufficient smooth, bounded controls. The \emph{\acl{OCP}} is defined as
\begin{mini}
{x,u}{E(x(T)) + \int_{t_0}^T L(x(t),u(t)) \mathrm{d}t} {\label{opt:ocp}} {}
\addConstraint{\dot x(t)}{= f(x(t),u(t))}{\hspace{0.5cm}\forall t \in [t_0, T]}
\addConstraint{x(t_0)}{= \bar x_0}{}
\addConstraint{x(T)}{= \bar x_T}{}
\addConstraint{u(t)}{\in \Omega \subseteq \mathbb R^{n_u}}{\hspace{0.5cm}\forall t \in [t_0, T]}
\end{mini}
\end{definition}

%\begin{equation}
%\label{eq:ocp}
%\begin{aligned}
%&\underset{x,u}{min}& &E(x(T)) + \int_{t_0}^T L(t,x(t),u(t)) \mathrm{d}t & &\\
%& s.t.&  \dot x(t) &= f(x(t),u(t)), &\forall t \in [t_0, T] \\
%& &x(t_0) &= x_0 \\
%& &x(T) &= x_T \\
%& &u(t) &\in \Omega \subseteq \mathbb R^{n_u}, &\forall t \in [t_0, T]
%\end{aligned}
%\end{equation}

For solving the \ac{OCP} (\ref{opt:ocp}) necessary conditions for optimality can be formulated. We will look at the \ac{PMP} as such a set of conditions. Additionally the discrete time version of the \ac{PMP} will be considered.

\begin{definition}[Hamilton-Function]
The \emph{Hamilton-Function} $H:\mathbb R^{n_x} \times \mathbb R^{n_u} \times\mathbb R^{n_x} \to \mathbb R$ of problem (\ref{opt:ocp}) writes
\begin{equation}
H(x,u,\lambda) := \lambda^T f(x,u) - L(x,u)
\label{eq:hamfunc}
\end{equation}
\end{definition}

\begin{myTheorem}[Pontryagin Maximum Principle]
Let $(x^*,u^*)$ be an optimal solution to problem \ref{opt:ocp}. Then a co-state process $\lambda^*:[t_0,T] \to \mathbb R^{n_x}$ exists, such that
\begin{align}
1.&\hspace{0.2cm} \dot x^*(t) =  \frac{\partial H}{\partial \lambda}(x^*(t),u^*(t),\lambda^*(t))^T &\forall t \in [t_0,T] \\
2.&\hspace{0.2cm} x^*(t_0) = \bar x_0 \\
3.&\hspace{0.2cm} \dot \lambda^*(t) = - \frac{\partial H}{\partial x}(x^*(t),u^*(t),\lambda^*(t))^T & \forall t \in [t_0,T] \\
4.&\hspace{0.2cm} \lambda^*(T) = \frac{\partial E}{\partial x}(x^*(T))^T \\
5.&\hspace{0.2cm} H(x^*(t),u^*(t),\lambda^*(t)) \geq H(x^*(t),u,\lambda^*(t)) & \forall u \in \Omega, \forall t \in [t_0,T]
\end{align}
\end{myTheorem}

The proof of the \ac{PMP} can be found in \cite{mtoop}.\newline

For the time discrete \ac{PMP} let $t_0 < t_1 < \dots < t_m = T$ be a discretization of $[t_0,T]$. A time discrete formulation of the \ac{OCP} (\ref{opt:ocp}) reads as follows
\begin{mini}
{\substack{x_1, \dots, x_m \\ u_0, \dots, u_{m-1}}}{E(x_m) + \sum_{k=0}^{m-1} L(x_k,u_k)} {\label{opt:ocpdt}} {}
\addConstraint{x_{k+1}}{= F(x_k,u_k)}{\hspace{0.5cm}\forall k = 0,\dots,m-1}
\addConstraint{x_0}{= \bar x_0}{}
\addConstraint{u_k}{\in \Omega \subseteq \mathbb R^{n_u}}{\hspace{0.5cm}\forall k=0,\dots,m}
\end{mini}


\begin{myTheorem}[Pontryagin Maximum Principle in discrete time]
\label{thm:pmpdt}
Let $x^*=\{x_0^*, \dots,x_m^*\}$, $x^*_k \in \mathbb R^{n_x}$, $k = 0,\dots, m$, $u^* = \{u_0^*, \dots,u_{m-1}^*\}$, $u_k^*\in \Omega$, $k = 0, \dots, m-1$ and $(x^*,u^*)$ be an optimal solution to problem (\ref{opt:ocpdt}). Then a co-state process $\lambda^* = \{\lambda_0^*, \dots, \lambda_m^*\}$, $\lambda^*_k \in \mathbb R^{n_x}$, $k=0,\dots,m$ exists, such that
\begin{align}
1.&\hspace{0.2cm} x^*_{k+1} =  \nabla_{\lambda_{k+1}} H(x^*_k,u^*_k,\lambda^*_{k+1}) &k=0,\dots,m-1 \\
2.&\hspace{0.2cm} x^*_0 = \bar x_0 \\
3.&\hspace{0.2cm} \lambda^*_k = \nabla_{x_k} H(x^*_k,u^*_k,\lambda^*_{k+1}) & k=m-1, \dots, 0 \\
4.&\hspace{0.2cm} \lambda^*_m = - \nabla_{x_m} E(x^*_m) \\
5.&\hspace{0.2cm} H(x^*_k,u^*_k,\lambda^*_{k+1}) \geq H(x^*_k,u,\lambda^*_{k+1}) & \forall u \in \Omega, k = 0,\dots, m-1
\end{align}
\end{myTheorem}

These prerequisites will be used in chapter \ref{chap:NNLaOCP} and chapter \ref{chap:AotPS}.
% -----------------------------------------------------------------------------------------
% Aufbau von Neuronalen Netzen
% -----------------------------------------------------------------------------------------

\chapter{Neural Network Architectures}
\label{chap:NNA}
The aim of the \acfp{NN} in this chapter is to minimize the difference between the network prediction $\mathcal{N}(x)$ and an output $y$ with respect to a given loss function. The output is associated with a given input as a pair $(x,y)$. 

At first we will have a look at a neural network definition motivated by graphs. Further we will see a different formulation and different architectures of neural networks. The overview will be limited to feedforward \acp{NN}.

\begin{definition}[Feedforward Neural Network]
A \emph{$T$-layer feedforward \acl{NN}} is a directed acyclic graph (DAG) with vertices $V = V_0 \hspace{0.1cm}\dot \cup\hspace{0.1cm} V_1 \hspace{0.1cm}\dot \cup \cdots \dot \cup\hspace{0.1cm} V_{T-1} \hspace{0.1cm}\dot \cup \hspace{0.1cm}V_T \dot \cup\hspace{0.1cm} B$ and edges $E \subseteq \{(v, u) \mid v \in V_i, u \in V_j, i < j\} \cup \{(b_i,v) \mid b_i \in B, v \in V_{i}, i \in \{1, ..., T\}\}$. The vertex set $V_0$  is called \emph{input-layer}, the $V_1, ..., V_{T-1}$ are called \emph{hidden-layers} and $V_T$ is called \emph{output-layer}. The vertices $b_i \in B$ are named \emph{biases}. A weight $w_e \in \Omega$ is assigned to each $e \in E$.

% It holds that $\delta_{in}\(v_0\)=0$ and $\delta_{out}\(v_0\) \neq 0$ for all $v_0 \in V_0$, $\delta_{in}\(v_i\) \neq 0$ and $\delta_{out}\(v_i\) \neq 0$ for all $v_i \in V_i, i \in \{1, ..., T-1\}$ and  $\delta_{in}\(v_T\) \neq 0$ and $\delta_{out}\(v_T\) = 0$ for all $v_T \in V_T$.
\end{definition}

In the forward propagation in a \ac{NN} an \emph{activation function} $\sigma(\cdot)$ is assigned to each vertex in the hidden-layers. The output $o(v)$ for a vertex $v$ is given by: 
\begin{equation}
\label{actFct}
o(v) = \sigma\left(\sum\limits_{\substack{e \in E :\\ e = (u,v)}} w_e o(u)\right)
\end{equation}

Some examples for activation functions:
\begin{itemize}
	\item Sigmoid: $\sigma_{sig}(x) = \frac{1}{1+e^{-x}}$
	\item Hyperbolic tangent: $\sigma_{tanh}(x) = tanh(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}$
	\item ReLU: $\sigma_{ReLU}(x) = \begin{cases} x, & x \geq 0 \\ 0, & x < 0\end{cases}$
	\item Softmax: $\sigma_{softmax}(x)_{i} = \frac{exp(x_i)}{\sum_{j=1}^{n}{exp(x_j)}}, \hspace{0.5cm} j=1, \dots, n, x\in \mathbb{R}^n$
\end{itemize}

The softmax function is used to transform the output values, such that each value is between $0$ and $1$ and they sum up to $1$.

A general example of a feedforward \ac{NN} is shown in figure \ref{fig:FFNN}.

\begin{figure}
\centering
\begin{tikzpicture}[draw=black!50, node distance=3.5cm] %shorten >=1pt,->,
		
    \tikzstyle{every pin edge}=[<-,shorten <=1pt]
    \tikzstyle{neuron}=[circle,fill=black!25,minimum size=35pt,inner sep=0pt]
    \tikzstyle{annot} = [text width=4.5em, text centered]

    % Draw the input layer nodes
    %\foreach \name / \y in {1,2}
    % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
     %   \node[neuron] (I-\name) at (0,-\y) {$x_{\name}$};%, pin=left:Input \#\y
				
		\node[neuron] (I-1) at (0,0) {$x_{1}$};%, pin=left:Input \#\y
		\node[neuron] (I-2) at (0,-2) {$x_{2}$};%, pin=left:Input \#\y		
		\node[neuron] (I-3) at (0,-4.5) {$x_{\vert V_0 \vert}$};%, pin=left:Input \#\y
				
		\path (I-2) -- (I-3) node [black, font=\huge, midway, sloped] {$\dots$};
				

    % Draw the hidden layer nodes
    %\foreach \name / \y in {1,2}
    %    \path[yshift=0.5cm]
    %        node[neuron] (H-\name) at (2.5cm,-\y cm) {$h^1_{\name}$};
						
		\path[yshift=0.5cm]
        node[neuron] (H-1) at (3.5cm,0 cm) {$h^1_{1}$};
		\path[yshift=0.5cm]
        node[neuron] (H-2) at (3.5cm,-2 cm) {$h^1_{2}$};
		\path[yshift=0.5cm]
				node[neuron] (H-3) at (3.5cm,-5.5 cm) {$h^1_{\vert V_1 \vert}$};
				
		\path (H-2) -- (H-3) node [black, font=\huge, midway, sloped] {$\dots$};
		
		
		%\foreach \name / \y in {1,2}
    %    \path[yshift=0.5cm]
    %        node[neuron] (H2-\name) at (5cm,-\y cm) {$h^{T-1}_{\name}$};
						
		\path[yshift=0.5cm]
        node[neuron] (H2-1) at (8cm,-0 cm) {$h^{T-1}_{1}$};
		\path[yshift=0.5cm]
        node[neuron] (H2-2) at (8cm,-2 cm) {$h^{T-1}_{2}$};				
		\path[yshift=0.5cm]
				node[neuron] (H2-3) at (8cm,-5.5 cm) {$h^{T-1}_{\vert V_{T-1} \vert}$};
				
		\path (H2-2) -- (H2-3) node [black, font=\huge, midway, sloped] {$\dots$};
		
		\path (H-2) -- (H2-2) node [black, font=\huge, midway, sloped] {$\dots$};
    % Draw the output layer node
		%\foreach \name / \y in {1,...,5}
		%\node[neuron, right of=H2-2] (O) {};%,pin={[pin edge={->}]right:Output}
		\node[neuron] (O-1) at (11.5,0) {$y_{1}$};%, pin=left:Input \#\y
		\node[neuron] (O-2) at (11.5,-2) {$y_{2}$};%, pin=left:Input \#\y		
		\node[neuron] (O-3) at (11.5,-4.5) {$y_{\vert V_T \vert}$};%, pin=left:Input \#\y
		
		\path (O-2) -- (O-3) node [black, font=\huge, midway, sloped] {$\dots$};
		
		\node[neuron] (B-1) at (0,-6.5) {$b_{1}$};
		\node[neuron] (B-2) at (4.5,-6.5) {$b_{T-1}$};
		\node[neuron] (B-3) at (8,-6.5) {$b_{T}$};
		
		\path (B-1) -- (B-2) node [black, font=\huge, midway, sloped] {$\dots$};
		
		\draw [thick,decoration={brace,mirror,raise=0.8cm},decorate] (B-1.west) -- (B-3.east) node [black,midway,yshift=-1.2cm] 
{Biases}; %, font=\huge

    % Connect every node in the input layer with every node in the
    % hidden layer.
		\begin{scope}[on background layer]
				\foreach \source in {1,...,3}
						\foreach \dest in {1,...,3}
								\path [->,shorten >=1pt] (I-\source) edge (H-\dest);
				\foreach \source in {1,...,3}
						\foreach \dest in {1,...,3}
								\path [->,shorten >=1pt] (I-\source) edge (H2-\dest);
				\foreach \source in {1,...,3}
						\foreach \dest in {1,...,3}
								\path [->,shorten >=1pt] (I-\source) edge (O-\dest);
				\foreach \source in {1,...,3}
						\foreach \dest in {1,...,3}
								\path [->,shorten >=1pt] (H-\source) edge (H2-\dest);
				\foreach \source in {1,...,3}
						\foreach \dest in {1,...,3}
								\path [->,shorten >=1pt] (H-\source) edge (O-\dest);
				\foreach \source in {1,...,3}
						\foreach \dest in {1,...,3}
								\path [->,shorten >=1pt] (H2-\source) edge (O-\dest);
				\foreach \dest in {1,...,3}
						\path [->,shorten >=1pt] (B-1) edge (H-\dest);
				\foreach \dest in {1,...,3}
						\path [->,shorten >=1pt] (B-2) edge (H2-\dest);
				\foreach \dest in {1,...,3}
						\path [->,shorten >=1pt] (B-3) edge (O-\dest);
		\end{scope}
						
		% Connect 			
		%\foreach \source in {1,...,3}
    %    \foreach \dest in {1,...,3}
    %        \path (I-\source) edge (H-\dest);

    % Connect every node in the hidden layer with the output layer
    %\foreach \source in {1,...,3}
		%		\foreach \dest in {1,...,3}
		%				\path (H2-\source) edge (O-\dest);

    % Annotate the layers
    \node[annot,above of=H-1, node distance=1.2cm] (hl) {Hidden layer $V_1$};
		\node[annot,above of=H2-1, node distance=1.2cm] (hl2) {Hidden layer $V_{T-1}$};
    \node[annot,left of=hl] {Input layer $V_0$};
    \node[annot,right of=hl2] {Output layer $V_T$};
\end{tikzpicture}

\caption{Example of a $T$-layer feedforward \ac{NN}}
\label{fig:FFNN}
\end{figure}


\section{Residual Neural Networks}
\label{sec:RNN}

\begin{definition}[Shortcut Connections and Residual Neural Networks]
An edge $e = (v,u) \in E$ is called a \emph{Shortcut Connection} if $v \in V_i, u \in V_j$ and $j \geq i+2$. The difference $d_e := j-i$ is the degree of the shortcut connection. 
A feedforward \ac{NN} $\mathcal{N}$ is called \emph{Residual} with a degree of $d_\mathcal{N}$ if all shortcut connections $e$ in $\mathcal{N}$ are of degree $d_e = d_\mathcal{N}$ and $w_e = 1$.

% A Feedforward Neural Network is called \emph{simple} if  $E \subseteq \{\(v, w\) \mid v \in V_i, w \in V_{i+1}, i \in {0, ..., T-1}\} \cup \{\(b_i,v\) \mid b_i \in B, v \in V_{i+1}, i \in {0, ..., T-1}\}$. It is called \emph{residual}, if 
\end{definition}

An example of a \ac{RNN} with degree 2 can be seen in figure \ref{fig:ResNet}. This type of \ac{RNN} is referred to as ResNet.



Apart from graphs we will now look at \acp{NN} as functions of the input $x$.

\section{Neural Networks as Functions}
\label{sec:NNaF}

\begin{definition}[Feedforward Neural Networks as Functions]
A \emph{feedforward \ac{NN}} can be seen as a function $\mathcal{N}:\mathbb{R}^{n_x} \to \mathbb{R}^{n_y}$ with
\begin{equation}
\mathcal{N}(x) := \tilde{\sigma}_T(\tilde{W}_T \tilde{\sigma}_{T-1}(\tilde{W}_{T-1} \dots \tilde{\sigma}_1(\tilde{W}_1 \tilde{x}) \dots ))
\label{eq:1}
\end{equation}
where $\tilde{x}$ is a vector containing the input $x$ and the biases $b$, the $\tilde{W}_i$ are matrices containing the weights for layer $i$ and an identity part for the shortcut connections and the biases $b_{i+1}$ to $b_{T}$, $i \in {1, \dots, T}$. The $\tilde{\sigma}_i$ are functions containing the activation functions $\sigma_i$ and identity functions for the shortcut connections and the biases. 
\end{definition}

The following example clarifies the structure of $\tilde{x}$, the $\tilde{W}_i$ and the $\tilde{\sigma}_i$:
%To clarify the structure of $\tilde{x}$, the $\tilde{W}_i$ and the $\tilde{\sigma}_i$ we look at the following example:

\begin{myExample}[Two Layer Neural Network as Function]
\label{exp:ResNet}

Consider the following two-layer \ac{NN} in figure \ref{fig:ResNet} with input-layer $x$, hidden-layer $h$ and output-layer $y$. For each vertex the output is identified with the name of the vertex.\newline



\begin{figure}%
\centering
\begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=3.5cm]
		
    \tikzstyle{every pin edge}=[<-,shorten <=1pt]
    \tikzstyle{neuron}=[circle,fill=black!25,minimum size=25pt,inner sep=0pt]
    \tikzstyle{annot} = [text width=4.5em, text centered]
		
		% Draw the Input layer
				
		\node[neuron] (I-1) at (0,0) {$x_{1}$};%, pin=left:Input \#\y
		\node[neuron] (I-2) at (0,-1.5) {$x_{2}$};%, pin=left:Input \#\y		
				
				

    % Draw the hidden layer nodes
						
		\path[yshift=0.5cm]
        node[neuron] (H-1) at (3.5cm,0 cm) {$x_{3}$};
		\path[yshift=0.5cm]
        node[neuron] (H-2) at (3.5cm,-1.5 cm) {$x_{4}$};
		\path[yshift=0.5cm]
				node[neuron] (H-3) at (3.5cm,-3 cm) {$x_{5}$};
				
	
		
    % Draw the output layer nodes
		
		\node[neuron] (O-1) at (7,0) {$y_{1}$};%, pin=left:Input \#\y
		\node[neuron] (O-2) at (7,-1.5) {$y_{2}$};%, pin=left:Input \#\y		
		
		
		\node[neuron] (B-1) at (0,-4) {$b_{1}$};
		\node[neuron] (B-2) at (3.5,-4) {$b_{2}$};
		

    % Connect every node in the input layer with every node in the
    % hidden layer.
		\begin{scope}[on background layer]
				\foreach \source in {1,2}
						\foreach \dest in {1,...,3}
								\path (I-\source) edge (H-\dest);
								
				\path (I-1) edge (O-1);
				
				\foreach \source in {1,...,3}
						\foreach \dest in {1,2}
								\path (H-\source) edge (O-\dest);
								
				\foreach \dest in {1,...,3}
						\path (B-1) edge (H-\dest);
				\foreach \dest in {1,2}
						\path (B-2) edge (O-\dest);
		\end{scope}


    % Annotate the layers
    %\node[annot,above of=H-1, node distance=1.5cm] (hl) {Hidden layer $V_1$};
		%\node[annot,above of=H2-1, node distance=1.5cm] (hl2) {Hidden layer $V_{T-1}$};
    %\node[annot,left of=hl] {Input layer $V_0$};
    %\node[annot,right of=hl2] {Output layer $V_T$};
\end{tikzpicture}

\caption{Example of a two-layer \ac{RNN} of degree 2 (ResNet)}%
\label{fig:ResNet}%
\end{figure}

Let
$
x=\begin{pmatrix}	x_1 \\ x_2 \end{pmatrix},
\tilde{x}=\begin{pmatrix}
	x_1 \\ x_2 \\ b_1 \\ b_2
\end{pmatrix}, 
\tilde{W}_1 = \begin{pmatrix}
	w_{1 3} & w_{2 3} & w_{b_1 3} & 0 \\
	w_{1 4} & w_{2 4} & w_{b_1 4} & 0 \\
	w_{1 5} & w_{2 5} & w_{b_1 5} & 0 \\
	1 & 0 & 0 & 0 \\
	0 & 0 & 0 & 1
\end{pmatrix}, 
\tilde{h}^p = \tilde{W}_1 \tilde{x} = \begin{pmatrix}
	x_3^p \\ x_4^p \\ x_5^p \\ x_1 \\ b_2
\end{pmatrix}, \\
\tilde{h} = \tilde{\sigma}_1(\tilde{h}^p) = \begin{pmatrix}
	\sigma_1(x_3^p) \\ \sigma_1(x_4^p) \\ \sigma_1(x_5^p) \\ x_1 \\ b_2 
\end{pmatrix} = \begin{pmatrix}
	x_3 \\ x_4 \\ x_5 \\ x_1 \\ b_2
\end{pmatrix},
\tilde{W}_2 = \begin{pmatrix}
	w_{3 1} & w_{4 1} & w_{5 1} & w_{1 1} & w_{b_2 1}  \\
	w_{3 2} & w_{4 2} & w_{5 2} & 0 & w_{b_2 2}  
\end{pmatrix}, \\
y^p = \tilde{W}_2 \tilde{h} = \begin{pmatrix}
	y_1^p \\ y_2^p
\end{pmatrix}$ and $
y = \tilde{\sigma}_2(y^p) = \begin{pmatrix}
	\sigma_2(y_1^p) \\ \sigma_2(y_2^p)
\end{pmatrix} = \begin{pmatrix}
	y_1 \\ y_2
\end{pmatrix}
%\label{eq:2}
$.

The input $x$ is combined with the biases to get $\tilde{x}$. Through multiplication with the weight-matrix $\tilde{W}_1$, where $w_{i j}$ is the weight of the edge form $x_i$ to $x_j$ and $w_{b_i j}$ the weight for the edge $e = (b_i, x_j)$, the pre-activation layer $\tilde{h}^p = \tilde{W}_1 \tilde{x}$ is computed. For the shortcut-connection and the unused bias we have an identity part in $\tilde{W}_1$. The following activation operates only on non-shortcut and non-bias input to get the output of the hidden-layer $\tilde{h}$.\newline
The pre-output $y^p$ is again computed through a matrix-vector multiplication with the weight matrix $\tilde{W}_2$ with the weights $w_{i j}$ for the edge $e=(x_i,y_j)$ and additional weights for the bias input. The output $y$ is the result of an activation of $y^p$.



\end{myExample} 

\begin{myLemma}
\label{lem:fwp}
The forward propagation in a \ac{NN} can be written as:
\begin{align*}
\mathcal{N}(x) &= x_T \\
x_{k+1} &= f_{k}(x_{k},\bar{W}_{k}) , \hspace{0.5cm}k = 0, \dots, T-1 \\
x_0 &= \bar{x} 
\end{align*}
where the subscripts denote the layer.
\end{myLemma}
\begin{proof}
With the choices $\bar{x} = \tilde{x}$, $\bar{W}_k = \tilde{W}_{k+1}$ and $f_k(x_k,\bar{W}_k)=\sigma_{k+1}(\bar{W}_k x_k)$ for $k=0, \dots, T-1$, the claim follows directly from recursive insertion of the $x_k$.
\end{proof}


\section{Convolutional Neural Networks}
\label{sec:CNN}
In the last section of this chapter we will look at \acp{CNN}. \acp{CNN} are specialized for image classification and consist of different types of layers. The input images can be seen as a $n_{px} \times n_{py} \times n_c$ tensors with $n_{px} \times n_{py}$ pixels and $n_c$ channels. The channels contain for example different color information of the image. This section is mainly based on \cite{cs231n}.

The architectures of \acp{RNN} and \acp{CNN} can be combined.

\subsection{Convolutional Layer}
\label{subsec:CL}
The convolutional layers are the main functionality of \acp{CNN}. For each layer there are hyperparameters such as the \emph{stride} $s \in \mathbb{N}$, a \emph{zero-padding} $p \in \mathbb{N}_0$, the number $d\in \mathbb{N}$ of  filters and their spatial size $n_f$. A filter $F$ can be interpreted as a tensor with dimensions $n_f \times n_f \times n_c, n_f \in 2\mathbb{N}+1, n_c \in \mathbb{N}$. 

The stride specifies the number of pixels a filter is shifted while it slides over the image. The number of zeros that are padded around the input image is given by the zero-padding, while the number of filters determines the number of channels in the output.

The weights $F_{j,k,l} \in \mathbb{R} $ for $j,k \in [n_f], l \in [n_c]$ of a filter $F$ are learnable parameters.\newline

\begin{definition}[Convolution]
Let $I,F \in \mathbb{R}^{n\times m \times c}$ be tensors. Then the \emph{convolution} $*:\mathbb{R}^{n\times m \times c}\times \mathbb{R}^{n\times m \times c} \to \mathbb{R}$ of $I$ and $F$ is defined as follows:
\begin{equation*}
I*F := \sum\limits_{j=1}^{n} \sum\limits_{k=1}^{m} \sum\limits_{l=1}^{c} I_{j,k,l}F_{j,k,l}
%\label{eq:3}
\end{equation*}
\end{definition}

The next definition shows how the convolution of an image with $d$ filters and stride $s$ works.
\begin{definition}[Image Convolution]
Let $I \in \mathbb{R}^{n_{px} \times n_{py} \times n_c}$ be an image, $F_1,\dots, F_d \in \mathbb{R}^{n_f \times n_f \times n_c}$ be  filters and $s \in \mathbb{N}$ the stride. The convolution $I^c := I *_s [F_1, \dots, F_d]$ of $I$ with the filters $F_1,\dots, F_d$ and stride $s$ is given by: 
\begin{equation*}
{I^c}_{j,k,l} := I_{J,K,L} * F_l, \hspace{0.5cm}\forall (j,k,l) \in \left(\frac{n_{px}-n_f+2p}{s}+1\right)\times \left(\frac{n_{py}-n_f+2p}{s}+1\right)  \times d
%I_{[(i-1)s+1:(i-1)s+n_f], [(j-1)s+1:(j-1)s+n_f], [1:n_c]} * F_k
%\label{eq:5}
\end{equation*}
where $I_{J,K,L}$ is a $n_f \times n_f \times n_c$ sub-tensor of $I$ and $J:=\{(j-1)s+1,\dots,(j-1)s+n_f\}, K:=\{(k-1)s+1,\dots,(k-1)s+n_f\}, L:=\{1, \dots, n_c\}$.
%$I_{[a+1:a+n_f],[b+1:b+n_f],[1:n_c]}$
\end{definition}

%An image of size $n_{px} \times n_{py}$ with $n_c$ channels gets convolved to an image of size $\left(\frac{n_{px}-n_f+2p}{s+1}\right)\times \left(\frac{n_{py}-n_f+2p}{s+1}\right)$ with $d$ channels.


\subsection{Pooling Layer}
\label{subsec:PL}
Pooling layers are inserted between convolutional layers for subsampling of the image and therefore reducing the amount of learnable parameters in the following layers. They also help to reduce the risk of overfitting.

As hyperparameters there are the spatial size $n_f$ of a pooling filter and the stride $s$.

\begin{definition}[Pooling]
Let $I \in \mathbb{R}^{n\times m \times c}$  be a tensor and $\phi : \mathbb{R}^{n_f \times n_f} \to \mathbb{R}$ a pooling-function. Then the \emph{pooling} $I^p$ of $I$ is defined via
\begin{equation*}
I^p_{j,k,l}:=\phi(I_{J,K,l}), \hspace{0.5cm} \forall (j,k,l) \in \left(\frac{n-n_f}{s}+1\right) \times \left(\frac{m-n_f}{s}+1\right) \times c
%\label{eq:}[(i-1)s+1:(i-1)s+n_f],[(j-1)s+1:(j-1)s+n_f]
\end{equation*}
where $I_{J,K,l}$ is a $n_f \times n_f \times 1$ sub-tensor of $I$ and $J:=\{(j-1)s+1,\dots,(j-1)s+n_f\}, K:=\{(k-1)s+1,\dots,(k-1)s+n_f\}$.
\end{definition}

%Given an image of size $n_{px} \times n_{py} \times n_c$ the pooling produces an output of size $\left(\frac{n_{px}-n_f}{s}+1\right) \times \left(\frac{n_{py}-n_f}{s}+1\right) \times n_c$. \newline

In practice different pooling-functions are used with max-pooling being the most popular. Some examples of pooling-functions:
\begin{itemize}
	\item Max-pooling: $\phi(I) = \underset{j \in [n],k \in [m]}{max}\{I_{j,k}\}, I\in \mathbb{R}^{n\times m}$ 
	\item Average-pooling: $\phi(I) = \frac{\sum\limits_{j=1}^{n}\sum\limits_{k=1}^{m}{I_{j,k}}}{nm}, I\in \mathbb{R}^{n\times m}$
\end{itemize}

Through the pooling no additional learnable parameters are added.

\subsection{Activation Layer}
\label{subsec:AL}
The activation-functions we introduced in \ref{actFct} are also important in \acp{CNN}. They perform elementwise operations but we seperate them from the convolutional layers for better clarity.

An activation layer needs no additional learnable parameters.

example structure: conv->relu->conv->relu->pool->...

\subsection{Fully-Connected Layer}
\label{subsec:FCL}
The last part of a \ac{CNN} consists of fully connected layers. This fully connected part is equivalent to a feedforward neural network without shortcut connections and the output of the last previous layer (e.g. pooling layer or activation layer) as input. 

Example of a complete \ac{CNN}.


% -----------------------------------------------------------------------------------------
% Neuronale Netze als OCP
% -----------------------------------------------------------------------------------------

\chapter{Neural Network Learning as Optimal Control Problem}
\label{chap:NNLaOCP}
In this chapter we look at the basic concepts of \ac{NN} learning and reformulate it as an \ac{OCP}. The prerequisites from chapter \ref{chap:mp} will be used, as well as \cite{mlcba}. \newline

There are two types of problems we want to solve with \acp{NN}. For both, pairs $(x^i,y^i) \in \mathbb R^{n_x} \times \mathcal{Y}, i = 1,\dots,s$ are given and the \ac{NN} should return the correct target $y^i$ when given $x^i$. The set $\{(x^i,y^i)\vert i=1,\dots,s\}$ is called training-set, where $s$ is the sample size. 

In the first case $\mathcal{Y}$ is a finite set. This task is called classification. The set $\mathcal{Y}$ for classification tasks can be $\{0,1\}^{n_y}$ or $\{-1,1\}^{n_y}$, with 
\begin{equation*}
y_j^i = \begin{cases} 1, & \text{if $x^i$ belongs to class $j$} \\
0, & \text{otherwise.}
\end{cases}
\end{equation*}
or
\begin{equation*}
y_j^i = \begin{cases} 1, & \text{if $x^i$ belongs to class $j$} \\
-1, & \text{otherwise.}
\end{cases}
\end{equation*}
and $n_y$ being the number of different classes. In practice it often holds that each $x^i$ belongs to exactly one of the $n_y$ classes. These are also the only cases we are considering here.

In the second problem, the $y^i \in \mathcal{Y} \subset \mathbb R^{n_y}$ belong to a continuous set. This is the regression task.

For both problem types it is necessary to measure the accuracy of the \ac{NN}. This accuracy can be used to train the network, meaning adapting the weights to increase the accuracy. Increasing the accuracy is equivalent to minimizing the error, which can be measured with loss-functions.


\section{Loss-Functions}
\label{sec:losfunc}

The result of the forward-propagation of $x^i$ in a \ac{NN} can be written as $x^i_T$ as seen in lemma \ref{lem:fwp}. Therefore a general loss-function can be seen as a function of $x^i_T$ and $y^i$. 

\begin{definition}[Loss-function]
Let $(x^i,y^i)$ be a sample from the training-set and $\mathcal{N}$ a \ac{NN} with $x^i_T = \mathcal{N}(x^i)$. A function $\phi : \mathbb{R}^{n_y} \times \mathcal{Y} \to \mathbb{R}^{\geq 0}$ is called \emph{loss-function}, if 
\begin{equation*}
x^i_{T,j} = y^i_j \hspace{0.5cm} \forall j=1,\dots, n_y \Rightarrow  \phi(x^i_T,y^i) = 0 
\end{equation*}
\end{definition}

How the correctness of the \ac{NN} is evaluated, depends on the problem type.

\subsection{Classification Task}
\label{subsec:clatas}
In this section we consider the set $\mathcal{Y}$ to be $\{-1,1\}^{n_y}$. The loss-functions for $\mathcal{Y} = \{0,1\}^{n_y}$ can be written in similar fashion.

%the maximum value of $x^i_{T}$ to be at the same index as the $1$ of $y^i$. Not only the index, but also the magnitude of the difference $x^i_{T,j}-y^i_j$ may play a role in the value of the loss-function. For this type of classification, a softmax function is often used on the output-layer, such that $x^i_{T,j} \in [0,1], j=1,\dots, n_y$ and $\sum_{j=1}^{n_y}{x^i_{T,j}} = 1$. This function was introduced in chapter \ref{chap:NNA}.

For the classification to be correct, we require matching signs of the entries of $x^i_T$ and $y^i$. Not only the sign, but also the magnitude of the difference $x^i_{T,j}-y^i_j$ may play a role in the value of the loss-function. Some examples of loss-functions for the classification are:

\begin{itemize}
	\item Simple misclassification: $\phi(x^i_T,y^i) = \begin{cases} 1, & \text{if } \exists j : sign(x^i_{T,j}) \neq sign(y^i_j) \\
0, & \text{otherwise.}
\end{cases}$
	\item Hinge loss 1: $\phi(x^i_T,y^i) = \sum_{j=1}^{n_y}{max\{0,1-y^i_j x^i_{T,j}\}}$
	\item Hinge loss 2: $\phi(x^i_T,y^i) = max\{0,\underset{j \in [n_y]}{max}\{1-y^i_j x^i_{T,j}\}\}$
\end{itemize}

Both hinge loss versions are extensions from the binary classification case, where $\mathcal{Y} = \{-1,1\}$ and the hinge loss is defined as $\phi(x^i_T,y^i) = max\{0,1-y^i x^i_{T}\}$.


\subsection{Regression Task}
In the regression the \ac{NN} output $x^i_{T}$ for the given pair $(x^i,y^i)$ should be as close as possible to $y^i$. One way to define "close" is to look at the general loss function 
\begin{equation*}
\phi_p(x^i_T,y^i)=\frac{1}{p}\Vert y^i - x^i_T \Vert_p^p
\end{equation*}

The most used loss functions for regression are

\begin{itemize}
	\item $p=1: \phi_1(x^i_T,y^i)=\sum_{j=1}^{n_y}\vert y_j^i - x^i_{T,j} \vert$
	\item $p=2: \phi_2(x^i_T,y^i)=\frac{1}{2}\sum_{j=1}^{n_y}(y_j^i - x^i_{T,j})^2$
	\item $p=\infty: \phi_{\infty}(x^i_T,y^i)=\underset{j=1,\dots,n_y}{max}\vert y_j^i - x^i_{T,j} \vert$
\end{itemize}

In these cases every difference of $x^i_{T}$ and $y^i$ are penalized. If the prediction $x^i_{T}$ is only required to be in a certain margin of $y^i$, the loss function
\begin{equation*}
\phi_{m_{\epsilon}}(x^i_T,y^i) = max\{0,\sum_{j=1}^{n_y}\vert y_j^i - x^i_{T,j} \vert - \epsilon\}
\end{equation*}
with the margin $\epsilon$ can be used.


\section{Forward Propagation and \acp{ODE}}
\label{sec:fpaode}
In this section the forward propagation of \ac{NN}, as presented in chapter \ref{chap:NNA} will be combined with \acp{ODE} and their discretization from section \ref{sec:na}.  \cite{mpbafdl} \newline

As seen in lemma (\ref{lem:fwp}) the forward propagation in an \ac{NN} can be written as
\begin{align*}
\mathcal{N}(x) &= x_T \\
x_{k+1} &= f_{k}(x_{k},\bar{W}_{k}) , \hspace{0.5cm}k = 0, \dots, T-1 \\
x_0 &= \bar{x} 
\end{align*}
This can be interpreted as a discretization of an \ac{ODE}. 

Looking at the special case of \acp{RNN} with a degree of 2 and multiplying the non-identity part of the activation function with an uncertainty parameter $h$, we derive following forward propagation, as in \cite{safdnn}:

\begin{align}
\begin{split}
\mathcal{N}(x) &= x_T \\
x_{k+1} &= x_k + h f_{k}(x_{k},\bar{W}_{k}) , \hspace{0.5cm}k = 0, \dots, T-1 \\
x_0 &= \bar{x} 
\end{split}
\label{eq:ResNetFwEul}
\end{align}

A ResNet formulation as in chapter \ref{chap:NNA} is attained by choosing $h=1$.

So the forward propagation of this ResNet yiels an forward Euler discretization, as seen in (\ref{eq:fweul}), of the \ac{ODE}
\begin{align*}
\dot x(t) &= f(x(t),\bar{W}(t)) , \hspace{0.5cm}t\in [t_0,T] \\
x(t_0) &= \bar{x} 
\end{align*}

Stability properties of the forward propagation will be discussed in chapter \ref{chap:AotPS}.

\section{Optimal Control Problem}

A formulation of the \ac{NN} learning as an optimal control problem, like in \ref{opt:ocpdt} is straight forward (see \cite{aocatdl}). \newline

A loss function $\phi(\cdot)$, introduced in \ref{sec:losfunc}, is minimized over a sample set with $s$ samples. This gives the Mayer-term in the cost-function. Possible regularizations on the weights result in Lagrange-term like functions, that also get minimized. Some regularizations will be regarded in chapter \ref{chap:AotPS}. 

The forward propagation yields \acp{ODE} as presented in the last section, with initial values from the samples.

Addtitional constraints on the weights can be introduced. Examples are binary and ternary networks as in \cite{aocatdl}. The weight-matrices $W_k$ are constrained being in $\Omega = \{-1,1\}^{n_{k+1}\times n_{k}}$ or $\{-1,0,1\}^{n_{k+1}\times n_{k}}$, for $k=0,\dots,T-1$ and $n_0 = n_x, n_T = n_y$. Analogously we have these constaints on the matrix-functions $W(t)$ for all $t\in[t_0,T]$, for the continuous case. \newline

With these considerations a continuous-time and a discrete-time \ac{OCP} can be formulated:

\begin{definition}[Continuous-Time \ac{OCP} for \ac{NN} Learning]
Let $(\bar{x}^i,y^i) \in \mathbb R^{n_x} \times \mathcal{Y}, i=1,\dots,s$ be given samples with sample size $s$. The continuous-time \ac{OCP} for \ac{NN} learning states as follows:
\begin{mini}
{x,W}{\frac{1}{s}\sum_{i=1}^{s}{\phi^i(x^i(T))} + \frac{1}{s}\sum_{i=1}^s{\int_{t_0}^T L(x^i(t),W(t)) \mathrm{d}t}} {\label{opt:nnlocp}} {}
\addConstraint{\dot x^i(t)}{= f(x^i(t),W(t))}{\hspace{0.5cm}\forall t \in [t_0, T], i=1,\dots,s}
\addConstraint{x^i(t_0)}{= \bar x^i}{\hspace{0.5cm}i=1,\dots,s}
\addConstraint{W(t)}{\in \Omega}{\hspace{0.5cm}\forall t \in [t_0, T]}
\end{mini}
where $\phi^i(x) := \phi(x,y^i)$ is a loss-function with the $y^i$ integrated into the definition.
\end{definition}

\begin{definition}[Discrete-Time \ac{OCP} for \ac{NN} Learning]
Let $(\bar{x}^i,y^i) \in \mathbb R^{n_x} \times \mathcal{Y}, i=1,\dots,s$ be given samples with sample size $s$. The discrete-time \ac{OCP} for \ac{NN} learning states as follows:
\begin{mini}
{x,W}{\frac{1}{s}\sum_{i=1}^{s}{\phi^i(x^i_T)} + \sum_{i=1}^s \sum_{k=0}^{T-1} L_k(x^i_k,W_k) } {\label{opt:nnldtocp}} {}
\addConstraint{x^i_{k+1}}{= f_k(x^i_k,W_k)}{\hspace{0.5cm} k= 0,\dots,T-1, i=1,\dots,s}
\addConstraint{x^i_0}{= \bar x^i}{\hspace{0.5cm}i=1,\dots,s}
\addConstraint{W_{k}}{\in \Omega}{\hspace{0.5cm}k=0,\dots, T-1}
\end{mini}
where $\phi^i(x) := \phi(x,y^i)$ is a loss-function with the $y^i$ integrated into the definition.
\end{definition}

For \acp{RNN} of degree 2, the forward propagation can be written as a forward Euler discretization of the \ac{ODE}, as in (\ref{eq:ResNetFwEul}).

% -----------------------------------------------------------------------------------------
% Analyse der Problemstruktur
% -----------------------------------------------------------------------------------------

\chapter{Analysis of the Problem Structure}
\label{chap:AotPS}
In this chapter we will consider stability properties of the forward propagation with the prerequisites of section \ref{sec:na} and use the \ac{PMP} of section \ref{sec:oc} to get necessary conditions for optimality for \ac{NN} learning.

\section{Stability of the Forward Propagation}
\label{sec:SotFP}
This section is based on \cite{safdnn}.

As seen in section \ref{sec:fpaode}, the forward propagation of a \ac{NN} can be interpreted as a discretization of an \ac{ODE}. We now consider the stability conditions of section \ref{sec:na} to ensure the stability of the forward propagation. 

\subsection{Goal of Stability}
There are two main reasons why stability is a desired property of the forward propagation. \newline

The first one is, that it results in a good generalization. A \ac{NN} is said to generalize well, if the classification or regression of new data (e.g. the test set) has a high accuracy. This directly corresponds to the stability property, as we want examples with similar features ($\widehat{=}$ close initial values) to result in a similar output in the regression, or the same class in classification ($\widehat{=}$ close solutions). \newline

The second one is, that stability ensures the preservation of the magnitude of feature values in the forward propagation. This helps against the problems of vanishing or exploding gradients in training with backpropagation. 

The vanishing gradient problem describes the problem, that small feature values and small value changes in between the hidden layers result in a vanishing of the backpropagated error in the first layers. This results in small weight updates and therefore in slow training. 

The exploding gradient problem describes the problem, that large feature values and large value changes in between the hidden layers result in a growth of the backpropagated error. This results in large weight updates and therefore may lead to divergence.

Both problems are more likely to occur in deep networks.

\subsection{Stability Conditions}
Consider \acp{RNN} with degree 2. The forward propagation can be written as a forward Euler discretization as in \ref{eq:ResNetFwEul}. The underlying \ac{ODE}, including the weights, biases and activation functions, writes as follows:

\begin{align}
\dot x(t) &= \phi(W(t) x(t) + b(t)) , \hspace{0.5cm}t\in [t_0,T] \\
x(t_0) &= \bar{x} 
\end{align} 

$\phi(x)$ denotes the element-wise application of the activation function $\phi$ on $x\in\mathbb{R}^n$.

For this \ac{ODE} the weak stability criterion (\ref{eq:wstabcrit}) requires the Jacobian $J$ of the right hand side to change sufficiently slow and its eigenvalues to be lesser than or equal to zero. 
With $f(t, x(t)) = \phi(W(t) x(t) + b(t))$ the Jacobian takes the following form:
\begin{equation}
J(t) = \frac{\partial (f_1, \dots, f_{n_x})}{\partial (x_1, \dots, x_{n_x})}(t) = \diag(\phi'(W(t) x(t) + b(t))) W(t)
\label{eq:jac}
\end{equation}

Since the activation function $\phi(\cdot)$ is element-wise non decreasing $\diag(\phi'(W(t) x(t) + b(t)))$ is a non negative diagonal matrix. A criterion on the eigenvalues of $W(t)$ is desired, but in contrast to the statement in \cite{safdnn} a pure restriction, namely
\begin{equation*}
\underset{i=1,\dots,n}{max}\Re(\lambda_i(W(t))) \leq 0, \hspace{0.5cm}\forall t \in [t_0,T]
\end{equation*}
is not sufficient, as the following example shows:

\begin{myExample}[Positive Real-Part Jacobian]
Let $D = \begin{pmatrix}
	4 & 0 \\
	0 & 1
\end{pmatrix}$ be a diagonal matrix with non negative entries and $W = \begin{pmatrix}
	1 & -2 \\
	2 & -2
\end{pmatrix}$ be a matrix with $\Re(\lambda_i(W)) = -\frac{1}{2}$ for $i=1,2$. For the product $J = D W = \begin{pmatrix}
	4 & -8 \\
	2 & -2
\end{pmatrix}$ the real parts of the two eigenvalues take the value $1$.
\end{myExample}

In the next section, we construct weight matrices using antisymmetric matrices. These matrices guarantee the weak stability criterion to hold.

\iffalse It follows that the weight-matrix $W(t)$ changing sufficiently slow and  
\begin{equation}
\underset{i=1,\dots,n}{max}\Re(\lambda_i(W(t))) \leq 0, \hspace{0.5cm}\forall t \in [t_0,T]
\label{eq:wstab}
\end{equation}
are sufficient for the weak stability criterion to hold.\newline
\fi

In addition to the stability of the underlying \ac{ODE}, the forward Euler discretization has to be stable. Analogously to \ref{eq:fweulstab}, with the time discretization $t_0 < t_1 < \dots < t_m = T$, we get the condition
\begin{equation}
\underset{i=1,\dots,n}{max} \vert 1+h\lambda_i(J_k) \vert \leq 1,\; \forall k=1,\dots,m
\label{eq:eulstab}
\end{equation}
with $J_k = \diag(\phi'(W(t_k) x(t_k) + b(t_k))) W(t_k)$ being the Jacobian at the discretization point $t_k$.

This condition shows the necessity to choose an $h$ that is small enough to stabilize the forward propagation.%\newline

%The next goal is to find a class of weight matrices that fulfill the condition (\ref{eq:wstab}). One possibility is to choose antisymmetric weight matrices.

\subsection{Antisymmetric Weight Matrices} 

\begin{definition}[Antisymmetric Matrix]
A matrix $W \in \mathbb{R}^{n\times n}$ is called \emph{antisymmetric} if
\begin{equation*}
W^T = -W
\end{equation*}
\end{definition}

What makes antisymmetric matrices interesting for constructing weight matrices are the following two properties:

\begin{itemize}
	\item For $W \in \mathbb{R}^{n\times n}$ antisymmetric it holds that $\Re(\lambda_i(W)) = 0, i=1,\dots,n$
	\item For any $M \in \mathbb{R}^{n\times n}$, $W = M-M^T$ is antisymmetric
\end{itemize}

In the following a weight matrix construction, that is sufficient for the weak stability criterion (\ref{eq:wstabcrit}) to hold, is derived. 

\begin{myLemma}[Eigenvalues of $AB$ and $BA$]
Let $A,B \in \mathbb{R}^{n\times n}$. It holds that 
\begin{equation}
\Lambda(AB)=\Lambda(BA)
\label{eq:eigs}
\end{equation} with $\Lambda(M) := \{\lambda \vert \lambda \text{ is eigenvalue of }M\}$.
\end{myLemma}
\begin{proof}
\end{proof}

\begin{myLemma}[Eigenvalues of $DW$]
Let $D\in \mathbb{R}^{n\times n}$ be a diagonal matrix with non negative entries and let $W \in \mathbb{R}^{n\times n}$ be an antisymmetric matrix. Then the matrix product $DW$ has only eigenvalues with $\Re(\lambda_i(DW)) = 0$ for $ i=1,\dots,n$.
\end{myLemma}

\section{Maximum Principle for Neural Networks}
In this section we make use of the work in \cite{aocatdl} and \cite{mpbafdl}.

For the problem (\ref{opt:nnldtocp}), a discrete time formulation of the \ac{PMP} as in theorem \ref{thm:pmpdt} can be made. For this purpose the layer-wise Hamilton-function $H_k: \mathbb{R}^{n_{k}} \times \Omega \times\mathbb{R}^{n_{k+1}} \to \mathbb{R} $ is defined as follows
\begin{equation}
H_k(x,W,\lambda) = \lambda^T f_k(x,W) - \frac{1}{s} L_k(x,W)
\label{eq:hamlw}
\end{equation}

With this Hamilton-function the \ac{PMP} as a necessary condition for optimality of the weights $W$ can be formulated.

\begin{myTheorem}[\ac{PMP} for \acp{NN}]
\label{thm:pmpnn}
Let $x^{i^*}=\{x_0^{i^*}, \dots,x_T^{i^*}\}$, $x_k^{i^*} \in \mathbb R^{n_k}$, $k = 0,\dots, T$, $i=1,\dots,s$, $W^* = \{W_0^*, \dots,W_{T-1}^*\}$, $W_k^*\in \Omega$, $k = 0, \dots, T-1$ and $(x^*,W^*)$ be an optimal solution to problem (\ref{opt:nnldtocp}). Then co-state processes $\lambda^{i^*} = \{\lambda_0^{i^*}, \dots, \lambda_T^{i^*}\}$, $\lambda_k^{i^*} \in \mathbb R^{n_k}$, $k=0,\dots,T$, $i=1,\dots,s$ exist, such that
\begin{align*}
1.&\hspace{0.2cm} x_{k+1}^{i^*} =  f_k(x_k^{i^*},W_k) &k=0,\dots,T-1, i=1,\dots,s \\
2.&\hspace{0.2cm} x_0^{i^*} = \bar x_0^i \\
3.&\hspace{0.2cm} \lambda_k^{i^*} = \nabla_{x_k^i} H_k(x_k^{i^*},W^*_k,\lambda_{k+1}^{i^*}) & k=T-1, \dots, 0, i=1,\dots,s \\
4.&\hspace{0.2cm} \lambda_T^{i^*} = - \frac{1}{s} \nabla_{x^i_T} \phi(x_T^{i^*}) \\
5.&\hspace{0.2cm} \sum_{i=1}^s{H_k(x_k^{i^*},W^*_k,\lambda_{k+1}^{i^*})} \geq \sum_{i=1}^s{H_k(x_k^{i^*},W,\lambda_{k+1}^{i^*})} & \forall W \in \Omega, k = 0,\dots, T-1
\end{align*}
\end{myTheorem}

With this theorem we can formulate an iterative algorithm to solve the learning problem, the \acf{MSA}.

\subsection{The \acl{MSA}}

For the \ac{MSA} an initial guess for the weights $W = \{W_0, \dots,W_{T-1}\}$ of a \ac{NN} is required. With these weights and the training samples $x^i, i \in [s]$, the intermediate values $x_k^i, i \in [s], k = 0,\dots,T$ are the result of the forward propagation. The $\lambda_T^i$ are computed using the gradient of the loss function $\phi(\cdot)$, as in theorem \ref{thm:pmpnn} no. 4. $\lambda_k^i$ is the result of the backward propagation of $\lambda_{k+1}^i$ through the hamiltion-function for $i\in [s], k=0,\dots,T-1$.
The final step requires the maximization of the hamilton-function with the previous computed $x_k^i$ and $\lambda_k^i$, to get a new guess for the weights $W$. This is repeated for a fixed number of iterations. \newline

\begin{algorithm}[H]
 \KwData{Samples $(x_i,y_i), i=1,\dots,s$, initial weights $W^0=\{W_0^0,\dots,W_{T-1}^0\}$, number of iterations $l$}
 \KwResult{Trained weights $W^l=\{W^l_0,\dots,W^l_{T-1}\}$}
 \For{$j=0$ \KwTo $l$}{
	1. $x_0^i = x_i$, \hspace{0.5cm}$i=1,\dots,s$\;
  2. $x_{k+1}^i = f_k(x_k^i,W_k^j)$, \hspace{0.5cm}$k=0,\dots,T-1, i=1,\dots,s$\;
  3. $\lambda_T^i = -\frac{1}{s}\nabla_{x_T^i}\phi(x_T^i)$, \hspace{0.5cm}$i=1,\dots,s$\;
	5. $\lambda_k^i = \nabla_{x_k^i}H_k(x_k^{i},W^j_k,\lambda_{k+1}^{i})$, \hspace{0.5cm}$k=T-1,\dots,0, i=1,\dots,s$\;
	6. $W_k^{j+1} = \underset{W\in \Omega}{\argmax} \sum_{i=1}^s{H_k(x_k^{i},W,\lambda_{k+1}^{i})}$, \hspace{0.5cm}$k=0,\dots,T-1$\;
 }
 \caption{The \ac{MSA}}
\label{alg:msa}
\end{algorithm}

This formulation of algotithm \ref{alg:msa}, as in \cite{aocatdl} and \cite{mpbafdl}, does not incorporate biases in the trainable layers. A step to find new biases is derived in section \ref{subsec:bnn}.

In \cite{aocatdl} an error estimate is stated for the \ac{MSA}. This error estimate will be used for regularization of the \ac{MSA} and further details can be found in the publication.

Under certain conditions the following theorem holds.

\begin{myTheorem}[Error Estimate for the \ac{MSA}]
\label{thm:eemsa}
Let $J(\cdot)$ be the cost-function of problem \ref{opt:nnldtocp}. A constant $C>0$ exists, such that for any weights $W, \tilde{W}$ we have
\begin{align*}
J(x,\tilde{W}) - J(x,W) \leq &- \sum_{k=0}^{T-1}\sum_{i=1}^s H_k(x_k^i,\tilde{W}_k,\lambda_{k+1}^i)-H_k(x_k^i,W_k,\lambda_{k+1}^i) \\
& + \frac{C}{s} \sum_{k=0}^{T-1}\sum_{i=1}^s \Vert f_k(x_k^i,\tilde{W}_k) - f_k(x_k^i,W_k) \Vert^2 \\
& + \frac{C}{s} \sum_{k=0}^{T-1}\sum_{i=1}^s \Vert \nabla_x f_k(x_k^i,\tilde{W}_k) - \nabla_x f_k(x_k^i,W_k) \Vert^2 \\
& + \frac{C}{s} \sum_{k=0}^{T-1}\sum_{i=1}^s \Vert \nabla_x L_k(x_k^i,\tilde{W}_k) - \nabla_x L_k(x_k^i,W_k) \Vert^2 \\
\end{align*}
with $x_k^i, \lambda_k^i, k=0,\dots,T, i=1,\dots,s$ being computed in one \ac{MSA} step, using $W$ as weights.
\end{myTheorem}

\subsection{Binary \aclp{NN}}
\label{subsec:bnn}

For binary \acp{NN} we restrict the weights $W=\{W_0,\dots,W_{T-1}\}$ so that $W_k \in \Omega = \{-1,1\}^{n_{k+1}\times n_{k}}$. These \acp{NN} require less memory on the computer and the discrete weights make the maximization step in the \ac{MSA} easier.\newline

For a feedforward fully-connected \ac{NN} we first consider the weight multiplication and the activation to be separate layers. This results in non-trainable layers of the form $x_{k+1} = f_k(x_k,W_k) = \sigma_k(x_k)$ and trainable layers of the form $x_{k+1} = f_k(x_k,W_k) = W_k x_k$ or $x_{k+1} = f_k(x_k,W_k) = W_k x_k + b_k$ if we incorporate biases.

With no additional regularization $L_k$ in the cost-function, the hamilton-function for the trainable layers writes as follows
\begin{equation}
H_k(x,W,\lambda) = \lambda^T W x 
\end{equation}
or
\begin{equation}
H_k(x,W,\lambda) = \lambda^T (W x+b) 
\end{equation}

The hamiltonian maximization for the second case will be stated explicitly in the following and also applies to the first case when setting the biases to zero.

Consider step no. 6 in algorithm \ref{alg:msa}. For a weight and bias optimization it holds that
\begin{equation*}
W_k^{j+1}, b_k^{j+1} = \underset{W\in \Omega, b \in \tilde\Omega}{\argmax} \sum_{i=1}^s{H_k(x_k^{i},W,\lambda_{k+1}^{i})} = \underset{W\in \Omega, b \in \tilde\Omega}{\argmax} \sum_{i=1}^s{{\lambda_{k+1}^{i}}^T (W x_k^{i} + b)}
%\label{eq:}
\end{equation*}
with $\tilde{\Omega} = \{-1,1\}^{n_{k+1}}$.

Further consider only one layer $k$. Therefore the subscripts in the next lines denote matrix or vector entries and not layers.
%\underset{W\in \Omega, b \in \tilde\Omega}{\argmax} \sum_{i=1}^s{{\lambda^{i}}^T (W x^{i} + b)} = 
\begin{equation*}
\underset{W\in \Omega, b \in \tilde\Omega}{\argmax} \sum_{i=1}^s \begin{pmatrix}
	\lambda^i_1 & \cdots & \lambda^i_{n_{k+1}}
\end{pmatrix} \left( \begin{pmatrix}
	w_{1,1} 					& \cdots & w_{1,n_{k}} \\
	\vdots  					& \ddots & \vdots \\
	w_{n_{k+1},1} & \cdots & w_{n_{k+1},n_{k}}
\end{pmatrix} \begin{pmatrix}
	x^i_1 \\
	\vdots \\
	x^i_{n_{k}}
\end{pmatrix} + \begin{pmatrix}
	b_1 \\
	\vdots \\
	b_{n_{k+1}}
\end{pmatrix}
\right)
%\label{eq:}
\end{equation*}

For a single sample $i$, the expression in the sum evaluates to the following:

\begin{align*}
\lambda^T (W x +b) =& \lambda_1 (w_{1,1} x_1 + \dots + w_{1,n_k} x_{n_k}) + \dots + \lambda_{n_{k+1}} (w_{n_{k+1},1} x_1 + \dots + w_{n_{k+1},n_k} x_{n_k}) \\
& + \lambda_1 b_1+ \dots + \lambda_{n_{k+1}} b_{n_{k+1}} \\
 =& \sum_{l=1}^{n_{k+1}} \sum_{m=1}^{n_{k}} {w_{l,m} \lambda_l x_m} + \sum_{l=1}^{n_{k+1}} {\lambda_l b_l}
\end{align*}

Inserting this into the hamiltonian maximization for layer $k$ yields:

\begin{align}
\underset{W\in \Omega, b \in \tilde\Omega}{\argmax} \sum_{i=1}^s{{\lambda^{i}}^T (W x^{i} + b)} =& \underset{w_{l,m}, b_l \in \{-1,1\}}{\argmax} \sum_{i=1}^s \left( \sum_{l=1}^{n_{k+1}} \sum_{m=1}^{n_{k}} {w_{l,m} \lambda^i_l x^i_m} + \sum_{l=1}^{n_{k+1}} {\lambda^i_l b_l} \right) \\
 =& \sum_{l=1}^{n_{k+1}} \sum_{m=1}^{n_{k}} w_{l,m} \left( \sum_{i=1}^s \lambda^i_l x^i_m \right) + \sum_{l=1}^{n_{k+1}} b_l \left( \sum_{i=1}^s \lambda^i_l \right)
\end{align}

Since the weights and biases only take values in $\{-1,1\}$ we maximize the expression by choosing $w_{l,m} = \sign\left( \sum_{i=1}^s \lambda^i_l x^i_m\right)$ and $b_l = \sign \left( \sum_{i=1}^s \lambda^i_l \right), l=1,\dots,n_{k+1}, m=1,\dots,n_k$.

Using this property the maximzation step 6 in algorithm \ref{alg:msa} can be replaced by:
\begin{equation}
W_k^{j+1} = \sign\left(\sum_{i=1}^s \lambda_{k+1}^i ({x_k^i})^T\right), k=0,\dots,T-1
\label{eq:wsgnmax}
\end{equation}

If the \ac{NN} incorporates biases, they can be updated with the following rule:
\begin{equation}
b_k^{j+1} = \sign\left( \sum_{i=1}^s \lambda_{k+1}^i \right), k=0,\dots,T-1 
\label{eq:bsgnmax}
\end{equation}

\subsection{Modification of the \acl{MSA} for Binary Networks}

The hamilton-maximization steps (\ref{eq:wsgnmax}) and (\ref{eq:bsgnmax}) can be modified to further stabilize the learning process. For this goal consider the error estimate from theorem \ref{thm:eemsa}. Since there are no regularization terms $L_k$ in binary \acp{NN} and the layers with learnable weight have the form $f_k(x_k,W_k) = W_k x_k + b_k$, the equation for the error estimate can be simplified to:

\begin{align*}
&J(x,\tilde{W}) - J(x,W) \leq  - \sum_{k=0}^{T-1}\sum_{i=1}^s H_k(x_k^i,\tilde{W}_k,\lambda_{k+1}^i)-H_k(x_k^i,W_k,\lambda_{k+1}^i) \\
& + \underbrace{\frac{C}{s} \sum_{k=0}^{T-1}\sum_{i=1}^s \Vert \tilde{W}_k x_k^i + \tilde{b}_k - (W_k x_k^i + b_k) \Vert^2 }_{T_1}
 +  \underbrace{\frac{C}{s} \sum_{k=0}^{T-1} s \Vert \tilde{W}_k - W_k \Vert^2_F }_{T_2}
\end{align*}

Looking at the error terms $T_1$ and $T_2$ we get the following estimation:

\begin{align*}
T_1 + T_2 \leq &  \frac{C}{s} \sum_{k=0}^{T-1}\sum_{i=1}^s \Vert (\tilde{W}_k - W_k) x_k^i\Vert^2 + \Vert \tilde{b}_k -  b_k \Vert^2 + 2 \langle (\tilde{W}_k - W_k) x_k^i,  \tilde{b}_k -  b_k \rangle +  T_2 \\
\leq &  \frac{C}{s} \sum_{k=0}^{T-1}\left(s + \sum_{i=1}^s \Vert x_k^i \Vert^2 \right) \Vert \tilde{W}_k - W_k \Vert^2_F + s \Vert \tilde{b}_k -  b_k \Vert^2 \\
& + \frac{2C}{s}  \sum_{k=0}^{T-1}\sum_{i=1}^s \langle (\tilde{W}_k - W_k) x_k^i,  \tilde{b}_k -  b_k \rangle
\end{align*}

With this estimation it can be seen, that not only the hamiltonian needs to be maximized to reduce the cost-function. Additionally the error terms introduced by changing the weights and biases need to be minimized. To incorporate this in the \ac{MSA} the weight update (step 6 in algorithm \ref{alg:msa} or (\ref{eq:wsgnmax})) changes to

\begin{equation*}
W_k^{j+1} = \underset{W\in \Omega}{\argmax} \sum_{i=1}^s{H_k(x_k^{i},W,\lambda_{k+1}^{i}) - \rho_k \Vert W - W_k^j \Vert^2_F}
\end{equation*}

or explicitly to

\begin{equation}
[W_k^{j+1}]_{lm} = \begin{cases} \sign\left([M_k]_{lm}\right), & \text{if }\vert[M_k]_{lm} \vert \geq \rho_k \\
																	[W_k^j]_{lm},                & \text{else} \end{cases}
, k=0,\dots,T-1
\label{eq:wsgnmaxpen}
\end{equation}

with $M_k = \sum_{i=1}^s \lambda_{k+1}^i ({x_k^i})^T$ and $\rho_k$ a layer-wise penalization parameter. 

For the biases there are different possibilities to modify the update step. The first one is analogous to the weight step

\begin{equation}
[b_k^{j+1}]_{l} = \begin{cases} \sign\left([m_k]_{l}\right), & \text{if }\vert[m_k]_{l} \vert \geq \rho_k \\
																	[b_k^j]_{l},                & \text{else} \end{cases}
, k=0,\dots,T-1
\label{eq:bsgnmaxpen}
\end{equation}
with $m_k = \sum_{i=1}^s \lambda_{k+1}^i $ and $\rho_k$ a layer-wise penalization parameter. 

Another way is to choose 

\begin{equation}
b_k^{j+1} = \sign\left( \sum_{i=1}^s \lambda_{k+1}^i-(W_k^{j+1}-W_k^j)x_k^i \right), k=0,\dots,T-1
\label{eq:bsgnmaxx}
\end{equation}
 with the intention to minimize the scalar product term in the estimation.\newline

The \ac{MSA} with the modified update steps is the foundation for the implementation in chapter \ref{chap:Impl}, where additional methods for stabilizing the training process are discussed.

% -----------------------------------------------------------------------------------------
% Implementierung
% -----------------------------------------------------------------------------------------
\chapter{Implementation}
\label{chap:Impl}









% -----------------------------------------------------------------------------------------
% Numerische Ergebnisse
% -----------------------------------------------------------------------------------------

\chapter{Numerical Results}
\label{chap:NR}


% -----------------------------------------------------------------------------------------
% Zusammenfassung
% -----------------------------------------------------------------------------------------

\chapter{Conclusion and Outlook}
\label{chap:CaO}

\clearpage
\appendix
\chapter{Appendix}


\clearpage
\bibliography{lit}
\bibliographystyle{alpha}

% ----------------------------------------------------------------------------------------
\newpage
\section*{Erklärung}		
\thispagestyle{empty}				


Hiermit erkläre ich, dass ich die vorliegende Arbeit selbstständig und ohne Benutzung anderer als der angegebenen Quellen und Hilfsmittel angefertigt habe.
\newline\newline
Ort, Datum, Unterschrift




\end{document}
