{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.808340  #  0.552083  #\n",
      "Correct predictions: 0.7833333333333333\n",
      "#  2  #  0.411628  #  0.831250  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  3  #  0.285570  #  0.945833  #\n",
      "Correct predictions: 0.9083333333333333\n",
      "#  4  #  0.200868  #  0.981250  #\n",
      "Correct predictions: 0.9666666666666667\n",
      "#  5  #  0.242199  #  0.945833  #\n",
      "Correct predictions: 0.975\n",
      "#  6  #  0.210706  #  0.981250  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  7  #  0.196120  #  0.972917  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  8  #  0.181256  #  0.983333  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  9  #  0.163337  #  0.989583  #\n",
      "Correct predictions: 1.0\n",
      "#  10  #  0.165084  #  0.989583  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  11  #  0.162165  #  0.989583  #\n",
      "Correct predictions: 0.9833333333333333\n",
      "#  12  #  0.162648  #  0.991667  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  13  #  0.172137  #  0.991667  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  14  #  0.166049  #  0.987500  #\n",
      "Correct predictions: 1.0\n",
      "#  15  #  0.202394  #  0.962500  #\n",
      "Correct predictions: 1.0\n",
      "#  16  #  0.156602  #  0.991667  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  17  #  0.178343  #  0.968750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  18  #  0.158333  #  0.987500  #\n",
      "Correct predictions: 1.0\n",
      "#  19  #  0.148993  #  0.989583  #\n",
      "Correct predictions: 1.0\n",
      "#  20  #  0.159728  #  0.989583  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  21  #  0.162459  #  0.987500  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  22  #  0.155231  #  0.989583  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  23  #  0.142529  #  1.000000  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  24  #  0.144426  #  0.995833  #\n",
      "Correct predictions: 0.975\n",
      "#  25  #  0.153088  #  0.991667  #\n",
      "Correct predictions: 0.975\n",
      "#  26  #  0.153835  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  27  #  0.151801  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  28  #  0.168344  #  0.977083  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  29  #  0.143604  #  0.995833  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  30  #  0.151432  #  0.991667  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  31  #  0.156119  #  0.985417  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  32  #  0.170007  #  0.979167  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  33  #  0.163274  #  0.983333  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  34  #  0.149747  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  35  #  0.155871  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  36  #  0.153484  #  0.989583  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  37  #  0.163589  #  0.985417  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  38  #  0.146085  #  0.995833  #\n",
      "Correct predictions: 0.9583333333333334\n",
      "#  39  #  0.162134  #  0.987500  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  40  #  0.144824  #  0.995833  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  41  #  0.147468  #  0.995833  #\n",
      "Correct predictions: 0.9833333333333333\n",
      "#  42  #  0.147761  #  0.989583  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  43  #  0.157261  #  0.985417  #\n",
      "Correct predictions: 1.0\n",
      "#  44  #  0.145169  #  0.991667  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  45  #  0.148623  #  0.991667  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  46  #  0.146692  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  47  #  0.146538  #  0.989583  #\n",
      "Correct predictions: 1.0\n",
      "#  48  #  0.162144  #  0.985417  #\n",
      "Correct predictions: 0.9833333333333333\n",
      "#  49  #  0.141463  #  0.995833  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  50  #  0.155941  #  0.985417  #\n",
      "Correct predictions: 0.925\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  51  #  0.162672  #  0.987500  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  52  #  0.178063  #  0.975000  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  53  #  0.156318  #  0.991667  #\n",
      "Correct predictions: 1.0\n",
      "#  54  #  0.154580  #  0.985417  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  55  #  0.156357  #  0.985417  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  56  #  0.149931  #  0.989583  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  57  #  0.144883  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  58  #  0.151132  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  59  #  0.161829  #  0.985417  #\n",
      "Correct predictions: 1.0\n",
      "#  60  #  0.153336  #  0.987500  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  61  #  0.150468  #  0.989583  #\n",
      "Correct predictions: 1.0\n",
      "#  62  #  0.148907  #  0.991667  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  63  #  0.147932  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  64  #  0.149734  #  0.989583  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  65  #  0.142000  #  0.997917  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  66  #  0.148420  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  67  #  0.149443  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  68  #  0.144668  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  69  #  0.147300  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  70  #  0.144443  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  71  #  0.152695  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  72  #  0.142211  #  0.997917  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  73  #  0.143431  #  0.995833  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  74  #  0.143691  #  0.995833  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  75  #  0.144267  #  0.995833  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  76  #  0.149862  #  0.991667  #\n",
      "Correct predictions: 1.0\n",
      "#  77  #  0.150395  #  0.995833  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  78  #  0.141215  #  0.995833  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  79  #  0.139038  #  0.997917  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  80  #  0.163974  #  0.983333  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  81  #  0.172182  #  0.975000  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  82  #  0.146684  #  0.995833  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  83  #  0.153879  #  0.987500  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  84  #  0.150215  #  0.987500  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  85  #  0.151643  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  86  #  0.150531  #  0.991667  #\n",
      "Correct predictions: 1.0\n",
      "#  87  #  0.142617  #  0.995833  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  88  #  0.148605  #  0.989583  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  89  #  0.154771  #  0.989583  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  90  #  0.139640  #  0.997917  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  91  #  0.147058  #  0.987500  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  92  #  0.144845  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  93  #  0.157150  #  0.989583  #\n",
      "Correct predictions: 1.0\n",
      "#  94  #  0.156387  #  0.987500  #\n",
      "Correct predictions: 1.0\n",
      "#  95  #  0.154718  #  0.989583  #\n",
      "Correct predictions: 1.0\n",
      "#  96  #  0.139922  #  0.995833  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  97  #  0.150784  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  98  #  0.139827  #  0.995833  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  99  #  0.142193  #  0.997917  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  100  #  0.139270  #  1.000000  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "Time elapsed:  31.295864621999996\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import FCMSANet\n",
    "import torch\n",
    "\n",
    "\n",
    "def make_uniform_layer_list(layers, num_features):\n",
    "    return [num_features] * (layers+1)\n",
    "    \n",
    "def make_uniform_hidden_layer_list(layers, num_features, num_classes, size_hidden):\n",
    "    res = [num_features]\n",
    "    res.extend([size_hidden]*(layers-1))\n",
    "    res.extend([num_classes])\n",
    "    return res\n",
    "\n",
    "\n",
    "dataset_size = 600\n",
    "batch_size = 50\n",
    "test_set_size = dataset_size * 0.2\n",
    "\n",
    "\n",
    "#torch.manual_seed(0)\n",
    "train_loader, test_loader = makeMoonsDataset(dataset_size, batch_size)\n",
    "#torch.manual_seed(3)\n",
    "\n",
    "num_layers = 10\n",
    "#layers = make_uniform_layer_list(num_layers, 2)\n",
    "layers = make_uniform_hidden_layer_list(num_layers, 2, 2, 64)\n",
    "#print(layers)\n",
    "net = FCMSANet(num_fc=num_layers, sizes_fc=layers, bias=False, batchnorm=True, test=False)   \n",
    "#net = FCMSANet(num_fc=3,sizes_fc=[2,2,4,2], bias=False, batchnorm=True, test=False)  \n",
    "net.set_rho(0.5)\n",
    "net.set_ema_alpha(0.99)\n",
    "net.set_test_tracking(True)\n",
    "net.train_msa(100,train_loader, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.027722  #  0.493750  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  2  #  0.017197  #  0.518750  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  3  #  0.011124  #  0.787500  #\n",
      "Correct predictions: 0.8083333333333333\n",
      "#  4  #  0.007607  #  0.875000  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  5  #  0.007319  #  0.877083  #\n",
      "Correct predictions: 0.85\n",
      "#  6  #  0.006946  #  0.887500  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  7  #  0.006866  #  0.879167  #\n",
      "Correct predictions: 0.8416666666666667\n",
      "#  8  #  0.006962  #  0.870833  #\n",
      "Correct predictions: 0.875\n",
      "#  9  #  0.007946  #  0.870833  #\n",
      "Correct predictions: 0.8083333333333333\n",
      "#  10  #  0.007471  #  0.866667  #\n",
      "Correct predictions: 0.8833333333333333\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  11  #  0.007847  #  0.872917  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  12  #  0.006727  #  0.866667  #\n",
      "Correct predictions: 0.8416666666666667\n",
      "#  13  #  0.006621  #  0.866667  #\n",
      "Correct predictions: 0.8833333333333333\n",
      "#  14  #  0.006840  #  0.862500  #\n",
      "Correct predictions: 0.8833333333333333\n",
      "#  15  #  0.007656  #  0.858333  #\n",
      "Correct predictions: 0.8166666666666667\n",
      "#  16  #  0.006500  #  0.866667  #\n",
      "Correct predictions: 0.8166666666666667\n",
      "#  17  #  0.006826  #  0.860417  #\n",
      "Correct predictions: 0.8833333333333333\n",
      "#  18  #  0.006802  #  0.877083  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  19  #  0.006608  #  0.881250  #\n",
      "Correct predictions: 0.85\n",
      "#  20  #  0.007035  #  0.872917  #\n",
      "Correct predictions: 0.8916666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  21  #  0.006911  #  0.877083  #\n",
      "Correct predictions: 0.8416666666666667\n",
      "#  22  #  0.006439  #  0.866667  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  23  #  0.006282  #  0.885417  #\n",
      "Correct predictions: 0.875\n",
      "#  24  #  0.006819  #  0.877083  #\n",
      "Correct predictions: 0.8833333333333333\n",
      "#  25  #  0.007895  #  0.862500  #\n",
      "Correct predictions: 0.8833333333333333\n",
      "#  26  #  0.006423  #  0.872917  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  27  #  0.006748  #  0.879167  #\n",
      "Correct predictions: 0.8833333333333333\n",
      "#  28  #  0.007281  #  0.877083  #\n",
      "Correct predictions: 0.8166666666666667\n",
      "#  29  #  0.006769  #  0.866667  #\n",
      "Correct predictions: 0.8166666666666667\n",
      "#  30  #  0.006854  #  0.862500  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  31  #  0.007032  #  0.868750  #\n",
      "Correct predictions: 0.875\n",
      "#  32  #  0.005929  #  0.881250  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  33  #  0.006449  #  0.875000  #\n",
      "Correct predictions: 0.9\n",
      "#  34  #  0.007465  #  0.845833  #\n",
      "Correct predictions: 0.8\n",
      "#  35  #  0.007506  #  0.872917  #\n",
      "Correct predictions: 0.875\n",
      "#  36  #  0.007189  #  0.862500  #\n",
      "Correct predictions: 0.75\n",
      "#  37  #  0.007588  #  0.860417  #\n",
      "Correct predictions: 0.8833333333333333\n",
      "#  38  #  0.006853  #  0.866667  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  39  #  0.006403  #  0.879167  #\n",
      "Correct predictions: 0.8833333333333333\n",
      "#  40  #  0.006367  #  0.870833  #\n",
      "Correct predictions: 0.8166666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  41  #  0.006625  #  0.870833  #\n",
      "Correct predictions: 0.8333333333333334\n",
      "#  42  #  0.007121  #  0.856250  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  43  #  0.006453  #  0.868750  #\n",
      "Correct predictions: 0.8333333333333334\n",
      "#  44  #  0.007573  #  0.854167  #\n",
      "Correct predictions: 0.85\n",
      "#  45  #  0.006510  #  0.872917  #\n",
      "Correct predictions: 0.9\n",
      "#  46  #  0.006193  #  0.877083  #\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import ResAntiSymNet\n",
    "import torch\n",
    "#import gc\n",
    "#gc.collect()\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset(600,40)\n",
    "gamma = 0.3\n",
    "h = 0.1\n",
    "net = ResAntiSymNet(features=2, classes=2, num_layers=100, gamma=gamma, h=h, bias=True, hidden_size=8)\n",
    "net.set_test_tracking(True)\n",
    "net.train(num_epochs=100, dataloader=train_loader, testloader=test_loader)\n",
    "print(net.avg_correct_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.029316  #  0.464583  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  2  #  0.017411  #  0.497917  #\n",
      "Correct predictions: 0.5416666666666666\n",
      "#  3  #  0.017303  #  0.531250  #\n",
      "Correct predictions: 0.6833333333333333\n",
      "#  4  #  0.016103  #  0.572917  #\n",
      "Correct predictions: 0.825\n",
      "#  5  #  0.009681  #  0.837500  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  6  #  0.009520  #  0.841667  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  7  #  0.007282  #  0.881250  #\n",
      "Correct predictions: 0.875\n",
      "#  8  #  0.007659  #  0.862500  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  9  #  0.007233  #  0.870833  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  10  #  0.007105  #  0.885417  #\n",
      "Correct predictions: 0.85\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  11  #  0.007785  #  0.862500  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  12  #  0.007215  #  0.870833  #\n",
      "Correct predictions: 0.85\n",
      "#  13  #  0.007417  #  0.864583  #\n",
      "Correct predictions: 0.85\n",
      "#  14  #  0.007197  #  0.864583  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  15  #  0.006995  #  0.877083  #\n",
      "Correct predictions: 0.875\n",
      "#  16  #  0.007821  #  0.866667  #\n",
      "Correct predictions: 0.875\n",
      "#  17  #  0.007211  #  0.872917  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  18  #  0.006832  #  0.875000  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  19  #  0.007244  #  0.879167  #\n",
      "Correct predictions: 0.875\n",
      "#  20  #  0.006833  #  0.868750  #\n",
      "Correct predictions: 0.875\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  21  #  0.006654  #  0.887500  #\n",
      "Correct predictions: 0.875\n",
      "#  22  #  0.006578  #  0.877083  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  23  #  0.006915  #  0.875000  #\n",
      "Correct predictions: 0.85\n",
      "#  24  #  0.007495  #  0.868750  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  25  #  0.007306  #  0.877083  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  26  #  0.007292  #  0.870833  #\n",
      "Correct predictions: 0.875\n",
      "#  27  #  0.006754  #  0.877083  #\n",
      "Correct predictions: 0.875\n",
      "#  28  #  0.006647  #  0.877083  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  29  #  0.007025  #  0.870833  #\n",
      "Correct predictions: 0.85\n",
      "#  30  #  0.007614  #  0.866667  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  31  #  0.006785  #  0.868750  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  32  #  0.006839  #  0.881250  #\n",
      "Correct predictions: 0.875\n",
      "#  33  #  0.007159  #  0.868750  #\n",
      "Correct predictions: 0.875\n",
      "#  34  #  0.006778  #  0.877083  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  35  #  0.007097  #  0.875000  #\n",
      "Correct predictions: 0.85\n",
      "#  36  #  0.006668  #  0.879167  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  37  #  0.006747  #  0.877083  #\n",
      "Correct predictions: 0.875\n",
      "#  38  #  0.007414  #  0.870833  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  39  #  0.006785  #  0.881250  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  40  #  0.007066  #  0.866667  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  41  #  0.006759  #  0.872917  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  42  #  0.006536  #  0.877083  #\n",
      "Correct predictions: 0.875\n",
      "#  43  #  0.006759  #  0.872917  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  44  #  0.006835  #  0.875000  #\n",
      "Correct predictions: 0.8833333333333333\n",
      "#  45  #  0.006507  #  0.870833  #\n",
      "Correct predictions: 0.875\n",
      "#  46  #  0.006652  #  0.883333  #\n",
      "Correct predictions: 0.875\n",
      "#  47  #  0.006843  #  0.879167  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  48  #  0.006360  #  0.879167  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  49  #  0.006938  #  0.872917  #\n",
      "Correct predictions: 0.875\n",
      "#  50  #  0.006670  #  0.872917  #\n",
      "Correct predictions: 0.875\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  51  #  0.006691  #  0.868750  #\n",
      "Correct predictions: 0.875\n",
      "#  52  #  0.006696  #  0.866667  #\n",
      "Correct predictions: 0.875\n",
      "#  53  #  0.006857  #  0.868750  #\n",
      "Correct predictions: 0.875\n",
      "#  54  #  0.006796  #  0.875000  #\n",
      "Correct predictions: 0.875\n",
      "#  55  #  0.006683  #  0.860417  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  56  #  0.006782  #  0.872917  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  57  #  0.006996  #  0.864583  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  58  #  0.006986  #  0.860417  #\n",
      "Correct predictions: 0.875\n",
      "#  59  #  0.006545  #  0.868750  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  60  #  0.006572  #  0.879167  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  61  #  0.006718  #  0.872917  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  62  #  0.006520  #  0.870833  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  63  #  0.006743  #  0.866667  #\n",
      "Correct predictions: 0.875\n",
      "#  64  #  0.006683  #  0.870833  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  65  #  0.006950  #  0.862500  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  66  #  0.006552  #  0.879167  #\n",
      "Correct predictions: 0.85\n",
      "#  67  #  0.006942  #  0.872917  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  68  #  0.006742  #  0.866667  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  69  #  0.006486  #  0.875000  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  70  #  0.006788  #  0.879167  #\n",
      "Correct predictions: 0.875\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  71  #  0.006659  #  0.872917  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  72  #  0.006510  #  0.875000  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  73  #  0.006604  #  0.870833  #\n",
      "Correct predictions: 0.875\n",
      "#  74  #  0.006660  #  0.872917  #\n",
      "Correct predictions: 0.85\n",
      "#  75  #  0.006817  #  0.866667  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  76  #  0.006496  #  0.866667  #\n",
      "Correct predictions: 0.875\n",
      "#  77  #  0.006636  #  0.864583  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  78  #  0.006638  #  0.881250  #\n",
      "Correct predictions: 0.875\n",
      "#  79  #  0.006558  #  0.877083  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  80  #  0.006843  #  0.868750  #\n",
      "Correct predictions: 0.875\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  81  #  0.006438  #  0.872917  #\n",
      "Correct predictions: 0.85\n",
      "#  82  #  0.006646  #  0.879167  #\n",
      "Correct predictions: 0.875\n",
      "#  83  #  0.006586  #  0.868750  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  84  #  0.006706  #  0.870833  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  85  #  0.006533  #  0.872917  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  86  #  0.006533  #  0.870833  #\n",
      "Correct predictions: 0.875\n",
      "#  87  #  0.006815  #  0.870833  #\n",
      "Correct predictions: 0.875\n",
      "#  88  #  0.006755  #  0.872917  #\n",
      "Correct predictions: 0.875\n",
      "#  89  #  0.006579  #  0.877083  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  90  #  0.006477  #  0.877083  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  91  #  0.006645  #  0.875000  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  92  #  0.006589  #  0.870833  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  93  #  0.006683  #  0.872917  #\n",
      "Correct predictions: 0.85\n",
      "#  94  #  0.006831  #  0.868750  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  95  #  0.006837  #  0.854167  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  96  #  0.006501  #  0.877083  #\n",
      "Correct predictions: 0.875\n",
      "#  97  #  0.006604  #  0.870833  #\n",
      "Correct predictions: 0.875\n",
      "#  98  #  0.006433  #  0.877083  #\n",
      "Correct predictions: 0.875\n",
      "#  99  #  0.006663  #  0.877083  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  100  #  0.006481  #  0.879167  #\n",
      "Correct predictions: 0.85\n",
      "Time elapsed:  8.941094796000016\n",
      "tensor([0.4646, 0.4979, 0.5312, 0.5729, 0.8375, 0.8417, 0.8813, 0.8625, 0.8708,\n",
      "        0.8854, 0.8625, 0.8708, 0.8646, 0.8646, 0.8771, 0.8667, 0.8729, 0.8750,\n",
      "        0.8792, 0.8687, 0.8875, 0.8771, 0.8750, 0.8687, 0.8771, 0.8708, 0.8771,\n",
      "        0.8771, 0.8708, 0.8667, 0.8687, 0.8813, 0.8687, 0.8771, 0.8750, 0.8792,\n",
      "        0.8771, 0.8708, 0.8813, 0.8667, 0.8729, 0.8771, 0.8729, 0.8750, 0.8708,\n",
      "        0.8833, 0.8792, 0.8792, 0.8729, 0.8729, 0.8687, 0.8667, 0.8687, 0.8750,\n",
      "        0.8604, 0.8729, 0.8646, 0.8604, 0.8687, 0.8792, 0.8729, 0.8708, 0.8667,\n",
      "        0.8708, 0.8625, 0.8792, 0.8729, 0.8667, 0.8750, 0.8792, 0.8729, 0.8750,\n",
      "        0.8708, 0.8729, 0.8667, 0.8667, 0.8646, 0.8813, 0.8771, 0.8687, 0.8729,\n",
      "        0.8792, 0.8687, 0.8708, 0.8729, 0.8708, 0.8708, 0.8729, 0.8771, 0.8771,\n",
      "        0.8750, 0.8708, 0.8729, 0.8687, 0.8542, 0.8771, 0.8708, 0.8771, 0.8771,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0.8792])\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.3\n",
    "h = 1\n",
    "net = None\n",
    "net = ResAntiSymNet(features=2, classes=2, num_layers=21, gamma=gamma, h=h, bias=True, hidden_size=8)\n",
    "net.set_test_tracking(True)\n",
    "net.train(num_epochs=100, dataloader=train_loader, testloader=test_loader)\n",
    "print(net.avg_correct_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.017344  #  0.506250  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  2  #  0.017221  #  0.543750  #\n",
      "Correct predictions: 0.7416666666666667\n",
      "#  3  #  0.017083  #  0.758333  #\n",
      "Correct predictions: 0.725\n",
      "#  4  #  0.016913  #  0.727083  #\n",
      "Correct predictions: 0.7416666666666667\n",
      "#  5  #  0.016583  #  0.750000  #\n",
      "Correct predictions: 0.725\n",
      "#  6  #  0.015957  #  0.750000  #\n",
      "Correct predictions: 0.7583333333333333\n",
      "#  7  #  0.014648  #  0.770833  #\n",
      "Correct predictions: 0.775\n",
      "#  8  #  0.012616  #  0.783333  #\n",
      "Correct predictions: 0.7833333333333333\n",
      "#  9  #  0.010690  #  0.810417  #\n",
      "Correct predictions: 0.8083333333333333\n",
      "#  10  #  0.009303  #  0.839583  #\n",
      "Correct predictions: 0.8166666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  11  #  0.008267  #  0.854167  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  12  #  0.007619  #  0.875000  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  13  #  0.007144  #  0.887500  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  14  #  0.007082  #  0.872917  #\n",
      "Correct predictions: 0.875\n",
      "#  15  #  0.006887  #  0.883333  #\n",
      "Correct predictions: 0.875\n",
      "#  16  #  0.006328  #  0.891667  #\n",
      "Correct predictions: 0.8833333333333333\n",
      "#  17  #  0.006506  #  0.889583  #\n",
      "Correct predictions: 0.8916666666666667\n",
      "#  18  #  0.005510  #  0.906250  #\n",
      "Correct predictions: 0.9166666666666666\n",
      "#  19  #  0.005588  #  0.910417  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  20  #  0.005392  #  0.897917  #\n",
      "Correct predictions: 0.9\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  21  #  0.004840  #  0.912500  #\n",
      "Correct predictions: 0.9333333333333333\n",
      "#  22  #  0.009136  #  0.854167  #\n",
      "Correct predictions: 0.925\n",
      "#  23  #  0.004225  #  0.935417  #\n",
      "Correct predictions: 0.9416666666666667\n",
      "#  24  #  0.003940  #  0.943750  #\n",
      "Correct predictions: 0.9416666666666667\n",
      "#  25  #  0.003696  #  0.950000  #\n",
      "Correct predictions: 0.95\n",
      "#  26  #  0.002610  #  0.962500  #\n",
      "Correct predictions: 0.95\n",
      "#  27  #  0.001950  #  0.979167  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  28  #  0.001676  #  0.977083  #\n",
      "Correct predictions: 0.9833333333333333\n",
      "#  29  #  0.001806  #  0.972917  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  30  #  0.000944  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  31  #  0.001162  #  0.989583  #\n",
      "Correct predictions: 1.0\n",
      "#  32  #  0.000738  #  0.991667  #\n",
      "Correct predictions: 1.0\n",
      "#  33  #  0.000679  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  34  #  0.000604  #  0.991667  #\n",
      "Correct predictions: 1.0\n",
      "#  35  #  0.000508  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  36  #  0.000421  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  37  #  0.010973  #  0.916667  #\n",
      "Correct predictions: 0.85\n",
      "#  38  #  0.004246  #  0.941667  #\n",
      "Correct predictions: 1.0\n",
      "#  39  #  0.001160  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  40  #  0.000843  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  41  #  0.000613  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  42  #  0.000616  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  43  #  0.000382  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  44  #  0.000414  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  45  #  0.000318  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  46  #  0.000278  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  47  #  0.000257  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  48  #  0.000217  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  49  #  0.000250  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  50  #  0.000203  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  51  #  0.000221  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  52  #  0.000165  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  53  #  0.000153  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  54  #  0.000189  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  55  #  0.000143  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  56  #  0.000138  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  57  #  0.000133  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  58  #  0.000194  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  59  #  0.000188  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  60  #  0.000139  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  61  #  0.000125  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  62  #  0.000165  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  63  #  0.000149  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  64  #  0.000108  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  65  #  0.000215  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  66  #  0.000100  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  67  #  0.000079  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  68  #  0.000134  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  69  #  0.000129  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  70  #  0.000067  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  71  #  0.000115  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  72  #  0.000141  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  73  #  0.000085  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  74  #  0.000079  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  75  #  0.000100  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  76  #  0.000092  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  77  #  0.000053  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  78  #  0.000077  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  79  #  0.000098  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  80  #  0.000071  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  81  #  0.000079  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  82  #  0.000053  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  83  #  0.000093  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  84  #  0.000060  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  85  #  0.000101  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  86  #  0.000095  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  87  #  0.000108  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  88  #  0.000078  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  89  #  0.000092  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  90  #  0.000062  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  91  #  0.000261  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  92  #  0.000047  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  93  #  0.000040  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  94  #  0.000057  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  95  #  0.000046  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  96  #  0.000077  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  97  #  0.000044  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  98  #  0.000041  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  99  #  0.000043  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  100  #  0.000064  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "Time elapsed:  4.904766947000098\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import FCNet\n",
    "import torch\n",
    "\n",
    "dataset_size = 600\n",
    "batch_size = 40\n",
    "test_set_size = dataset_size * 0.2\n",
    "num_layers = 11\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset(dataset_size,batch_size)\n",
    "layers = make_uniform_hidden_layer_list(num_layers, 2, 2, 64)\n",
    "# net = FCNet(num_layers=num_layers, layers=layers, bias=True)\n",
    "net = FCNet(num_layers=6, layers=[2,16,32,64,128,128,2], bias=True)\n",
    "net.set_test_tracking(True)\n",
    "net.train(100, train_loader, test_loader)\n",
    "\n",
    "#net.test(test_loader, test_set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.472702  #  0.641667  #\n",
      "#  2  #  0.005676  #  0.914583  #\n",
      "#  3  #  0.003665  #  0.952083  #\n",
      "#  4  #  0.002344  #  0.975000  #\n",
      "#  5  #  0.001609  #  0.987500  #\n",
      "#  6  #  0.001037  #  0.991667  #\n",
      "#  7  #  0.000751  #  0.995833  #\n",
      "#  8  #  0.000607  #  0.995833  #\n",
      "#  9  #  0.000501  #  0.995833  #\n",
      "#  10  #  0.000413  #  0.997917  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  11  #  0.000426  #  0.993750  #\n",
      "#  12  #  0.000313  #  0.997917  #\n",
      "#  13  #  0.000319  #  0.995833  #\n",
      "#  14  #  0.000263  #  0.995833  #\n",
      "#  15  #  0.000228  #  1.000000  #\n",
      "#  16  #  0.000187  #  0.997917  #\n",
      "#  17  #  0.000175  #  0.997917  #\n",
      "#  18  #  0.000161  #  1.000000  #\n",
      "#  19  #  0.000157  #  1.000000  #\n",
      "#  20  #  0.000161  #  0.997917  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  21  #  0.000129  #  1.000000  #\n",
      "#  22  #  0.000172  #  1.000000  #\n",
      "#  23  #  0.000179  #  0.997917  #\n",
      "#  24  #  0.000146  #  0.997917  #\n",
      "#  25  #  0.000090  #  1.000000  #\n",
      "#  26  #  0.000087  #  1.000000  #\n",
      "#  27  #  0.000106  #  0.997917  #\n",
      "#  28  #  0.000091  #  1.000000  #\n",
      "#  29  #  0.000135  #  0.997917  #\n",
      "#  30  #  0.000126  #  0.997917  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  31  #  0.000100  #  1.000000  #\n",
      "#  32  #  0.000090  #  1.000000  #\n",
      "#  33  #  0.000109  #  0.997917  #\n",
      "#  34  #  0.000074  #  1.000000  #\n",
      "#  35  #  0.000074  #  1.000000  #\n",
      "#  36  #  0.000158  #  0.997917  #\n",
      "#  37  #  0.000055  #  1.000000  #\n",
      "#  38  #  0.000051  #  1.000000  #\n",
      "#  39  #  0.000048  #  1.000000  #\n",
      "#  40  #  0.000041  #  1.000000  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  41  #  0.000037  #  1.000000  #\n",
      "#  42  #  0.000050  #  1.000000  #\n",
      "#  43  #  0.000072  #  0.997917  #\n",
      "#  44  #  0.000137  #  0.995833  #\n",
      "#  45  #  0.000062  #  1.000000  #\n",
      "#  46  #  0.000165  #  0.995833  #\n",
      "#  47  #  0.000074  #  1.000000  #\n",
      "#  48  #  0.000147  #  0.995833  #\n",
      "#  49  #  0.000153  #  0.995833  #\n",
      "#  50  #  0.000083  #  0.997917  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  51  #  0.000075  #  1.000000  #\n",
      "#  52  #  0.000029  #  1.000000  #\n",
      "#  53  #  0.000053  #  1.000000  #\n",
      "#  54  #  0.000063  #  0.997917  #\n",
      "#  55  #  0.000051  #  1.000000  #\n",
      "#  56  #  0.000056  #  1.000000  #\n",
      "#  57  #  0.000041  #  1.000000  #\n",
      "#  58  #  0.000039  #  1.000000  #\n",
      "#  59  #  0.000030  #  1.000000  #\n",
      "#  60  #  0.000038  #  1.000000  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  61  #  0.000067  #  1.000000  #\n",
      "#  62  #  0.000058  #  0.997917  #\n",
      "#  63  #  0.000214  #  0.993750  #\n",
      "#  64  #  0.000050  #  1.000000  #\n",
      "#  65  #  0.000022  #  1.000000  #\n",
      "#  66  #  0.000025  #  1.000000  #\n",
      "#  67  #  0.000024  #  1.000000  #\n",
      "#  68  #  0.000030  #  1.000000  #\n",
      "#  69  #  0.000020  #  1.000000  #\n",
      "#  70  #  0.000021  #  1.000000  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  71  #  0.000028  #  1.000000  #\n",
      "#  72  #  0.000018  #  1.000000  #\n",
      "#  73  #  0.000019  #  1.000000  #\n",
      "#  74  #  0.000018  #  1.000000  #\n",
      "#  75  #  0.000019  #  1.000000  #\n",
      "#  76  #  0.000014  #  1.000000  #\n",
      "#  77  #  0.000015  #  1.000000  #\n",
      "#  78  #  0.000015  #  1.000000  #\n",
      "#  79  #  0.000020  #  1.000000  #\n",
      "#  80  #  0.000014  #  1.000000  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  81  #  0.000014  #  1.000000  #\n",
      "#  82  #  0.000013  #  1.000000  #\n",
      "#  83  #  0.000018  #  1.000000  #\n",
      "#  84  #  0.000015  #  1.000000  #\n",
      "#  85  #  0.000014  #  1.000000  #\n",
      "#  86  #  0.000012  #  1.000000  #\n",
      "#  87  #  0.000012  #  1.000000  #\n",
      "#  88  #  0.000013  #  1.000000  #\n",
      "#  89  #  0.000011  #  1.000000  #\n",
      "#  90  #  0.000012  #  1.000000  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  91  #  0.000011  #  1.000000  #\n",
      "#  92  #  0.000012  #  1.000000  #\n",
      "#  93  #  0.000012  #  1.000000  #\n",
      "#  94  #  0.000011  #  1.000000  #\n",
      "#  95  #  0.000010  #  1.000000  #\n",
      "#  96  #  0.000012  #  1.000000  #\n",
      "#  97  #  0.000011  #  1.000000  #\n",
      "#  98  #  0.000010  #  1.000000  #\n",
      "#  99  #  0.000010  #  1.000000  #\n",
      "#  100  #  0.000009  #  1.000000  #\n",
      "Time elapsed:  6.256591480996576\n",
      "Correct predictions: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import ResFCNet\n",
    "import torch\n",
    "\n",
    "def make_uniform_layer_list(layers, num_features):\n",
    "        return [num_features] * (layers+1)\n",
    "\n",
    "def make_uniform_hidden_layer_list(layers, num_features, num_classes, size_hidden):\n",
    "    res = [num_features]\n",
    "    res.extend([size_hidden]*(layers-1))\n",
    "    res.extend([num_classes])\n",
    "    return res\n",
    "\n",
    "\n",
    "dataset_size = 600\n",
    "batch_size = 40\n",
    "test_set_size = dataset_size * 0.2\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset(dataset_size,batch_size)\n",
    "\n",
    "num_layers = 14\n",
    "#layers = make_uniform_layer_list(num_layers, 2)\n",
    "layers = make_uniform_hidden_layer_list(num_layers, 2, 2, 64)\n",
    "print(len(layers))\n",
    "net = ResFCNet(num_layers=num_layers, layers=layers, bias=True)\n",
    "\n",
    "#net = ResFCNet(num_layers=30, layers=[2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2], bias=True)\n",
    "net.train(100, train_loader)\n",
    "\n",
    "net.test(test_loader, test_set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:340: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:343: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  compared_weight_signs = torch.tensor(torch.ne(new_weight_signs, old_weight_signs).detach(), dtype=torch.float32)\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:148: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_weights = new_weight_signs*torch.tensor(torch.ge(torch.abs(self.m_accumulated),self.rho*max_weight_elem*torch.ones_like(self.m_accumulated)).detach(),dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed:  10.146643088\n",
      "Seed 0   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.60604992\n",
      "Seed 1   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.699145381999998\n",
      "Seed 2   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.273395311000002\n",
      "Seed 3   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.354167834000002\n",
      "Seed 4   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.854717102000002\n",
      "Seed 5   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.360114429000006\n",
      "Seed 6   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.212407759000001\n",
      "Seed 7   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.896629595000007\n",
      "Seed 8   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.603672412999998\n",
      "Seed 9   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.097800613000004\n",
      "Seed 10   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.248547408000007\n",
      "Seed 11   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.193569678999978\n",
      "Seed 12   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.238679597000015\n",
      "Seed 13   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.202587755999986\n",
      "Seed 14   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.563680431999984\n",
      "Seed 15   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.800362409000002\n",
      "Seed 16   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.172943199000002\n",
      "Seed 17   ###   Best-Avg 0.8\n",
      "Time elapsed:  11.664164929999998\n",
      "Seed 18   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.64493668099999\n",
      "Seed 19   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.808216150000021\n",
      "Seed 20   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.073397882000023\n",
      "Seed 21   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.22652783800001\n",
      "Seed 22   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.820626478999998\n",
      "Seed 23   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.476716109999984\n",
      "Seed 24   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.439701024999977\n",
      "Seed 25   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.500485010999967\n",
      "Seed 26   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.450659146999953\n",
      "Seed 27   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.663561942000001\n",
      "Seed 28   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.897770284000046\n",
      "Seed 29   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.072501369000008\n",
      "Seed 30   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.462130296999987\n",
      "Seed 31   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.181674433000012\n",
      "Seed 32   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.720457097999997\n",
      "Seed 33   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.911982378000005\n",
      "Seed 34   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.12446210600001\n",
      "Seed 35   ###   Best-Avg 0.8\n",
      "Time elapsed:  11.751844816999949\n",
      "Seed 36   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.914494053999988\n",
      "Seed 37   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.721946830999968\n",
      "Seed 38   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.854742291999969\n",
      "Seed 39   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.348190395000017\n",
      "Seed 40   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.857938689000036\n",
      "Seed 41   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.154126195999993\n",
      "Seed 42   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.182050207999964\n",
      "Seed 43   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.263982440000007\n",
      "Seed 44   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.456949645000066\n",
      "Seed 45   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.78960939500007\n",
      "Seed 46   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.985499520000076\n",
      "Seed 47   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.699192676000052\n",
      "Seed 48   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.425626183999952\n",
      "Seed 49   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.54046722499993\n",
      "Seed 50   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.030507655000065\n",
      "Seed 51   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.125098506000086\n",
      "Seed 52   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.919794993999972\n",
      "Seed 53   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.273062202999995\n",
      "Seed 54   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.648992066000005\n",
      "Seed 55   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.420492824999997\n",
      "Seed 56   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.824312771999985\n",
      "Seed 57   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.768213887999991\n",
      "Seed 58   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.478139016\n",
      "Seed 59   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.264060061999999\n",
      "Seed 60   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.428517747\n",
      "Seed 61   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.804717488000051\n",
      "Seed 62   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.830327735999958\n",
      "Seed 63   ###   Best-Avg 0.84375\n",
      "Time elapsed:  13.149567551000018\n",
      "Seed 64   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.077774036999926\n",
      "Seed 65   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.304072090999966\n",
      "Seed 66   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.241903238999953\n",
      "Seed 67   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.745793354000057\n",
      "Seed 68   ###   Best-Avg 0.84375\n",
      "Time elapsed:  13.113821154999982\n",
      "Seed 69   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.56083924699999\n",
      "Seed 70   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.023275922000039\n",
      "Seed 71   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.34733552099999\n",
      "Seed 72   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.205158547999986\n",
      "Seed 73   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.781024666000008\n",
      "Seed 74   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.818582595000066\n",
      "Seed 75   ###   Best-Avg 0.8\n",
      "Time elapsed:  12.775833219999981\n",
      "Seed 76   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.596671489999949\n",
      "Seed 77   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.738887018000014\n",
      "Seed 78   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.079107496999995\n",
      "Seed 79   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.085618524999973\n",
      "Seed 80   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.447313674000043\n",
      "Seed 81   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.547355056000015\n",
      "Seed 82   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.684601208999993\n",
      "Seed 83   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.680478483000002\n",
      "Seed 84   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.97397130999991\n",
      "Seed 85   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.147157143999948\n",
      "Seed 86   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.294491636999965\n",
      "Seed 87   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.387910954999938\n",
      "Seed 88   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.786750217999952\n",
      "Seed 89   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.385938010000018\n",
      "Seed 90   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.857001564999791\n",
      "Seed 91   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.818907478000028\n",
      "Seed 92   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.13523568200003\n",
      "Seed 93   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.777255096999852\n",
      "Seed 94   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.35629602400013\n",
      "Seed 95   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.564049009999962\n",
      "Seed 96   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.523687422999956\n",
      "Seed 97   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.850773782000033\n",
      "Seed 98   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.6953454840002\n",
      "Seed 99   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.015339932000188\n",
      "Seed 100   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.39123638000001\n",
      "Seed 101   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.594417870000143\n",
      "Seed 102   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.783619105999833\n",
      "Seed 103   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.876350617000071\n",
      "Seed 104   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.439500544000111\n",
      "Seed 105   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.181604522000043\n",
      "Seed 106   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.076896543999965\n",
      "Seed 107   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.832593179000014\n",
      "Seed 108   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.307717772999922\n",
      "Seed 109   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.43920239099998\n",
      "Seed 110   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.498897093999858\n",
      "Seed 111   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.657795781000004\n",
      "Seed 112   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.814789377999887\n",
      "Seed 113   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.931495412999993\n",
      "Seed 114   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.288754776999895\n",
      "Seed 115   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.244204666000087\n",
      "Seed 116   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.550813107000067\n",
      "Seed 117   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.388158215999965\n",
      "Seed 118   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.750045622000016\n",
      "Seed 119   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.235819392000167\n",
      "Seed 120   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.996203699000034\n",
      "Seed 121   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.989480880999963\n",
      "Seed 122   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.056708553999897\n",
      "Seed 123   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.285523938999859\n",
      "Seed 124   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.525031678000005\n",
      "Seed 125   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.853770211999972\n",
      "Seed 126   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.874691244999894\n",
      "Seed 127   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.638378874000182\n",
      "Seed 128   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.042981213999838\n",
      "Seed 129   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.147106766999968\n",
      "Seed 130   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.252185378999911\n",
      "Seed 131   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.136590218000038\n",
      "Seed 132   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.631353791000038\n",
      "Seed 133   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.084455215999924\n",
      "Seed 134   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.278537409000137\n",
      "Seed 135   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.179119578000154\n",
      "Seed 136   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.569564312000011\n",
      "Seed 137   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.627897280999832\n",
      "Seed 138   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.784591183999964\n",
      "Seed 139   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.15707995899993\n",
      "Seed 140   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.181525357000055\n",
      "Seed 141   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.311887792000107\n",
      "Seed 142   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.489734566999914\n",
      "Seed 143   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.879751607999879\n",
      "Seed 144   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.325272771000073\n",
      "Seed 145   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.095041162999905\n",
      "Seed 146   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.463122424999938\n",
      "Seed 147   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.492982883999957\n",
      "Seed 148   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.48772615300004\n",
      "Seed 149   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.673118235000175\n",
      "Seed 150   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.028434469000103\n",
      "Seed 151   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.975339212000108\n",
      "Seed 152   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.189310215000205\n",
      "Seed 153   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.902211723999926\n",
      "Seed 154   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.252953891999823\n",
      "Seed 155   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.101709491000065\n",
      "Seed 156   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.322991391999949\n",
      "Seed 157   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.231441695000058\n",
      "Seed 158   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.062093285999936\n",
      "Seed 159   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.669411380999918\n",
      "Seed 160   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.712783790999993\n",
      "Seed 161   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.289970005000214\n",
      "Seed 162   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.054718644999866\n",
      "Seed 163   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.402621168999985\n",
      "Seed 164   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.74273369599996\n",
      "Seed 165   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.994736071000034\n",
      "Seed 166   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.28945646399984\n",
      "Seed 167   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.997000486000161\n",
      "Seed 168   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.741073809999989\n",
      "Seed 169   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.930781389999993\n",
      "Seed 170   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.058178237999982\n",
      "Seed 171   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.299073929000087\n",
      "Seed 172   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.528790453000056\n",
      "Seed 173   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.644680681000182\n",
      "Seed 174   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.82024248000016\n",
      "Seed 175   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.990435996000087\n",
      "Seed 176   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.8661229679999\n",
      "Seed 177   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.123140467999974\n",
      "Seed 178   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.590671946999919\n",
      "Seed 179   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.689152142000012\n",
      "Seed 180   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.67604732399991\n",
      "Seed 181   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.032750995000015\n",
      "Seed 182   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.450169252000023\n",
      "Seed 183   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.201859338999839\n",
      "Seed 184   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.017197729000145\n",
      "Seed 185   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.891306548999637\n",
      "Seed 186   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.39308081199988\n",
      "Seed 187   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.244145035999736\n",
      "Seed 188   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.604870163000214\n",
      "Seed 189   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.960879856999782\n",
      "Seed 190   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.77327322300016\n",
      "Seed 191   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.03098315699981\n",
      "Seed 192   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.10578955100027\n",
      "Seed 193   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.368450354000288\n",
      "Seed 194   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.467802901000141\n",
      "Seed 195   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.05645203999984\n",
      "Seed 196   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.349908370000321\n",
      "Seed 197   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.354014645000007\n",
      "Seed 198   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.494870496999738\n",
      "Seed 199   ###   Best-Avg 0.825\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import ConvNet, FCMSANet\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset()\n",
    "\n",
    "for seed in range(200):\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    net = FCMSANet(num_fc=2,sizes_fc=[2,4,2], bias=False, test=False)\n",
    "    \n",
    "    net.train_msa(60,train_loader)\n",
    "    print('Seed '+str(seed)+'   ###   Best-Avg '+str(net.best_avg))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  ###   Avg-Loss 0.016693341732025146   ###   Correct predictions 0.7354166666666667\n",
      "Epoch 2  ###   Avg-Loss 0.01539247731367747   ###   Correct predictions 0.7895833333333333\n",
      "Epoch 3  ###   Avg-Loss 0.013269580403963725   ###   Correct predictions 0.7958333333333333\n",
      "Epoch 4  ###   Avg-Loss 0.010812340180079143   ###   Correct predictions 0.8125\n",
      "Epoch 5  ###   Avg-Loss 0.009092676639556884   ###   Correct predictions 0.8375\n",
      "Epoch 6  ###   Avg-Loss 0.007869457205136618   ###   Correct predictions 0.8583333333333333\n",
      "Epoch 7  ###   Avg-Loss 0.007090519865353902   ###   Correct predictions 0.8625\n",
      "Epoch 8  ###   Avg-Loss 0.00648987740278244   ###   Correct predictions 0.8916666666666667\n",
      "Epoch 9  ###   Avg-Loss 0.006088708837827046   ###   Correct predictions 0.8979166666666667\n",
      "Epoch 10  ###   Avg-Loss 0.005733463664849599   ###   Correct predictions 0.9\n",
      "Epoch 11  ###   Avg-Loss 0.005439147353172302   ###   Correct predictions 0.9041666666666667\n",
      "Epoch 12  ###   Avg-Loss 0.005182546377182007   ###   Correct predictions 0.9125\n",
      "Epoch 13  ###   Avg-Loss 0.0049934898813565574   ###   Correct predictions 0.9104166666666667\n",
      "Epoch 14  ###   Avg-Loss 0.004753189285596212   ###   Correct predictions 0.91875\n",
      "Epoch 15  ###   Avg-Loss 0.004416432480017344   ###   Correct predictions 0.9229166666666667\n",
      "Epoch 16  ###   Avg-Loss 0.004125663638114929   ###   Correct predictions 0.93125\n",
      "Epoch 17  ###   Avg-Loss 0.003898448000351588   ###   Correct predictions 0.93125\n",
      "Epoch 18  ###   Avg-Loss 0.003615226596593857   ###   Correct predictions 0.9375\n",
      "Epoch 19  ###   Avg-Loss 0.0030299144486586253   ###   Correct predictions 0.95\n",
      "Epoch 20  ###   Avg-Loss 0.0029660664498806   ###   Correct predictions 0.95625\n",
      "Epoch 21  ###   Avg-Loss 0.00286577045917511   ###   Correct predictions 0.9479166666666666\n",
      "Epoch 22  ###   Avg-Loss 0.002026676634947459   ###   Correct predictions 0.9770833333333333\n",
      "Epoch 23  ###   Avg-Loss 0.00176669346789519   ###   Correct predictions 0.9791666666666666\n",
      "Epoch 24  ###   Avg-Loss 0.0016901899129152299   ###   Correct predictions 0.98125\n",
      "Epoch 25  ###   Avg-Loss 0.001406506821513176   ###   Correct predictions 0.9854166666666667\n",
      "Epoch 26  ###   Avg-Loss 0.0012301721920569737   ###   Correct predictions 0.9895833333333334\n",
      "Epoch 27  ###   Avg-Loss 0.0012656405568122863   ###   Correct predictions 0.9895833333333334\n",
      "Epoch 28  ###   Avg-Loss 0.0009948708117008208   ###   Correct predictions 0.9916666666666667\n",
      "Epoch 29  ###   Avg-Loss 0.0008995652198791504   ###   Correct predictions 0.99375\n",
      "Epoch 30  ###   Avg-Loss 0.0008839219808578491   ###   Correct predictions 0.99375\n",
      "Epoch 31  ###   Avg-Loss 0.0007204620788494746   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 32  ###   Avg-Loss 0.0006353583186864853   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 33  ###   Avg-Loss 0.0006963463500142097   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 34  ###   Avg-Loss 0.0006097466374437014   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 35  ###   Avg-Loss 0.0006425640856226285   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 36  ###   Avg-Loss 0.0004952810704708099   ###   Correct predictions 1.0\n",
      "Epoch 37  ###   Avg-Loss 0.00046397863576809565   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 38  ###   Avg-Loss 0.00047715206940968834   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 39  ###   Avg-Loss 0.0004436716747780641   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 40  ###   Avg-Loss 0.00044246241450309756   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 41  ###   Avg-Loss 0.0003799566999077797   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 42  ###   Avg-Loss 0.00037729634592930473   ###   Correct predictions 1.0\n",
      "Epoch 43  ###   Avg-Loss 0.00042406826590498287   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 44  ###   Avg-Loss 0.0003210838573674361   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 45  ###   Avg-Loss 0.00035742980738480884   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 46  ###   Avg-Loss 0.0003395354375243187   ###   Correct predictions 1.0\n",
      "Epoch 47  ###   Avg-Loss 0.0003530730493366718   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 48  ###   Avg-Loss 0.0003453268048663934   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 49  ###   Avg-Loss 0.0003007656273742517   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 50  ###   Avg-Loss 0.0002979370454947154   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 51  ###   Avg-Loss 0.0002575600054115057   ###   Correct predictions 1.0\n",
      "Epoch 52  ###   Avg-Loss 0.0003250787034630775   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 53  ###   Avg-Loss 0.00023143263533711433   ###   Correct predictions 1.0\n",
      "Epoch 54  ###   Avg-Loss 0.0002476739386717478   ###   Correct predictions 1.0\n",
      "Epoch 55  ###   Avg-Loss 0.0002808337099850178   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 56  ###   Avg-Loss 0.00022636189435919126   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 57  ###   Avg-Loss 0.00020725494250655173   ###   Correct predictions 1.0\n",
      "Epoch 58  ###   Avg-Loss 0.0002392620158692201   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 59  ###   Avg-Loss 0.0002071976816902558   ###   Correct predictions 1.0\n",
      "Epoch 60  ###   Avg-Loss 0.00020230164130528768   ###   Correct predictions 1.0\n",
      "Epoch 61  ###   Avg-Loss 0.00018292929356296858   ###   Correct predictions 1.0\n",
      "Epoch 62  ###   Avg-Loss 0.00022179240671296914   ###   Correct predictions 1.0\n",
      "Epoch 63  ###   Avg-Loss 0.00019362818760176498   ###   Correct predictions 1.0\n",
      "Epoch 64  ###   Avg-Loss 0.00022219737681249778   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 65  ###   Avg-Loss 0.00018464084714651108   ###   Correct predictions 1.0\n",
      "Epoch 66  ###   Avg-Loss 0.00021055651207764944   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 67  ###   Avg-Loss 0.00020628821415205796   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 68  ###   Avg-Loss 0.00019506152408818405   ###   Correct predictions 1.0\n",
      "Epoch 69  ###   Avg-Loss 0.0001632713247090578   ###   Correct predictions 1.0\n",
      "Epoch 70  ###   Avg-Loss 0.00017592293831209343   ###   Correct predictions 1.0\n",
      "Epoch 71  ###   Avg-Loss 0.00016277733569343886   ###   Correct predictions 1.0\n",
      "Epoch 72  ###   Avg-Loss 0.0001429748721420765   ###   Correct predictions 1.0\n",
      "Epoch 73  ###   Avg-Loss 0.0001682426780462265   ###   Correct predictions 1.0\n",
      "Epoch 74  ###   Avg-Loss 0.00015255645848810673   ###   Correct predictions 1.0\n",
      "Epoch 75  ###   Avg-Loss 0.00013346169143915176   ###   Correct predictions 1.0\n",
      "Epoch 76  ###   Avg-Loss 0.00015274204003314177   ###   Correct predictions 1.0\n",
      "Epoch 77  ###   Avg-Loss 0.0001632209246357282   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 78  ###   Avg-Loss 0.00016444246284663677   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 79  ###   Avg-Loss 0.0001559902292986711   ###   Correct predictions 1.0\n",
      "Epoch 80  ###   Avg-Loss 0.00012979219512393076   ###   Correct predictions 1.0\n",
      "Epoch 81  ###   Avg-Loss 0.00014563739920655887   ###   Correct predictions 1.0\n",
      "Epoch 82  ###   Avg-Loss 0.00012268397646645704   ###   Correct predictions 1.0\n",
      "Epoch 83  ###   Avg-Loss 0.00014513544738292694   ###   Correct predictions 1.0\n",
      "Epoch 84  ###   Avg-Loss 0.00012508279954393705   ###   Correct predictions 1.0\n",
      "Epoch 85  ###   Avg-Loss 0.00013901602166394394   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 86  ###   Avg-Loss 0.0001333181746304035   ###   Correct predictions 1.0\n",
      "Epoch 87  ###   Avg-Loss 0.00012044358688096205   ###   Correct predictions 1.0\n",
      "Epoch 88  ###   Avg-Loss 0.00014618256439765295   ###   Correct predictions 1.0\n",
      "Epoch 89  ###   Avg-Loss 0.00012452843754241864   ###   Correct predictions 1.0\n",
      "Epoch 90  ###   Avg-Loss 0.00011009617398182551   ###   Correct predictions 1.0\n",
      "Epoch 91  ###   Avg-Loss 0.00012323612657686074   ###   Correct predictions 1.0\n",
      "Epoch 92  ###   Avg-Loss 0.00011581191793084145   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 93  ###   Avg-Loss 0.0001325206986318032   ###   Correct predictions 1.0\n",
      "Epoch 94  ###   Avg-Loss 0.00010437506716698408   ###   Correct predictions 1.0\n",
      "Epoch 95  ###   Avg-Loss 0.00010432829149067401   ###   Correct predictions 1.0\n",
      "Epoch 96  ###   Avg-Loss 0.00011434933015455803   ###   Correct predictions 1.0\n",
      "Epoch 97  ###   Avg-Loss 0.00010554267403980096   ###   Correct predictions 1.0\n",
      "Epoch 98  ###   Avg-Loss 0.00013531527171532312   ###   Correct predictions 1.0\n",
      "Epoch 99  ###   Avg-Loss 9.843817291160425e-05   ###   Correct predictions 1.0\n",
      "Epoch 100  ###   Avg-Loss 9.126464525858562e-05   ###   Correct predictions 1.0\n",
      "Epoch 101  ###   Avg-Loss 8.715117195000251e-05   ###   Correct predictions 1.0\n",
      "Epoch 102  ###   Avg-Loss 9.70254031320413e-05   ###   Correct predictions 1.0\n",
      "Epoch 103  ###   Avg-Loss 0.00010043517686426639   ###   Correct predictions 1.0\n",
      "Epoch 104  ###   Avg-Loss 8.699353784322739e-05   ###   Correct predictions 1.0\n",
      "Epoch 105  ###   Avg-Loss 9.705725436409315e-05   ###   Correct predictions 1.0\n",
      "Epoch 106  ###   Avg-Loss 8.577681922664246e-05   ###   Correct predictions 1.0\n",
      "Epoch 107  ###   Avg-Loss 0.00010882464703172445   ###   Correct predictions 1.0\n",
      "Epoch 108  ###   Avg-Loss 0.00010686044115573167   ###   Correct predictions 1.0\n",
      "Epoch 109  ###   Avg-Loss 0.00010626811999827623   ###   Correct predictions 1.0\n",
      "Epoch 110  ###   Avg-Loss 0.00010530042927712203   ###   Correct predictions 1.0\n",
      "Epoch 111  ###   Avg-Loss 0.00010232297548403342   ###   Correct predictions 1.0\n",
      "Epoch 112  ###   Avg-Loss 8.531966401884954e-05   ###   Correct predictions 1.0\n",
      "Epoch 113  ###   Avg-Loss 8.699455453703801e-05   ###   Correct predictions 1.0\n",
      "Epoch 114  ###   Avg-Loss 8.856581213573615e-05   ###   Correct predictions 1.0\n",
      "Epoch 115  ###   Avg-Loss 7.67768050233523e-05   ###   Correct predictions 1.0\n",
      "Epoch 116  ###   Avg-Loss 8.174949325621129e-05   ###   Correct predictions 1.0\n",
      "Epoch 117  ###   Avg-Loss 7.995864531646172e-05   ###   Correct predictions 1.0\n",
      "Epoch 118  ###   Avg-Loss 7.204515859484672e-05   ###   Correct predictions 1.0\n",
      "Epoch 119  ###   Avg-Loss 8.846645553906758e-05   ###   Correct predictions 1.0\n",
      "Epoch 120  ###   Avg-Loss 8.003233621517817e-05   ###   Correct predictions 1.0\n",
      "Epoch 121  ###   Avg-Loss 7.330861408263444e-05   ###   Correct predictions 1.0\n",
      "Epoch 122  ###   Avg-Loss 6.652111187577248e-05   ###   Correct predictions 1.0\n",
      "Epoch 123  ###   Avg-Loss 7.577384822070599e-05   ###   Correct predictions 1.0\n",
      "Epoch 124  ###   Avg-Loss 6.504503932471076e-05   ###   Correct predictions 1.0\n",
      "Epoch 125  ###   Avg-Loss 9.361816725383202e-05   ###   Correct predictions 1.0\n",
      "Epoch 126  ###   Avg-Loss 6.556252483278513e-05   ###   Correct predictions 1.0\n",
      "Epoch 127  ###   Avg-Loss 6.442156542713443e-05   ###   Correct predictions 1.0\n",
      "Epoch 128  ###   Avg-Loss 7.758416080226501e-05   ###   Correct predictions 1.0\n",
      "Epoch 129  ###   Avg-Loss 7.533366636683544e-05   ###   Correct predictions 1.0\n",
      "Epoch 130  ###   Avg-Loss 6.837865803390741e-05   ###   Correct predictions 1.0\n",
      "Epoch 131  ###   Avg-Loss 7.118641709287961e-05   ###   Correct predictions 1.0\n",
      "Epoch 132  ###   Avg-Loss 8.623798688252766e-05   ###   Correct predictions 1.0\n",
      "Epoch 133  ###   Avg-Loss 6.864879590769608e-05   ###   Correct predictions 1.0\n",
      "Epoch 134  ###   Avg-Loss 7.340631758173307e-05   ###   Correct predictions 1.0\n",
      "Epoch 135  ###   Avg-Loss 5.6213835099091135e-05   ###   Correct predictions 1.0\n",
      "Epoch 136  ###   Avg-Loss 5.9551175218075515e-05   ###   Correct predictions 1.0\n",
      "Epoch 137  ###   Avg-Loss 7.160236903776725e-05   ###   Correct predictions 1.0\n",
      "Epoch 138  ###   Avg-Loss 6.109048845246435e-05   ###   Correct predictions 1.0\n",
      "Epoch 139  ###   Avg-Loss 7.094782777130603e-05   ###   Correct predictions 1.0\n",
      "Epoch 140  ###   Avg-Loss 7.791101622084776e-05   ###   Correct predictions 1.0\n",
      "Epoch 141  ###   Avg-Loss 6.113395793363451e-05   ###   Correct predictions 1.0\n",
      "Epoch 142  ###   Avg-Loss 6.176692356045048e-05   ###   Correct predictions 1.0\n",
      "Epoch 143  ###   Avg-Loss 7.38037284463644e-05   ###   Correct predictions 1.0\n",
      "Epoch 144  ###   Avg-Loss 6.984003509084384e-05   ###   Correct predictions 1.0\n",
      "Epoch 145  ###   Avg-Loss 6.579244509339333e-05   ###   Correct predictions 1.0\n",
      "Epoch 146  ###   Avg-Loss 5.51832839846611e-05   ###   Correct predictions 1.0\n",
      "Epoch 147  ###   Avg-Loss 5.711079575121403e-05   ###   Correct predictions 1.0\n",
      "Epoch 148  ###   Avg-Loss 5.0937927638490996e-05   ###   Correct predictions 1.0\n",
      "Epoch 149  ###   Avg-Loss 5.608038045465946e-05   ###   Correct predictions 1.0\n",
      "Epoch 150  ###   Avg-Loss 6.162119098007679e-05   ###   Correct predictions 1.0\n",
      "Epoch 151  ###   Avg-Loss 5.776884887988369e-05   ###   Correct predictions 1.0\n",
      "Epoch 152  ###   Avg-Loss 6.204020076741776e-05   ###   Correct predictions 1.0\n",
      "Epoch 153  ###   Avg-Loss 5.5288333290567e-05   ###   Correct predictions 1.0\n",
      "Epoch 154  ###   Avg-Loss 4.973360725368063e-05   ###   Correct predictions 1.0\n",
      "Epoch 155  ###   Avg-Loss 5.3552817553281785e-05   ###   Correct predictions 1.0\n",
      "Epoch 156  ###   Avg-Loss 4.901509576787551e-05   ###   Correct predictions 1.0\n",
      "Epoch 157  ###   Avg-Loss 5.7395625238617264e-05   ###   Correct predictions 1.0\n",
      "Epoch 158  ###   Avg-Loss 4.948605395232638e-05   ###   Correct predictions 1.0\n",
      "Epoch 159  ###   Avg-Loss 4.963681955511371e-05   ###   Correct predictions 1.0\n",
      "Epoch 160  ###   Avg-Loss 5.433883440370361e-05   ###   Correct predictions 1.0\n",
      "Epoch 161  ###   Avg-Loss 5.3806684445589784e-05   ###   Correct predictions 1.0\n",
      "Epoch 162  ###   Avg-Loss 4.746093569944302e-05   ###   Correct predictions 1.0\n",
      "Epoch 163  ###   Avg-Loss 7.585322794814905e-05   ###   Correct predictions 1.0\n",
      "Epoch 164  ###   Avg-Loss 4.612901248037815e-05   ###   Correct predictions 1.0\n",
      "Epoch 165  ###   Avg-Loss 4.46078289921085e-05   ###   Correct predictions 1.0\n",
      "Epoch 166  ###   Avg-Loss 4.7858059406280515e-05   ###   Correct predictions 1.0\n",
      "Epoch 167  ###   Avg-Loss 5.252285239597161e-05   ###   Correct predictions 1.0\n",
      "Epoch 168  ###   Avg-Loss 6.123439331228535e-05   ###   Correct predictions 1.0\n",
      "Epoch 169  ###   Avg-Loss 5.4478478462745744e-05   ###   Correct predictions 1.0\n",
      "Epoch 170  ###   Avg-Loss 5.318818924327691e-05   ###   Correct predictions 1.0\n",
      "Epoch 171  ###   Avg-Loss 4.756053676828742e-05   ###   Correct predictions 1.0\n",
      "Epoch 172  ###   Avg-Loss 4.761051774645845e-05   ###   Correct predictions 1.0\n",
      "Epoch 173  ###   Avg-Loss 5.6479397850732006e-05   ###   Correct predictions 1.0\n",
      "Epoch 174  ###   Avg-Loss 4.788977870096763e-05   ###   Correct predictions 1.0\n",
      "Epoch 175  ###   Avg-Loss 6.446146095792452e-05   ###   Correct predictions 1.0\n",
      "Epoch 176  ###   Avg-Loss 4.3726297250638405e-05   ###   Correct predictions 1.0\n",
      "Epoch 177  ###   Avg-Loss 5.430232267826796e-05   ###   Correct predictions 1.0\n",
      "Epoch 178  ###   Avg-Loss 5.736845002199213e-05   ###   Correct predictions 1.0\n",
      "Epoch 179  ###   Avg-Loss 5.051289529850086e-05   ###   Correct predictions 1.0\n",
      "Epoch 180  ###   Avg-Loss 5.14243069725732e-05   ###   Correct predictions 1.0\n",
      "Epoch 181  ###   Avg-Loss 3.913740317026774e-05   ###   Correct predictions 1.0\n",
      "Epoch 182  ###   Avg-Loss 5.9283819670478506e-05   ###   Correct predictions 1.0\n",
      "Epoch 183  ###   Avg-Loss 4.4025426420072714e-05   ###   Correct predictions 1.0\n",
      "Epoch 184  ###   Avg-Loss 4.923796902100245e-05   ###   Correct predictions 1.0\n",
      "Epoch 185  ###   Avg-Loss 4.7920217427114645e-05   ###   Correct predictions 1.0\n",
      "Epoch 186  ###   Avg-Loss 4.310249350965023e-05   ###   Correct predictions 1.0\n",
      "Epoch 187  ###   Avg-Loss 4.640198312699795e-05   ###   Correct predictions 1.0\n",
      "Epoch 188  ###   Avg-Loss 4.455156158655882e-05   ###   Correct predictions 1.0\n",
      "Epoch 189  ###   Avg-Loss 4.795944939057032e-05   ###   Correct predictions 1.0\n",
      "Epoch 190  ###   Avg-Loss 3.5239538798729576e-05   ###   Correct predictions 1.0\n",
      "Epoch 191  ###   Avg-Loss 4.360287372643749e-05   ###   Correct predictions 1.0\n",
      "Epoch 192  ###   Avg-Loss 4.576954136913021e-05   ###   Correct predictions 1.0\n",
      "Epoch 193  ###   Avg-Loss 3.716207187001904e-05   ###   Correct predictions 1.0\n",
      "Epoch 194  ###   Avg-Loss 4.478615010157227e-05   ###   Correct predictions 1.0\n",
      "Epoch 195  ###   Avg-Loss 4.2566253493229546e-05   ###   Correct predictions 1.0\n",
      "Epoch 196  ###   Avg-Loss 4.4678539658586185e-05   ###   Correct predictions 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197  ###   Avg-Loss 3.961453524728616e-05   ###   Correct predictions 1.0\n",
      "Epoch 198  ###   Avg-Loss 4.014573448027174e-05   ###   Correct predictions 1.0\n",
      "Epoch 199  ###   Avg-Loss 4.041891467447082e-05   ###   Correct predictions 1.0\n",
      "Epoch 200  ###   Avg-Loss 3.8891894898066916e-05   ###   Correct predictions 1.0\n",
      "Epoch 201  ###   Avg-Loss 3.913562589635452e-05   ###   Correct predictions 1.0\n",
      "Epoch 202  ###   Avg-Loss 4.7289044596254824e-05   ###   Correct predictions 1.0\n",
      "Epoch 203  ###   Avg-Loss 3.683992739145954e-05   ###   Correct predictions 1.0\n",
      "Epoch 204  ###   Avg-Loss 3.6930983575681846e-05   ###   Correct predictions 1.0\n",
      "Epoch 205  ###   Avg-Loss 3.837041634445389e-05   ###   Correct predictions 1.0\n",
      "Epoch 206  ###   Avg-Loss 3.6403711419552565e-05   ###   Correct predictions 1.0\n",
      "Epoch 207  ###   Avg-Loss 3.761501672367255e-05   ###   Correct predictions 1.0\n",
      "Epoch 208  ###   Avg-Loss 3.68371062601606e-05   ###   Correct predictions 1.0\n",
      "Epoch 209  ###   Avg-Loss 4.079292993992567e-05   ###   Correct predictions 1.0\n",
      "Epoch 210  ###   Avg-Loss 3.946068463847041e-05   ###   Correct predictions 1.0\n",
      "Epoch 211  ###   Avg-Loss 4.385570452238123e-05   ###   Correct predictions 1.0\n",
      "Epoch 212  ###   Avg-Loss 4.163978931804498e-05   ###   Correct predictions 1.0\n",
      "Epoch 213  ###   Avg-Loss 3.332675745089849e-05   ###   Correct predictions 1.0\n",
      "Epoch 214  ###   Avg-Loss 4.257323065151771e-05   ###   Correct predictions 1.0\n",
      "Epoch 215  ###   Avg-Loss 4.2936753015965226e-05   ###   Correct predictions 1.0\n",
      "Epoch 216  ###   Avg-Loss 3.636674955487251e-05   ###   Correct predictions 1.0\n",
      "Epoch 217  ###   Avg-Loss 3.94138313519458e-05   ###   Correct predictions 1.0\n",
      "Epoch 218  ###   Avg-Loss 3.3846831259628135e-05   ###   Correct predictions 1.0\n",
      "Epoch 219  ###   Avg-Loss 3.627579426392913e-05   ###   Correct predictions 1.0\n",
      "Epoch 220  ###   Avg-Loss 4.044672629485528e-05   ###   Correct predictions 1.0\n",
      "Epoch 221  ###   Avg-Loss 3.899487977226575e-05   ###   Correct predictions 1.0\n",
      "Epoch 222  ###   Avg-Loss 3.2388655624041956e-05   ###   Correct predictions 1.0\n",
      "Epoch 223  ###   Avg-Loss 3.148379425207774e-05   ###   Correct predictions 1.0\n",
      "Epoch 224  ###   Avg-Loss 4.479126849522193e-05   ###   Correct predictions 1.0\n",
      "Epoch 225  ###   Avg-Loss 3.618796666463216e-05   ###   Correct predictions 1.0\n",
      "Epoch 226  ###   Avg-Loss 3.777043893933296e-05   ###   Correct predictions 1.0\n",
      "Epoch 227  ###   Avg-Loss 3.232196516667803e-05   ###   Correct predictions 1.0\n",
      "Epoch 228  ###   Avg-Loss 3.010243138608833e-05   ###   Correct predictions 1.0\n",
      "Epoch 229  ###   Avg-Loss 3.525512292981148e-05   ###   Correct predictions 1.0\n",
      "Epoch 230  ###   Avg-Loss 3.3884646836668256e-05   ###   Correct predictions 1.0\n",
      "Epoch 231  ###   Avg-Loss 3.234480197230975e-05   ###   Correct predictions 1.0\n",
      "Epoch 232  ###   Avg-Loss 3.662161761894822e-05   ###   Correct predictions 1.0\n",
      "Epoch 233  ###   Avg-Loss 3.697317248831193e-05   ###   Correct predictions 1.0\n",
      "Epoch 234  ###   Avg-Loss 2.878650751275321e-05   ###   Correct predictions 1.0\n",
      "Epoch 235  ###   Avg-Loss 3.556391845146815e-05   ###   Correct predictions 1.0\n",
      "Epoch 236  ###   Avg-Loss 2.7765481111903984e-05   ###   Correct predictions 1.0\n",
      "Epoch 237  ###   Avg-Loss 3.6760074241707726e-05   ###   Correct predictions 1.0\n",
      "Epoch 238  ###   Avg-Loss 3.500080201774835e-05   ###   Correct predictions 1.0\n",
      "Epoch 239  ###   Avg-Loss 3.9084527331093946e-05   ###   Correct predictions 1.0\n",
      "Epoch 240  ###   Avg-Loss 3.469903022050858e-05   ###   Correct predictions 1.0\n",
      "Epoch 241  ###   Avg-Loss 3.173545701429248e-05   ###   Correct predictions 1.0\n",
      "Epoch 242  ###   Avg-Loss 3.360264624158541e-05   ###   Correct predictions 1.0\n",
      "Epoch 243  ###   Avg-Loss 3.289517092828949e-05   ###   Correct predictions 1.0\n",
      "Epoch 244  ###   Avg-Loss 2.8689632502694926e-05   ###   Correct predictions 1.0\n",
      "Epoch 245  ###   Avg-Loss 3.048427946244677e-05   ###   Correct predictions 1.0\n",
      "Epoch 246  ###   Avg-Loss 3.113332592571775e-05   ###   Correct predictions 1.0\n",
      "Epoch 247  ###   Avg-Loss 2.8724937389294308e-05   ###   Correct predictions 1.0\n",
      "Epoch 248  ###   Avg-Loss 3.149114587965111e-05   ###   Correct predictions 1.0\n",
      "Epoch 249  ###   Avg-Loss 3.239394476016362e-05   ###   Correct predictions 1.0\n",
      "Epoch 250  ###   Avg-Loss 2.6755715953186155e-05   ###   Correct predictions 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('lin1',nn.Linear(2, 16, bias=True)),\n",
    "    ('relu1',nn.ReLU()),\n",
    "    ('lin2',nn.Linear(16, 32, bias=True)),\n",
    "    ('relu2',nn.ReLU()),\n",
    "    ('lin3',nn.Linear(32, 64, bias=True)),\n",
    "    ('relu3',nn.ReLU()),\n",
    "    ('lin4',nn.Linear(64, 2, bias=True))\n",
    "])\n",
    ")\n",
    "\n",
    "output_frequency = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(250):\n",
    "    loss_sum = 0\n",
    "    correct_pred = 0\n",
    "    for index, (data, target) in enumerate(train_loader):\n",
    "        #print(index)\n",
    "        output = model.forward(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss_sum = loss_sum + loss.data\n",
    "        for i in range(len(output)):\n",
    "            _, ind = torch.max(output[i],0)\n",
    "            label = target[i]\n",
    "            \n",
    "            if ind.data == label.data:\n",
    "                correct_pred +=1\n",
    "        #if index % (10*(output_frequency)) == 0:\n",
    "        #    print(\"#  Epoch  #  Batch  #  Avg-Loss ###############\")\n",
    "        #if index % (output_frequency) == 0 and index > 0:\n",
    "        #    print(\"#  %d  #  %d  #  %f  #\" % (epoch+1, index, loss_sum/output_frequency))\n",
    "        #    loss_sum = 0\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch '+str(epoch+1)+'  ###   Avg-Loss '+str(loss_sum.item()/480)+'   ###   Correct predictions '+str(correct_pred/480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  ###   Avg-Loss 0.019352535406748455   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 2  ###   Avg-Loss 0.017910422881444295   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 3  ###   Avg-Loss 0.01750417153040568   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 4  ###   Avg-Loss 0.017391308148701986   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 5  ###   Avg-Loss 0.017361233631769817   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 6  ###   Avg-Loss 0.017345829804738363   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 7  ###   Avg-Loss 0.017331127325693765   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 8  ###   Avg-Loss 0.01733660101890564   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 9  ###   Avg-Loss 0.01734957695007324   ###   Correct predictions 0.4895833333333333\n",
      "Epoch 10  ###   Avg-Loss 0.01734344959259033   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 11  ###   Avg-Loss 0.017335693041483562   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 12  ###   Avg-Loss 0.01733403404553731   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 13  ###   Avg-Loss 0.017345335086186728   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 14  ###   Avg-Loss 0.017336952686309814   ###   Correct predictions 0.50625\n",
      "Epoch 15  ###   Avg-Loss 0.017330414056777953   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 16  ###   Avg-Loss 0.01733735203742981   ###   Correct predictions 0.49375\n",
      "Epoch 17  ###   Avg-Loss 0.017340999841690064   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 18  ###   Avg-Loss 0.01733502944310506   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 19  ###   Avg-Loss 0.017336159944534302   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 20  ###   Avg-Loss 0.01733076572418213   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 21  ###   Avg-Loss 0.017335595687230428   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 22  ###   Avg-Loss 0.017331228653589884   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 23  ###   Avg-Loss 0.017345523834228514   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 24  ###   Avg-Loss 0.017328786849975585   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 25  ###   Avg-Loss 0.017338613669077556   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 26  ###   Avg-Loss 0.017339388529459637   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 27  ###   Avg-Loss 0.017334789037704468   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 28  ###   Avg-Loss 0.017332659165064494   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 29  ###   Avg-Loss 0.017332019408543904   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 30  ###   Avg-Loss 0.017343803246816   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 31  ###   Avg-Loss 0.017337658007939658   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 32  ###   Avg-Loss 0.017327052354812623   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 33  ###   Avg-Loss 0.017350995540618898   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 34  ###   Avg-Loss 0.017341856161753336   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 35  ###   Avg-Loss 0.01733092466990153   ###   Correct predictions 0.49375\n",
      "Epoch 36  ###   Avg-Loss 0.017343801259994508   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 37  ###   Avg-Loss 0.017331286271413168   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 38  ###   Avg-Loss 0.0173449436823527   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 39  ###   Avg-Loss 0.017336622873942057   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 40  ###   Avg-Loss 0.01733430624008179   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 41  ###   Avg-Loss 0.01733665664990743   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 42  ###   Avg-Loss 0.01732741594314575   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 43  ###   Avg-Loss 0.017335259914398195   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 44  ###   Avg-Loss 0.017344200611114503   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 45  ###   Avg-Loss 0.017335255940755207   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 46  ###   Avg-Loss 0.01732981006304423   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 47  ###   Avg-Loss 0.017340336243311563   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 48  ###   Avg-Loss 0.017337270577748618   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 49  ###   Avg-Loss 0.01734388470649719   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 50  ###   Avg-Loss 0.017336700359980264   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 51  ###   Avg-Loss 0.017336754004160564   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 52  ###   Avg-Loss 0.017329061031341554   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 53  ###   Avg-Loss 0.017343024412790935   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 54  ###   Avg-Loss 0.017335404952367146   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 55  ###   Avg-Loss 0.017333406209945678   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 56  ###   Avg-Loss 0.01734092434247335   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 57  ###   Avg-Loss 0.017334075768788655   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 58  ###   Avg-Loss 0.017333996295928956   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 59  ###   Avg-Loss 0.01734001040458679   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 60  ###   Avg-Loss 0.017343183358510334   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 61  ###   Avg-Loss 0.017334266503651937   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 62  ###   Avg-Loss 0.017326943079630532   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 63  ###   Avg-Loss 0.017331435283025106   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 64  ###   Avg-Loss 0.01734351714452108   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 65  ###   Avg-Loss 0.01733464002609253   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 66  ###   Avg-Loss 0.017335947354634604   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 67  ###   Avg-Loss 0.017339940865834555   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 68  ###   Avg-Loss 0.017335885763168336   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 69  ###   Avg-Loss 0.017330139875411987   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 70  ###   Avg-Loss 0.017336839437484743   ###   Correct predictions 0.48125\n",
      "Epoch 71  ###   Avg-Loss 0.01733196576436361   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 72  ###   Avg-Loss 0.01733160416285197   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 73  ###   Avg-Loss 0.017341158787409463   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 74  ###   Avg-Loss 0.01734090844790141   ###   Correct predictions 0.4895833333333333\n",
      "Epoch 75  ###   Avg-Loss 0.01733473539352417   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 76  ###   Avg-Loss 0.01733032464981079   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 77  ###   Avg-Loss 0.01734740932782491   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 78  ###   Avg-Loss 0.017335404952367146   ###   Correct predictions 0.49375\n",
      "Epoch 79  ###   Avg-Loss 0.017347671588261924   ###   Correct predictions 0.4895833333333333\n",
      "Epoch 80  ###   Avg-Loss 0.017328637838363647   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 81  ###   Avg-Loss 0.017335180441538492   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 82  ###   Avg-Loss 0.017335520188013712   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 83  ###   Avg-Loss 0.01733883221944173   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 84  ###   Avg-Loss 0.017346135775248208   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 85  ###   Avg-Loss 0.0173315425713857   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 86  ###   Avg-Loss 0.017339418331782024   ###   Correct predictions 0.48125\n",
      "Epoch 87  ###   Avg-Loss 0.01734487811724345   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 88  ###   Avg-Loss 0.01733728249867757   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 89  ###   Avg-Loss 0.01733392079671224   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 90  ###   Avg-Loss 0.017338260014851888   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 91  ###   Avg-Loss 0.017348573605219523   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 92  ###   Avg-Loss 0.017336924870808918   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 93  ###   Avg-Loss 0.017339259386062622   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 94  ###   Avg-Loss 0.017346447706222533   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 95  ###   Avg-Loss 0.01733930706977844   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 96  ###   Avg-Loss 0.01734011769294739   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 97  ###   Avg-Loss 0.01733176310857137   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 98  ###   Avg-Loss 0.017335659265518187   ###   Correct predictions 0.48125\n",
      "Epoch 99  ###   Avg-Loss 0.017331586281458537   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 100  ###   Avg-Loss 0.017337832848230997   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 101  ###   Avg-Loss 0.017333618799845376   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 102  ###   Avg-Loss 0.017344723145167034   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 103  ###   Avg-Loss 0.0173422376314799   ###   Correct predictions 0.48541666666666666\n",
      "Epoch 104  ###   Avg-Loss 0.017340991894404092   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 105  ###   Avg-Loss 0.01734160582224528   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 106  ###   Avg-Loss 0.017327898740768434   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 107  ###   Avg-Loss 0.01732935905456543   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 108  ###   Avg-Loss 0.017331963777542113   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 109  ###   Avg-Loss 0.01732992132504781   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 110  ###   Avg-Loss 0.017332820097605388   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 111  ###   Avg-Loss 0.017328357696533202   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 112  ###   Avg-Loss 0.017340288559595744   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 113  ###   Avg-Loss 0.017335466543833413   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 114  ###   Avg-Loss 0.017333012819290162   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 115  ###   Avg-Loss 0.017334548632303874   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 116  ###   Avg-Loss 0.017332889636357627   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 117  ###   Avg-Loss 0.017335166533788044   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 118  ###   Avg-Loss 0.01733022133509318   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 119  ###   Avg-Loss 0.017332543929417927   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 120  ###   Avg-Loss 0.017333388328552246   ###   Correct predictions 0.48541666666666666\n",
      "Epoch 121  ###   Avg-Loss 0.017331175009409585   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 122  ###   Avg-Loss 0.017329037189483643   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 123  ###   Avg-Loss 0.017340620358784992   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 124  ###   Avg-Loss 0.01733303666114807   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 125  ###   Avg-Loss 0.0173516849676768   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 126  ###   Avg-Loss 0.0173428475856781   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 127  ###   Avg-Loss 0.017335009574890137   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 128  ###   Avg-Loss 0.017333370447158814   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 129  ###   Avg-Loss 0.017340290546417236   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 130  ###   Avg-Loss 0.01732499400774638   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 131  ###   Avg-Loss 0.01733763813972473   ###   Correct predictions 0.48541666666666666\n",
      "Epoch 132  ###   Avg-Loss 0.017332053184509276   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 133  ###   Avg-Loss 0.017353246609369915   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 134  ###   Avg-Loss 0.017330745855967205   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 135  ###   Avg-Loss 0.01732763648033142   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 136  ###   Avg-Loss 0.017330714066823325   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 137  ###   Avg-Loss 0.017336773872375488   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 138  ###   Avg-Loss 0.017343024412790935   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 139  ###   Avg-Loss 0.01733009417851766   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 140  ###   Avg-Loss 0.017354045311609903   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 141  ###   Avg-Loss 0.017332154512405395   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 142  ###   Avg-Loss 0.017334461212158203   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 143  ###   Avg-Loss 0.017343628406524658   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 144  ###   Avg-Loss 0.017340620358784992   ###   Correct predictions 0.46875\n",
      "Epoch 145  ###   Avg-Loss 0.01733053723971049   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 146  ###   Avg-Loss 0.017334272464116413   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 147  ###   Avg-Loss 0.017340207099914552   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 148  ###   Avg-Loss 0.01733244260152181   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 149  ###   Avg-Loss 0.017334314187367757   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 150  ###   Avg-Loss 0.0173399825890859   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 151  ###   Avg-Loss 0.017332218090693154   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 152  ###   Avg-Loss 0.017343036333719888   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 153  ###   Avg-Loss 0.01734464367230733   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 154  ###   Avg-Loss 0.017339040835698444   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 155  ###   Avg-Loss 0.01733132799466451   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 156  ###   Avg-Loss 0.0173313041528066   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 157  ###   Avg-Loss 0.017345698674519856   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 158  ###   Avg-Loss 0.01734340190887451   ###   Correct predictions 0.47708333333333336\n",
      "Epoch 159  ###   Avg-Loss 0.017333271106084187   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 160  ###   Avg-Loss 0.01733576456705729   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 161  ###   Avg-Loss 0.01733560562133789   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 162  ###   Avg-Loss 0.017329166332880657   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 163  ###   Avg-Loss 0.017341987291971842   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 164  ###   Avg-Loss 0.017338371276855467   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 165  ###   Avg-Loss 0.017340058088302614   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 166  ###   Avg-Loss 0.017344150940577188   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 167  ###   Avg-Loss 0.017344385385513306   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 168  ###   Avg-Loss 0.017346356312433878   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 169  ###   Avg-Loss 0.017333459854125977   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 170  ###   Avg-Loss 0.01733231743176778   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 171  ###   Avg-Loss 0.01732916235923767   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 172  ###   Avg-Loss 0.017331536610921225   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 173  ###   Avg-Loss 0.01733509699503581   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 174  ###   Avg-Loss 0.01732465426127116   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 175  ###   Avg-Loss 0.017342160145441692   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 176  ###   Avg-Loss 0.017330139875411987   ###   Correct predictions 0.5104166666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177  ###   Avg-Loss 0.017332039276758828   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 178  ###   Avg-Loss 0.01733199159304301   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 179  ###   Avg-Loss 0.017338985204696657   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 180  ###   Avg-Loss 0.017342575391133628   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 181  ###   Avg-Loss 0.017335110902786256   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 182  ###   Avg-Loss 0.017331095536549886   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 183  ###   Avg-Loss 0.017341347535451253   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 184  ###   Avg-Loss 0.017332384983698528   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 185  ###   Avg-Loss 0.017330108086268108   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 186  ###   Avg-Loss 0.017338589827219645   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 187  ###   Avg-Loss 0.017340189218521117   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 188  ###   Avg-Loss 0.017331349849700927   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 189  ###   Avg-Loss 0.017359145482381187   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 190  ###   Avg-Loss 0.017331101497014365   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 191  ###   Avg-Loss 0.017330982287724814   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 192  ###   Avg-Loss 0.017326368888219198   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 193  ###   Avg-Loss 0.017343854904174803   ###   Correct predictions 0.47708333333333336\n",
      "Epoch 194  ###   Avg-Loss 0.017337441444396973   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 195  ###   Avg-Loss 0.017333298921585083   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 196  ###   Avg-Loss 0.017332341273625693   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 197  ###   Avg-Loss 0.017343161503473918   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 198  ###   Avg-Loss 0.017330855131149292   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 199  ###   Avg-Loss 0.01732876698176066   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 200  ###   Avg-Loss 0.017356745402018228   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 201  ###   Avg-Loss 0.017346465587615968   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 202  ###   Avg-Loss 0.017329835891723634   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 203  ###   Avg-Loss 0.01734454035758972   ###   Correct predictions 0.4895833333333333\n",
      "Epoch 204  ###   Avg-Loss 0.01733029286066691   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 205  ###   Avg-Loss 0.01733484069506327   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 206  ###   Avg-Loss 0.017335607608159383   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 207  ###   Avg-Loss 0.017337212959925335   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 208  ###   Avg-Loss 0.017347023884455363   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 209  ###   Avg-Loss 0.017332309484481813   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 210  ###   Avg-Loss 0.01733231743176778   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 211  ###   Avg-Loss 0.01734496553738912   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 212  ###   Avg-Loss 0.0173362930615743   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 213  ###   Avg-Loss 0.017337344090143838   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 214  ###   Avg-Loss 0.017353427410125733   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 215  ###   Avg-Loss 0.017331113417943318   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 216  ###   Avg-Loss 0.01733746329943339   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 217  ###   Avg-Loss 0.017332746585210165   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 218  ###   Avg-Loss 0.01733715335528056   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 219  ###   Avg-Loss 0.017337143421173096   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 220  ###   Avg-Loss 0.0173479954401652   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 221  ###   Avg-Loss 0.01733072797457377   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 222  ###   Avg-Loss 0.01734137535095215   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 223  ###   Avg-Loss 0.017333173751831056   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 224  ###   Avg-Loss 0.017326873540878297   ###   Correct predictions 0.50625\n",
      "Epoch 225  ###   Avg-Loss 0.01732667684555054   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 226  ###   Avg-Loss 0.017343149582544962   ###   Correct predictions 0.47708333333333336\n",
      "Epoch 227  ###   Avg-Loss 0.017348565657933555   ###   Correct predictions 0.49375\n",
      "Epoch 228  ###   Avg-Loss 0.017334616184234618   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 229  ###   Avg-Loss 0.017330666383107502   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 230  ###   Avg-Loss 0.017340850830078126   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 231  ###   Avg-Loss 0.01733234922091166   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 232  ###   Avg-Loss 0.01734009782473246   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 233  ###   Avg-Loss 0.017341488599777223   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 234  ###   Avg-Loss 0.017334944009780882   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 235  ###   Avg-Loss 0.017334401607513428   ###   Correct predictions 0.50625\n",
      "Epoch 236  ###   Avg-Loss 0.01733740170796712   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 237  ###   Avg-Loss 0.017334469159444175   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 238  ###   Avg-Loss 0.01733429233233134   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 239  ###   Avg-Loss 0.017332722743352253   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 240  ###   Avg-Loss 0.01733097235361735   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 241  ###   Avg-Loss 0.017334298292795817   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 242  ###   Avg-Loss 0.017329289515813192   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 243  ###   Avg-Loss 0.01733185847600301   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 244  ###   Avg-Loss 0.017335520188013712   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 245  ###   Avg-Loss 0.017344415187835693   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 246  ###   Avg-Loss 0.017336014906565347   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 247  ###   Avg-Loss 0.017329887549082438   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 248  ###   Avg-Loss 0.017341379324595133   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 249  ###   Avg-Loss 0.01733956535657247   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 250  ###   Avg-Loss 0.01733067234357198   ###   Correct predictions 0.5104166666666666\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "from Dataset.Dataset import makeMoonsDataset\n",
    "\n",
    "dataset_size = 600\n",
    "batch_size = 40\n",
    "train_size = dataset_size*0.8\n",
    "test_size = dataset_size*0.2\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset(dataset_size,batch_size)\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('lin1',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu1',nn.ReLU()),\n",
    "    ('lin2',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu2',nn.ReLU()),\n",
    "    ('lin3',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu3',nn.ReLU()),\n",
    "    ('lin4',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu4',nn.ReLU()),\n",
    "    ('lin5',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu5',nn.ReLU()),\n",
    "    ('lin6',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu6',nn.ReLU()),\n",
    "    ('lin7',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu7',nn.ReLU()),\n",
    "    ('lin8',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu8',nn.ReLU()),\n",
    "    ('lin9',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu9',nn.ReLU()),\n",
    "    ('lin10',nn.Linear(2, 2, bias=True))\n",
    "])\n",
    ")\n",
    "\n",
    "output_frequency = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(250):\n",
    "    loss_sum = 0\n",
    "    correct_pred = 0\n",
    "    for index, (data, target) in enumerate(train_loader):\n",
    "        #print(index)\n",
    "        output = model.forward(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss_sum = loss_sum + loss.data\n",
    "        for i in range(len(output)):\n",
    "            _, ind = torch.max(output[i],0)\n",
    "            label = target[i]\n",
    "            \n",
    "            if ind.data == label.data:\n",
    "                correct_pred +=1\n",
    "        #if index % (10*(output_frequency)) == 0:\n",
    "        #    print(\"#  Epoch  #  Batch  #  Avg-Loss ###############\")\n",
    "        #if index % (output_frequency) == 0 and index > 0:\n",
    "        #    print(\"#  %d  #  %d  #  %f  #\" % (epoch+1, index, loss_sum/output_frequency))\n",
    "        #    loss_sum = 0\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch '+str(epoch+1)+'  ###   Avg-Loss '+str(loss_sum.item()/train_size)+'   ###   Correct predictions '+str(correct_pred/train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7115,  1.4547]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.6434, -2.7388]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3886,  2.0311]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.9741, -3.1041]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-0.8089,  0.1275]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3526, -2.2834]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.8463, -2.8233]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.9055, -2.8244]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-1.7085,  1.4306]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.7206,  2.3536]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 0.7751, -1.6380]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.4437,  2.1093]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.1609,  1.7324]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-0.5344,  0.0122]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3842,  2.1159]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-1.5523,  1.2432]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.5676, -2.6339]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 2.0958, -3.2221]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 0.9023, -1.6219]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.7551,  2.3714]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3445, -2.3139]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.5533,  2.1857]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3841, -2.4257]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3780,  2.0977]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 0.0020, -0.6190]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 0.1119, -0.9363]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3832, -2.3543]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.5189, -2.5425]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-1.9774,  1.4456]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.2206, -2.2302]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-0.1666, -0.6191]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.3856,  2.0124]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.4732, -2.4705]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.4295,  2.1136]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-0.6206,  0.0895]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.7165, -2.7979]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.8907, -2.8468]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.8076, -2.8020]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3121,  1.9327]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.0975,  1.7790]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "for index, (data, target) in enumerate(test_loader):\n",
    "    print(model.forward(data))\n",
    "    print(target)\n",
    "    print('####################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 1.0\n"
     ]
    }
   ],
   "source": [
    "net.test(test_loader,120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
      "        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1])\n",
      "tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,\n",
      "        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1])\n",
      "tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n",
      "        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1])\n",
      "tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1])\n",
      "tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1])\n",
      "tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n",
      "        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0])\n",
      "tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,\n",
      "        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1])\n",
      "tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
      "        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,\n",
      "        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0])\n",
      "tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
      "        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0])\n",
      "tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0])\n",
      "tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1])\n",
      "[0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "from pandas import DataFrame\n",
    "import torch\n",
    "\n",
    "x1 = []\n",
    "x2 = []\n",
    "y = []\n",
    "for (data,label) in train_loader:\n",
    "    res = torch.argmax(net.forward(data), dim=1)\n",
    "    print(res)\n",
    "    for point in range(len(data)):\n",
    "        x1.append(data[point][0].item())\n",
    "        x2.append(data[point][1].item())\n",
    "        y.append(res[point].item())\n",
    "#print(x1)\n",
    "df = DataFrame(dict(x=x1, y=x2, label=y))\n",
    "print(y)\n",
    "colors = {0:'red', 1:'blue'}\n",
    "fig, ax = pyplot.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 1.0\n"
     ]
    }
   ],
   "source": [
    "correct_pred = 0\n",
    "for _, (data, label) in enumerate(test_loader):\n",
    "    prediction = net.forward(data)\n",
    "    _, ind = torch.max(prediction,1)\n",
    "    if ind.data == label.data:\n",
    "        correct_pred +=1\n",
    "print('Correct predictions: '+str(correct_pred/120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:168: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  compared_weight_signs = torch.tensor(torch.ne(new_weight_signs, old_weight_signs).detach(), dtype=torch.float32)\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:173: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_weights = new_weight_signs*torch.tensor(torch.ge(torch.abs(self.m_accumulated),self.rho*max_weight_elem*torch.ones_like(self.m_accumulated)).detach(),dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import loadMNIST\n",
    "from Networks.ResNet import ConvNet, FCMSANet\n",
    "\n",
    "#net = ConvNet(3, [1], [3,6], num_fc=2, sizes_fc=[100,10])\n",
    "net = FCMSANet(num_fc=4,sizes_fc=[784,1024,1024,1024,10], bias=False, batchnorm=True, test=False)\n",
    "train_loader, test_loader = loadMNIST('Dataset/input/mnist_train.csv', 'Dataset/input/mnist_test.csv')\n",
    "#net.train_backprop(1,train_loader)\n",
    "#net.test(test_loader,10000)\n",
    "print(train_loader.batch_size)\n",
    "\n",
    "net.train_msa(1,train_loader)\n",
    "\n",
    "#for index, (data, target) in enumerate(train_loader):\n",
    "#    if index == 1:\n",
    "#        print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "60000\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.021136  #  0.286883  #\n",
      "#  2  #  0.013666  #  0.829800  #\n",
      "#  3  #  0.013053  #  0.897417  #\n",
      "#  4  #  0.012986  #  0.906250  #\n",
      "#  5  #  0.012988  #  0.906350  #\n",
      "#  6  #  0.012940  #  0.911467  #\n",
      "#  7  #  0.012933  #  0.911933  #\n",
      "#  8  #  0.012929  #  0.911917  #\n",
      "#  9  #  0.012904  #  0.915100  #\n",
      "#  10  #  0.012890  #  0.917233  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  11  #  0.012865  #  0.920600  #\n",
      "#  12  #  0.012873  #  0.919233  #\n",
      "#  13  #  0.012850  #  0.921300  #\n",
      "#  14  #  0.012837  #  0.922417  #\n",
      "#  15  #  0.012826  #  0.923083  #\n",
      "#  16  #  0.012819  #  0.925200  #\n",
      "#  17  #  0.012815  #  0.925233  #\n",
      "#  18  #  0.012803  #  0.924800  #\n",
      "#  19  #  0.012793  #  0.925500  #\n",
      "#  20  #  0.012786  #  0.927700  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  21  #  0.012767  #  0.928617  #\n",
      "#  22  #  0.012764  #  0.929817  #\n",
      "#  23  #  0.012761  #  0.929783  #\n",
      "#  24  #  0.012750  #  0.931117  #\n",
      "#  25  #  0.012755  #  0.930150  #\n",
      "#  26  #  0.012739  #  0.930950  #\n",
      "#  27  #  0.012737  #  0.932067  #\n",
      "#  28  #  0.012714  #  0.934367  #\n",
      "#  29  #  0.012710  #  0.933683  #\n",
      "#  30  #  0.012709  #  0.934067  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  31  #  0.012702  #  0.935467  #\n",
      "#  32  #  0.012690  #  0.937067  #\n",
      "#  33  #  0.012683  #  0.936800  #\n",
      "#  34  #  0.012679  #  0.937833  #\n",
      "#  35  #  0.012673  #  0.937717  #\n",
      "#  36  #  0.012665  #  0.939350  #\n",
      "#  37  #  0.012664  #  0.938350  #\n",
      "#  38  #  0.012651  #  0.939567  #\n",
      "#  39  #  0.012656  #  0.939817  #\n",
      "#  40  #  0.012646  #  0.941567  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  41  #  0.012644  #  0.940967  #\n",
      "#  42  #  0.012632  #  0.942317  #\n",
      "#  43  #  0.012623  #  0.944083  #\n",
      "#  44  #  0.012625  #  0.943300  #\n",
      "#  45  #  0.012613  #  0.944450  #\n",
      "#  46  #  0.012618  #  0.943500  #\n",
      "#  47  #  0.012606  #  0.944933  #\n",
      "#  48  #  0.012599  #  0.945317  #\n",
      "#  49  #  0.012598  #  0.946067  #\n",
      "#  50  #  0.012588  #  0.946917  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  51  #  0.012578  #  0.947050  #\n",
      "#  52  #  0.012578  #  0.947650  #\n",
      "#  53  #  0.012573  #  0.948067  #\n",
      "#  54  #  0.012572  #  0.948083  #\n",
      "#  55  #  0.012566  #  0.948533  #\n",
      "#  56  #  0.012561  #  0.948933  #\n",
      "#  57  #  0.012555  #  0.949333  #\n",
      "#  58  #  0.012557  #  0.949850  #\n",
      "#  59  #  0.012548  #  0.950500  #\n",
      "#  60  #  0.012550  #  0.950417  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  61  #  0.012548  #  0.950367  #\n",
      "#  62  #  0.012538  #  0.951200  #\n",
      "#  63  #  0.012539  #  0.951067  #\n",
      "#  64  #  0.012532  #  0.952583  #\n",
      "#  65  #  0.012539  #  0.951133  #\n",
      "#  66  #  0.012519  #  0.952433  #\n",
      "#  67  #  0.012516  #  0.953233  #\n",
      "#  68  #  0.012518  #  0.953417  #\n",
      "#  69  #  0.012520  #  0.952833  #\n",
      "#  70  #  0.012513  #  0.954000  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  71  #  0.012516  #  0.955067  #\n",
      "#  72  #  0.012513  #  0.955033  #\n",
      "#  73  #  0.012505  #  0.955567  #\n",
      "#  74  #  0.012507  #  0.954783  #\n",
      "#  75  #  0.012495  #  0.955800  #\n",
      "#  76  #  0.012499  #  0.955583  #\n",
      "#  77  #  0.012494  #  0.956133  #\n",
      "#  78  #  0.012495  #  0.955817  #\n",
      "#  79  #  0.012485  #  0.957150  #\n",
      "#  80  #  0.012484  #  0.956950  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  81  #  0.012489  #  0.956650  #\n",
      "#  82  #  0.012486  #  0.956533  #\n",
      "#  83  #  0.012485  #  0.956917  #\n",
      "#  84  #  0.012485  #  0.957567  #\n",
      "#  85  #  0.012472  #  0.958333  #\n",
      "#  86  #  0.012476  #  0.957617  #\n",
      "#  87  #  0.012479  #  0.957600  #\n",
      "#  88  #  0.012475  #  0.957917  #\n",
      "#  89  #  0.012474  #  0.957683  #\n",
      "#  90  #  0.012469  #  0.958500  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  91  #  0.012469  #  0.957800  #\n",
      "#  92  #  0.012466  #  0.958800  #\n",
      "#  93  #  0.012467  #  0.958267  #\n",
      "#  94  #  0.012462  #  0.959483  #\n",
      "#  95  #  0.012464  #  0.958783  #\n",
      "#  96  #  0.012465  #  0.959083  #\n",
      "#  97  #  0.012455  #  0.960100  #\n",
      "#  98  #  0.012458  #  0.959733  #\n",
      "#  99  #  0.012456  #  0.960383  #\n",
      "#  100  #  0.012450  #  0.960350  #\n",
      "Time elapsed:  42948.913511782994\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import loadMNIST\n",
    "from Networks.ResNet import ConvMSANet\n",
    "\n",
    "#net = ConvNet(3, [1], [3,6], num_fc=2, sizes_fc=[100,10])\n",
    "net = ConvMSANet(num_conv=5, num_channels=[1,3,6,6,12,12], subsample_points=[2,4], num_fc=2,sizes_fc=[588,900,10], batchnorm=True, test=False)\n",
    "train_loader, test_loader = loadMNIST('Dataset/input/mnist_train.csv', 'Dataset/input/mnist_test.csv')\n",
    "#net.train_backprop(1,train_loader)\n",
    "#net.test(test_loader,10000)\n",
    "print(train_loader.batch_size)\n",
    "print(len(train_loader.dataset))\n",
    "\n",
    "net.train_msa(100,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.019112  #  0.406533  #\n",
      "Test set accuracy:  0.8661\n",
      "#  2  #  0.006457  #  0.915850  #\n",
      "Test set accuracy:  0.9351\n",
      "#  3  #  0.005586  #  0.939183  #\n",
      "Test set accuracy:  0.9308\n",
      "#  4  #  0.005552  #  0.940833  #\n",
      "Test set accuracy:  0.926\n",
      "#  5  #  0.005510  #  0.941217  #\n",
      "Test set accuracy:  0.9341\n",
      "#  6  #  0.005494  #  0.941800  #\n",
      "Test set accuracy:  0.9359\n",
      "#  7  #  0.005494  #  0.940317  #\n",
      "Test set accuracy:  0.9336\n",
      "#  8  #  0.005440  #  0.943433  #\n",
      "Test set accuracy:  0.9445\n",
      "#  9  #  0.005444  #  0.943083  #\n",
      "Test set accuracy:  0.9367\n",
      "#  10  #  0.005400  #  0.943900  #\n",
      "Test set accuracy:  0.9439\n",
      "Time elapsed:  722.9656050030001\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import loadMNIST\n",
    "from Networks.ResNet import ConvMSANet\n",
    "\n",
    "#net = ConvNet(3, [1], [3,6], num_fc=2, sizes_fc=[100,10])\n",
    "net = ConvMSANet(num_conv=3, num_channels=[1,3,6,12], subsample_points=[0,1], num_fc=2,sizes_fc=[588,900,10], batchnorm=True, test=False)\n",
    "train_loader, test_loader = loadMNIST('Dataset/input/mnist_train.csv', 'Dataset/input/mnist_test.csv')\n",
    "net.set_test_tracking(True)\n",
    "\n",
    "net.train_msa(10,train_loader, testloader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n"
     ]
    }
   ],
   "source": [
    "net.train_msa(1,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy:  0.9439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9439"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.test(test_loader,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "60000\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.010105  #  0.672850  #\n",
      "#  2  #  0.001262  #  0.961583  #\n",
      "#  3  #  0.000785  #  0.976367  #\n",
      "#  4  #  0.000575  #  0.982483  #\n",
      "#  5  #  0.000435  #  0.986500  #\n",
      "#  6  #  0.000330  #  0.989950  #\n",
      "#  7  #  0.000251  #  0.992450  #\n",
      "#  8  #  0.000219  #  0.993333  #\n",
      "#  9  #  0.000174  #  0.994683  #\n",
      "#  10  #  0.000160  #  0.994983  #\n",
      "Time elapsed:  110.92861831799999\n"
     ]
    }
   ],
   "source": [
    "from Networks.ResNet import ConvNet\n",
    "from Dataset.Dataset import loadMNIST\n",
    "\n",
    "net = ConvNet(num_conv=3, num_channels=[1,3,6,12], subsample_points=[0,1], num_fc=2,sizes_fc=[588,900,10])\n",
    "train_loader, test_loader = loadMNIST('Dataset/input/mnist_train.csv', 'Dataset/input/mnist_test.csv')\n",
    "#net.train_backprop(1,train_loader)\n",
    "#net.test(test_loader,10000)\n",
    "print(train_loader.batch_size)\n",
    "print(len(train_loader.dataset))\n",
    "\n",
    "net.train(10,train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 0.9812\n"
     ]
    }
   ],
   "source": [
    "net.test(test_loader, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1574, -0.1913,  0.3191],\n",
      "        [ 0.4584,  0.3066, -0.3247],\n",
      "        [ 0.2495,  0.4553,  0.0500],\n",
      "        [ 0.3111, -0.3222,  0.2152]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.4820, -0.5667, -0.2615,  0.5324], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "test_layer = torch.nn.Linear(3, 4)\n",
    "print(test_layer.weight)\n",
    "print(test_layer.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6., 18., 36.], grad_fn=<SumBackward2>)\n",
      "tensor([[1., 1., 1.],\n",
      "        [2., 2., 2.],\n",
      "        [3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([[1,2,3],[2,3,4],[3,4,5]],dtype=torch.float32, requires_grad=True)\n",
    "y = torch.tensor([[1,1,1],[2,2,2],[3,3,3]],dtype=torch.float32)\n",
    "\n",
    "z = torch.sum(x*y, dim=1)\n",
    "print(z)\n",
    "#z.backward(torch.FloatTensor([0,0,1]), retain_graph=True)\n",
    "z.backward(torch.FloatTensor([1,1,1]), retain_graph=True)\n",
    "#z[0].backward(retain_graph=True)\n",
    "#z[1].backward(retain_graph=True)\n",
    "#z[2].backward(retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "tensor([[[[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]],\n",
      "\n",
      "         [[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]],\n",
      "\n",
      "         [[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]]],\n",
      "\n",
      "\n",
      "        [[[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]],\n",
      "\n",
      "         [[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]],\n",
      "\n",
      "         [[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]]],\n",
      "\n",
      "\n",
      "        [[[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]],\n",
      "\n",
      "         [[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]],\n",
      "\n",
      "         [[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]]]], dtype=torch.float64)\n",
      "tensor([120., 120., 120.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor((), dtype=torch.float64)\n",
    "y = torch.tensor((), dtype=torch.float64)\n",
    "x = x.new_ones((3,3,4,4))\n",
    "y = y.new_full((3,3,4,4), 2.5)\n",
    "print(x.dim())\n",
    "print(x*y)\n",
    "print(torch.sum(x*y, (1,2,3)))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 7, 9]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([[1,2,3]])\n",
    "y=torch.tensor([[4,5,6]])\n",
    "print(x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.218333333333334\n"
     ]
    }
   ],
   "source": [
    "time =0\n",
    "for i in range(61):\n",
    "    time += i*14.2\n",
    "print(time/60/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6)\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([[1,2,3,0,0,1,3,2]])\n",
    "y=torch.tensor([[1,2,2,1,0,1,3,2]])\n",
    "print(torch.sum(x==y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(120)\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0])\n",
    "b=torch.tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0])\n",
    "c=torch.tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1])\n",
    "d=torch.tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1])\n",
    "e=torch.tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0])\n",
    "f=torch.tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0])\n",
    "\n",
    "print(torch.sum(a==b)+torch.sum(c==d)+torch.sum(e==f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from Dataset.Dataset import makeMoonsDataset\n",
    "\n",
    "dataset_size = 600\n",
    "batch_size = 40\n",
    "test_set_size = dataset_size * 0.2\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset(dataset_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from pandas import DataFrame\n",
    "\n",
    "#x1 = []\n",
    "#x2 = []\n",
    "#y = []\n",
    "#for (data,label) in train_loader:\n",
    "#    res = label#torch.argmax(net.forward, dim=1)\n",
    "#    for point in range(len(data)):\n",
    "#        x1.append(data[point][0].item())\n",
    "#        x2.append(data[point][1].item())\n",
    "#        y.append(res[point].item())\n",
    "##print(x1)\n",
    "#df = DataFrame(dict(x=x1, y=x2, label=y))\n",
    "#colors = {0:'red', 1:'blue'}\n",
    "fig, ax2 = plt.subplots()\n",
    "#grouped = df.groupby('label')\n",
    "#for key, group in grouped:\n",
    "#    group.plot(ax=ax1, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "#ax1.set_title('Training Set')\n",
    "    \n",
    "x1 = []\n",
    "x2 = []\n",
    "y = []\n",
    "for (data,label) in test_loader:\n",
    "    res = label#torch.argmax(label, dim=1)\n",
    "    for point in range(len(data)):\n",
    "        x1.append(data[point][0].item())\n",
    "        x2.append(data[point][1].item())\n",
    "        y.append(res[point].item())\n",
    "#print(x1)\n",
    "df = DataFrame(dict(x=x1, y=x2, label=y))\n",
    "colors = {0:'red', 1:'blue'}\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax2, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "ax2.set_title('Test Set')\n",
    "#plt.tight_layout()\n",
    "plt.savefig(\"Plots/MoonsTest.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import loadMNIST\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "train_loader, test_loader = loadMNIST('Dataset/input/mnist_train.csv', 'Dataset/input/mnist_test.csv')\n",
    "arr_dict = {'arr0':[],'c0':0, 'arr1':[],'c1':0, 'arr2':[],'c2':0, 'arr3':[],'c3':0, 'arr4':[],'c4':0, 'arr5':[],'c5':0, 'arr6':[],'c6':0, 'arr7':[],'c7':0, 'arr8':[],'c8':0, 'arr9':[],'c9':0}\n",
    "\n",
    "\n",
    "for index, (data, label) in enumerate(train_loader):\n",
    "    if index < 200:\n",
    "        #print(data[0],label[0])\n",
    "        key = 'arr'+str(label[0].item())\n",
    "        key_c = 'c'+str(label[0].item())\n",
    "        arr = data[0][0].numpy()*255\n",
    "        if arr_dict[key] == []:\n",
    "            arr_dict[key] = arr\n",
    "            arr_dict[key_c] += 1\n",
    "        elif arr_dict[key_c] < 10:\n",
    "            arr_dict[key] = np.concatenate((arr_dict[key], arr), axis=1) \n",
    "            arr_dict[key_c] += 1\n",
    "            \n",
    "        #img = Image.fromarray(arr)\n",
    "        #if img.mode != 'RGB':\n",
    "        #    img = img.convert('RGB')\n",
    "        #img.save('Plots/'+str(label[0].item())+'/mnist'+str(index)+'.png')\n",
    "        #img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(10):\n",
    "#    print()\n",
    "arr = np.concatenate((arr_dict['arr0'],arr_dict['arr1'],arr_dict['arr2'],arr_dict['arr3'],arr_dict['arr4'],arr_dict['arr5'],arr_dict['arr6'],arr_dict['arr7'],arr_dict['arr8'],arr_dict['arr9']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.fromarray(arr)\n",
    "if img.mode != 'RGB':            \n",
    "    img = img.convert('RGB')\n",
    "    img.save('Plots/mnistall.png')\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "num_epochs = 100\n",
    "num_layers = 5\n",
    "layers = [2,16,32,64,128,2]\n",
    "bias = False\n",
    "test_set_size = 120\n",
    "\n",
    "basic_save_key = 'Convergence_results//FCNet//FCNet'#MSAMSA\n",
    "\n",
    "train_save_key = '_train_per_epoch_layers_'+str(num_layers)+'_max_hidden_size_'+str(max(layers))+'_bias_'+str(bias)\n",
    "test_save_key = '_test_per_epoch_layers_'+str(num_layers)+'_max_hidden_size_'+str(max(layers))+'_bias_'+str(bias)\n",
    "\n",
    "train_results_per_epoch = np.loadtxt(basic_save_key+train_save_key+'.csv')\n",
    "test_results_per_epoch = np.loadtxt(basic_save_key+test_save_key+'.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8822916746139526\n"
     ]
    }
   ],
   "source": [
    "print(train_results_per_epoch[15])#/test_set_sizemax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "layers = np.arange(2,max_layers+2,1)\n",
    "ax.plot(layers, train_results_per_epoch, label='train')\n",
    "ax.plot(layers, test_results_per_epoch/test_set_size, label='test')\n",
    "ax.set_ylim(0,1.1)\n",
    "ax.set_xlabel('Number of layers')\n",
    "ax.set_ylabel('Average Accuracy')\n",
    "ax.legend(loc='best')\n",
    "plt.savefig(\"Plots/\"+train_save_key+\".pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 16\n",
      "[0.71458334 0.8645833  0.87291664 0.84166664 0.89166665 0.88125\n",
      " 0.8666667  0.88958335 0.87083334 0.91041666 0.87291664 0.90833336\n",
      " 0.94166666 0.84375    0.93333334 0.97291666 0.9895833  0.9583333\n",
      " 0.9895833  0.99375    0.99583334 0.98541665 0.95208335 0.99375\n",
      " 0.99583334 0.99791664 0.96458334 0.9916667  0.99375    0.99375\n",
      " 0.99791664 1.         0.99583334 0.99375    0.99583334 0.99791664\n",
      " 0.99583334 1.         0.69375    0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[109. 110. 111. 112. 107. 112. 114. 114. 114. 115. 111. 116.  85. 113.\n",
      " 117. 120. 115. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.]\n",
      "[0.68125    0.84791666 0.86875    0.85625    0.8645833  0.8520833\n",
      " 0.8541667  0.86875    0.86875    0.86875    0.85625    0.86041665\n",
      " 0.8645833  0.87083334 0.87708336 0.8854167  0.8979167  0.84791666\n",
      " 0.9145833  0.9270833  0.9479167  0.96875    0.9291667  0.97083336\n",
      " 0.9916667  0.9791667  0.97083336 0.99375    0.99583334 0.99791664\n",
      " 0.97083336 1.         1.         1.         0.99791664 1.\n",
      " 1.         0.99375    0.99791664 0.93333334 0.99583334 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.99791664 0.99791664 1.         1.         0.99791664\n",
      " 0.99791664 1.         0.99791664 0.99791664 0.99791664 0.99791664\n",
      " 0.99583334 1.         0.99375    0.99791664 1.         0.99791664\n",
      " 1.         0.99791664 0.99583334 0.99791664 1.         0.99791664\n",
      " 1.         1.         0.99791664 1.         1.         1.\n",
      " 1.         0.99791664 1.         1.         1.         1.\n",
      " 1.         1.         0.99791664 1.         1.         0.99583334\n",
      " 1.         0.99583334 0.99791664 1.         1.         1.\n",
      " 1.         1.         1.         1.        ]\n",
      "[106. 109. 111. 106. 112. 111. 110. 106. 110. 111. 106. 112. 110. 111.\n",
      " 110. 111. 102. 111. 114. 114. 115. 118. 120. 120. 119. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.]\n",
      "[0.64375    0.8541667  0.86875    0.8520833  0.88125    0.8875\n",
      " 0.9270833  0.87291664 0.88125    0.9291667  0.96875    0.98333335\n",
      " 0.92083335 0.9270833  0.9916667  0.99375    0.99375    0.99375\n",
      " 0.99791664 0.94375    0.60833335 0.83958334 0.85625    0.8645833\n",
      " 0.86875    0.8645833  0.8666667  0.84583336 0.8520833  0.8541667\n",
      " 0.84583336 0.8666667  0.8645833  0.8666667  0.86041665 0.86041665\n",
      " 0.87083334 0.87083334 0.85833335 0.87291664 0.8541667  0.8625\n",
      " 0.87708336 0.8666667  0.8645833  0.86041665 0.87083334 0.87291664\n",
      " 0.85625    0.87083334 0.8666667  0.87291664 0.8666667  0.8625\n",
      " 0.86875    0.87291664 0.86875    0.875      0.87708336 0.86875\n",
      " 0.86875    0.8645833  0.87083334 0.87083334 0.87291664 0.87708336\n",
      " 0.86875    0.8625     0.86875    0.8645833  0.8645833  0.86041665\n",
      " 0.8625     0.86041665 0.87083334 0.875      0.8666667  0.85833335\n",
      " 0.8666667  0.8541667  0.8666667  0.8625     0.86041665 0.8645833\n",
      " 0.8645833  0.8625     0.8645833  0.86875    0.8666667  0.8625\n",
      " 0.86875    0.87291664 0.8666667  0.8666667  0.8645833  0.87083334\n",
      " 0.8666667  0.87083334 0.86041665 0.87708336]\n",
      "[111. 103. 112. 108. 111. 114. 117. 105. 111. 114. 120. 119. 110. 120.\n",
      " 120. 120. 120. 120. 120.  62. 109. 110. 111. 112. 110. 111. 106. 110.\n",
      " 110. 110. 111. 111. 111. 107. 110. 110. 111. 109. 110. 111. 112. 111.\n",
      " 110. 112. 107. 112. 111. 111. 111. 108. 112. 112. 112. 111. 110. 111.\n",
      " 112. 112. 110. 112. 112. 111. 112. 112. 109. 110. 106. 112. 109. 110.\n",
      " 108. 110. 110. 111. 112. 111. 110. 112. 110. 109. 111. 112. 111. 112.\n",
      " 111. 111. 111. 112. 110. 108. 112. 111. 108. 110. 110. 112. 112. 111.\n",
      " 111. 109.]\n",
      "[0.65833336 0.84583336 0.85625    0.86875    0.8833333  0.875\n",
      " 0.86875    0.8666667  0.88125    0.88125    0.8875     0.86875\n",
      " 0.8958333  0.89166665 0.8875     0.89375    0.9125     0.92083335\n",
      " 0.8958333  0.9291667  0.95       0.9604167  0.9604167  0.96875\n",
      " 0.85833335 0.875      0.94375    0.95625    0.95625    0.97291666\n",
      " 0.98541665 0.97291666 0.925      0.9875     0.9916667  0.99375\n",
      " 0.9916667  0.9916667  0.99375    0.8854167  0.9583333  0.9770833\n",
      " 0.9791667  0.98541665 0.98333335 0.99375    0.9875     0.99375\n",
      " 0.99583334 0.99791664 0.99791664 0.99583334 0.99791664 0.99583334\n",
      " 0.79375    0.90208334 0.97291666 0.9916667  0.9916667  0.99375\n",
      " 0.99583334 0.99375    0.99791664 0.99583334 0.99791664 0.99791664\n",
      " 0.99375    0.99791664 0.9916667  0.99791664 0.99791664 0.99791664\n",
      " 0.99791664 0.99791664 0.99791664 0.99791664 0.99583334 0.9916667\n",
      " 0.99583334 0.99791664 0.99791664 0.99583334 0.99583334 0.99375\n",
      " 1.         0.99791664 0.99583334 0.99791664 0.99583334 0.99791664\n",
      " 1.         0.99583334 0.99791664 0.99791664 0.99583334 0.99791664\n",
      " 0.9916667  0.99583334 0.99791664 0.99583334]\n",
      "[108. 111. 106. 110. 110. 111. 111. 111. 111. 111. 111. 111. 113. 113.\n",
      " 111. 115. 113. 106. 118. 117. 118. 118. 119. 118. 109. 116. 118. 118.\n",
      " 120. 120. 118. 120. 120. 120. 120. 120. 120. 120. 120. 115. 118. 118.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 107. 118.\n",
      " 120. 119. 120. 118. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.]\n",
      "[0.6354167  0.80625    0.81458336 0.875      0.8666667  0.86875\n",
      " 0.84791666 0.88125    0.8645833  0.8666667  0.89166665 0.87291664\n",
      " 0.87291664 0.8979167  0.8833333  0.88125    0.8854167  0.87708336\n",
      " 0.875      0.8625     0.8666667  0.8666667  0.8833333  0.8833333\n",
      " 0.8875     0.89375    0.86041665 0.87916666 0.87916666 0.87708336\n",
      " 0.875      0.8854167  0.87291664 0.87083334 0.8833333  0.88958335\n",
      " 0.89375    0.88125    0.87708336 0.8666667  0.8645833  0.88125\n",
      " 0.88958335 0.87916666 0.88125    0.8854167  0.87916666 0.8833333\n",
      " 0.8854167  0.8854167  0.8854167  0.86875    0.8979167  0.87083334\n",
      " 0.88125    0.87916666 0.87291664 0.88958335 0.87708336 0.875\n",
      " 0.88125    0.87708336 0.87916666 0.88125    0.8854167  0.8854167\n",
      " 0.87291664 0.8645833  0.8854167  0.87291664 0.87291664 0.87083334\n",
      " 0.89166665 0.89166665 0.9        0.8958333  0.8833333  0.87083334\n",
      " 0.875      0.88958335 0.8833333  0.88125    0.9        0.90625\n",
      " 0.9125     0.92083335 0.9479167  0.93958336 0.95625    0.975\n",
      " 0.98333335 0.9875     0.96666664 0.97291666 0.99583334 0.99583334\n",
      " 0.99375    1.         0.99791664 0.93541664]\n",
      "[100. 111. 109. 112. 110. 111. 110. 110. 109. 109. 109. 111. 111. 111.\n",
      " 111. 110. 109. 109. 111. 112. 112. 111. 110. 111. 111. 112. 110. 111.\n",
      " 106. 107. 110. 111. 111. 111. 112. 111. 111. 111. 110. 111. 110. 110.\n",
      " 111. 109. 112. 111. 111. 111. 111. 111. 112. 111. 111. 111. 111. 111.\n",
      " 110. 112. 111. 112. 111. 111. 112. 111. 112. 112. 111. 112. 111. 111.\n",
      " 111. 112. 112. 111. 109. 111. 112. 112. 111. 112. 111. 113. 113. 113.\n",
      " 114. 115. 118. 119. 119. 119. 120. 120. 112. 120. 120. 120. 120. 120.\n",
      " 114. 120.]\n",
      "[0.68333334 0.8020833  0.8645833  0.85833335 0.8645833  0.8541667\n",
      " 0.8666667  0.87708336 0.88958335 0.875      0.9        0.83958334\n",
      " 0.9125     0.9270833  0.94375    0.96458334 0.9916667  0.99375\n",
      " 0.99583334 0.97083336 0.9916667  0.99791664 0.99583334 0.99375\n",
      " 0.99583334 0.99583334 0.99791664 0.9916667  0.98125    0.99791664\n",
      " 1.         1.         1.         0.99791664 0.99583334 0.99791664\n",
      " 1.         0.99583334 1.         0.99583334 0.99791664 0.99791664\n",
      " 0.9916667  0.99583334 0.99791664 0.99791664 0.99791664 0.99791664\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.99791664 1.         1.         1.         0.99791664\n",
      " 1.         0.99583334 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1.         1.         1.         1.        ]\n",
      "[106. 107. 113. 112. 109. 113. 111. 111. 111. 114. 114. 113. 117. 116.\n",
      " 118. 120. 120. 120. 120. 120. 118. 120. 120. 120. 120. 120. 120. 117.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 116. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.]\n",
      "[0.51666665 0.82708335 0.84166664 0.86041665 0.93125    0.88125\n",
      " 0.93958336 0.83958334 0.96875    0.98333335 0.9791667  0.99791664\n",
      " 0.98125    0.99791664 0.99583334 0.99791664 0.98541665 0.99375\n",
      " 0.98541665 1.         1.         0.99791664 0.9875     0.99375\n",
      " 0.98333335 0.99791664 0.99791664 1.         0.99791664 1.\n",
      " 0.99791664 0.99583334 0.99791664 1.         1.         0.99375\n",
      " 0.8666667  0.9583333  0.97291666 0.98541665 0.99583334 0.9916667\n",
      " 0.99583334 1.         0.99583334 0.99791664 1.         0.99791664\n",
      " 0.99791664 0.99583334 1.         0.99791664 0.99583334 1.\n",
      " 1.         1.         0.99791664 0.99583334 0.9166667  0.9895833\n",
      " 0.99791664 0.99791664 0.99791664 1.         0.99791664 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.99791664 0.99791664 1.         1.         0.99791664 1.\n",
      " 1.         0.99791664 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         0.99791664 0.99791664 1.\n",
      " 1.         1.         1.         1.        ]\n",
      "[ 97. 110. 114. 112. 113. 114. 109. 116. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 117. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 115. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 119. 120. 120. 120. 120.\n",
      " 120. 107. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.]\n",
      "[0.625      0.84791666 0.85625    0.8666667  0.86875    0.8979167\n",
      " 0.90833336 0.92083335 0.94166666 0.975      0.98333335 0.99583334\n",
      " 0.99583334 1.         0.99791664 0.81875    0.90416664 0.92083335\n",
      " 0.9583333  0.98541665 0.99375    0.98541665 0.95       0.99583334\n",
      " 0.99791664 0.99791664 0.99791664 0.99583334 1.         0.99791664\n",
      " 1.         0.98125    0.90625    0.9875     0.99583334 0.99375\n",
      " 0.99791664 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         0.99791664 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.99583334\n",
      " 1.         1.         1.         1.         0.99791664 1.\n",
      " 1.         1.         1.         1.         1.         0.99791664\n",
      " 1.         1.         1.         1.         1.         0.99791664\n",
      " 0.99583334 0.99791664 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.99791664\n",
      " 1.         0.99375    0.99583334 0.99583334]\n",
      "[ 99. 110. 112. 112. 113. 109. 119. 120. 118. 120. 120. 120. 120. 120.\n",
      " 110. 113. 114. 117. 117. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120.  88. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.]\n",
      "[0.675      0.8333333  0.84583336 0.8645833  0.88958335 0.87291664\n",
      " 0.92291665 0.93541664 0.93958336 0.86041665 0.8333333  0.88125\n",
      " 0.90416664 0.9270833  0.93958336 0.97291666 0.84791666 0.96875\n",
      " 0.975      0.99375    0.93958336 0.9875     0.99791664 0.99791664\n",
      " 0.99375    0.99583334 1.         0.8208333  0.7875     0.8541667\n",
      " 0.86041665 0.8666667  0.87708336 0.88125    0.87916666 0.8854167\n",
      " 0.8854167  0.8854167  0.93333334 0.93333334 0.9479167  0.9625\n",
      " 0.9583333  0.9625     0.9770833  0.9875     0.97291666 0.99375\n",
      " 0.9895833  0.9625     0.75208336 0.8125     0.8666667  0.88125\n",
      " 0.88125    0.87916666 0.8625     0.875      0.87291664 0.87708336\n",
      " 0.87291664 0.8875     0.8979167  0.8833333  0.8854167  0.87916666\n",
      " 0.875      0.87916666 0.8645833  0.87708336 0.87916666 0.87291664\n",
      " 0.89166665 0.8666667  0.88958335 0.8854167  0.8833333  0.8833333\n",
      " 0.88125    0.87291664 0.8854167  0.875      0.87916666 0.87708336\n",
      " 0.8833333  0.875      0.88125    0.88958335 0.8854167  0.8833333\n",
      " 0.87916666 0.8875     0.88958335 0.8854167  0.87083334 0.87708336\n",
      " 0.8875     0.8854167  0.88125    0.8875    ]\n",
      "[104. 112. 108. 111. 112. 112. 115. 117. 118. 109. 111. 112. 116. 117.\n",
      " 116. 116. 117. 119. 120. 105. 120. 120. 120. 120. 120. 120. 120. 102.\n",
      " 109. 109. 110. 112. 112. 111. 111. 111. 113. 112. 117. 116. 116. 118.\n",
      " 118. 119. 120. 113. 120. 120. 120. 100. 106. 108. 109. 111. 111. 112.\n",
      " 111. 111. 111. 110. 112. 111. 110. 111. 111. 111. 112. 112. 111. 111.\n",
      " 112. 111. 111. 112. 111. 111. 111. 112. 111. 111. 110. 111. 110. 112.\n",
      " 111. 111. 111. 111. 111. 112. 111. 111. 112. 112. 111. 111. 110. 112.\n",
      " 111. 111.]\n",
      "[0.68541664 0.8645833  0.8645833  0.90416664 0.9166667  0.98125\n",
      " 0.92291665 0.99583334 0.8333333  0.8520833  0.87708336 0.8854167\n",
      " 0.8645833  0.8625     0.88125    0.85       0.88125    0.8854167\n",
      " 0.86041665 0.87916666 0.875      0.88125    0.8645833  0.87916666\n",
      " 0.87708336 0.88958335 0.8958333  0.87291664 0.87916666 0.87291664\n",
      " 0.8854167  0.87083334 0.88125    0.8854167  0.8833333  0.88958335\n",
      " 0.89166665 0.87708336 0.875      0.86875    0.88125    0.88125\n",
      " 0.8666667  0.875      0.87291664 0.86875    0.87916666 0.88125\n",
      " 0.87291664 0.8833333  0.86041665 0.88958335 0.8666667  0.8833333\n",
      " 0.86875    0.86875    0.87291664 0.8666667  0.875      0.8833333\n",
      " 0.87291664 0.8854167  0.8875     0.87291664 0.87916666 0.8645833\n",
      " 0.89166665 0.87291664 0.8666667  0.87083334 0.875      0.8833333\n",
      " 0.89166665 0.88125    0.87708336 0.87916666 0.8875     0.88958335\n",
      " 0.87708336 0.8833333  0.8875     0.8833333  0.88125    0.8645833\n",
      " 0.89375    0.8854167  0.8875     0.87291664 0.875      0.8854167\n",
      " 0.87916666 0.87916666 0.88125    0.88125    0.88125    0.8666667\n",
      " 0.875      0.8833333  0.8875     0.8875    ]\n",
      "[108. 108. 114. 110. 117. 118. 118. 120. 111. 109. 112. 111. 112. 112.\n",
      " 109. 112. 111. 111. 111. 112. 110. 110. 112. 110. 109. 111. 111. 110.\n",
      " 110. 111. 112. 111. 112. 110. 109. 111. 111. 111. 112. 110. 112. 111.\n",
      " 111. 111. 111. 111. 110. 112. 111. 111. 111. 111. 111. 111. 111. 111.\n",
      " 111. 111. 111. 111. 111. 111. 112. 112. 111. 112. 111. 111. 112. 110.\n",
      " 111. 112. 111. 111. 112. 111. 112. 111. 111. 111. 111. 111. 111. 112.\n",
      " 111. 111. 111. 111. 111. 111. 110. 111. 111. 111. 111. 111. 112. 111.\n",
      " 111. 111.]\n",
      "Number of layers: 17\n",
      "[0.60625    0.85       0.83125    0.88125    0.91041666 0.93958336\n",
      " 0.98541665 0.84166664 0.81041664 0.84375    0.8541667  0.87291664\n",
      " 0.92083335 0.93541664 0.9        0.94375    0.93333334 0.96875\n",
      " 0.9625     0.9791667  0.92083335 0.96666664 0.9770833  0.98333335\n",
      " 0.9895833  0.99583334 0.9875     0.99583334 0.99375    0.9875\n",
      " 0.99583334 0.9875     0.98541665 0.99583334 0.9458333  0.95625\n",
      " 0.9916667  0.98125    1.         0.99583334 0.9916667  0.9895833\n",
      " 0.99791664 0.99791664 0.99791664 0.99791664 0.99791664 1.\n",
      " 0.98333335 0.97083336 0.99583334 0.99375    1.         1.\n",
      " 0.99791664 0.99791664 0.99791664 0.99791664 0.99791664 0.99583334\n",
      " 1.         0.99791664 1.         1.         0.99791664 1.\n",
      " 0.99583334 1.         1.         0.99791664 0.99583334 0.99583334\n",
      " 0.99791664 0.99791664 1.         0.96458334 0.99583334 0.99791664\n",
      " 0.99791664 1.         0.99791664 1.         0.99791664 0.9916667\n",
      " 0.99791664 0.99583334 0.99375    0.99791664 0.99791664 1.\n",
      " 0.99583334 0.99791664 0.99791664 1.         1.         0.99791664\n",
      " 0.99375    0.99583334 0.99791664 1.        ]\n",
      "[ 97. 109. 108. 113. 117. 118. 120. 105. 110. 110.  97. 113. 113. 117.\n",
      " 116. 116. 117. 119. 119. 101. 114. 120. 120. 120. 120. 120. 120. 119.\n",
      " 120. 120. 117. 120. 118. 120.  93. 120. 120. 120. 120. 120. 120. 118.\n",
      " 120. 120. 120. 120. 120. 120. 116. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 120. 120.]\n",
      "[0.6125     0.8625     0.88125    0.8625     0.8666667  0.7875\n",
      " 0.8666667  0.87291664 0.8833333  0.8833333  0.8875     0.875\n",
      " 0.87291664 0.875      0.8833333  0.89166665 0.8833333  0.88125\n",
      " 0.87083334 0.87708336 0.87083334 0.86875    0.8833333  0.8645833\n",
      " 0.875      0.89375    0.875      0.88958335 0.87083334 0.8833333\n",
      " 0.89375    0.88958335 0.88125    0.9        0.90833336 0.9145833\n",
      " 0.93125    0.92083335 0.93958336 0.94375    0.95625    0.93125\n",
      " 0.96458334 0.97291666 0.96875    0.9875     0.96458334 0.97291666\n",
      " 0.98333335 0.87083334 0.95625    0.9770833  0.98541665 0.9895833\n",
      " 0.99583334 0.98333335 0.97291666 0.9916667  0.99791664 0.99583334\n",
      " 0.99583334 0.975      0.99375    0.9895833  0.99375    0.92291665\n",
      " 0.98333335 1.         0.99583334 0.99583334 0.99583334 1.\n",
      " 0.99791664 0.99375    0.99791664 0.99791664 0.99791664 0.99791664\n",
      " 0.95208335 0.9895833  0.99583334 0.99791664 0.99791664 1.\n",
      " 1.         1.         1.         1.         0.99791664 0.99583334\n",
      " 1.         1.         0.9916667  0.99583334 0.99791664 1.\n",
      " 0.99583334 0.99375    0.99583334 0.99791664]\n",
      "[109. 110. 112. 109. 112. 108. 111. 111. 110. 112. 110. 111. 111. 111.\n",
      " 112. 111. 111. 112. 111. 110. 111. 111. 111. 111. 112. 111. 111. 111.\n",
      " 110. 112. 112. 113. 112. 115. 115. 115. 115. 116. 116. 118. 117. 118.\n",
      " 119. 117. 118. 118. 120. 120. 107. 115. 120. 119. 119. 120. 118. 110.\n",
      " 120. 120. 120. 120. 112. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.]\n",
      "[0.62916666 0.85833335 0.84583336 0.84583336 0.8666667  0.85\n",
      " 0.87083334 0.87083334 0.8854167  0.87291664 0.8666667  0.8854167\n",
      " 0.86875    0.87291664 0.88125    0.8833333  0.88958335 0.87916666\n",
      " 0.8875     0.88125    0.8833333  0.86875    0.88125    0.8875\n",
      " 0.87083334 0.87708336 0.8666667  0.87708336 0.8854167  0.8833333\n",
      " 0.88125    0.87708336 0.8854167  0.875      0.875      0.87916666\n",
      " 0.86875    0.8833333  0.8666667  0.8833333  0.88125    0.87708336\n",
      " 0.88125    0.875      0.88125    0.875      0.8666667  0.8875\n",
      " 0.8854167  0.87916666 0.88125    0.8958333  0.8854167  0.87916666\n",
      " 0.8854167  0.8875     0.87708336 0.88125    0.87708336 0.8833333\n",
      " 0.87916666 0.87291664 0.87291664 0.89166665 0.87916666 0.89166665\n",
      " 0.875      0.89375    0.88958335 0.8854167  0.88125    0.88125\n",
      " 0.8854167  0.87708336 0.88958335 0.89166665 0.88125    0.8833333\n",
      " 0.875      0.88125    0.8833333  0.8854167  0.88125    0.87916666\n",
      " 0.87708336 0.8875     0.89166665 0.875      0.88958335 0.8833333\n",
      " 0.8958333  0.89375    0.88125    0.87916666 0.88125    0.8854167\n",
      " 0.875      0.8875     0.8833333  0.88125   ]\n",
      "[106. 112. 110. 104. 109. 112. 112. 112. 110. 110. 112. 112. 110. 110.\n",
      " 112. 110. 110. 111. 110. 111. 111. 111. 111. 110. 111. 110. 111. 111.\n",
      " 111. 110. 111. 111. 111. 111. 111. 111. 112. 111. 111. 109. 111. 111.\n",
      " 112. 111. 110. 111. 111. 110. 112. 111. 111. 110. 111. 111. 111. 111.\n",
      " 112. 111. 111. 111. 111. 112. 111. 111. 111. 111. 111. 111. 111. 111.\n",
      " 111. 112. 111. 112. 112. 111. 111. 112. 111. 109. 111. 112. 111. 111.\n",
      " 110. 111. 111. 111. 109. 111. 111. 110. 111. 112. 112. 111. 111. 111.\n",
      " 111. 111.]\n",
      "[0.46666667 0.48333332 0.4875     0.48333332 0.5208333  0.50416666\n",
      " 0.525      0.5125     0.49583334 0.49583334 0.48333332 0.49583334\n",
      " 0.47083333 0.48333332 0.46666667 0.49583334 0.5083333  0.50416666\n",
      " 0.5125     0.45       0.475      0.5        0.5083333  0.50416666\n",
      " 0.50416666 0.5125     0.50416666 0.49583334 0.4625     0.4625\n",
      " 0.475      0.50416666 0.49166667 0.50416666 0.475      0.4875\n",
      " 0.45416668 0.49166667 0.4875     0.5        0.45416668 0.48333332\n",
      " 0.4625     0.51666665 0.49583334 0.49166667 0.47916666 0.5\n",
      " 0.5125     0.49166667 0.5208333  0.4875     0.475      0.52916664\n",
      " 0.48333332 0.4875     0.5083333  0.45416668 0.5        0.49583334\n",
      " 0.48333332 0.47916666 0.49166667 0.49166667 0.49166667 0.47916666\n",
      " 0.44583333 0.48333332 0.48333332 0.5083333  0.475      0.46666667\n",
      " 0.51666665 0.49583334 0.475      0.475      0.48333332 0.475\n",
      " 0.5        0.5083333  0.5        0.48333332 0.5        0.47916666\n",
      " 0.5125     0.47916666 0.49166667 0.47083333 0.48333332 0.48333332\n",
      " 0.45833334 0.45       0.44166666 0.45416668 0.50416666 0.4625\n",
      " 0.5083333  0.4875     0.47083333 0.47083333]\n",
      "[62. 62. 58. 62. 58. 58. 62. 62. 58. 62. 62. 62. 58. 62. 62. 62. 58. 58.\n",
      " 62. 58. 62. 62. 62. 58. 58. 58. 58. 58. 58. 58. 58. 58. 58. 62. 62. 58.\n",
      " 58. 58. 62. 62. 58. 58. 58. 62. 58. 62. 62. 62. 58. 62. 58. 58. 58. 62.\n",
      " 62. 62. 62. 62. 58. 62. 58. 58. 62. 62. 58. 58. 58. 58. 58. 58. 58. 58.\n",
      " 62. 58. 58. 62. 62. 62. 62. 62. 58. 58. 58. 62. 58. 58. 62. 62. 62. 62.\n",
      " 62. 58. 62. 58. 58. 62. 58. 58. 58. 58.]\n",
      "[0.68333334 0.82708335 0.8625     0.8645833  0.875      0.875\n",
      " 0.8645833  0.8625     0.87708336 0.8666667  0.87708336 0.87708336\n",
      " 0.87291664 0.87916666 0.87291664 0.875      0.89375    0.87708336\n",
      " 0.875      0.8833333  0.88125    0.87708336 0.87083334 0.87083334\n",
      " 0.875      0.88125    0.87291664 0.86875    0.87708336 0.87916666\n",
      " 0.8875     0.86875    0.86875    0.875      0.8833333  0.87916666\n",
      " 0.87708336 0.88125    0.89166665 0.87916666 0.8875     0.87916666\n",
      " 0.87916666 0.8875     0.89375    0.88125    0.88958335 0.8833333\n",
      " 0.88125    0.875      0.89166665 0.87916666 0.8833333  0.88958335\n",
      " 0.8854167  0.87291664 0.8854167  0.87291664 0.8854167  0.88958335\n",
      " 0.88125    0.87708336 0.88958335 0.87083334 0.88125    0.89375\n",
      " 0.89166665 0.8958333  0.89166665 0.90625    0.8979167  0.8958333\n",
      " 0.90625    0.91875    0.92083335 0.93333334 0.9583333  0.9375\n",
      " 0.95       0.9604167  0.97083336 0.97291666 0.975      0.9458333\n",
      " 0.98541665 0.99375    0.98541665 0.98125    0.99375    0.96875\n",
      " 0.9895833  0.98541665 0.99583334 0.99583334 0.9791667  0.99375\n",
      " 0.6479167  0.49583334 0.49583334 0.49583334]\n",
      "[107. 112. 112. 107. 111. 110. 110. 110. 111. 111. 110. 112. 110. 111.\n",
      " 110. 112. 111. 110. 112. 111. 111. 111. 111. 111. 111. 111. 109. 111.\n",
      " 112. 111. 111. 111. 111. 111. 111. 112. 112. 111. 112. 111. 112. 111.\n",
      " 110. 111. 112. 111. 112. 111. 112. 110. 110. 111. 112. 112. 111. 111.\n",
      " 111. 110. 112. 110. 111. 112. 112. 111. 112. 109. 112. 113. 113. 113.\n",
      " 108. 114. 114. 115. 115. 116. 116. 117. 118. 119. 119. 118. 117. 119.\n",
      " 120. 118. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.  62.  62.\n",
      "  62.  62.]\n",
      "[0.50416666 0.82708335 0.8520833  0.86041665 0.86875    0.87291664\n",
      " 0.86875    0.85       0.8645833  0.85625    0.8833333  0.86875\n",
      " 0.86875    0.8520833  0.8625     0.8520833  0.85625    0.86041665\n",
      " 0.87291664 0.87083334 0.875      0.86041665 0.8625     0.86875\n",
      " 0.8375     0.8645833  0.8645833  0.8645833  0.87708336 0.86875\n",
      " 0.87916666 0.875      0.8645833  0.8875     0.875      0.88125\n",
      " 0.8833333  0.8875     0.88125    0.8833333  0.8875     0.86875\n",
      " 0.8833333  0.8854167  0.87291664 0.88958335 0.87708336 0.88958335\n",
      " 0.8854167  0.8833333  0.87916666 0.87708336 0.8854167  0.88958335\n",
      " 0.8854167  0.89166665 0.8833333  0.86875    0.88125    0.8854167\n",
      " 0.8875     0.8833333  0.8833333  0.8854167  0.8833333  0.87708336\n",
      " 0.87708336 0.89166665 0.8645833  0.87916666 0.88958335 0.87916666\n",
      " 0.88958335 0.88958335 0.8666667  0.89375    0.8854167  0.8875\n",
      " 0.87916666 0.8875     0.88958335 0.8833333  0.8979167  0.8854167\n",
      " 0.87916666 0.87708336 0.88958335 0.89166665 0.87291664 0.88958335\n",
      " 0.8875     0.89375    0.8833333  0.87083334 0.89166665 0.88958335\n",
      " 0.87916666 0.89375    0.88958335 0.87916666]\n",
      "[ 85. 109. 105. 111. 111. 111. 111. 109. 112. 110. 111. 111. 111. 110.\n",
      " 111. 109. 112. 111. 111. 112. 110. 112. 110. 110. 110. 112. 110. 110.\n",
      " 110. 110. 111. 111. 111. 110. 111. 111. 111. 110. 111. 110. 111. 111.\n",
      " 112. 110. 111. 109. 111. 111. 112. 110. 111. 111. 112. 112. 111. 109.\n",
      " 111. 112. 111. 111. 111. 111. 111. 111. 112. 111. 111. 110. 111. 111.\n",
      " 110. 111. 111. 111. 111. 111. 112. 112. 110. 111. 112. 112. 111. 111.\n",
      " 112. 110. 112. 111. 112. 111. 111. 111. 111. 112. 111. 112. 111. 111.\n",
      " 111. 111.]\n",
      "[0.68125    0.8520833  0.8541667  0.875      0.85625    0.87083334\n",
      " 0.88125    0.88958335 0.86875    0.86875    0.88125    0.86875\n",
      " 0.88958335 0.8875     0.87916666 0.89166665 0.8833333  0.8979167\n",
      " 0.9125     0.91041666 0.925      0.9583333  0.97083336 0.98541665\n",
      " 0.99375    0.99583334 0.99791664 1.         1.         0.99375\n",
      " 1.         0.99791664 0.99791664 0.99791664 0.99583334 0.99791664\n",
      " 0.99791664 0.5375     0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[112. 107. 111. 110. 110. 109. 111. 109. 110. 111. 110. 111. 109. 111.\n",
      " 111. 111. 111. 112. 114. 115. 115. 118. 120. 120. 120. 119. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.]\n",
      "[0.65208334 0.84166664 0.875      0.85833335 0.85833335 0.87083334\n",
      " 0.86041665 0.8833333  0.87083334 0.87708336 0.87083334 0.87708336\n",
      " 0.8833333  0.88958335 0.8854167  0.88958335 0.89166665 0.87916666\n",
      " 0.88125    0.87916666 0.8666667  0.8833333  0.88958335 0.8833333\n",
      " 0.8854167  0.8875     0.8875     0.87916666 0.8833333  0.89375\n",
      " 0.8958333  0.8854167  0.90416664 0.8979167  0.8854167  0.8958333\n",
      " 0.88958335 0.88125    0.88125    0.87916666 0.8854167  0.8875\n",
      " 0.89375    0.8833333  0.8979167  0.88958335 0.8833333  0.89375\n",
      " 0.89375    0.89166665 0.89375    0.8958333  0.8875     0.8854167\n",
      " 0.8875     0.8854167  0.89166665 0.8854167  0.90208334 0.8958333\n",
      " 0.90208334 0.89166665 0.9        0.89166665 0.87083334 0.8854167\n",
      " 0.90625    0.8958333  0.8979167  0.9        0.89375    0.8979167\n",
      " 0.90416664 0.89166665 0.90416664 0.90416664 0.90208334 0.89166665\n",
      " 0.89166665 0.89166665 0.90833336 0.90208334 0.90625    0.88125\n",
      " 0.88958335 0.8958333  0.8958333  0.9125     0.9166667  0.925\n",
      " 0.93958336 0.95416665 0.97291666 0.98125    0.98125    0.67291665\n",
      " 0.6        0.82916665 0.86041665 0.8625    ]\n",
      "[108. 111. 112. 111. 112. 109. 112. 111. 112. 113. 114. 110. 113. 112.\n",
      " 113. 112. 109. 113. 111. 113. 110. 113. 111. 111. 110. 110. 111. 111.\n",
      " 114. 113. 113. 112. 114. 113. 113. 113. 113. 112. 114. 114. 114. 113.\n",
      " 111. 112. 112. 111. 113. 113. 114. 113. 111. 110. 114. 114. 113. 113.\n",
      " 113. 109. 110. 114. 109. 114. 114. 110. 113. 113. 113. 112. 113. 114.\n",
      " 113. 111. 112. 112. 112. 113. 112. 111. 113. 113. 113. 111. 113. 112.\n",
      " 110. 110. 113. 113. 113. 116. 116. 116. 118. 120.  94.  68.  95. 110.\n",
      " 110. 110.]\n",
      "[0.675      0.8354167  0.87291664 0.9166667  0.95208335 0.9791667\n",
      " 0.93958336 0.9375     0.96458334 0.98125    1.         1.\n",
      " 0.99791664 0.99791664 0.99583334 0.99791664 0.9895833  0.99791664\n",
      " 1.         0.99791664 1.         0.99791664 0.99375    0.99583334\n",
      " 0.98541665 0.99375    0.99791664 0.99791664 1.         0.99791664\n",
      " 0.99791664 0.99791664 0.99791664 0.99791664 1.         0.99791664\n",
      " 1.         0.99791664 0.99375    1.         0.99791664 1.\n",
      " 1.         0.99791664 0.99791664 0.99791664 1.         0.99583334\n",
      " 0.99583334 0.99583334 0.99791664 0.99791664 0.99791664 1.\n",
      " 1.         0.99791664 0.99791664 0.99791664 0.99583334 0.73541665\n",
      " 0.49166667 0.4875     0.47916666 0.58125    0.7104167  0.73333335\n",
      " 0.78125    0.79791665 0.8333333  0.7895833  0.8625     0.86875\n",
      " 0.87083334 0.8625     0.8875     0.8625     0.87916666 0.8854167\n",
      " 0.8645833  0.88958335 0.8854167  0.875      0.8666667  0.88125\n",
      " 0.87916666 0.8875     0.86875    0.86875    0.88125    0.88958335\n",
      " 0.89166665 0.875      0.88125    0.87916666 0.8541667  0.87083334\n",
      " 0.8625     0.8833333  0.8875     0.8833333 ]\n",
      "[111. 109. 113. 114. 118. 120. 114. 113. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 118. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120.  62.  62.  62.  69.  76.  99. 101. 105.  97. 111. 111.\n",
      " 109. 111. 108. 111. 110. 110. 111. 110. 110. 111. 111. 110. 111. 111.\n",
      " 111. 110. 111. 112. 109. 111. 111. 111. 111. 111. 112. 107. 111. 111.\n",
      " 112. 111.]\n",
      "[0.55625    0.775      0.825      0.86875    0.85833335 0.86875\n",
      " 0.88958335 0.80625    0.8666667  0.89166665 0.9        0.8979167\n",
      " 0.94166666 0.89375    0.9145833  0.87916666 0.9458333  0.9125\n",
      " 0.96458334 0.9375     0.9583333  0.975      0.9604167  0.90833336\n",
      " 0.95       0.9875     0.975      0.96458334 0.9916667  0.975\n",
      " 0.98541665 0.9791667  0.9916667  0.99583334 0.9895833  0.99375\n",
      " 0.99791664 0.99583334 0.99583334 0.99583334 0.99375    0.6875\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[ 97. 108. 111. 112. 111. 109. 113. 110. 112. 113. 111. 114.  93. 117.\n",
      " 117. 117. 119. 117. 117. 120. 117. 120. 111. 119. 120. 120. 120. 120.\n",
      " 120. 112. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 117.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.]\n",
      "Number of layers: 18\n",
      "[0.5083333  0.76875    0.81666666 0.85833335 0.8541667  0.87291664\n",
      " 0.8520833  0.875      0.84791666 0.87291664 0.8666667  0.8666667\n",
      " 0.8645833  0.86041665 0.8625     0.8625     0.86875    0.8645833\n",
      " 0.85625    0.85833335 0.87083334 0.8520833  0.87083334 0.86041665\n",
      " 0.8666667  0.8645833  0.85833335 0.86875    0.8645833  0.87708336\n",
      " 0.86875    0.86041665 0.8520833  0.86875    0.8625     0.86875\n",
      " 0.85833335 0.85833335 0.87291664 0.87291664 0.8541667  0.86875\n",
      " 0.8666667  0.86875    0.8625     0.875      0.86041665 0.87291664\n",
      " 0.87083334 0.8666667  0.87083334 0.85625    0.8666667  0.8625\n",
      " 0.86875    0.86041665 0.8645833  0.87083334 0.8645833  0.875\n",
      " 0.86875    0.8625     0.86041665 0.8645833  0.87291664 0.8666667\n",
      " 0.8666667  0.85833335 0.8666667  0.85833335 0.8645833  0.86875\n",
      " 0.85833335 0.85625    0.8645833  0.8625     0.86875    0.87291664\n",
      " 0.8645833  0.85833335 0.8645833  0.87083334 0.8625     0.8625\n",
      " 0.86875    0.87291664 0.86041665 0.8645833  0.86041665 0.8645833\n",
      " 0.8625     0.86041665 0.86875    0.87916666 0.85625    0.86875\n",
      " 0.8625     0.87083334 0.8645833  0.87291664]\n",
      "[102. 105. 109. 112. 107. 107. 105. 109. 109. 111. 110. 108. 112. 106.\n",
      " 112. 112. 111. 111. 110. 110. 110. 110. 112. 112. 110. 106. 111. 110.\n",
      " 111. 111. 112. 112. 112. 111. 112. 111. 112. 111. 112. 111. 112. 111.\n",
      " 111. 107. 110. 109. 111. 111. 112. 112. 109. 111. 112. 111. 111. 110.\n",
      " 111. 110. 112. 106. 111. 113. 110. 110. 112. 112. 111. 113. 112. 111.\n",
      " 111. 112. 111. 112. 110. 112. 112. 112. 111. 112. 110. 108. 112. 112.\n",
      " 111. 111. 112. 112. 111. 112. 110. 112. 111. 110. 109. 112. 111. 110.\n",
      " 111. 111.]\n",
      "[0.62291664 0.84583336 0.8625     0.85833335 0.85833335 0.84375\n",
      " 0.85833335 0.8645833  0.87916666 0.87916666 0.89166665 0.87708336\n",
      " 0.8979167  0.90625    0.90208334 0.9166667  0.9291667  0.94375\n",
      " 0.9625     0.9479167  0.86875    0.9458333  0.96458334 0.98333335\n",
      " 0.9791667  0.9875     0.9895833  0.9583333  0.9895833  0.99375\n",
      " 0.9916667  0.99375    0.99583334 0.9895833  0.99375    0.99375\n",
      " 0.9875     0.99375    0.99583334 0.98541665 0.99583334 0.99791664\n",
      " 1.         0.99791664 0.99791664 1.         0.9895833  0.99583334\n",
      " 1.         0.99583334 0.99583334 0.99791664 1.         0.99791664\n",
      " 1.         1.         0.99791664 1.         0.99791664 0.99583334\n",
      " 0.6479167  0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[109. 109. 107. 111. 110. 110. 112. 111. 112. 110. 112. 111. 112. 114.\n",
      " 114. 115. 118. 119. 118.  76. 116. 117. 120. 120. 120. 120. 108. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 117. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  62.  62.]\n",
      "[0.70208335 0.8020833  0.8833333  0.8979167  0.90833336 0.90416664\n",
      " 0.9270833  0.975      0.9875     0.92291665 0.9875     0.99375\n",
      " 0.99375    0.99375    0.99583334 0.99791664 0.99791664 0.99583334\n",
      " 0.99791664 0.99791664 0.9791667  0.99791664 0.99791664 0.99791664\n",
      " 0.99791664 0.99583334 0.99791664 0.99791664 0.99791664 0.99791664\n",
      " 0.99791664 0.99791664 0.99791664 0.99375    0.9916667  0.99791664\n",
      " 0.99791664 0.99375    0.99375    0.99791664 0.99791664 0.99791664\n",
      " 0.99791664 1.         0.99791664 0.99583334 0.99791664 0.99791664\n",
      " 0.99791664 0.99791664 1.         0.99791664 0.99583334 1.\n",
      " 1.         1.         1.         1.         1.         0.9916667\n",
      " 0.99791664 0.99375    0.99791664 0.99583334 1.         1.\n",
      " 1.         1.         1.         0.99583334 0.99583334 0.99791664\n",
      " 1.         1.         1.         0.99583334 1.         1.\n",
      " 0.99791664 0.99791664 1.         1.         0.99791664 0.99375\n",
      " 1.         1.         1.         0.99791664 0.99583334 1.\n",
      " 1.         1.         1.         1.         1.         0.99583334\n",
      " 0.59375    0.49583334 0.49583334 0.49583334]\n",
      "[110. 109. 113. 114. 117. 117. 120. 120. 118. 120. 119. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.  62.  62.\n",
      "  62.  62.]\n",
      "[0.60625    0.79375    0.8625     0.8625     0.87291664 0.87291664\n",
      " 0.8520833  0.8875     0.86875    0.88958335 0.85       0.8875\n",
      " 0.8833333  0.8833333  0.87708336 0.87708336 0.875      0.87916666\n",
      " 0.8833333  0.87708336 0.8645833  0.88125    0.87291664 0.8833333\n",
      " 0.87291664 0.87708336 0.87916666 0.87083334 0.875      0.87916666\n",
      " 0.88125    0.89166665 0.88125    0.8833333  0.87708336 0.8666667\n",
      " 0.89166665 0.87291664 0.875      0.8875     0.89375    0.88125\n",
      " 0.875      0.87916666 0.88958335 0.8854167  0.9        0.875\n",
      " 0.875      0.86875    0.87916666 0.8875     0.8854167  0.8875\n",
      " 0.87708336 0.875      0.875      0.875      0.86875    0.8875\n",
      " 0.8666667  0.8854167  0.8875     0.87916666 0.87083334 0.88125\n",
      " 0.875      0.8875     0.8854167  0.8875     0.88958335 0.8875\n",
      " 0.8875     0.8875     0.875      0.875      0.87708336 0.875\n",
      " 0.8854167  0.8854167  0.8854167  0.87291664 0.8875     0.87291664\n",
      " 0.8625     0.8833333  0.87291664 0.89166665 0.875      0.8875\n",
      " 0.875      0.87916666 0.89375    0.8625     0.8875     0.87916666\n",
      " 0.87291664 0.8854167  0.87708336 0.8875    ]\n",
      "[ 94. 109. 102. 111. 111.  77. 111. 111. 111. 109. 110. 111. 110. 111.\n",
      " 110. 109. 110. 111. 111. 111. 111. 111. 112. 111. 112. 111. 109. 111.\n",
      " 111. 111. 111. 111. 111. 111. 112. 111. 108. 109. 110. 111. 110. 111.\n",
      " 112. 111. 111. 111. 111. 110. 110. 111. 110. 111. 111. 111. 111. 111.\n",
      " 107. 110. 110. 111. 110. 110. 111. 111. 110. 111. 111. 111. 111. 111.\n",
      " 111. 111. 111. 111. 111. 111. 111. 110. 111. 112. 111. 111. 110. 111.\n",
      " 110. 111. 111. 111. 111. 111. 110. 111. 108. 111. 111. 111. 111. 111.\n",
      " 111. 111.]\n",
      "[0.7        0.85       0.8375     0.85       0.8666667  0.8666667\n",
      " 0.88125    0.88125    0.88125    0.87916666 0.875      0.87083334\n",
      " 0.8875     0.9291667  0.96666664 0.94166666 0.99375    0.9895833\n",
      " 0.99791664 1.         0.99583334 0.99375    0.99791664 0.99583334\n",
      " 0.99791664 0.99791664 1.         1.         1.         1.\n",
      " 0.6145833  0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[107. 107. 109. 109. 111. 112. 112. 111. 110. 112. 110. 113. 113. 116.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.]\n",
      "[0.68333334 0.85       0.85       0.87083334 0.8833333  0.88125\n",
      " 0.8875     0.8854167  0.87916666 0.88125    0.8854167  0.87708336\n",
      " 0.86875    0.87708336 0.87916666 0.89166665 0.87708336 0.8666667\n",
      " 0.8833333  0.8854167  0.8854167  0.88125    0.88958335 0.8958333\n",
      " 0.875      0.87291664 0.87916666 0.87291664 0.88125    0.89375\n",
      " 0.8854167  0.88125    0.89166665 0.875      0.88125    0.87291664\n",
      " 0.86041665 0.875      0.88125    0.875      0.8833333  0.8875\n",
      " 0.875      0.88125    0.87708336 0.87291664 0.87916666 0.8666667\n",
      " 0.8854167  0.8854167  0.8875     0.9        0.87291664 0.8875\n",
      " 0.87083334 0.88125    0.8875     0.87708336 0.89166665 0.875\n",
      " 0.8854167  0.875      0.88125    0.87916666 0.87708336 0.8833333\n",
      " 0.8854167  0.8666667  0.8875     0.88958335 0.8833333  0.88125\n",
      " 0.8833333  0.88958335 0.875      0.88958335 0.88125    0.88125\n",
      " 0.8875     0.875      0.87916666 0.87708336 0.87708336 0.88958335\n",
      " 0.8833333  0.8833333  0.88125    0.8854167  0.8833333  0.8854167\n",
      " 0.8833333  0.87083334 0.875      0.87916666 0.8854167  0.8833333\n",
      " 0.87916666 0.87708336 0.87083334 0.8875    ]\n",
      "[111. 103. 110. 111. 111. 111. 110. 110. 110. 110. 110. 111. 111. 111.\n",
      " 111. 111. 111. 111. 111. 112. 111. 111. 111. 111. 110. 111. 108. 111.\n",
      " 111. 111. 111. 111. 112. 111. 110. 111. 111. 111. 111. 108. 112. 111.\n",
      " 110. 112. 111. 110. 109. 111. 111. 111. 111. 111. 109. 111. 112. 111.\n",
      " 111. 111. 111. 110. 111. 111. 111. 111. 112. 110. 111. 111. 111. 111.\n",
      " 109. 111. 111. 111. 111. 111. 111. 111. 111. 111. 111. 111. 111. 110.\n",
      " 111. 111. 111. 111. 111. 110. 110. 110. 110. 111. 111. 111. 111. 110.\n",
      " 111. 111.]\n",
      "[0.52916664 0.78333336 0.8645833  0.84166664 0.8541667  0.8875\n",
      " 0.88958335 0.90416664 0.92291665 0.94166666 0.76458335 0.85625\n",
      " 0.87083334 0.8625     0.86875    0.8645833  0.87708336 0.86875\n",
      " 0.875      0.8666667  0.88125    0.8875     0.8645833  0.88958335\n",
      " 0.86041665 0.87708336 0.8625     0.88125    0.8833333  0.88125\n",
      " 0.875      0.87083334 0.87916666 0.8833333  0.87291664 0.8833333\n",
      " 0.88125    0.88958335 0.9        0.89166665 0.86875    0.875\n",
      " 0.8645833  0.8833333  0.8875     0.88958335 0.87291664 0.87916666\n",
      " 0.875      0.87916666 0.875      0.8875     0.87708336 0.8958333\n",
      " 0.8645833  0.88958335 0.8833333  0.87083334 0.8854167  0.875\n",
      " 0.87916666 0.87708336 0.8854167  0.86875    0.87708336 0.87291664\n",
      " 0.89166665 0.8875     0.87291664 0.87291664 0.8833333  0.8875\n",
      " 0.8833333  0.8875     0.8875     0.87916666 0.88125    0.87291664\n",
      " 0.8958333  0.8833333  0.8833333  0.8854167  0.8875     0.88125\n",
      " 0.87083334 0.86041665 0.89166665 0.8979167  0.875      0.87916666\n",
      " 0.8875     0.8854167  0.8833333  0.89166665 0.86875    0.8875\n",
      " 0.8833333  0.87916666 0.8833333  0.88125   ]\n",
      "[104. 112. 110. 109. 111. 112. 114. 113. 116. 119. 108. 110. 112. 111.\n",
      " 111. 111. 112. 110. 112. 110. 110. 110. 110. 111. 110. 111. 111. 111.\n",
      " 111. 108. 111. 110. 111. 111. 111. 111. 111. 112. 111. 111. 111. 111.\n",
      " 111. 111. 111. 109. 112. 110. 111. 111. 112. 110. 111. 111. 111. 110.\n",
      " 111. 111. 109. 111. 111. 112. 111. 109. 109. 111. 111. 109. 110. 111.\n",
      " 111. 111. 111. 111. 111. 112. 111. 111. 111. 111. 111. 111. 110. 111.\n",
      " 111. 111. 111. 111. 110. 112. 111. 111. 111. 110. 111. 111. 111. 111.\n",
      " 111. 111.]\n",
      "[0.66875    0.8354167  0.8041667  0.8875     0.9270833  0.9625\n",
      " 0.9145833  0.98333335 0.98541665 0.9625     0.9895833  0.99583334\n",
      " 0.99791664 0.99375    0.99375    0.99375    0.99791664 0.99791664\n",
      " 0.99791664 0.99583334 0.84583336 0.51458335 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[111. 111. 109. 112. 114. 118. 120. 120. 119. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120.  78.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.]\n",
      "[0.69375    0.8354167  0.84375    0.8666667  0.89166665 0.85625\n",
      " 0.9291667  0.96458334 0.96875    0.99583334 0.99375    0.99583334\n",
      " 0.62083334 0.7375     0.75625    0.77708334 0.825      0.87083334\n",
      " 0.8854167  0.88958335 0.8666667  0.89166665 0.875      0.8854167\n",
      " 0.8833333  0.8625     0.87083334 0.8854167  0.8854167  0.8645833\n",
      " 0.88125    0.87291664 0.8833333  0.87708336 0.8666667  0.875\n",
      " 0.8854167  0.8666667  0.8625     0.88125    0.87708336 0.87916666\n",
      " 0.875      0.86041665 0.875      0.89166665 0.8854167  0.8875\n",
      " 0.88958335 0.87083334 0.86875    0.89166665 0.88125    0.8833333\n",
      " 0.87291664 0.875      0.88958335 0.87708336 0.87083334 0.8854167\n",
      " 0.8854167  0.8854167  0.87916666 0.86875    0.8875     0.87916666\n",
      " 0.8854167  0.8854167  0.87083334 0.86875    0.8833333  0.88125\n",
      " 0.875      0.875      0.88125    0.87708336 0.875      0.88958335\n",
      " 0.87708336 0.8625     0.8833333  0.8854167  0.88958335 0.88958335\n",
      " 0.8854167  0.8833333  0.87916666 0.88125    0.87916666 0.89375\n",
      " 0.87083334 0.88958335 0.87916666 0.88958335 0.875      0.88125\n",
      " 0.87916666 0.8875     0.8833333  0.8666667 ]\n",
      "[107. 107. 112. 113. 113. 113. 116. 111. 120. 120. 120. 117.  96.  98.\n",
      " 103. 107. 107. 111. 110. 111. 109. 110. 111. 112. 109. 111. 111. 111.\n",
      " 110. 111. 111. 110. 112. 111. 110. 112. 111. 103. 111. 112. 111. 111.\n",
      " 111. 111. 112. 111. 111. 111. 111. 111. 110. 111. 111. 111. 111. 112.\n",
      " 111. 110. 111. 111. 111. 112. 110. 112. 110. 111. 111. 111. 111. 111.\n",
      " 111. 111. 109. 111. 112. 109. 110. 111. 112. 111. 111. 111. 111. 111.\n",
      " 112. 111. 110. 112. 111. 111. 111. 111. 112. 112. 111. 111. 112. 111.\n",
      " 111. 111.]\n",
      "[0.47916666 0.48541668 0.54375    0.59583336 0.7270833  0.76458335\n",
      " 0.78541666 0.8625     0.8645833  0.85625    0.8354167  0.85833335\n",
      " 0.86041665 0.87083334 0.86875    0.8625     0.85833335 0.84791666\n",
      " 0.8520833  0.8645833  0.8666667  0.87083334 0.8645833  0.8541667\n",
      " 0.8645833  0.8625     0.8645833  0.8666667  0.8666667  0.85\n",
      " 0.8645833  0.8541667  0.84375    0.86875    0.87083334 0.87083334\n",
      " 0.8625     0.8625     0.84791666 0.8666667  0.86041665 0.8625\n",
      " 0.8666667  0.8666667  0.87083334 0.86041665 0.87291664 0.86041665\n",
      " 0.8666667  0.87291664 0.8625     0.8645833  0.85625    0.875\n",
      " 0.8645833  0.8625     0.8520833  0.87083334 0.8520833  0.8645833\n",
      " 0.8625     0.8666667  0.8541667  0.85833335 0.84375    0.86041665\n",
      " 0.86041665 0.86875    0.87291664 0.8625     0.8666667  0.8666667\n",
      " 0.8666667  0.86875    0.875      0.85       0.8645833  0.8666667\n",
      " 0.87291664 0.86875    0.85625    0.85625    0.86041665 0.8541667\n",
      " 0.8645833  0.8645833  0.86875    0.8645833  0.86041665 0.875\n",
      " 0.86041665 0.87291664 0.86041665 0.87083334 0.8625     0.86041665\n",
      " 0.8666667  0.85625    0.87083334 0.87083334]\n",
      "[ 62.  62.  59.  92. 100. 103. 107. 111. 109. 106. 106. 111. 110. 112.\n",
      " 110. 111. 108. 108. 111. 112. 110. 108. 111. 106. 112. 111. 112. 112.\n",
      " 112. 111. 111. 111. 111. 111. 110. 112. 104. 109. 107. 111. 111. 111.\n",
      " 110. 110. 113. 109. 111. 110. 110. 111. 107. 110. 112. 111. 111. 110.\n",
      " 112. 110. 111. 112. 112. 111. 111. 108. 110. 111. 110. 113. 110. 111.\n",
      " 110. 110. 112. 111. 109. 111. 110. 111. 113. 110. 112. 106. 111. 110.\n",
      " 112. 111. 111. 112. 110. 112. 112. 110. 112. 112. 112. 109. 111. 112.\n",
      " 111. 108.]\n",
      "Number of layers: 19\n",
      "[0.7104167  0.8666667  0.8625     0.8833333  0.87708336 0.87291664\n",
      " 0.8625     0.8958333  0.8854167  0.86041665 0.87291664 0.8833333\n",
      " 0.86041665 0.8625     0.8645833  0.875      0.875      0.8645833\n",
      " 0.87708336 0.87916666 0.8645833  0.89166665 0.87708336 0.86875\n",
      " 0.87083334 0.87083334 0.8854167  0.87708336 0.8958333  0.8854167\n",
      " 0.85       0.87291664 0.8958333  0.87916666 0.87291664 0.87708336\n",
      " 0.87083334 0.86875    0.8666667  0.8854167  0.8875     0.87291664\n",
      " 0.8833333  0.87083334 0.8645833  0.87291664 0.88125    0.87916666\n",
      " 0.88125    0.875      0.87916666 0.88125    0.87916666 0.8666667\n",
      " 0.87708336 0.8854167  0.89375    0.86041665 0.87708336 0.88958335\n",
      " 0.87916666 0.88125    0.88125    0.8854167  0.8854167  0.8833333\n",
      " 0.875      0.8833333  0.8833333  0.8833333  0.8645833  0.87291664\n",
      " 0.8854167  0.8645833  0.8833333  0.8875     0.8854167  0.88125\n",
      " 0.875      0.875      0.87916666 0.89375    0.86875    0.875\n",
      " 0.8875     0.8666667  0.87916666 0.87291664 0.8875     0.87916666\n",
      " 0.87916666 0.87708336 0.8875     0.8833333  0.8833333  0.88125\n",
      " 0.8833333  0.85833335 0.87291664 0.87916666]\n",
      "[109. 111. 111. 112. 111. 111. 112. 112. 112. 110. 111. 109. 111. 111.\n",
      " 111. 111. 110. 111. 111. 111. 111. 112. 109. 110. 112. 111. 111. 111.\n",
      " 111. 112. 111. 112. 111. 111. 111. 111. 111. 112. 111. 112. 111. 111.\n",
      " 111. 111. 111. 111. 111. 112. 112. 111. 111. 112. 111. 110. 112. 111.\n",
      " 111. 112. 111. 112. 111. 111. 112. 112. 112. 111. 111. 112. 112. 112.\n",
      " 110. 111. 111. 108. 110. 112. 110. 111. 111. 111. 111. 112. 111. 111.\n",
      " 112. 112. 112. 112. 112. 112. 112. 112. 112. 112. 112. 111. 112. 111.\n",
      " 111. 112.]\n",
      "[0.7395833  0.88125    0.8375     0.87083334 0.8833333  0.89375\n",
      " 0.8875     0.875      0.875      0.8625     0.8875     0.88958335\n",
      " 0.8979167  0.88125    0.88958335 0.9        0.8854167  0.875\n",
      " 0.87291664 0.89166665 0.87916666 0.8979167  0.88958335 0.8854167\n",
      " 0.9        0.89375    0.90625    0.88125    0.88958335 0.89166665\n",
      " 0.89375    0.89375    0.90208334 0.89166665 0.87916666 0.9\n",
      " 0.8854167  0.8958333  0.8854167  0.8854167  0.89375    0.8979167\n",
      " 0.9        0.8979167  0.8979167  0.9        0.8833333  0.9\n",
      " 0.90208334 0.8958333  0.89375    0.8875     0.89375    0.90208334\n",
      " 0.9        0.9        0.88958335 0.8875     0.9        0.8958333\n",
      " 0.8875     0.8854167  0.89166665 0.90625    0.89375    0.89166665\n",
      " 0.88958335 0.89166665 0.8958333  0.8979167  0.8979167  0.8854167\n",
      " 0.8979167  0.90416664 0.90208334 0.90208334 0.9        0.90208334\n",
      " 0.88958335 0.88958335 0.8958333  0.89375    0.8958333  0.89166665\n",
      " 0.8854167  0.8979167  0.88125    0.90208334 0.8979167  0.8979167\n",
      " 0.89166665 0.90416664 0.90208334 0.89166665 0.8958333  0.90208334\n",
      " 0.89166665 0.89375    0.8979167  0.89375   ]\n",
      "[113. 111. 109. 114. 113. 112. 113. 114. 109. 112. 113. 112. 111. 113.\n",
      " 114. 113. 114. 113. 113. 113. 110. 110. 114. 110. 113. 110. 111. 112.\n",
      " 109. 113. 112. 113. 110. 113. 114. 113. 114. 110. 112. 111. 111. 112.\n",
      " 113. 113. 113. 110. 113. 113. 111. 113. 112. 113. 112. 109. 114. 113.\n",
      " 112. 113. 112. 113. 113. 113. 114. 113. 113. 112. 113. 112. 113. 111.\n",
      " 112. 114. 112. 111. 110. 113. 113. 112. 113. 113. 113. 111. 109. 111.\n",
      " 112. 112. 111. 111. 111. 113. 114. 113. 114. 113. 112. 113. 111. 113.\n",
      " 111. 113.]\n",
      "[0.6666667  0.84583336 0.85625    0.87708336 0.8520833  0.87708336\n",
      " 0.89166665 0.9145833  0.90833336 0.93958336 0.94375    0.975\n",
      " 0.95625    0.5083333  0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[111. 105. 109. 111. 112. 112. 113. 108. 111. 118. 116. 120.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  62.  62.]\n",
      "[0.50416666 0.5395833  0.50416666 0.525      0.5083333  0.47916666\n",
      " 0.51458335 0.475      0.4875     0.48333332 0.5125     0.7270833\n",
      " 0.8520833  0.86041665 0.85       0.8520833  0.8645833  0.85625\n",
      " 0.87083334 0.8520833  0.87291664 0.87083334 0.8625     0.8645833\n",
      " 0.85833335 0.85833335 0.86041665 0.8625     0.84791666 0.86875\n",
      " 0.85625    0.8875     0.85833335 0.8666667  0.85833335 0.8520833\n",
      " 0.8645833  0.87291664 0.8666667  0.8645833  0.8520833  0.86875\n",
      " 0.87083334 0.8645833  0.8625     0.8625     0.86875    0.8645833\n",
      " 0.84791666 0.8645833  0.85625    0.8625     0.87708336 0.8625\n",
      " 0.8625     0.87916666 0.8666667  0.85833335 0.87291664 0.87083334\n",
      " 0.8625     0.8666667  0.875      0.85625    0.86041665 0.8666667\n",
      " 0.87708336 0.86041665 0.8666667  0.87291664 0.8645833  0.85625\n",
      " 0.86875    0.86875    0.86875    0.8666667  0.8541667  0.86875\n",
      " 0.86041665 0.86041665 0.86041665 0.8625     0.87083334 0.87708336\n",
      " 0.8666667  0.86875    0.85833335 0.85833335 0.87291664 0.85833335\n",
      " 0.85833335 0.87083334 0.87291664 0.87291664 0.875      0.8645833\n",
      " 0.8625     0.8666667  0.87708336 0.86875   ]\n",
      "[ 62.  58.  58.  62.  58.  58.  62.  58.  58.  58.  62. 107. 109. 111.\n",
      " 111. 108. 112. 110. 110. 112. 108. 112. 111. 111. 111. 111. 106. 113.\n",
      " 111. 110. 111. 110. 110. 110. 108. 112. 111. 111. 112. 112. 110. 112.\n",
      " 109. 113. 111. 112. 112. 111. 109. 110. 112. 110. 113. 111. 110. 110.\n",
      " 112. 110. 112. 112. 111. 110. 110. 110. 111. 111. 113. 112. 111. 111.\n",
      " 110. 112. 109. 110. 110. 110. 111. 112. 111. 111. 111. 111. 110. 111.\n",
      " 112. 110. 113. 111. 110. 111. 111. 113. 111. 110. 112. 109. 110. 111.\n",
      " 112. 112.]\n",
      "[0.5375     0.80625    0.85625    0.86041665 0.8520833  0.8333333\n",
      " 0.8541667  0.8541667  0.85833335 0.8625     0.8541667  0.8520833\n",
      " 0.87083334 0.8645833  0.87083334 0.8645833  0.85833335 0.85833335\n",
      " 0.8666667  0.8520833  0.8625     0.85625    0.86875    0.85833335\n",
      " 0.87083334 0.8625     0.86041665 0.86875    0.87083334 0.86875\n",
      " 0.8666667  0.8645833  0.8625     0.8625     0.85833335 0.87083334\n",
      " 0.8666667  0.8625     0.86875    0.825      0.8666667  0.8666667\n",
      " 0.8625     0.8625     0.8625     0.8625     0.8625     0.875\n",
      " 0.86875    0.875      0.8645833  0.8666667  0.87083334 0.8645833\n",
      " 0.8541667  0.8645833  0.86875    0.875      0.8541667  0.86875\n",
      " 0.8645833  0.8666667  0.8645833  0.8541667  0.8645833  0.87916666\n",
      " 0.8645833  0.86041665 0.87708336 0.86875    0.8666667  0.87083334\n",
      " 0.8645833  0.86875    0.86875    0.8666667  0.86875    0.87291664\n",
      " 0.8625     0.8645833  0.8625     0.8666667  0.8625     0.8645833\n",
      " 0.8645833  0.85833335 0.85833335 0.8645833  0.875      0.87083334\n",
      " 0.8666667  0.87916666 0.85625    0.8645833  0.87083334 0.86875\n",
      " 0.8625     0.8625     0.8666667  0.85833335]\n",
      "[101. 109. 108. 110. 110. 103. 110. 110. 111. 108. 110. 111. 111. 111.\n",
      " 112. 111. 111. 112. 112. 110. 110. 111. 110. 112. 112. 111. 111. 110.\n",
      " 113. 109. 111. 106. 111. 111. 110. 111. 112. 111. 111. 109. 111. 109.\n",
      " 112. 112. 111. 110. 111. 111. 110. 112. 112. 111. 109. 109. 112. 111.\n",
      " 111. 112. 111. 111. 112. 112. 111. 110. 112. 112. 112. 109. 113. 112.\n",
      " 112. 112. 110. 112. 111. 110. 110. 110. 111. 113. 112. 112. 112. 111.\n",
      " 112. 112. 111. 112. 111. 112. 111. 109. 112. 111. 111. 111. 111. 111.\n",
      " 111. 109.]\n",
      "[0.62083334 0.84166664 0.8541667  0.86041665 0.85833335 0.85833335\n",
      " 0.84583336 0.8520833  0.87708336 0.87083334 0.8666667  0.86041665\n",
      " 0.87291664 0.875      0.87916666 0.8833333  0.8979167  0.90208334\n",
      " 0.9166667  0.91875    0.65625    0.83958334 0.875      0.87291664\n",
      " 0.8645833  0.8645833  0.8625     0.8645833  0.86041665 0.87083334\n",
      " 0.87083334 0.86875    0.85625    0.88125    0.8875     0.8645833\n",
      " 0.8854167  0.87916666 0.86875    0.87916666 0.8666667  0.89166665\n",
      " 0.87708336 0.87291664 0.87083334 0.87708336 0.88958335 0.87916666\n",
      " 0.8854167  0.8833333  0.8645833  0.8666667  0.8854167  0.85833335\n",
      " 0.88958335 0.87708336 0.88125    0.87916666 0.87708336 0.8958333\n",
      " 0.87291664 0.87083334 0.86875    0.8875     0.89375    0.87708336\n",
      " 0.87916666 0.875      0.85625    0.8833333  0.8854167  0.8645833\n",
      " 0.88125    0.8666667  0.89166665 0.8875     0.88958335 0.88958335\n",
      " 0.8833333  0.8833333  0.8875     0.87916666 0.85833335 0.8875\n",
      " 0.875      0.87708336 0.88958335 0.8833333  0.89375    0.87083334\n",
      " 0.8854167  0.8833333  0.87291664 0.8854167  0.8875     0.8833333\n",
      " 0.87708336 0.8854167  0.8833333  0.87708336]\n",
      "[111. 112. 103. 110. 112. 110. 112. 111. 109. 111. 111. 110. 113. 110.\n",
      " 111. 112. 108. 116. 117.  93. 104. 111. 111. 110. 110. 110. 106. 111.\n",
      " 112. 110. 111. 111. 112. 110. 111. 108. 110. 109. 112. 112. 110. 110.\n",
      " 111. 111. 111. 110. 111. 111. 111. 109. 111. 111. 111. 111. 111. 112.\n",
      " 111. 110. 112. 111. 111. 111. 112. 111. 111. 111. 112. 110. 111. 111.\n",
      " 111. 110. 111. 111. 110. 111. 112. 110. 111. 110. 110. 111. 112. 111.\n",
      " 111. 111. 110. 111. 112. 111. 111. 111. 111. 111. 112. 111. 111. 109.\n",
      " 111. 111.]\n",
      "[0.6354167  0.85       0.84166664 0.86875    0.86041665 0.8541667\n",
      " 0.85625    0.875      0.8520833  0.8645833  0.87916666 0.8625\n",
      " 0.85       0.85833335 0.8666667  0.85833335 0.8541667  0.8666667\n",
      " 0.8625     0.8645833  0.86041665 0.875      0.8645833  0.875\n",
      " 0.88125    0.875      0.87708336 0.86875    0.89166665 0.89375\n",
      " 0.87708336 0.89166665 0.8833333  0.93541664 0.91875    0.9125\n",
      " 0.93333334 0.9375     0.86875    0.86875    0.9375     0.92083335\n",
      " 0.89166665 0.9604167  0.9375     0.9145833  0.9770833  0.9895833\n",
      " 0.9916667  0.9875     0.9916667  0.9916667  1.         0.99791664\n",
      " 0.99375    0.99375    0.99791664 0.99791664 0.99791664 0.99583334\n",
      " 1.         1.         0.99583334 0.99791664 0.99583334 0.99375\n",
      " 0.99375    0.99791664 0.99583334 0.99791664 0.99583334 0.99791664\n",
      " 0.99583334 0.99791664 0.99583334 0.99583334 0.99583334 0.99791664\n",
      " 1.         0.99583334 0.99583334 0.9916667  0.99583334 0.99791664\n",
      " 0.99791664 0.9895833  0.99375    1.         1.         0.99791664\n",
      " 0.99791664 1.         0.99791664 1.         1.         0.99375\n",
      " 0.99791664 0.99375    0.99791664 1.        ]\n",
      "[108. 108. 103. 110. 108. 113. 112. 107. 111. 111. 108. 107. 111. 111.\n",
      " 112. 112. 109. 106. 112. 112. 111. 109. 112. 112. 111. 109. 111. 112.\n",
      " 110. 112. 112. 113. 112. 112. 118. 114. 112. 116.  95. 112. 113.  76.\n",
      " 116. 120.  97. 119. 118. 120. 120. 120. 120. 120. 120. 120. 119. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 119. 120. 120. 120. 120. 120. 120. 117. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.]\n",
      "[0.6333333  0.76875    0.82916665 0.83125    0.8645833  0.875\n",
      " 0.84583336 0.8625     0.8541667  0.88958335 0.875      0.85\n",
      " 0.89375    0.89375    0.89166665 0.9145833  0.9625     0.96666664\n",
      " 0.9916667  0.8354167  0.84583336 0.86875    0.8625     0.85625\n",
      " 0.85833335 0.84791666 0.8666667  0.85       0.85833335 0.8645833\n",
      " 0.8541667  0.8625     0.8666667  0.86875    0.8645833  0.8625\n",
      " 0.85625    0.86875    0.85       0.87083334 0.85       0.86875\n",
      " 0.8666667  0.86041665 0.8666667  0.8645833  0.8666667  0.8541667\n",
      " 0.8625     0.8645833  0.875      0.8666667  0.8666667  0.8625\n",
      " 0.85625    0.87083334 0.87083334 0.8625     0.8645833  0.8645833\n",
      " 0.8666667  0.85625    0.85625    0.86875    0.8645833  0.85833335\n",
      " 0.8645833  0.85833335 0.8541667  0.8625     0.8666667  0.8520833\n",
      " 0.8666667  0.86041665 0.87083334 0.87291664 0.8625     0.8645833\n",
      " 0.8625     0.8625     0.87083334 0.86875    0.8645833  0.86875\n",
      " 0.8625     0.8625     0.8541667  0.8645833  0.87291664 0.87083334\n",
      " 0.8625     0.87083334 0.8666667  0.85833335 0.85833335 0.8625\n",
      " 0.86875    0.8645833  0.8666667  0.86041665]\n",
      "[ 58. 106. 109. 109. 110. 112. 106. 110. 110. 107. 106. 110. 114. 109.\n",
      " 111. 118. 114. 120. 120.  92. 111. 109. 112. 110. 112. 110. 112. 111.\n",
      " 110. 111. 111. 110. 106. 110. 112. 113. 111. 108. 111. 110. 106. 112.\n",
      " 112. 112. 111. 112. 111. 111. 112. 111. 112. 112. 111. 110. 109. 111.\n",
      " 111. 106. 111. 112. 110. 112. 111. 112. 112. 112. 111. 110. 111. 111.\n",
      " 110. 111. 112. 110. 111. 110. 111. 110. 112. 111. 111. 110. 110. 113.\n",
      " 112. 106. 110. 109. 112. 110. 111. 110. 109. 111. 112. 109. 112. 112.\n",
      " 111. 112.]\n",
      "[0.6166667  0.84166664 0.8520833  0.85       0.8375     0.87083334\n",
      " 0.8520833  0.8625     0.87083334 0.86875    0.8625     0.85\n",
      " 0.85833335 0.8666667  0.87083334 0.86041665 0.8666667  0.8645833\n",
      " 0.84375    0.8625     0.8645833  0.86875    0.8541667  0.8645833\n",
      " 0.87916666 0.87708336 0.86875    0.84375    0.86875    0.87083334\n",
      " 0.86041665 0.87708336 0.87291664 0.87291664 0.8625     0.8645833\n",
      " 0.8541667  0.85625    0.8645833  0.88125    0.87083334 0.88125\n",
      " 0.9291667  0.93958336 0.95208335 0.99583334 0.99375    0.99375\n",
      " 0.90416664 0.6666667  0.8229167  0.8541667  0.8666667  0.8645833\n",
      " 0.87083334 0.87708336 0.8833333  0.87916666 0.88958335 0.87916666\n",
      " 0.8666667  0.88958335 0.87083334 0.8875     0.8875     0.88125\n",
      " 0.875      0.875      0.875      0.87083334 0.87916666 0.88125\n",
      " 0.88958335 0.875      0.87083334 0.875      0.875      0.8625\n",
      " 0.8833333  0.87916666 0.8854167  0.875      0.8833333  0.8833333\n",
      " 0.86875    0.8625     0.875      0.875      0.88125    0.86875\n",
      " 0.87708336 0.8854167  0.8854167  0.8666667  0.87708336 0.87083334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.8645833  0.85833335 0.875      0.875     ]\n",
      "[110. 108. 112. 107. 110. 111. 108. 112. 112. 109. 110. 111. 111. 110.\n",
      " 110. 112. 112. 111. 111. 112. 107. 111. 111. 110. 111. 110. 111. 111.\n",
      " 109. 110. 109. 110. 112. 111. 106. 111. 110. 110. 111. 112. 104. 112.\n",
      " 113. 116. 120. 120. 120. 120.  75.  86. 111. 112. 111. 110. 110. 111.\n",
      " 110. 110. 112. 110. 111. 110. 112. 111. 110. 106. 111. 111. 111. 111.\n",
      " 111. 111. 107. 111. 111. 111. 110. 111. 111. 110. 111. 111. 112. 112.\n",
      " 111. 112. 109. 111. 111. 111. 112. 112. 108. 109. 111. 112. 111. 111.\n",
      " 110. 112.]\n",
      "[0.675      0.84791666 0.83125    0.85625    0.8625     0.86041665\n",
      " 0.8666667  0.86875    0.84583336 0.8666667  0.84583336 0.87291664\n",
      " 0.86041665 0.86875    0.8666667  0.85833335 0.8520833  0.86041665\n",
      " 0.875      0.8645833  0.87291664 0.8625     0.84375    0.86875\n",
      " 0.87291664 0.86875    0.86875    0.8666667  0.86041665 0.87916666\n",
      " 0.8666667  0.875      0.875      0.87916666 0.87916666 0.8875\n",
      " 0.85       0.88958335 0.8958333  0.88125    0.8875     0.89166665\n",
      " 0.89166665 0.8875     0.8875     0.88958335 0.89166665 0.89375\n",
      " 0.8979167  0.88125    0.8958333  0.88958335 0.88958335 0.86875\n",
      " 0.8979167  0.8875     0.87291664 0.8875     0.89375    0.89375\n",
      " 0.88958335 0.87916666 0.8958333  0.88958335 0.8833333  0.8833333\n",
      " 0.87916666 0.86875    0.8854167  0.8833333  0.8854167  0.88958335\n",
      " 0.8875     0.8875     0.8833333  0.89166665 0.88958335 0.8875\n",
      " 0.8854167  0.89375    0.89375    0.89166665 0.89375    0.875\n",
      " 0.89166665 0.87916666 0.89166665 0.8958333  0.89166665 0.8958333\n",
      " 0.8979167  0.88958335 0.89375    0.8854167  0.89375    0.87916666\n",
      " 0.90208334 0.89375    0.89375    0.8833333 ]\n",
      "[ 89. 111. 111. 110. 111. 111. 110. 111. 108. 111. 111. 108. 112. 112.\n",
      " 109. 111. 111. 108. 112. 112. 110. 111. 110. 111. 112. 109. 108. 110.\n",
      " 113. 112. 111. 112. 112. 107. 112. 113. 112. 112. 112. 112. 112. 113.\n",
      " 109. 112. 113. 112. 113. 113. 113. 112. 112. 111. 111. 114. 113. 110.\n",
      " 113. 113. 109. 111. 112. 112. 112. 111. 112. 114. 109. 111. 111. 111.\n",
      " 113. 112. 113. 113. 111. 113. 113. 113. 113. 113. 114. 112. 111. 113.\n",
      " 112. 113. 112. 111. 113. 113. 112. 113. 110. 114. 112. 113. 111. 112.\n",
      " 114. 113.]\n",
      "Number of layers: 20\n",
      "[0.49166667 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.]\n",
      "[0.74375    0.85       0.8645833  0.8645833  0.84375    0.83958334\n",
      " 0.8520833  0.86041665 0.8625     0.84791666 0.86875    0.8666667\n",
      " 0.8541667  0.87708336 0.8666667  0.8666667  0.86041665 0.8666667\n",
      " 0.8625     0.8666667  0.8666667  0.87708336 0.8645833  0.8666667\n",
      " 0.87291664 0.87291664 0.8666667  0.8541667  0.8625     0.8645833\n",
      " 0.85625    0.87291664 0.8666667  0.8645833  0.87291664 0.8625\n",
      " 0.85833335 0.8645833  0.8625     0.8666667  0.86875    0.86875\n",
      " 0.87708336 0.8645833  0.8645833  0.8625     0.8625     0.8625\n",
      " 0.8666667  0.87708336 0.8645833  0.85625    0.8645833  0.8625\n",
      " 0.875      0.8645833  0.85833335 0.85625    0.8645833  0.86041665\n",
      " 0.8645833  0.8645833  0.87083334 0.85833335 0.85625    0.8625\n",
      " 0.8666667  0.87291664 0.875      0.8625     0.8666667  0.86875\n",
      " 0.87083334 0.8666667  0.87916666 0.8625     0.85833335 0.85625\n",
      " 0.87291664 0.87916666 0.875      0.85625    0.86875    0.86875\n",
      " 0.85625    0.8625     0.8625     0.86875    0.8666667  0.8625\n",
      " 0.8645833  0.8645833  0.8625     0.85833335 0.8666667  0.875\n",
      " 0.8645833  0.86041665 0.8666667  0.86875   ]\n",
      "[108. 111. 110. 110. 111. 111. 111. 111. 108. 111. 111. 111. 110. 111.\n",
      " 109. 112. 112. 112. 109. 112. 110. 111. 111. 111. 112. 112. 112. 111.\n",
      " 112. 111. 111. 112. 111. 107. 112. 111. 110. 112. 112. 111. 110. 112.\n",
      " 113. 112. 111. 111. 112. 112. 110. 112. 111. 111. 110. 111. 112. 112.\n",
      " 110. 111. 110. 109. 111. 113. 110. 109. 112. 111. 111. 112. 112. 110.\n",
      " 111. 111. 110. 111. 112. 110. 112. 111. 113. 111. 110. 112. 112. 111.\n",
      " 110. 112. 112. 110. 111. 112. 112. 112. 112. 111. 112. 111. 112. 111.\n",
      " 109. 111.]\n",
      "[0.65833336 0.8208333  0.86041665 0.875      0.86041665 0.8833333\n",
      " 0.84791666 0.89166665 0.90625    0.9166667  0.9291667  0.9\n",
      " 0.90416664 0.95       0.93125    0.92291665 0.97291666 0.98333335\n",
      " 0.92291665 0.9791667  0.94375    0.9875     0.9895833  0.97291666\n",
      " 1.         0.99583334 0.99583334 0.99791664 0.99583334 1.\n",
      " 1.         1.         0.975      0.99791664 1.         1.\n",
      " 0.99791664 0.99791664 0.99791664 0.99791664 0.99583334 0.99375\n",
      " 0.98541665 0.99791664 1.         1.         0.97083336 0.90833336\n",
      " 0.9916667  1.         1.         0.99791664 0.99791664 0.99583334\n",
      " 0.99583334 0.99791664 0.99791664 1.         0.99791664 1.\n",
      " 0.99583334 1.         1.         1.         1.         0.99791664\n",
      " 0.99791664 1.         1.         1.         1.         1.\n",
      " 1.         0.99791664 1.         0.99791664 1.         1.\n",
      " 0.775      0.7875     0.87291664 0.89166665 0.93958336 0.9625\n",
      " 0.95625    0.9625     0.9916667  0.95       1.         1.\n",
      " 0.99583334 0.97291666 0.71666664 0.82708335 0.88958335 0.96666664\n",
      " 0.98541665 0.9875     0.9916667  0.98333335]\n",
      "[107. 108. 110. 109. 109. 112. 112. 113. 115. 100. 118. 114. 118. 111.\n",
      " 110. 118. 118. 118. 120. 116. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 107. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 100. 107. 112. 116. 118. 113.\n",
      " 120. 118. 115. 116. 120. 120. 120.  87. 104. 110. 115. 119. 120. 120.\n",
      " 120. 120.]\n",
      "[0.5416667  0.8125     0.82916665 0.85625    0.8854167  0.8875\n",
      " 0.8833333  0.89166665 0.87083334 0.8854167  0.88958335 0.89375\n",
      " 0.89166665 0.87291664 0.8979167  0.8958333  0.8833333  0.8854167\n",
      " 0.8875     0.87916666 0.8854167  0.8958333  0.87708336 0.8854167\n",
      " 0.8854167  0.8833333  0.89375    0.88958335 0.87083334 0.87916666\n",
      " 0.89166665 0.86875    0.89375    0.8854167  0.87916666 0.88125\n",
      " 0.87916666 0.8854167  0.8875     0.8875     0.8875     0.9\n",
      " 0.89166665 0.88958335 0.88958335 0.8875     0.8854167  0.89375\n",
      " 0.88958335 0.8833333  0.8875     0.8854167  0.89166665 0.875\n",
      " 0.8833333  0.89375    0.8833333  0.8854167  0.8833333  0.87708336\n",
      " 0.87916666 0.89166665 0.8854167  0.87708336 0.87916666 0.88125\n",
      " 0.8833333  0.8854167  0.8833333  0.88125    0.87916666 0.88125\n",
      " 0.8854167  0.90208334 0.89166665 0.9        0.88958335 0.89375\n",
      " 0.88958335 0.89166665 0.88958335 0.88958335 0.87708336 0.89166665\n",
      " 0.8854167  0.8854167  0.8854167  0.88958335 0.8958333  0.8833333\n",
      " 0.89166665 0.88125    0.8875     0.89166665 0.8854167  0.9\n",
      " 0.8854167  0.8958333  0.88958335 0.89166665]\n",
      "[105.  85. 110. 113. 113. 112. 111. 112. 113. 113. 114. 111. 112. 113.\n",
      " 111. 112. 111. 113. 112. 113. 113. 109. 109. 112. 111. 111. 112. 113.\n",
      " 112. 113. 111. 112. 113. 111. 109. 110. 110. 111. 112. 112. 110. 112.\n",
      " 112. 113. 112. 112. 112. 111. 111. 112. 111. 112. 113. 114. 111. 112.\n",
      " 113. 110. 108. 112. 113. 112. 111. 111. 112. 112. 114. 113. 112. 113.\n",
      " 112. 112. 112. 112. 112. 112. 112. 109. 113. 112. 112. 111. 113. 112.\n",
      " 114. 110. 113. 110. 112. 113. 113. 111. 112. 111. 112. 109. 114. 110.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 114. 112.]\n",
      "[0.65833336 0.84375    0.90416664 0.87916666 0.94375    0.95208335\n",
      " 0.99375    0.9916667  0.99583334 0.99583334 0.99583334 1.\n",
      " 1.         0.99583334 1.         0.99791664 0.99791664 0.57916665\n",
      " 0.53125    0.5541667  0.56875    0.575      0.5833333  0.62083334\n",
      " 0.6333333  0.6354167  0.6354167  0.6458333  0.6479167  0.6479167\n",
      " 0.65       0.65       0.65       0.65       0.65       0.65\n",
      " 0.65       0.65       0.65       0.65       0.65       0.65\n",
      " 0.65       0.65       0.65       0.65       0.65       0.65\n",
      " 0.65       0.65       0.65       0.65       0.65       0.65\n",
      " 0.65       0.65       0.65       0.65       0.65       0.65\n",
      " 0.65       0.65       0.65       0.65       0.65       0.65\n",
      " 0.65       0.65       0.65       0.65       0.65       0.65\n",
      " 0.65       0.6625     0.6770833  0.68125    0.68125    0.68125\n",
      " 0.68125    0.68125    0.68333334 0.6979167  0.68958336 0.70416665\n",
      " 0.71458334 0.71875    0.71875    0.72083336 0.72083336 0.72083336\n",
      " 0.72083336 0.72083336 0.72083336 0.73541665 0.7395833  0.7416667\n",
      " 0.7375     0.7375     0.74375    0.74791664]\n",
      "[106. 110. 115. 113. 119. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 119.  59.  61.  62.  64.  65.  71.  75.  76.  76.  77.  79.\n",
      "  79.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.\n",
      "  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.\n",
      "  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.\n",
      "  82.  82.  82.  86.  86.  86.  86.  86.  86.  86.  87.  88.  87.  88.\n",
      "  90.  90.  90.  91.  91.  91.  92.  92.  93.  93.  93.  93.  94.  94.\n",
      "  94.  94.]\n",
      "[0.5125     0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.]\n",
      "[0.71666664 0.8375     0.86875    0.86875    0.87291664 0.83958334\n",
      " 0.8645833  0.8666667  0.87083334 0.8833333  0.83125    0.85833335\n",
      " 0.87291664 0.8666667  0.85833335 0.8666667  0.88125    0.89375\n",
      " 0.89375    0.95       0.9        0.93541664 0.94375    0.97291666\n",
      " 0.9479167  0.89166665 0.92083335 0.95       0.9604167  0.9583333\n",
      " 0.96666664 0.9291667  0.95416665 0.9791667  0.9604167  0.95\n",
      " 0.95208335 0.98541665 0.92291665 0.9791667  0.975      0.86875\n",
      " 0.95416665 0.9916667  0.9791667  1.         0.925      0.9791667\n",
      " 0.9895833  0.99791664 0.9916667  0.99375    0.9895833  0.74375\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[111. 109. 111. 112. 105. 112. 111. 112. 110. 104. 112. 107. 113. 112.\n",
      " 111. 113. 115. 113. 116. 117. 113. 111. 111. 119. 117. 116. 118. 119.\n",
      " 120. 115. 120. 118. 118. 114. 112. 120. 120.  90. 120. 120. 116. 117.\n",
      " 118. 112. 120. 120. 118. 120. 120. 120. 120. 120. 120.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.]\n",
      "[0.4875     0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.]\n",
      "[0.48333332 0.67083335 0.87083334 0.84583336 0.87916666 0.8854167\n",
      " 0.87083334 0.87916666 0.88958335 0.86875    0.87916666 0.88125\n",
      " 0.87083334 0.8979167  0.88125    0.88125    0.8854167  0.88125\n",
      " 0.88125    0.88958335 0.86875    0.87708336 0.8875     0.88125\n",
      " 0.8875     0.89166665 0.8979167  0.88958335 0.88125    0.8854167\n",
      " 0.89375    0.89375    0.89375    0.9        0.88958335 0.8958333\n",
      " 0.89166665 0.8875     0.88958335 0.88958335 0.90625    0.8979167\n",
      " 0.8979167  0.8958333  0.8958333  0.8958333  0.89166665 0.89375\n",
      " 0.8979167  0.8958333  0.8958333  0.8958333  0.88958335 0.8958333\n",
      " 0.8875     0.90625    0.89166665 0.89166665 0.89375    0.8958333\n",
      " 0.89375    0.9        0.89375    0.8958333  0.9        0.8875\n",
      " 0.9        0.88958335 0.9        0.88958335 0.89166665 0.90208334\n",
      " 0.8979167  0.89375    0.9        0.88958335 0.90208334 0.8875\n",
      " 0.89375    0.9        0.89166665 0.9        0.8979167  0.90208334\n",
      " 0.89375    0.8979167  0.8833333  0.8958333  0.90208334 0.9\n",
      " 0.8979167  0.90208334 0.9        0.8979167  0.90208334 0.9\n",
      " 0.8958333  0.8958333  0.90625    0.9       ]\n",
      "[ 58. 109. 107. 109. 111. 111. 113. 111. 111. 114. 108. 111. 114. 113.\n",
      " 114. 113. 111. 114. 114. 111. 111. 111. 112. 112. 112. 114. 111. 111.\n",
      " 113. 113. 113. 114. 114. 111. 114. 113. 112. 113. 111. 111. 111. 110.\n",
      " 113. 112. 112. 113. 112. 113. 113. 112. 112. 113. 112. 111. 114. 110.\n",
      " 112. 114. 113. 112. 112. 113. 114. 111. 113. 113. 110. 113. 112. 111.\n",
      " 114. 113. 114. 111. 112. 112. 112. 113. 111. 113. 111. 111. 113. 111.\n",
      " 113. 113. 111. 113. 113. 113. 112. 113. 113. 112. 112. 112. 113. 112.\n",
      " 113. 112.]\n",
      "[0.53125    0.7625     0.83958334 0.88125    0.89166665 0.87083334\n",
      " 0.85833335 0.90625    0.94375    0.97083336 0.89375    0.97291666\n",
      " 0.95       0.9875     0.9916667  0.99791664 0.9916667  0.8354167\n",
      " 0.96458334 0.97291666 0.9895833  0.99375    0.99375    0.99791664\n",
      " 0.99583334 0.99583334 0.99791664 0.99375    0.99791664 0.99791664\n",
      " 0.99791664 0.99375    1.         0.99583334 0.99583334 1.\n",
      " 0.99791664 1.         1.         0.99791664 0.9895833  0.99791664\n",
      " 0.99583334 0.99583334 0.9916667  1.         1.         0.99791664\n",
      " 0.99583334 0.99791664 1.         1.         1.         0.99791664\n",
      " 1.         0.99791664 1.         0.99583334 0.8541667  0.9458333\n",
      " 0.98125    0.99583334 0.99583334 1.         0.9916667  0.99791664\n",
      " 0.99583334 1.         1.         1.         1.         0.99791664\n",
      " 0.99791664 1.         0.99791664 1.         1.         0.99791664\n",
      " 0.99791664 0.99791664 1.         1.         1.         0.99791664\n",
      " 1.         1.         1.         0.99791664 0.99791664 1.\n",
      " 1.         1.         1.         0.99791664 1.         1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1.         1.         0.99791664 0.99791664]\n",
      "[106. 111. 111. 112. 108. 114. 113. 115. 118. 102. 120. 120. 120. 120.\n",
      " 120. 120. 120. 117. 118. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 119. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 114. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.]\n",
      "Number of layers: 21\n",
      "[0.5208333  0.51875    0.68125    0.73333335 0.76458335 0.81041664\n",
      " 0.82708335 0.84791666 0.86041665 0.8520833  0.80625    0.8666667\n",
      " 0.85625    0.85625    0.8625     0.85       0.8645833  0.8625\n",
      " 0.86875    0.84791666 0.8645833  0.85833335 0.86875    0.85833335\n",
      " 0.875      0.8666667  0.875      0.87916666 0.8833333  0.88125\n",
      " 0.88958335 0.88125    0.88958335 0.8854167  0.87708336 0.87916666\n",
      " 0.8833333  0.88125    0.8854167  0.875      0.87291664 0.88958335\n",
      " 0.8833333  0.87708336 0.8854167  0.88125    0.87083334 0.87708336\n",
      " 0.87291664 0.875      0.89375    0.85833335 0.87708336 0.87291664\n",
      " 0.87916666 0.87708336 0.8875     0.88125    0.8854167  0.8666667\n",
      " 0.88958335 0.88125    0.89375    0.8854167  0.87708336 0.8875\n",
      " 0.89166665 0.87083334 0.87916666 0.86875    0.875      0.87708336\n",
      " 0.88125    0.87916666 0.88958335 0.8854167  0.87708336 0.88125\n",
      " 0.8854167  0.86875    0.875      0.8875     0.8833333  0.87291664\n",
      " 0.8666667  0.86875    0.87291664 0.8833333  0.8875     0.88958335\n",
      " 0.88125    0.87916666 0.86875    0.88958335 0.8875     0.87291664\n",
      " 0.86875    0.87291664 0.88125    0.875     ]\n",
      "[ 62.  80.  92. 100. 102. 108. 108. 106. 106. 111. 111. 111. 107. 112.\n",
      " 112. 111. 105. 111. 110. 111. 112. 112. 112. 109. 111. 109. 111. 110.\n",
      " 110. 112. 110. 111. 111. 109. 111. 110. 111. 111. 112. 111. 111. 111.\n",
      " 112. 112. 112. 110. 110. 111. 111. 110. 111. 111. 110. 112. 110. 110.\n",
      " 111. 111. 112. 111. 112. 111. 112. 111. 111. 111. 110. 111. 111. 111.\n",
      " 111. 112. 112. 112. 112. 111. 112. 111. 109. 111. 111. 111. 111. 111.\n",
      " 111. 110. 111. 111. 111. 110. 108. 111. 112. 111. 112. 112. 111. 111.\n",
      " 112. 111.]\n",
      "[0.66875    0.84166664 0.8645833  0.87291664 0.8854167  0.8854167\n",
      " 0.86875    0.86875    0.86875    0.8645833  0.87291664 0.8833333\n",
      " 0.875      0.87083334 0.8854167  0.88125    0.8833333  0.8833333\n",
      " 0.87708336 0.86875    0.87291664 0.87708336 0.88125    0.89375\n",
      " 0.87083334 0.87083334 0.8854167  0.87291664 0.875      0.87708336\n",
      " 0.87916666 0.8833333  0.86875    0.87083334 0.87291664 0.88125\n",
      " 0.87083334 0.86875    0.8833333  0.89166665 0.8833333  0.89375\n",
      " 0.875      0.87083334 0.87708336 0.8645833  0.875      0.88125\n",
      " 0.87708336 0.88125    0.8854167  0.88125    0.88125    0.8854167\n",
      " 0.8833333  0.8645833  0.88958335 0.8833333  0.87708336 0.8645833\n",
      " 0.89375    0.8666667  0.88958335 0.8854167  0.8875     0.8854167\n",
      " 0.88125    0.8833333  0.8833333  0.875      0.86875    0.8875\n",
      " 0.875      0.8833333  0.87916666 0.88125    0.87916666 0.89375\n",
      " 0.87708336 0.88125    0.88125    0.8666667  0.88125    0.8833333\n",
      " 0.88125    0.8833333  0.8875     0.87916666 0.87708336 0.87291664\n",
      " 0.88125    0.87291664 0.86875    0.87708336 0.87916666 0.87916666\n",
      " 0.8854167  0.88125    0.88125    0.8875    ]\n",
      "[104. 110. 111. 112. 109. 110. 111. 111. 109. 110. 111. 111. 111. 111.\n",
      " 111. 112. 110. 110. 109. 111. 110. 111. 111. 111. 112. 111. 111. 111.\n",
      " 110. 112. 110. 110. 112. 111. 111. 111. 110. 111. 111. 111. 111. 111.\n",
      " 111. 111. 110. 111. 110. 111. 112. 112. 111. 112. 112. 111. 112. 112.\n",
      " 111. 111. 109. 111. 111. 111. 112. 111. 111. 111. 112. 111. 112. 111.\n",
      " 110. 112. 111. 112. 112. 110. 111. 112. 112. 111. 112. 111. 111. 111.\n",
      " 112. 112. 111. 111. 109. 111. 109. 111. 111. 111. 111. 111. 112. 111.\n",
      " 111. 111.]\n",
      "[0.5        0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.]\n",
      "[0.66041666 0.84166664 0.84791666 0.8625     0.88125    0.8645833\n",
      " 0.875      0.875      0.88125    0.8854167  0.87708336 0.87916666\n",
      " 0.89375    0.8875     0.87708336 0.87708336 0.875      0.88958335\n",
      " 0.87083334 0.88958335 0.89166665 0.89375    0.925      0.88958335\n",
      " 0.9270833  0.925      0.95208335 0.93125    0.9375     0.95416665\n",
      " 0.95       0.9625     0.91875    0.95416665 0.97291666 0.95625\n",
      " 0.98333335 0.95416665 0.98125    0.98541665 0.99583334 0.9770833\n",
      " 0.975      0.9916667  0.99791664 0.99375    0.9791667  0.99375\n",
      " 0.99583334 0.99375    0.9875     0.99583334 0.99583334 0.99791664\n",
      " 1.         0.99791664 0.99791664 0.99375    0.9916667  0.9895833\n",
      " 0.99583334 0.99375    0.99791664 0.99375    1.         0.99583334\n",
      " 1.         0.99583334 0.99375    0.99791664 0.99583334 0.99583334\n",
      " 0.98125    0.9375     1.         1.         0.9895833  0.99583334\n",
      " 0.99791664 0.99791664 0.99791664 0.99583334 0.99583334 0.99375\n",
      " 0.99791664 0.99791664 0.99583334 0.99791664 0.99583334 0.99375\n",
      " 0.99375    0.99791664 0.99375    0.99791664 0.99583334 0.99791664\n",
      " 0.99791664 0.99583334 0.99583334 0.99791664]\n",
      "[103.  58. 108. 111. 111. 109. 111. 112. 110. 110. 111. 112. 113. 109.\n",
      " 112. 112. 111. 112. 113. 113. 113. 115. 115. 115. 117. 118. 116. 114.\n",
      " 118. 119. 120. 118. 115. 116. 120. 120. 119. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 118. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 117. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.  96. 120. 120. 120. 120. 120. 120. 120. 120. 120. 118. 120.\n",
      " 120. 120. 120. 120. 117. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.]\n",
      "[0.6125     0.81041664 0.85625    0.87291664 0.9        0.94375\n",
      " 0.8979167  0.98125    0.98125    0.9604167  0.81041664 0.86875\n",
      " 0.87083334 0.8645833  0.8833333  0.8875     0.8875     0.8854167\n",
      " 0.90208334 0.8833333  0.8958333  0.89166665 0.87916666 0.8833333\n",
      " 0.8854167  0.88125    0.8875     0.89375    0.89375    0.87291664\n",
      " 0.89166665 0.8958333  0.8875     0.8854167  0.8833333  0.89375\n",
      " 0.89166665 0.8875     0.89166665 0.9        0.8833333  0.8625\n",
      " 0.87916666 0.8875     0.88125    0.87083334 0.88958335 0.89375\n",
      " 0.87916666 0.8833333  0.89166665 0.87708336 0.89166665 0.8958333\n",
      " 0.8979167  0.89166665 0.88958335 0.8854167  0.88958335 0.89166665\n",
      " 0.90416664 0.8958333  0.8979167  0.90208334 0.8875     0.89375\n",
      " 0.8979167  0.89166665 0.8958333  0.8979167  0.90416664 0.8979167\n",
      " 0.8979167  0.8875     0.89375    0.89166665 0.9        0.90208334\n",
      " 0.8875     0.8958333  0.9        0.8958333  0.8979167  0.8958333\n",
      " 0.89375    0.8979167  0.9        0.90208334 0.8979167  0.89375\n",
      " 0.8833333  0.90625    0.8979167  0.8979167  0.9        0.90208334\n",
      " 0.8854167  0.90208334 0.90416664 0.90416664]\n",
      "[ 62. 106. 106. 110. 117.  81. 118. 120. 120.  86. 111. 113. 112. 107.\n",
      " 111. 113. 114. 114. 109. 113. 109. 110. 113. 113. 109. 109. 114. 112.\n",
      " 112. 110. 109. 112. 111. 110. 106. 113. 112. 113. 111. 113. 114. 110.\n",
      " 112. 112. 109. 110. 111. 111. 113. 110. 114. 112. 113. 111. 112. 109.\n",
      " 113. 113. 111. 113. 114. 111. 113. 111. 111. 114. 112. 111. 113. 112.\n",
      " 111. 112. 111. 113. 113. 112. 111. 114. 111. 113. 114. 112. 113. 113.\n",
      " 111. 111. 112. 111. 111. 111. 114. 110. 111. 112. 111. 112. 110. 113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 110. 109.]\n",
      "[0.53333336 0.8125     0.86041665 0.8645833  0.87708336 0.88125\n",
      " 0.8645833  0.87083334 0.85833335 0.8833333  0.88958335 0.86875\n",
      " 0.87291664 0.86875    0.8645833  0.88125    0.875      0.87916666\n",
      " 0.875      0.875      0.8833333  0.87291664 0.875      0.87708336\n",
      " 0.88958335 0.87291664 0.8875     0.8833333  0.87916666 0.89166665\n",
      " 0.88958335 0.87291664 0.87708336 0.86875    0.875      0.87916666\n",
      " 0.8833333  0.87083334 0.88125    0.86875    0.87291664 0.8625\n",
      " 0.87083334 0.89166665 0.88125    0.8875     0.8875     0.87916666\n",
      " 0.87083334 0.8833333  0.87916666 0.89166665 0.8833333  0.87708336\n",
      " 0.8958333  0.87291664 0.875      0.8833333  0.875      0.87916666\n",
      " 0.89166665 0.86875    0.87708336 0.88125    0.87083334 0.87916666\n",
      " 0.8875     0.8854167  0.88958335 0.875      0.8875     0.87916666\n",
      " 0.88125    0.88958335 0.87916666 0.87291664 0.8833333  0.8645833\n",
      " 0.8854167  0.85625    0.87916666 0.8854167  0.87083334 0.8854167\n",
      " 0.86875    0.8854167  0.88125    0.88958335 0.88958335 0.8625\n",
      " 0.875      0.8645833  0.8833333  0.875      0.8854167  0.88958335\n",
      " 0.89375    0.8854167  0.87708336 0.8833333 ]\n",
      "[106. 111. 109. 111. 111. 112. 109. 111. 111. 111. 112. 112. 109. 111.\n",
      " 111. 111. 110. 111. 111. 111. 110. 110. 111. 110. 111. 111. 110. 111.\n",
      " 111. 108. 110. 109. 110. 110. 111. 111. 111. 111. 111. 111. 109. 111.\n",
      " 110. 110. 110. 111. 110. 111. 111. 111. 112. 111. 112. 111. 112. 111.\n",
      " 111. 111. 111. 111. 110. 111. 112. 112. 111. 111. 111. 111. 109. 111.\n",
      " 111. 112. 111. 111. 111. 112. 111. 111. 111. 111. 112. 112. 111. 112.\n",
      " 111. 111. 111. 112. 111. 112. 110. 112. 111. 111. 111. 111. 111. 111.\n",
      " 111. 112.]\n",
      "[0.60625    0.8354167  0.8541667  0.8333333  0.8520833  0.87291664\n",
      " 0.86875    0.8625     0.87291664 0.8666667  0.86041665 0.86875\n",
      " 0.8645833  0.8645833  0.8645833  0.85833335 0.85625    0.8625\n",
      " 0.84791666 0.86875    0.8666667  0.8666667  0.87916666 0.8666667\n",
      " 0.8666667  0.8520833  0.87708336 0.86041665 0.8541667  0.86875\n",
      " 0.86875    0.8625     0.85833335 0.8520833  0.85833335 0.8625\n",
      " 0.86875    0.86875    0.8625     0.875      0.86041665 0.8541667\n",
      " 0.86041665 0.86041665 0.8625     0.8645833  0.8625     0.8625\n",
      " 0.86041665 0.86041665 0.85625    0.85625    0.85833335 0.8666667\n",
      " 0.8520833  0.85833335 0.8666667  0.86875    0.8666667  0.8666667\n",
      " 0.8666667  0.86875    0.8666667  0.85625    0.8625     0.8625\n",
      " 0.87083334 0.85833335 0.8666667  0.87291664 0.8875     0.8875\n",
      " 0.9458333  0.9        0.925      0.9458333  0.94375    0.95208335\n",
      " 0.9291667  0.74791664 0.8645833  0.8666667  0.8541667  0.8645833\n",
      " 0.84791666 0.8520833  0.86041665 0.86875    0.8541667  0.86875\n",
      " 0.87916666 0.85625    0.86875    0.87708336 0.86041665 0.86875\n",
      " 0.87291664 0.8625     0.86041665 0.8666667 ]\n",
      "[112. 108. 108. 112. 112. 112. 110. 112. 111. 111. 111. 111. 112. 109.\n",
      " 110. 111. 112. 106. 111. 110. 112. 110. 110. 112. 111. 111. 111. 112.\n",
      " 111. 110. 107. 109. 108. 112. 111. 112. 112. 111. 111. 111. 111. 108.\n",
      " 112. 109. 106. 112. 111. 111. 112. 109. 106. 110. 111. 108. 109. 112.\n",
      " 112. 108. 111. 110. 110. 111. 108. 110. 110. 111. 112. 112. 112. 112.\n",
      " 111. 113. 110. 116. 115. 113. 117. 117.  58. 112. 111. 111. 110. 112.\n",
      " 111. 109. 111. 112. 112. 111. 107. 111. 109. 112. 110. 108. 110. 111.\n",
      " 110. 112.]\n",
      "[0.48333332 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.]\n",
      "[0.5208333  0.5729167  0.8020833  0.82916665 0.84583336 0.87708336\n",
      " 0.87916666 0.87291664 0.87708336 0.87916666 0.87916666 0.87083334\n",
      " 0.8854167  0.8875     0.88125    0.8875     0.88125    0.8854167\n",
      " 0.8875     0.8958333  0.875      0.87916666 0.875      0.8854167\n",
      " 0.87916666 0.86041665 0.875      0.8854167  0.88125    0.88958335\n",
      " 0.87291664 0.88958335 0.8666667  0.875      0.8875     0.88958335\n",
      " 0.88125    0.88125    0.8875     0.89375    0.8854167  0.8833333\n",
      " 0.87291664 0.8979167  0.90833336 0.8979167  0.8958333  0.8833333\n",
      " 0.88958335 0.89375    0.8854167  0.89166665 0.88958335 0.91041666\n",
      " 0.88958335 0.8854167  0.87708336 0.87708336 0.89375    0.88958335\n",
      " 0.8833333  0.89375    0.89166665 0.87916666 0.8979167  0.87083334\n",
      " 0.8833333  0.89166665 0.87916666 0.89166665 0.88125    0.87708336\n",
      " 0.89375    0.8958333  0.8854167  0.8875     0.88958335 0.90625\n",
      " 0.9        0.89166665 0.89166665 0.88958335 0.88958335 0.8854167\n",
      " 0.88125    0.88958335 0.8875     0.8875     0.8958333  0.8979167\n",
      " 0.89166665 0.8979167  0.8854167  0.8979167  0.88958335 0.87916666\n",
      " 0.8958333  0.87916666 0.88125    0.8979167 ]\n",
      "[ 92.  97. 109. 110. 111. 110. 110. 106. 113. 113. 113. 112. 107. 110.\n",
      " 113. 111. 113. 113. 109. 111. 109. 112. 111. 113. 112. 112. 112. 111.\n",
      " 111. 112. 111. 112. 111. 112. 110. 111. 110. 112. 110. 113. 113. 110.\n",
      " 111. 112. 112. 112. 112. 114. 113. 112. 113. 111. 112. 111. 113. 112.\n",
      " 114. 111. 111. 111. 112. 114. 113. 113. 114. 112. 113. 113. 113. 110.\n",
      " 113. 110. 113. 113. 112. 111. 113. 110. 109. 113. 112. 111. 111. 112.\n",
      " 113. 111. 114. 112. 113. 114. 113. 114. 114. 109. 112. 113. 114. 113.\n",
      " 111. 113.]\n",
      "[0.54583335 0.6875     0.75625    0.80625    0.85625    0.8666667\n",
      " 0.86875    0.875      0.8854167  0.875      0.8833333  0.87291664\n",
      " 0.89375    0.87916666 0.87916666 0.89166665 0.88125    0.87708336\n",
      " 0.8854167  0.8833333  0.8875     0.8666667  0.87083334 0.88958335\n",
      " 0.88125    0.8875     0.87708336 0.89166665 0.88958335 0.8833333\n",
      " 0.8958333  0.9125     0.9145833  0.8354167  0.84583336 0.8833333\n",
      " 0.8875     0.8875     0.88958335 0.89166665 0.87916666 0.8875\n",
      " 0.87708336 0.8833333  0.89375    0.88958335 0.87291664 0.8833333\n",
      " 0.875      0.87708336 0.87083334 0.8833333  0.87708336 0.88125\n",
      " 0.8875     0.87291664 0.875      0.8625     0.8854167  0.8875\n",
      " 0.8645833  0.87708336 0.88958335 0.8833333  0.87083334 0.8875\n",
      " 0.89375    0.8854167  0.87708336 0.86875    0.87708336 0.8833333\n",
      " 0.8625     0.87291664 0.875      0.875      0.88125    0.875\n",
      " 0.87916666 0.88125    0.8854167  0.87291664 0.87708336 0.8979167\n",
      " 0.87708336 0.87916666 0.89375    0.86875    0.87708336 0.88125\n",
      " 0.87916666 0.88125    0.8875     0.8666667  0.88958335 0.8875\n",
      " 0.875      0.88958335 0.88125    0.87916666]\n",
      "[ 79.  99. 105. 107. 109. 110. 112. 112. 112. 110. 112. 110. 112. 112.\n",
      " 112. 111. 111. 112. 111. 111. 111. 111. 111. 111. 111. 112. 112. 111.\n",
      " 112. 112. 113. 113. 116. 107. 109. 111. 111. 110. 111. 111. 111. 112.\n",
      " 112. 111. 112. 112. 109. 111. 112. 111. 111. 111. 111. 110. 112. 111.\n",
      " 111. 111. 111. 111. 112. 112. 111. 112. 111. 112. 111. 111. 111. 111.\n",
      " 112. 111. 110. 110. 111. 111. 111. 111. 110. 112. 111. 111. 112. 109.\n",
      " 111. 111. 111. 111. 111. 111. 111. 111. 109. 111. 112. 111. 111. 111.\n",
      " 111. 112.]\n"
     ]
    }
   ],
   "source": [
    "from Networks.ResNet import ResAntiSymNet\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "num_epochs = 100\n",
    "num_features = 2\n",
    "num_classes = 2\n",
    "size_hidden = 8\n",
    "gamma = 0.3\n",
    "h = 1\n",
    "lr = 0.5\n",
    "\n",
    "max_layers = 20\n",
    "train_results_per_layer = np.zeros(max_layers)\n",
    "test_results_per_layer = np.zeros(max_layers)\n",
    "basic_save_key = 'AntiSym_Comparison_results//AntiSymNet'\n",
    "\n",
    "for num_hidden_layers in range(15,max_layers+1):\n",
    "    tic = timeit.default_timer()\n",
    "    num_layers = num_hidden_layers + 1\n",
    "    print('Number of layers: '+str(num_layers))\n",
    "    train_results_per_epoch = np.zeros(num_epochs)\n",
    "    test_results_per_epoch = np.zeros(num_epochs)\n",
    "    for i in range(10):\n",
    "        net = ResAntiSymNet(features=num_features, classes=num_classes, num_layers=num_layers, gamma=gamma, h=h, bias=True, hidden_size=size_hidden)\n",
    "        net.set_test_tracking(True)\n",
    "        net.train(num_epochs, train_loader, test_loader, print_output=False)\n",
    "        print(net.avg_correct_pred.numpy())\n",
    "        print(net.test_results.numpy())\n",
    "        del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x215af6b9240>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from Networks.ResNet import ResAntiSymNet\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "num_epochs = 100\n",
    "num_features = 2\n",
    "num_classes = 2\n",
    "size_hidden = 8\n",
    "gamma = 0.3\n",
    "h = 0.1\n",
    "lr = 0.5\n",
    "test_set_size = 120\n",
    "\n",
    "\n",
    "num_layers = 50\n",
    "basic_save_key = 'AntiSym_Comparison_results//AntiSymNet'\n",
    "\n",
    "train_save_key = basic_save_key+'_train_per_epoch_layers_'+str(num_layers)+'_h_'+str(h)+'_lr_'+str(lr)+'.csv'\n",
    "test_save_key = basic_save_key+'_test_per_epoch_layers_'+str(num_layers)+'_h_'+str(h)+'_lr_'+str(lr)+'.csv'\n",
    "\n",
    "train_results_per_epoch = np.loadtxt(train_save_key)\n",
    "test_results_per_epoch = np.loadtxt(test_save_key)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "epochs = np.arange(1,101,1)\n",
    "ax.plot(epochs, train_results_per_epoch, label='train')\n",
    "ax.plot(epochs, test_results_per_epoch/test_set_size, label='test')\n",
    "ax.set_ylim(0,1.1)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Average Accuracy')\n",
    "ax.legend(loc='best')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
