{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.771797  #  0.575000  #\n",
      "Correct predictions: 0.5916666666666667\n",
      "#  2  #  0.427235  #  0.822917  #\n",
      "Correct predictions: 0.9166666666666666\n",
      "#  3  #  0.252071  #  0.952083  #\n",
      "Correct predictions: 0.975\n",
      "#  4  #  0.226529  #  0.952083  #\n",
      "Correct predictions: 0.975\n",
      "#  5  #  0.220710  #  0.956250  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  6  #  0.193539  #  0.975000  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  7  #  0.223180  #  0.952083  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  8  #  0.183156  #  0.977083  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  9  #  0.182346  #  0.983333  #\n",
      "Correct predictions: 0.9333333333333333\n",
      "#  10  #  0.181505  #  0.985417  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  11  #  0.182598  #  0.979167  #\n",
      "Correct predictions: 1.0\n",
      "#  12  #  0.164622  #  0.985417  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  13  #  0.176332  #  0.979167  #\n",
      "Correct predictions: 1.0\n",
      "#  14  #  0.166670  #  0.987500  #\n",
      "Correct predictions: 0.9666666666666667\n",
      "#  15  #  0.176271  #  0.981250  #\n",
      "Correct predictions: 1.0\n",
      "#  16  #  0.160454  #  0.989583  #\n",
      "Correct predictions: 1.0\n",
      "#  17  #  0.156161  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  18  #  0.189422  #  0.972917  #\n",
      "Correct predictions: 0.9166666666666666\n",
      "#  19  #  0.185322  #  0.975000  #\n",
      "Correct predictions: 1.0\n",
      "#  20  #  0.150124  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  21  #  0.167365  #  0.983333  #\n",
      "Correct predictions: 1.0\n",
      "#  22  #  0.159297  #  0.991667  #\n",
      "Correct predictions: 1.0\n",
      "#  23  #  0.154967  #  0.989583  #\n",
      "Correct predictions: 1.0\n",
      "#  24  #  0.170451  #  0.981250  #\n",
      "Correct predictions: 1.0\n",
      "#  25  #  0.182023  #  0.972917  #\n",
      "Correct predictions: 1.0\n",
      "#  26  #  0.171579  #  0.981250  #\n",
      "Correct predictions: 1.0\n",
      "#  27  #  0.166192  #  0.985417  #\n",
      "Correct predictions: 1.0\n",
      "#  28  #  0.157507  #  0.989583  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  29  #  0.163559  #  0.985417  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  30  #  0.151593  #  0.991667  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  31  #  0.155712  #  0.985417  #\n",
      "Correct predictions: 1.0\n",
      "#  32  #  0.168362  #  0.985417  #\n",
      "Correct predictions: 1.0\n",
      "#  33  #  0.160126  #  0.981250  #\n",
      "Correct predictions: 1.0\n",
      "#  34  #  0.146433  #  0.991667  #\n",
      "Correct predictions: 1.0\n",
      "#  35  #  0.148630  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  36  #  0.161294  #  0.983333  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  37  #  0.161148  #  0.989583  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  38  #  0.153750  #  0.991667  #\n",
      "Correct predictions: 0.9833333333333333\n",
      "#  39  #  0.142887  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  40  #  0.146618  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  41  #  0.151692  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  42  #  0.152706  #  0.987500  #\n",
      "Correct predictions: 1.0\n",
      "#  43  #  0.177530  #  0.977083  #\n",
      "Correct predictions: 1.0\n",
      "#  44  #  0.162201  #  0.983333  #\n",
      "Correct predictions: 1.0\n",
      "#  45  #  0.159113  #  0.987500  #\n",
      "Correct predictions: 0.975\n",
      "#  46  #  0.157848  #  0.987500  #\n",
      "Correct predictions: 1.0\n",
      "#  47  #  0.161896  #  0.983333  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  48  #  0.178220  #  0.981250  #\n",
      "Correct predictions: 1.0\n",
      "#  49  #  0.142404  #  0.995833  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  50  #  0.151090  #  0.991667  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  51  #  0.148658  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  52  #  0.162305  #  0.983333  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  53  #  0.161849  #  0.985417  #\n",
      "Correct predictions: 1.0\n",
      "#  54  #  0.164337  #  0.983333  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  55  #  0.168665  #  0.987500  #\n",
      "Correct predictions: 1.0\n",
      "#  56  #  0.164917  #  0.985417  #\n",
      "Correct predictions: 1.0\n",
      "#  57  #  0.163079  #  0.987500  #\n",
      "Correct predictions: 1.0\n",
      "#  58  #  0.161450  #  0.983333  #\n",
      "Correct predictions: 1.0\n",
      "#  59  #  0.168457  #  0.981250  #\n",
      "Correct predictions: 1.0\n",
      "#  60  #  0.168957  #  0.972917  #\n",
      "Correct predictions: 0.975\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  61  #  0.151165  #  0.989583  #\n",
      "Correct predictions: 0.975\n",
      "#  62  #  0.149153  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  63  #  0.148219  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  64  #  0.144542  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  65  #  0.146904  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  66  #  0.146803  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  67  #  0.152221  #  0.989583  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  68  #  0.148291  #  0.991667  #\n",
      "Correct predictions: 1.0\n",
      "#  69  #  0.161934  #  0.987500  #\n",
      "Correct predictions: 0.975\n",
      "#  70  #  0.143357  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  71  #  0.157789  #  0.981250  #\n",
      "Correct predictions: 1.0\n",
      "#  72  #  0.164482  #  0.983333  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  73  #  0.141512  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  74  #  0.146042  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  75  #  0.146262  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  76  #  0.155908  #  0.989583  #\n",
      "Correct predictions: 1.0\n",
      "#  77  #  0.181834  #  0.981250  #\n",
      "Correct predictions: 1.0\n",
      "#  78  #  0.142356  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  79  #  0.166088  #  0.987500  #\n",
      "Correct predictions: 1.0\n",
      "#  80  #  0.141874  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  81  #  0.149083  #  0.991667  #\n",
      "Correct predictions: 1.0\n",
      "#  82  #  0.153010  #  0.991667  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  83  #  0.157650  #  0.989583  #\n",
      "Correct predictions: 0.9833333333333333\n",
      "#  84  #  0.168660  #  0.985417  #\n",
      "Correct predictions: 1.0\n",
      "#  85  #  0.142296  #  0.995833  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  86  #  0.146330  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  87  #  0.146023  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  88  #  0.153826  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  89  #  0.148369  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  90  #  0.140617  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  91  #  0.155460  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  92  #  0.160497  #  0.987500  #\n",
      "Correct predictions: 1.0\n",
      "#  93  #  0.179562  #  0.979167  #\n",
      "Correct predictions: 1.0\n",
      "#  94  #  0.150417  #  0.991667  #\n",
      "Correct predictions: 1.0\n",
      "#  95  #  0.139858  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  96  #  0.152690  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  97  #  0.143044  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  98  #  0.151723  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  99  #  0.141466  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  100  #  0.145241  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "Time elapsed:  37.459640685\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import FCMSANet\n",
    "import torch\n",
    "\n",
    "\n",
    "def make_uniform_layer_list(layers, num_features):\n",
    "    return [num_features] * (layers+1)\n",
    "    \n",
    "def make_uniform_hidden_layer_list(layers, num_features, num_classes, size_hidden):\n",
    "    res = [num_features]\n",
    "    res.extend([size_hidden]*(layers-1))\n",
    "    res.extend([num_classes])\n",
    "    return res\n",
    "\n",
    "\n",
    "dataset_size = 600\n",
    "batch_size = 50\n",
    "test_set_size = dataset_size * 0.2\n",
    "\n",
    "\n",
    "#torch.manual_seed(0)\n",
    "train_loader, test_loader = makeMoonsDataset(dataset_size, batch_size)\n",
    "#torch.manual_seed(3)\n",
    "\n",
    "num_layers = 10\n",
    "#layers = make_uniform_layer_list(num_layers, 2)\n",
    "layers = make_uniform_hidden_layer_list(num_layers, 2, 2, 64)\n",
    "#print(layers)\n",
    "net = FCMSANet(num_fc=num_layers, sizes_fc=layers, bias=False, batchnorm=True, test=False)   \n",
    "#net = FCMSANet(num_fc=3,sizes_fc=[2,2,4,2], bias=False, batchnorm=True, test=False)  \n",
    "net.set_rho(0.5)\n",
    "net.set_ema_alpha(0.99)\n",
    "net.set_test_tracking(True)\n",
    "net.train_msa(100,train_loader, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.027722  #  0.493750  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  2  #  0.017197  #  0.518750  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  3  #  0.011124  #  0.787500  #\n",
      "Correct predictions: 0.8083333333333333\n",
      "#  4  #  0.007607  #  0.875000  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  5  #  0.007319  #  0.877083  #\n",
      "Correct predictions: 0.85\n",
      "#  6  #  0.006946  #  0.887500  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  7  #  0.006866  #  0.879167  #\n",
      "Correct predictions: 0.8416666666666667\n",
      "#  8  #  0.006962  #  0.870833  #\n",
      "Correct predictions: 0.875\n",
      "#  9  #  0.007946  #  0.870833  #\n",
      "Correct predictions: 0.8083333333333333\n",
      "#  10  #  0.007471  #  0.866667  #\n",
      "Correct predictions: 0.8833333333333333\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  11  #  0.007847  #  0.872917  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  12  #  0.006727  #  0.866667  #\n",
      "Correct predictions: 0.8416666666666667\n",
      "#  13  #  0.006621  #  0.866667  #\n",
      "Correct predictions: 0.8833333333333333\n",
      "#  14  #  0.006840  #  0.862500  #\n",
      "Correct predictions: 0.8833333333333333\n",
      "#  15  #  0.007656  #  0.858333  #\n",
      "Correct predictions: 0.8166666666666667\n",
      "#  16  #  0.006500  #  0.866667  #\n",
      "Correct predictions: 0.8166666666666667\n",
      "#  17  #  0.006826  #  0.860417  #\n",
      "Correct predictions: 0.8833333333333333\n",
      "#  18  #  0.006802  #  0.877083  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  19  #  0.006608  #  0.881250  #\n",
      "Correct predictions: 0.85\n",
      "#  20  #  0.007035  #  0.872917  #\n",
      "Correct predictions: 0.8916666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  21  #  0.006911  #  0.877083  #\n",
      "Correct predictions: 0.8416666666666667\n",
      "#  22  #  0.006439  #  0.866667  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  23  #  0.006282  #  0.885417  #\n",
      "Correct predictions: 0.875\n",
      "#  24  #  0.006819  #  0.877083  #\n",
      "Correct predictions: 0.8833333333333333\n",
      "#  25  #  0.007895  #  0.862500  #\n",
      "Correct predictions: 0.8833333333333333\n",
      "#  26  #  0.006423  #  0.872917  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  27  #  0.006748  #  0.879167  #\n",
      "Correct predictions: 0.8833333333333333\n",
      "#  28  #  0.007281  #  0.877083  #\n",
      "Correct predictions: 0.8166666666666667\n",
      "#  29  #  0.006769  #  0.866667  #\n",
      "Correct predictions: 0.8166666666666667\n",
      "#  30  #  0.006854  #  0.862500  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  31  #  0.007032  #  0.868750  #\n",
      "Correct predictions: 0.875\n",
      "#  32  #  0.005929  #  0.881250  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  33  #  0.006449  #  0.875000  #\n",
      "Correct predictions: 0.9\n",
      "#  34  #  0.007465  #  0.845833  #\n",
      "Correct predictions: 0.8\n",
      "#  35  #  0.007506  #  0.872917  #\n",
      "Correct predictions: 0.875\n",
      "#  36  #  0.007189  #  0.862500  #\n",
      "Correct predictions: 0.75\n",
      "#  37  #  0.007588  #  0.860417  #\n",
      "Correct predictions: 0.8833333333333333\n",
      "#  38  #  0.006853  #  0.866667  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  39  #  0.006403  #  0.879167  #\n",
      "Correct predictions: 0.8833333333333333\n",
      "#  40  #  0.006367  #  0.870833  #\n",
      "Correct predictions: 0.8166666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  41  #  0.006625  #  0.870833  #\n",
      "Correct predictions: 0.8333333333333334\n",
      "#  42  #  0.007121  #  0.856250  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  43  #  0.006453  #  0.868750  #\n",
      "Correct predictions: 0.8333333333333334\n",
      "#  44  #  0.007573  #  0.854167  #\n",
      "Correct predictions: 0.85\n",
      "#  45  #  0.006510  #  0.872917  #\n",
      "Correct predictions: 0.9\n",
      "#  46  #  0.006193  #  0.877083  #\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import ResAntiSymNet\n",
    "import torch\n",
    "#import gc\n",
    "#gc.collect()\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset(600,40)\n",
    "gamma = 0.3\n",
    "h = 0.1\n",
    "net = ResAntiSymNet(features=2, classes=2, num_layers=100, gamma=gamma, h=h, bias=True, hidden_size=8)\n",
    "net.set_test_tracking(True)\n",
    "net.train(num_epochs=100, dataloader=train_loader, testloader=test_loader)\n",
    "print(net.avg_correct_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.029316  #  0.464583  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  2  #  0.017411  #  0.497917  #\n",
      "Correct predictions: 0.5416666666666666\n",
      "#  3  #  0.017303  #  0.531250  #\n",
      "Correct predictions: 0.6833333333333333\n",
      "#  4  #  0.016103  #  0.572917  #\n",
      "Correct predictions: 0.825\n",
      "#  5  #  0.009681  #  0.837500  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  6  #  0.009520  #  0.841667  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  7  #  0.007282  #  0.881250  #\n",
      "Correct predictions: 0.875\n",
      "#  8  #  0.007659  #  0.862500  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  9  #  0.007233  #  0.870833  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  10  #  0.007105  #  0.885417  #\n",
      "Correct predictions: 0.85\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  11  #  0.007785  #  0.862500  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  12  #  0.007215  #  0.870833  #\n",
      "Correct predictions: 0.85\n",
      "#  13  #  0.007417  #  0.864583  #\n",
      "Correct predictions: 0.85\n",
      "#  14  #  0.007197  #  0.864583  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  15  #  0.006995  #  0.877083  #\n",
      "Correct predictions: 0.875\n",
      "#  16  #  0.007821  #  0.866667  #\n",
      "Correct predictions: 0.875\n",
      "#  17  #  0.007211  #  0.872917  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  18  #  0.006832  #  0.875000  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  19  #  0.007244  #  0.879167  #\n",
      "Correct predictions: 0.875\n",
      "#  20  #  0.006833  #  0.868750  #\n",
      "Correct predictions: 0.875\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  21  #  0.006654  #  0.887500  #\n",
      "Correct predictions: 0.875\n",
      "#  22  #  0.006578  #  0.877083  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  23  #  0.006915  #  0.875000  #\n",
      "Correct predictions: 0.85\n",
      "#  24  #  0.007495  #  0.868750  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  25  #  0.007306  #  0.877083  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  26  #  0.007292  #  0.870833  #\n",
      "Correct predictions: 0.875\n",
      "#  27  #  0.006754  #  0.877083  #\n",
      "Correct predictions: 0.875\n",
      "#  28  #  0.006647  #  0.877083  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  29  #  0.007025  #  0.870833  #\n",
      "Correct predictions: 0.85\n",
      "#  30  #  0.007614  #  0.866667  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  31  #  0.006785  #  0.868750  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  32  #  0.006839  #  0.881250  #\n",
      "Correct predictions: 0.875\n",
      "#  33  #  0.007159  #  0.868750  #\n",
      "Correct predictions: 0.875\n",
      "#  34  #  0.006778  #  0.877083  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  35  #  0.007097  #  0.875000  #\n",
      "Correct predictions: 0.85\n",
      "#  36  #  0.006668  #  0.879167  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  37  #  0.006747  #  0.877083  #\n",
      "Correct predictions: 0.875\n",
      "#  38  #  0.007414  #  0.870833  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  39  #  0.006785  #  0.881250  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  40  #  0.007066  #  0.866667  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  41  #  0.006759  #  0.872917  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  42  #  0.006536  #  0.877083  #\n",
      "Correct predictions: 0.875\n",
      "#  43  #  0.006759  #  0.872917  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  44  #  0.006835  #  0.875000  #\n",
      "Correct predictions: 0.8833333333333333\n",
      "#  45  #  0.006507  #  0.870833  #\n",
      "Correct predictions: 0.875\n",
      "#  46  #  0.006652  #  0.883333  #\n",
      "Correct predictions: 0.875\n",
      "#  47  #  0.006843  #  0.879167  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  48  #  0.006360  #  0.879167  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  49  #  0.006938  #  0.872917  #\n",
      "Correct predictions: 0.875\n",
      "#  50  #  0.006670  #  0.872917  #\n",
      "Correct predictions: 0.875\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  51  #  0.006691  #  0.868750  #\n",
      "Correct predictions: 0.875\n",
      "#  52  #  0.006696  #  0.866667  #\n",
      "Correct predictions: 0.875\n",
      "#  53  #  0.006857  #  0.868750  #\n",
      "Correct predictions: 0.875\n",
      "#  54  #  0.006796  #  0.875000  #\n",
      "Correct predictions: 0.875\n",
      "#  55  #  0.006683  #  0.860417  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  56  #  0.006782  #  0.872917  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  57  #  0.006996  #  0.864583  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  58  #  0.006986  #  0.860417  #\n",
      "Correct predictions: 0.875\n",
      "#  59  #  0.006545  #  0.868750  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  60  #  0.006572  #  0.879167  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  61  #  0.006718  #  0.872917  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  62  #  0.006520  #  0.870833  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  63  #  0.006743  #  0.866667  #\n",
      "Correct predictions: 0.875\n",
      "#  64  #  0.006683  #  0.870833  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  65  #  0.006950  #  0.862500  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  66  #  0.006552  #  0.879167  #\n",
      "Correct predictions: 0.85\n",
      "#  67  #  0.006942  #  0.872917  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  68  #  0.006742  #  0.866667  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  69  #  0.006486  #  0.875000  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  70  #  0.006788  #  0.879167  #\n",
      "Correct predictions: 0.875\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  71  #  0.006659  #  0.872917  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  72  #  0.006510  #  0.875000  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  73  #  0.006604  #  0.870833  #\n",
      "Correct predictions: 0.875\n",
      "#  74  #  0.006660  #  0.872917  #\n",
      "Correct predictions: 0.85\n",
      "#  75  #  0.006817  #  0.866667  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  76  #  0.006496  #  0.866667  #\n",
      "Correct predictions: 0.875\n",
      "#  77  #  0.006636  #  0.864583  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  78  #  0.006638  #  0.881250  #\n",
      "Correct predictions: 0.875\n",
      "#  79  #  0.006558  #  0.877083  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  80  #  0.006843  #  0.868750  #\n",
      "Correct predictions: 0.875\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  81  #  0.006438  #  0.872917  #\n",
      "Correct predictions: 0.85\n",
      "#  82  #  0.006646  #  0.879167  #\n",
      "Correct predictions: 0.875\n",
      "#  83  #  0.006586  #  0.868750  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  84  #  0.006706  #  0.870833  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  85  #  0.006533  #  0.872917  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  86  #  0.006533  #  0.870833  #\n",
      "Correct predictions: 0.875\n",
      "#  87  #  0.006815  #  0.870833  #\n",
      "Correct predictions: 0.875\n",
      "#  88  #  0.006755  #  0.872917  #\n",
      "Correct predictions: 0.875\n",
      "#  89  #  0.006579  #  0.877083  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  90  #  0.006477  #  0.877083  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  91  #  0.006645  #  0.875000  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  92  #  0.006589  #  0.870833  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  93  #  0.006683  #  0.872917  #\n",
      "Correct predictions: 0.85\n",
      "#  94  #  0.006831  #  0.868750  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  95  #  0.006837  #  0.854167  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  96  #  0.006501  #  0.877083  #\n",
      "Correct predictions: 0.875\n",
      "#  97  #  0.006604  #  0.870833  #\n",
      "Correct predictions: 0.875\n",
      "#  98  #  0.006433  #  0.877083  #\n",
      "Correct predictions: 0.875\n",
      "#  99  #  0.006663  #  0.877083  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  100  #  0.006481  #  0.879167  #\n",
      "Correct predictions: 0.85\n",
      "Time elapsed:  8.941094796000016\n",
      "tensor([0.4646, 0.4979, 0.5312, 0.5729, 0.8375, 0.8417, 0.8813, 0.8625, 0.8708,\n",
      "        0.8854, 0.8625, 0.8708, 0.8646, 0.8646, 0.8771, 0.8667, 0.8729, 0.8750,\n",
      "        0.8792, 0.8687, 0.8875, 0.8771, 0.8750, 0.8687, 0.8771, 0.8708, 0.8771,\n",
      "        0.8771, 0.8708, 0.8667, 0.8687, 0.8813, 0.8687, 0.8771, 0.8750, 0.8792,\n",
      "        0.8771, 0.8708, 0.8813, 0.8667, 0.8729, 0.8771, 0.8729, 0.8750, 0.8708,\n",
      "        0.8833, 0.8792, 0.8792, 0.8729, 0.8729, 0.8687, 0.8667, 0.8687, 0.8750,\n",
      "        0.8604, 0.8729, 0.8646, 0.8604, 0.8687, 0.8792, 0.8729, 0.8708, 0.8667,\n",
      "        0.8708, 0.8625, 0.8792, 0.8729, 0.8667, 0.8750, 0.8792, 0.8729, 0.8750,\n",
      "        0.8708, 0.8729, 0.8667, 0.8667, 0.8646, 0.8813, 0.8771, 0.8687, 0.8729,\n",
      "        0.8792, 0.8687, 0.8708, 0.8729, 0.8708, 0.8708, 0.8729, 0.8771, 0.8771,\n",
      "        0.8750, 0.8708, 0.8729, 0.8687, 0.8542, 0.8771, 0.8708, 0.8771, 0.8771,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0.8792])\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.3\n",
    "h = 1\n",
    "net = None\n",
    "net = ResAntiSymNet(features=2, classes=2, num_layers=21, gamma=gamma, h=h, bias=True, hidden_size=8)\n",
    "net.set_test_tracking(True)\n",
    "net.train(num_epochs=100, dataloader=train_loader, testloader=test_loader)\n",
    "print(net.avg_correct_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.017344  #  0.506250  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  2  #  0.017221  #  0.543750  #\n",
      "Correct predictions: 0.7416666666666667\n",
      "#  3  #  0.017083  #  0.758333  #\n",
      "Correct predictions: 0.725\n",
      "#  4  #  0.016913  #  0.727083  #\n",
      "Correct predictions: 0.7416666666666667\n",
      "#  5  #  0.016583  #  0.750000  #\n",
      "Correct predictions: 0.725\n",
      "#  6  #  0.015957  #  0.750000  #\n",
      "Correct predictions: 0.7583333333333333\n",
      "#  7  #  0.014648  #  0.770833  #\n",
      "Correct predictions: 0.775\n",
      "#  8  #  0.012616  #  0.783333  #\n",
      "Correct predictions: 0.7833333333333333\n",
      "#  9  #  0.010690  #  0.810417  #\n",
      "Correct predictions: 0.8083333333333333\n",
      "#  10  #  0.009303  #  0.839583  #\n",
      "Correct predictions: 0.8166666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  11  #  0.008267  #  0.854167  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  12  #  0.007619  #  0.875000  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  13  #  0.007144  #  0.887500  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  14  #  0.007082  #  0.872917  #\n",
      "Correct predictions: 0.875\n",
      "#  15  #  0.006887  #  0.883333  #\n",
      "Correct predictions: 0.875\n",
      "#  16  #  0.006328  #  0.891667  #\n",
      "Correct predictions: 0.8833333333333333\n",
      "#  17  #  0.006506  #  0.889583  #\n",
      "Correct predictions: 0.8916666666666667\n",
      "#  18  #  0.005510  #  0.906250  #\n",
      "Correct predictions: 0.9166666666666666\n",
      "#  19  #  0.005588  #  0.910417  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  20  #  0.005392  #  0.897917  #\n",
      "Correct predictions: 0.9\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  21  #  0.004840  #  0.912500  #\n",
      "Correct predictions: 0.9333333333333333\n",
      "#  22  #  0.009136  #  0.854167  #\n",
      "Correct predictions: 0.925\n",
      "#  23  #  0.004225  #  0.935417  #\n",
      "Correct predictions: 0.9416666666666667\n",
      "#  24  #  0.003940  #  0.943750  #\n",
      "Correct predictions: 0.9416666666666667\n",
      "#  25  #  0.003696  #  0.950000  #\n",
      "Correct predictions: 0.95\n",
      "#  26  #  0.002610  #  0.962500  #\n",
      "Correct predictions: 0.95\n",
      "#  27  #  0.001950  #  0.979167  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  28  #  0.001676  #  0.977083  #\n",
      "Correct predictions: 0.9833333333333333\n",
      "#  29  #  0.001806  #  0.972917  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  30  #  0.000944  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  31  #  0.001162  #  0.989583  #\n",
      "Correct predictions: 1.0\n",
      "#  32  #  0.000738  #  0.991667  #\n",
      "Correct predictions: 1.0\n",
      "#  33  #  0.000679  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  34  #  0.000604  #  0.991667  #\n",
      "Correct predictions: 1.0\n",
      "#  35  #  0.000508  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  36  #  0.000421  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  37  #  0.010973  #  0.916667  #\n",
      "Correct predictions: 0.85\n",
      "#  38  #  0.004246  #  0.941667  #\n",
      "Correct predictions: 1.0\n",
      "#  39  #  0.001160  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  40  #  0.000843  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  41  #  0.000613  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  42  #  0.000616  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  43  #  0.000382  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  44  #  0.000414  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  45  #  0.000318  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  46  #  0.000278  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  47  #  0.000257  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  48  #  0.000217  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  49  #  0.000250  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  50  #  0.000203  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  51  #  0.000221  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  52  #  0.000165  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  53  #  0.000153  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  54  #  0.000189  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  55  #  0.000143  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  56  #  0.000138  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  57  #  0.000133  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  58  #  0.000194  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  59  #  0.000188  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  60  #  0.000139  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  61  #  0.000125  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  62  #  0.000165  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  63  #  0.000149  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  64  #  0.000108  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  65  #  0.000215  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  66  #  0.000100  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  67  #  0.000079  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  68  #  0.000134  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  69  #  0.000129  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  70  #  0.000067  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  71  #  0.000115  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  72  #  0.000141  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  73  #  0.000085  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  74  #  0.000079  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  75  #  0.000100  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  76  #  0.000092  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  77  #  0.000053  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  78  #  0.000077  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  79  #  0.000098  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  80  #  0.000071  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  81  #  0.000079  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  82  #  0.000053  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  83  #  0.000093  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  84  #  0.000060  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  85  #  0.000101  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  86  #  0.000095  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  87  #  0.000108  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  88  #  0.000078  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  89  #  0.000092  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  90  #  0.000062  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  91  #  0.000261  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  92  #  0.000047  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  93  #  0.000040  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  94  #  0.000057  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  95  #  0.000046  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  96  #  0.000077  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  97  #  0.000044  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  98  #  0.000041  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  99  #  0.000043  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  100  #  0.000064  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "Time elapsed:  4.904766947000098\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import FCNet\n",
    "import torch\n",
    "\n",
    "dataset_size = 600\n",
    "batch_size = 40\n",
    "test_set_size = dataset_size * 0.2\n",
    "num_layers = 11\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset(dataset_size,batch_size)\n",
    "layers = make_uniform_hidden_layer_list(num_layers, 2, 2, 64)\n",
    "# net = FCNet(num_layers=num_layers, layers=layers, bias=True)\n",
    "net = FCNet(num_layers=6, layers=[2,16,32,64,128,128,2], bias=True)\n",
    "net.set_test_tracking(True)\n",
    "net.train(100, train_loader, test_loader)\n",
    "\n",
    "#net.test(test_loader, test_set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.472702  #  0.641667  #\n",
      "#  2  #  0.005676  #  0.914583  #\n",
      "#  3  #  0.003665  #  0.952083  #\n",
      "#  4  #  0.002344  #  0.975000  #\n",
      "#  5  #  0.001609  #  0.987500  #\n",
      "#  6  #  0.001037  #  0.991667  #\n",
      "#  7  #  0.000751  #  0.995833  #\n",
      "#  8  #  0.000607  #  0.995833  #\n",
      "#  9  #  0.000501  #  0.995833  #\n",
      "#  10  #  0.000413  #  0.997917  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  11  #  0.000426  #  0.993750  #\n",
      "#  12  #  0.000313  #  0.997917  #\n",
      "#  13  #  0.000319  #  0.995833  #\n",
      "#  14  #  0.000263  #  0.995833  #\n",
      "#  15  #  0.000228  #  1.000000  #\n",
      "#  16  #  0.000187  #  0.997917  #\n",
      "#  17  #  0.000175  #  0.997917  #\n",
      "#  18  #  0.000161  #  1.000000  #\n",
      "#  19  #  0.000157  #  1.000000  #\n",
      "#  20  #  0.000161  #  0.997917  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  21  #  0.000129  #  1.000000  #\n",
      "#  22  #  0.000172  #  1.000000  #\n",
      "#  23  #  0.000179  #  0.997917  #\n",
      "#  24  #  0.000146  #  0.997917  #\n",
      "#  25  #  0.000090  #  1.000000  #\n",
      "#  26  #  0.000087  #  1.000000  #\n",
      "#  27  #  0.000106  #  0.997917  #\n",
      "#  28  #  0.000091  #  1.000000  #\n",
      "#  29  #  0.000135  #  0.997917  #\n",
      "#  30  #  0.000126  #  0.997917  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  31  #  0.000100  #  1.000000  #\n",
      "#  32  #  0.000090  #  1.000000  #\n",
      "#  33  #  0.000109  #  0.997917  #\n",
      "#  34  #  0.000074  #  1.000000  #\n",
      "#  35  #  0.000074  #  1.000000  #\n",
      "#  36  #  0.000158  #  0.997917  #\n",
      "#  37  #  0.000055  #  1.000000  #\n",
      "#  38  #  0.000051  #  1.000000  #\n",
      "#  39  #  0.000048  #  1.000000  #\n",
      "#  40  #  0.000041  #  1.000000  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  41  #  0.000037  #  1.000000  #\n",
      "#  42  #  0.000050  #  1.000000  #\n",
      "#  43  #  0.000072  #  0.997917  #\n",
      "#  44  #  0.000137  #  0.995833  #\n",
      "#  45  #  0.000062  #  1.000000  #\n",
      "#  46  #  0.000165  #  0.995833  #\n",
      "#  47  #  0.000074  #  1.000000  #\n",
      "#  48  #  0.000147  #  0.995833  #\n",
      "#  49  #  0.000153  #  0.995833  #\n",
      "#  50  #  0.000083  #  0.997917  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  51  #  0.000075  #  1.000000  #\n",
      "#  52  #  0.000029  #  1.000000  #\n",
      "#  53  #  0.000053  #  1.000000  #\n",
      "#  54  #  0.000063  #  0.997917  #\n",
      "#  55  #  0.000051  #  1.000000  #\n",
      "#  56  #  0.000056  #  1.000000  #\n",
      "#  57  #  0.000041  #  1.000000  #\n",
      "#  58  #  0.000039  #  1.000000  #\n",
      "#  59  #  0.000030  #  1.000000  #\n",
      "#  60  #  0.000038  #  1.000000  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  61  #  0.000067  #  1.000000  #\n",
      "#  62  #  0.000058  #  0.997917  #\n",
      "#  63  #  0.000214  #  0.993750  #\n",
      "#  64  #  0.000050  #  1.000000  #\n",
      "#  65  #  0.000022  #  1.000000  #\n",
      "#  66  #  0.000025  #  1.000000  #\n",
      "#  67  #  0.000024  #  1.000000  #\n",
      "#  68  #  0.000030  #  1.000000  #\n",
      "#  69  #  0.000020  #  1.000000  #\n",
      "#  70  #  0.000021  #  1.000000  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  71  #  0.000028  #  1.000000  #\n",
      "#  72  #  0.000018  #  1.000000  #\n",
      "#  73  #  0.000019  #  1.000000  #\n",
      "#  74  #  0.000018  #  1.000000  #\n",
      "#  75  #  0.000019  #  1.000000  #\n",
      "#  76  #  0.000014  #  1.000000  #\n",
      "#  77  #  0.000015  #  1.000000  #\n",
      "#  78  #  0.000015  #  1.000000  #\n",
      "#  79  #  0.000020  #  1.000000  #\n",
      "#  80  #  0.000014  #  1.000000  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  81  #  0.000014  #  1.000000  #\n",
      "#  82  #  0.000013  #  1.000000  #\n",
      "#  83  #  0.000018  #  1.000000  #\n",
      "#  84  #  0.000015  #  1.000000  #\n",
      "#  85  #  0.000014  #  1.000000  #\n",
      "#  86  #  0.000012  #  1.000000  #\n",
      "#  87  #  0.000012  #  1.000000  #\n",
      "#  88  #  0.000013  #  1.000000  #\n",
      "#  89  #  0.000011  #  1.000000  #\n",
      "#  90  #  0.000012  #  1.000000  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  91  #  0.000011  #  1.000000  #\n",
      "#  92  #  0.000012  #  1.000000  #\n",
      "#  93  #  0.000012  #  1.000000  #\n",
      "#  94  #  0.000011  #  1.000000  #\n",
      "#  95  #  0.000010  #  1.000000  #\n",
      "#  96  #  0.000012  #  1.000000  #\n",
      "#  97  #  0.000011  #  1.000000  #\n",
      "#  98  #  0.000010  #  1.000000  #\n",
      "#  99  #  0.000010  #  1.000000  #\n",
      "#  100  #  0.000009  #  1.000000  #\n",
      "Time elapsed:  6.256591480996576\n",
      "Correct predictions: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import ResFCNet\n",
    "import torch\n",
    "\n",
    "def make_uniform_layer_list(layers, num_features):\n",
    "        return [num_features] * (layers+1)\n",
    "\n",
    "def make_uniform_hidden_layer_list(layers, num_features, num_classes, size_hidden):\n",
    "    res = [num_features]\n",
    "    res.extend([size_hidden]*(layers-1))\n",
    "    res.extend([num_classes])\n",
    "    return res\n",
    "\n",
    "\n",
    "dataset_size = 600\n",
    "batch_size = 40\n",
    "test_set_size = dataset_size * 0.2\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset(dataset_size,batch_size)\n",
    "\n",
    "num_layers = 14\n",
    "#layers = make_uniform_layer_list(num_layers, 2)\n",
    "layers = make_uniform_hidden_layer_list(num_layers, 2, 2, 64)\n",
    "print(len(layers))\n",
    "net = ResFCNet(num_layers=num_layers, layers=layers, bias=True)\n",
    "\n",
    "#net = ResFCNet(num_layers=30, layers=[2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2], bias=True)\n",
    "net.train(100, train_loader)\n",
    "\n",
    "net.test(test_loader, test_set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:340: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:343: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  compared_weight_signs = torch.tensor(torch.ne(new_weight_signs, old_weight_signs).detach(), dtype=torch.float32)\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:148: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_weights = new_weight_signs*torch.tensor(torch.ge(torch.abs(self.m_accumulated),self.rho*max_weight_elem*torch.ones_like(self.m_accumulated)).detach(),dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed:  10.146643088\n",
      "Seed 0   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.60604992\n",
      "Seed 1   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.699145381999998\n",
      "Seed 2   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.273395311000002\n",
      "Seed 3   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.354167834000002\n",
      "Seed 4   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.854717102000002\n",
      "Seed 5   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.360114429000006\n",
      "Seed 6   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.212407759000001\n",
      "Seed 7   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.896629595000007\n",
      "Seed 8   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.603672412999998\n",
      "Seed 9   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.097800613000004\n",
      "Seed 10   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.248547408000007\n",
      "Seed 11   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.193569678999978\n",
      "Seed 12   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.238679597000015\n",
      "Seed 13   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.202587755999986\n",
      "Seed 14   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.563680431999984\n",
      "Seed 15   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.800362409000002\n",
      "Seed 16   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.172943199000002\n",
      "Seed 17   ###   Best-Avg 0.8\n",
      "Time elapsed:  11.664164929999998\n",
      "Seed 18   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.64493668099999\n",
      "Seed 19   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.808216150000021\n",
      "Seed 20   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.073397882000023\n",
      "Seed 21   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.22652783800001\n",
      "Seed 22   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.820626478999998\n",
      "Seed 23   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.476716109999984\n",
      "Seed 24   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.439701024999977\n",
      "Seed 25   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.500485010999967\n",
      "Seed 26   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.450659146999953\n",
      "Seed 27   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.663561942000001\n",
      "Seed 28   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.897770284000046\n",
      "Seed 29   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.072501369000008\n",
      "Seed 30   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.462130296999987\n",
      "Seed 31   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.181674433000012\n",
      "Seed 32   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.720457097999997\n",
      "Seed 33   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.911982378000005\n",
      "Seed 34   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.12446210600001\n",
      "Seed 35   ###   Best-Avg 0.8\n",
      "Time elapsed:  11.751844816999949\n",
      "Seed 36   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.914494053999988\n",
      "Seed 37   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.721946830999968\n",
      "Seed 38   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.854742291999969\n",
      "Seed 39   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.348190395000017\n",
      "Seed 40   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.857938689000036\n",
      "Seed 41   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.154126195999993\n",
      "Seed 42   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.182050207999964\n",
      "Seed 43   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.263982440000007\n",
      "Seed 44   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.456949645000066\n",
      "Seed 45   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.78960939500007\n",
      "Seed 46   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.985499520000076\n",
      "Seed 47   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.699192676000052\n",
      "Seed 48   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.425626183999952\n",
      "Seed 49   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.54046722499993\n",
      "Seed 50   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.030507655000065\n",
      "Seed 51   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.125098506000086\n",
      "Seed 52   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.919794993999972\n",
      "Seed 53   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.273062202999995\n",
      "Seed 54   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.648992066000005\n",
      "Seed 55   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.420492824999997\n",
      "Seed 56   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.824312771999985\n",
      "Seed 57   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.768213887999991\n",
      "Seed 58   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.478139016\n",
      "Seed 59   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.264060061999999\n",
      "Seed 60   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.428517747\n",
      "Seed 61   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.804717488000051\n",
      "Seed 62   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.830327735999958\n",
      "Seed 63   ###   Best-Avg 0.84375\n",
      "Time elapsed:  13.149567551000018\n",
      "Seed 64   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.077774036999926\n",
      "Seed 65   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.304072090999966\n",
      "Seed 66   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.241903238999953\n",
      "Seed 67   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.745793354000057\n",
      "Seed 68   ###   Best-Avg 0.84375\n",
      "Time elapsed:  13.113821154999982\n",
      "Seed 69   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.56083924699999\n",
      "Seed 70   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.023275922000039\n",
      "Seed 71   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.34733552099999\n",
      "Seed 72   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.205158547999986\n",
      "Seed 73   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.781024666000008\n",
      "Seed 74   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.818582595000066\n",
      "Seed 75   ###   Best-Avg 0.8\n",
      "Time elapsed:  12.775833219999981\n",
      "Seed 76   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.596671489999949\n",
      "Seed 77   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.738887018000014\n",
      "Seed 78   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.079107496999995\n",
      "Seed 79   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.085618524999973\n",
      "Seed 80   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.447313674000043\n",
      "Seed 81   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.547355056000015\n",
      "Seed 82   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.684601208999993\n",
      "Seed 83   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.680478483000002\n",
      "Seed 84   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.97397130999991\n",
      "Seed 85   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.147157143999948\n",
      "Seed 86   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.294491636999965\n",
      "Seed 87   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.387910954999938\n",
      "Seed 88   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.786750217999952\n",
      "Seed 89   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.385938010000018\n",
      "Seed 90   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.857001564999791\n",
      "Seed 91   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.818907478000028\n",
      "Seed 92   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.13523568200003\n",
      "Seed 93   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.777255096999852\n",
      "Seed 94   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.35629602400013\n",
      "Seed 95   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.564049009999962\n",
      "Seed 96   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.523687422999956\n",
      "Seed 97   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.850773782000033\n",
      "Seed 98   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.6953454840002\n",
      "Seed 99   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.015339932000188\n",
      "Seed 100   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.39123638000001\n",
      "Seed 101   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.594417870000143\n",
      "Seed 102   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.783619105999833\n",
      "Seed 103   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.876350617000071\n",
      "Seed 104   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.439500544000111\n",
      "Seed 105   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.181604522000043\n",
      "Seed 106   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.076896543999965\n",
      "Seed 107   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.832593179000014\n",
      "Seed 108   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.307717772999922\n",
      "Seed 109   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.43920239099998\n",
      "Seed 110   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.498897093999858\n",
      "Seed 111   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.657795781000004\n",
      "Seed 112   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.814789377999887\n",
      "Seed 113   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.931495412999993\n",
      "Seed 114   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.288754776999895\n",
      "Seed 115   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.244204666000087\n",
      "Seed 116   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.550813107000067\n",
      "Seed 117   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.388158215999965\n",
      "Seed 118   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.750045622000016\n",
      "Seed 119   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.235819392000167\n",
      "Seed 120   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.996203699000034\n",
      "Seed 121   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.989480880999963\n",
      "Seed 122   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.056708553999897\n",
      "Seed 123   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.285523938999859\n",
      "Seed 124   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.525031678000005\n",
      "Seed 125   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.853770211999972\n",
      "Seed 126   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.874691244999894\n",
      "Seed 127   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.638378874000182\n",
      "Seed 128   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.042981213999838\n",
      "Seed 129   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.147106766999968\n",
      "Seed 130   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.252185378999911\n",
      "Seed 131   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.136590218000038\n",
      "Seed 132   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.631353791000038\n",
      "Seed 133   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.084455215999924\n",
      "Seed 134   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.278537409000137\n",
      "Seed 135   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.179119578000154\n",
      "Seed 136   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.569564312000011\n",
      "Seed 137   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.627897280999832\n",
      "Seed 138   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.784591183999964\n",
      "Seed 139   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.15707995899993\n",
      "Seed 140   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.181525357000055\n",
      "Seed 141   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.311887792000107\n",
      "Seed 142   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.489734566999914\n",
      "Seed 143   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.879751607999879\n",
      "Seed 144   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.325272771000073\n",
      "Seed 145   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.095041162999905\n",
      "Seed 146   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.463122424999938\n",
      "Seed 147   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.492982883999957\n",
      "Seed 148   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.48772615300004\n",
      "Seed 149   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.673118235000175\n",
      "Seed 150   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.028434469000103\n",
      "Seed 151   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.975339212000108\n",
      "Seed 152   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.189310215000205\n",
      "Seed 153   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.902211723999926\n",
      "Seed 154   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.252953891999823\n",
      "Seed 155   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.101709491000065\n",
      "Seed 156   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.322991391999949\n",
      "Seed 157   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.231441695000058\n",
      "Seed 158   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.062093285999936\n",
      "Seed 159   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.669411380999918\n",
      "Seed 160   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.712783790999993\n",
      "Seed 161   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.289970005000214\n",
      "Seed 162   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.054718644999866\n",
      "Seed 163   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.402621168999985\n",
      "Seed 164   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.74273369599996\n",
      "Seed 165   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.994736071000034\n",
      "Seed 166   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.28945646399984\n",
      "Seed 167   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.997000486000161\n",
      "Seed 168   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.741073809999989\n",
      "Seed 169   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.930781389999993\n",
      "Seed 170   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.058178237999982\n",
      "Seed 171   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.299073929000087\n",
      "Seed 172   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.528790453000056\n",
      "Seed 173   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.644680681000182\n",
      "Seed 174   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.82024248000016\n",
      "Seed 175   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.990435996000087\n",
      "Seed 176   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.8661229679999\n",
      "Seed 177   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.123140467999974\n",
      "Seed 178   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.590671946999919\n",
      "Seed 179   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.689152142000012\n",
      "Seed 180   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.67604732399991\n",
      "Seed 181   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.032750995000015\n",
      "Seed 182   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.450169252000023\n",
      "Seed 183   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.201859338999839\n",
      "Seed 184   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.017197729000145\n",
      "Seed 185   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.891306548999637\n",
      "Seed 186   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.39308081199988\n",
      "Seed 187   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.244145035999736\n",
      "Seed 188   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.604870163000214\n",
      "Seed 189   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.960879856999782\n",
      "Seed 190   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.77327322300016\n",
      "Seed 191   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.03098315699981\n",
      "Seed 192   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.10578955100027\n",
      "Seed 193   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.368450354000288\n",
      "Seed 194   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.467802901000141\n",
      "Seed 195   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.05645203999984\n",
      "Seed 196   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.349908370000321\n",
      "Seed 197   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.354014645000007\n",
      "Seed 198   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.494870496999738\n",
      "Seed 199   ###   Best-Avg 0.825\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import ConvNet, FCMSANet\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset()\n",
    "\n",
    "for seed in range(200):\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    net = FCMSANet(num_fc=2,sizes_fc=[2,4,2], bias=False, test=False)\n",
    "    \n",
    "    net.train_msa(60,train_loader)\n",
    "    print('Seed '+str(seed)+'   ###   Best-Avg '+str(net.best_avg))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  ###   Avg-Loss 0.016693341732025146   ###   Correct predictions 0.7354166666666667\n",
      "Epoch 2  ###   Avg-Loss 0.01539247731367747   ###   Correct predictions 0.7895833333333333\n",
      "Epoch 3  ###   Avg-Loss 0.013269580403963725   ###   Correct predictions 0.7958333333333333\n",
      "Epoch 4  ###   Avg-Loss 0.010812340180079143   ###   Correct predictions 0.8125\n",
      "Epoch 5  ###   Avg-Loss 0.009092676639556884   ###   Correct predictions 0.8375\n",
      "Epoch 6  ###   Avg-Loss 0.007869457205136618   ###   Correct predictions 0.8583333333333333\n",
      "Epoch 7  ###   Avg-Loss 0.007090519865353902   ###   Correct predictions 0.8625\n",
      "Epoch 8  ###   Avg-Loss 0.00648987740278244   ###   Correct predictions 0.8916666666666667\n",
      "Epoch 9  ###   Avg-Loss 0.006088708837827046   ###   Correct predictions 0.8979166666666667\n",
      "Epoch 10  ###   Avg-Loss 0.005733463664849599   ###   Correct predictions 0.9\n",
      "Epoch 11  ###   Avg-Loss 0.005439147353172302   ###   Correct predictions 0.9041666666666667\n",
      "Epoch 12  ###   Avg-Loss 0.005182546377182007   ###   Correct predictions 0.9125\n",
      "Epoch 13  ###   Avg-Loss 0.0049934898813565574   ###   Correct predictions 0.9104166666666667\n",
      "Epoch 14  ###   Avg-Loss 0.004753189285596212   ###   Correct predictions 0.91875\n",
      "Epoch 15  ###   Avg-Loss 0.004416432480017344   ###   Correct predictions 0.9229166666666667\n",
      "Epoch 16  ###   Avg-Loss 0.004125663638114929   ###   Correct predictions 0.93125\n",
      "Epoch 17  ###   Avg-Loss 0.003898448000351588   ###   Correct predictions 0.93125\n",
      "Epoch 18  ###   Avg-Loss 0.003615226596593857   ###   Correct predictions 0.9375\n",
      "Epoch 19  ###   Avg-Loss 0.0030299144486586253   ###   Correct predictions 0.95\n",
      "Epoch 20  ###   Avg-Loss 0.0029660664498806   ###   Correct predictions 0.95625\n",
      "Epoch 21  ###   Avg-Loss 0.00286577045917511   ###   Correct predictions 0.9479166666666666\n",
      "Epoch 22  ###   Avg-Loss 0.002026676634947459   ###   Correct predictions 0.9770833333333333\n",
      "Epoch 23  ###   Avg-Loss 0.00176669346789519   ###   Correct predictions 0.9791666666666666\n",
      "Epoch 24  ###   Avg-Loss 0.0016901899129152299   ###   Correct predictions 0.98125\n",
      "Epoch 25  ###   Avg-Loss 0.001406506821513176   ###   Correct predictions 0.9854166666666667\n",
      "Epoch 26  ###   Avg-Loss 0.0012301721920569737   ###   Correct predictions 0.9895833333333334\n",
      "Epoch 27  ###   Avg-Loss 0.0012656405568122863   ###   Correct predictions 0.9895833333333334\n",
      "Epoch 28  ###   Avg-Loss 0.0009948708117008208   ###   Correct predictions 0.9916666666666667\n",
      "Epoch 29  ###   Avg-Loss 0.0008995652198791504   ###   Correct predictions 0.99375\n",
      "Epoch 30  ###   Avg-Loss 0.0008839219808578491   ###   Correct predictions 0.99375\n",
      "Epoch 31  ###   Avg-Loss 0.0007204620788494746   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 32  ###   Avg-Loss 0.0006353583186864853   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 33  ###   Avg-Loss 0.0006963463500142097   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 34  ###   Avg-Loss 0.0006097466374437014   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 35  ###   Avg-Loss 0.0006425640856226285   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 36  ###   Avg-Loss 0.0004952810704708099   ###   Correct predictions 1.0\n",
      "Epoch 37  ###   Avg-Loss 0.00046397863576809565   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 38  ###   Avg-Loss 0.00047715206940968834   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 39  ###   Avg-Loss 0.0004436716747780641   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 40  ###   Avg-Loss 0.00044246241450309756   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 41  ###   Avg-Loss 0.0003799566999077797   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 42  ###   Avg-Loss 0.00037729634592930473   ###   Correct predictions 1.0\n",
      "Epoch 43  ###   Avg-Loss 0.00042406826590498287   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 44  ###   Avg-Loss 0.0003210838573674361   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 45  ###   Avg-Loss 0.00035742980738480884   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 46  ###   Avg-Loss 0.0003395354375243187   ###   Correct predictions 1.0\n",
      "Epoch 47  ###   Avg-Loss 0.0003530730493366718   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 48  ###   Avg-Loss 0.0003453268048663934   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 49  ###   Avg-Loss 0.0003007656273742517   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 50  ###   Avg-Loss 0.0002979370454947154   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 51  ###   Avg-Loss 0.0002575600054115057   ###   Correct predictions 1.0\n",
      "Epoch 52  ###   Avg-Loss 0.0003250787034630775   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 53  ###   Avg-Loss 0.00023143263533711433   ###   Correct predictions 1.0\n",
      "Epoch 54  ###   Avg-Loss 0.0002476739386717478   ###   Correct predictions 1.0\n",
      "Epoch 55  ###   Avg-Loss 0.0002808337099850178   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 56  ###   Avg-Loss 0.00022636189435919126   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 57  ###   Avg-Loss 0.00020725494250655173   ###   Correct predictions 1.0\n",
      "Epoch 58  ###   Avg-Loss 0.0002392620158692201   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 59  ###   Avg-Loss 0.0002071976816902558   ###   Correct predictions 1.0\n",
      "Epoch 60  ###   Avg-Loss 0.00020230164130528768   ###   Correct predictions 1.0\n",
      "Epoch 61  ###   Avg-Loss 0.00018292929356296858   ###   Correct predictions 1.0\n",
      "Epoch 62  ###   Avg-Loss 0.00022179240671296914   ###   Correct predictions 1.0\n",
      "Epoch 63  ###   Avg-Loss 0.00019362818760176498   ###   Correct predictions 1.0\n",
      "Epoch 64  ###   Avg-Loss 0.00022219737681249778   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 65  ###   Avg-Loss 0.00018464084714651108   ###   Correct predictions 1.0\n",
      "Epoch 66  ###   Avg-Loss 0.00021055651207764944   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 67  ###   Avg-Loss 0.00020628821415205796   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 68  ###   Avg-Loss 0.00019506152408818405   ###   Correct predictions 1.0\n",
      "Epoch 69  ###   Avg-Loss 0.0001632713247090578   ###   Correct predictions 1.0\n",
      "Epoch 70  ###   Avg-Loss 0.00017592293831209343   ###   Correct predictions 1.0\n",
      "Epoch 71  ###   Avg-Loss 0.00016277733569343886   ###   Correct predictions 1.0\n",
      "Epoch 72  ###   Avg-Loss 0.0001429748721420765   ###   Correct predictions 1.0\n",
      "Epoch 73  ###   Avg-Loss 0.0001682426780462265   ###   Correct predictions 1.0\n",
      "Epoch 74  ###   Avg-Loss 0.00015255645848810673   ###   Correct predictions 1.0\n",
      "Epoch 75  ###   Avg-Loss 0.00013346169143915176   ###   Correct predictions 1.0\n",
      "Epoch 76  ###   Avg-Loss 0.00015274204003314177   ###   Correct predictions 1.0\n",
      "Epoch 77  ###   Avg-Loss 0.0001632209246357282   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 78  ###   Avg-Loss 0.00016444246284663677   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 79  ###   Avg-Loss 0.0001559902292986711   ###   Correct predictions 1.0\n",
      "Epoch 80  ###   Avg-Loss 0.00012979219512393076   ###   Correct predictions 1.0\n",
      "Epoch 81  ###   Avg-Loss 0.00014563739920655887   ###   Correct predictions 1.0\n",
      "Epoch 82  ###   Avg-Loss 0.00012268397646645704   ###   Correct predictions 1.0\n",
      "Epoch 83  ###   Avg-Loss 0.00014513544738292694   ###   Correct predictions 1.0\n",
      "Epoch 84  ###   Avg-Loss 0.00012508279954393705   ###   Correct predictions 1.0\n",
      "Epoch 85  ###   Avg-Loss 0.00013901602166394394   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 86  ###   Avg-Loss 0.0001333181746304035   ###   Correct predictions 1.0\n",
      "Epoch 87  ###   Avg-Loss 0.00012044358688096205   ###   Correct predictions 1.0\n",
      "Epoch 88  ###   Avg-Loss 0.00014618256439765295   ###   Correct predictions 1.0\n",
      "Epoch 89  ###   Avg-Loss 0.00012452843754241864   ###   Correct predictions 1.0\n",
      "Epoch 90  ###   Avg-Loss 0.00011009617398182551   ###   Correct predictions 1.0\n",
      "Epoch 91  ###   Avg-Loss 0.00012323612657686074   ###   Correct predictions 1.0\n",
      "Epoch 92  ###   Avg-Loss 0.00011581191793084145   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 93  ###   Avg-Loss 0.0001325206986318032   ###   Correct predictions 1.0\n",
      "Epoch 94  ###   Avg-Loss 0.00010437506716698408   ###   Correct predictions 1.0\n",
      "Epoch 95  ###   Avg-Loss 0.00010432829149067401   ###   Correct predictions 1.0\n",
      "Epoch 96  ###   Avg-Loss 0.00011434933015455803   ###   Correct predictions 1.0\n",
      "Epoch 97  ###   Avg-Loss 0.00010554267403980096   ###   Correct predictions 1.0\n",
      "Epoch 98  ###   Avg-Loss 0.00013531527171532312   ###   Correct predictions 1.0\n",
      "Epoch 99  ###   Avg-Loss 9.843817291160425e-05   ###   Correct predictions 1.0\n",
      "Epoch 100  ###   Avg-Loss 9.126464525858562e-05   ###   Correct predictions 1.0\n",
      "Epoch 101  ###   Avg-Loss 8.715117195000251e-05   ###   Correct predictions 1.0\n",
      "Epoch 102  ###   Avg-Loss 9.70254031320413e-05   ###   Correct predictions 1.0\n",
      "Epoch 103  ###   Avg-Loss 0.00010043517686426639   ###   Correct predictions 1.0\n",
      "Epoch 104  ###   Avg-Loss 8.699353784322739e-05   ###   Correct predictions 1.0\n",
      "Epoch 105  ###   Avg-Loss 9.705725436409315e-05   ###   Correct predictions 1.0\n",
      "Epoch 106  ###   Avg-Loss 8.577681922664246e-05   ###   Correct predictions 1.0\n",
      "Epoch 107  ###   Avg-Loss 0.00010882464703172445   ###   Correct predictions 1.0\n",
      "Epoch 108  ###   Avg-Loss 0.00010686044115573167   ###   Correct predictions 1.0\n",
      "Epoch 109  ###   Avg-Loss 0.00010626811999827623   ###   Correct predictions 1.0\n",
      "Epoch 110  ###   Avg-Loss 0.00010530042927712203   ###   Correct predictions 1.0\n",
      "Epoch 111  ###   Avg-Loss 0.00010232297548403342   ###   Correct predictions 1.0\n",
      "Epoch 112  ###   Avg-Loss 8.531966401884954e-05   ###   Correct predictions 1.0\n",
      "Epoch 113  ###   Avg-Loss 8.699455453703801e-05   ###   Correct predictions 1.0\n",
      "Epoch 114  ###   Avg-Loss 8.856581213573615e-05   ###   Correct predictions 1.0\n",
      "Epoch 115  ###   Avg-Loss 7.67768050233523e-05   ###   Correct predictions 1.0\n",
      "Epoch 116  ###   Avg-Loss 8.174949325621129e-05   ###   Correct predictions 1.0\n",
      "Epoch 117  ###   Avg-Loss 7.995864531646172e-05   ###   Correct predictions 1.0\n",
      "Epoch 118  ###   Avg-Loss 7.204515859484672e-05   ###   Correct predictions 1.0\n",
      "Epoch 119  ###   Avg-Loss 8.846645553906758e-05   ###   Correct predictions 1.0\n",
      "Epoch 120  ###   Avg-Loss 8.003233621517817e-05   ###   Correct predictions 1.0\n",
      "Epoch 121  ###   Avg-Loss 7.330861408263444e-05   ###   Correct predictions 1.0\n",
      "Epoch 122  ###   Avg-Loss 6.652111187577248e-05   ###   Correct predictions 1.0\n",
      "Epoch 123  ###   Avg-Loss 7.577384822070599e-05   ###   Correct predictions 1.0\n",
      "Epoch 124  ###   Avg-Loss 6.504503932471076e-05   ###   Correct predictions 1.0\n",
      "Epoch 125  ###   Avg-Loss 9.361816725383202e-05   ###   Correct predictions 1.0\n",
      "Epoch 126  ###   Avg-Loss 6.556252483278513e-05   ###   Correct predictions 1.0\n",
      "Epoch 127  ###   Avg-Loss 6.442156542713443e-05   ###   Correct predictions 1.0\n",
      "Epoch 128  ###   Avg-Loss 7.758416080226501e-05   ###   Correct predictions 1.0\n",
      "Epoch 129  ###   Avg-Loss 7.533366636683544e-05   ###   Correct predictions 1.0\n",
      "Epoch 130  ###   Avg-Loss 6.837865803390741e-05   ###   Correct predictions 1.0\n",
      "Epoch 131  ###   Avg-Loss 7.118641709287961e-05   ###   Correct predictions 1.0\n",
      "Epoch 132  ###   Avg-Loss 8.623798688252766e-05   ###   Correct predictions 1.0\n",
      "Epoch 133  ###   Avg-Loss 6.864879590769608e-05   ###   Correct predictions 1.0\n",
      "Epoch 134  ###   Avg-Loss 7.340631758173307e-05   ###   Correct predictions 1.0\n",
      "Epoch 135  ###   Avg-Loss 5.6213835099091135e-05   ###   Correct predictions 1.0\n",
      "Epoch 136  ###   Avg-Loss 5.9551175218075515e-05   ###   Correct predictions 1.0\n",
      "Epoch 137  ###   Avg-Loss 7.160236903776725e-05   ###   Correct predictions 1.0\n",
      "Epoch 138  ###   Avg-Loss 6.109048845246435e-05   ###   Correct predictions 1.0\n",
      "Epoch 139  ###   Avg-Loss 7.094782777130603e-05   ###   Correct predictions 1.0\n",
      "Epoch 140  ###   Avg-Loss 7.791101622084776e-05   ###   Correct predictions 1.0\n",
      "Epoch 141  ###   Avg-Loss 6.113395793363451e-05   ###   Correct predictions 1.0\n",
      "Epoch 142  ###   Avg-Loss 6.176692356045048e-05   ###   Correct predictions 1.0\n",
      "Epoch 143  ###   Avg-Loss 7.38037284463644e-05   ###   Correct predictions 1.0\n",
      "Epoch 144  ###   Avg-Loss 6.984003509084384e-05   ###   Correct predictions 1.0\n",
      "Epoch 145  ###   Avg-Loss 6.579244509339333e-05   ###   Correct predictions 1.0\n",
      "Epoch 146  ###   Avg-Loss 5.51832839846611e-05   ###   Correct predictions 1.0\n",
      "Epoch 147  ###   Avg-Loss 5.711079575121403e-05   ###   Correct predictions 1.0\n",
      "Epoch 148  ###   Avg-Loss 5.0937927638490996e-05   ###   Correct predictions 1.0\n",
      "Epoch 149  ###   Avg-Loss 5.608038045465946e-05   ###   Correct predictions 1.0\n",
      "Epoch 150  ###   Avg-Loss 6.162119098007679e-05   ###   Correct predictions 1.0\n",
      "Epoch 151  ###   Avg-Loss 5.776884887988369e-05   ###   Correct predictions 1.0\n",
      "Epoch 152  ###   Avg-Loss 6.204020076741776e-05   ###   Correct predictions 1.0\n",
      "Epoch 153  ###   Avg-Loss 5.5288333290567e-05   ###   Correct predictions 1.0\n",
      "Epoch 154  ###   Avg-Loss 4.973360725368063e-05   ###   Correct predictions 1.0\n",
      "Epoch 155  ###   Avg-Loss 5.3552817553281785e-05   ###   Correct predictions 1.0\n",
      "Epoch 156  ###   Avg-Loss 4.901509576787551e-05   ###   Correct predictions 1.0\n",
      "Epoch 157  ###   Avg-Loss 5.7395625238617264e-05   ###   Correct predictions 1.0\n",
      "Epoch 158  ###   Avg-Loss 4.948605395232638e-05   ###   Correct predictions 1.0\n",
      "Epoch 159  ###   Avg-Loss 4.963681955511371e-05   ###   Correct predictions 1.0\n",
      "Epoch 160  ###   Avg-Loss 5.433883440370361e-05   ###   Correct predictions 1.0\n",
      "Epoch 161  ###   Avg-Loss 5.3806684445589784e-05   ###   Correct predictions 1.0\n",
      "Epoch 162  ###   Avg-Loss 4.746093569944302e-05   ###   Correct predictions 1.0\n",
      "Epoch 163  ###   Avg-Loss 7.585322794814905e-05   ###   Correct predictions 1.0\n",
      "Epoch 164  ###   Avg-Loss 4.612901248037815e-05   ###   Correct predictions 1.0\n",
      "Epoch 165  ###   Avg-Loss 4.46078289921085e-05   ###   Correct predictions 1.0\n",
      "Epoch 166  ###   Avg-Loss 4.7858059406280515e-05   ###   Correct predictions 1.0\n",
      "Epoch 167  ###   Avg-Loss 5.252285239597161e-05   ###   Correct predictions 1.0\n",
      "Epoch 168  ###   Avg-Loss 6.123439331228535e-05   ###   Correct predictions 1.0\n",
      "Epoch 169  ###   Avg-Loss 5.4478478462745744e-05   ###   Correct predictions 1.0\n",
      "Epoch 170  ###   Avg-Loss 5.318818924327691e-05   ###   Correct predictions 1.0\n",
      "Epoch 171  ###   Avg-Loss 4.756053676828742e-05   ###   Correct predictions 1.0\n",
      "Epoch 172  ###   Avg-Loss 4.761051774645845e-05   ###   Correct predictions 1.0\n",
      "Epoch 173  ###   Avg-Loss 5.6479397850732006e-05   ###   Correct predictions 1.0\n",
      "Epoch 174  ###   Avg-Loss 4.788977870096763e-05   ###   Correct predictions 1.0\n",
      "Epoch 175  ###   Avg-Loss 6.446146095792452e-05   ###   Correct predictions 1.0\n",
      "Epoch 176  ###   Avg-Loss 4.3726297250638405e-05   ###   Correct predictions 1.0\n",
      "Epoch 177  ###   Avg-Loss 5.430232267826796e-05   ###   Correct predictions 1.0\n",
      "Epoch 178  ###   Avg-Loss 5.736845002199213e-05   ###   Correct predictions 1.0\n",
      "Epoch 179  ###   Avg-Loss 5.051289529850086e-05   ###   Correct predictions 1.0\n",
      "Epoch 180  ###   Avg-Loss 5.14243069725732e-05   ###   Correct predictions 1.0\n",
      "Epoch 181  ###   Avg-Loss 3.913740317026774e-05   ###   Correct predictions 1.0\n",
      "Epoch 182  ###   Avg-Loss 5.9283819670478506e-05   ###   Correct predictions 1.0\n",
      "Epoch 183  ###   Avg-Loss 4.4025426420072714e-05   ###   Correct predictions 1.0\n",
      "Epoch 184  ###   Avg-Loss 4.923796902100245e-05   ###   Correct predictions 1.0\n",
      "Epoch 185  ###   Avg-Loss 4.7920217427114645e-05   ###   Correct predictions 1.0\n",
      "Epoch 186  ###   Avg-Loss 4.310249350965023e-05   ###   Correct predictions 1.0\n",
      "Epoch 187  ###   Avg-Loss 4.640198312699795e-05   ###   Correct predictions 1.0\n",
      "Epoch 188  ###   Avg-Loss 4.455156158655882e-05   ###   Correct predictions 1.0\n",
      "Epoch 189  ###   Avg-Loss 4.795944939057032e-05   ###   Correct predictions 1.0\n",
      "Epoch 190  ###   Avg-Loss 3.5239538798729576e-05   ###   Correct predictions 1.0\n",
      "Epoch 191  ###   Avg-Loss 4.360287372643749e-05   ###   Correct predictions 1.0\n",
      "Epoch 192  ###   Avg-Loss 4.576954136913021e-05   ###   Correct predictions 1.0\n",
      "Epoch 193  ###   Avg-Loss 3.716207187001904e-05   ###   Correct predictions 1.0\n",
      "Epoch 194  ###   Avg-Loss 4.478615010157227e-05   ###   Correct predictions 1.0\n",
      "Epoch 195  ###   Avg-Loss 4.2566253493229546e-05   ###   Correct predictions 1.0\n",
      "Epoch 196  ###   Avg-Loss 4.4678539658586185e-05   ###   Correct predictions 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197  ###   Avg-Loss 3.961453524728616e-05   ###   Correct predictions 1.0\n",
      "Epoch 198  ###   Avg-Loss 4.014573448027174e-05   ###   Correct predictions 1.0\n",
      "Epoch 199  ###   Avg-Loss 4.041891467447082e-05   ###   Correct predictions 1.0\n",
      "Epoch 200  ###   Avg-Loss 3.8891894898066916e-05   ###   Correct predictions 1.0\n",
      "Epoch 201  ###   Avg-Loss 3.913562589635452e-05   ###   Correct predictions 1.0\n",
      "Epoch 202  ###   Avg-Loss 4.7289044596254824e-05   ###   Correct predictions 1.0\n",
      "Epoch 203  ###   Avg-Loss 3.683992739145954e-05   ###   Correct predictions 1.0\n",
      "Epoch 204  ###   Avg-Loss 3.6930983575681846e-05   ###   Correct predictions 1.0\n",
      "Epoch 205  ###   Avg-Loss 3.837041634445389e-05   ###   Correct predictions 1.0\n",
      "Epoch 206  ###   Avg-Loss 3.6403711419552565e-05   ###   Correct predictions 1.0\n",
      "Epoch 207  ###   Avg-Loss 3.761501672367255e-05   ###   Correct predictions 1.0\n",
      "Epoch 208  ###   Avg-Loss 3.68371062601606e-05   ###   Correct predictions 1.0\n",
      "Epoch 209  ###   Avg-Loss 4.079292993992567e-05   ###   Correct predictions 1.0\n",
      "Epoch 210  ###   Avg-Loss 3.946068463847041e-05   ###   Correct predictions 1.0\n",
      "Epoch 211  ###   Avg-Loss 4.385570452238123e-05   ###   Correct predictions 1.0\n",
      "Epoch 212  ###   Avg-Loss 4.163978931804498e-05   ###   Correct predictions 1.0\n",
      "Epoch 213  ###   Avg-Loss 3.332675745089849e-05   ###   Correct predictions 1.0\n",
      "Epoch 214  ###   Avg-Loss 4.257323065151771e-05   ###   Correct predictions 1.0\n",
      "Epoch 215  ###   Avg-Loss 4.2936753015965226e-05   ###   Correct predictions 1.0\n",
      "Epoch 216  ###   Avg-Loss 3.636674955487251e-05   ###   Correct predictions 1.0\n",
      "Epoch 217  ###   Avg-Loss 3.94138313519458e-05   ###   Correct predictions 1.0\n",
      "Epoch 218  ###   Avg-Loss 3.3846831259628135e-05   ###   Correct predictions 1.0\n",
      "Epoch 219  ###   Avg-Loss 3.627579426392913e-05   ###   Correct predictions 1.0\n",
      "Epoch 220  ###   Avg-Loss 4.044672629485528e-05   ###   Correct predictions 1.0\n",
      "Epoch 221  ###   Avg-Loss 3.899487977226575e-05   ###   Correct predictions 1.0\n",
      "Epoch 222  ###   Avg-Loss 3.2388655624041956e-05   ###   Correct predictions 1.0\n",
      "Epoch 223  ###   Avg-Loss 3.148379425207774e-05   ###   Correct predictions 1.0\n",
      "Epoch 224  ###   Avg-Loss 4.479126849522193e-05   ###   Correct predictions 1.0\n",
      "Epoch 225  ###   Avg-Loss 3.618796666463216e-05   ###   Correct predictions 1.0\n",
      "Epoch 226  ###   Avg-Loss 3.777043893933296e-05   ###   Correct predictions 1.0\n",
      "Epoch 227  ###   Avg-Loss 3.232196516667803e-05   ###   Correct predictions 1.0\n",
      "Epoch 228  ###   Avg-Loss 3.010243138608833e-05   ###   Correct predictions 1.0\n",
      "Epoch 229  ###   Avg-Loss 3.525512292981148e-05   ###   Correct predictions 1.0\n",
      "Epoch 230  ###   Avg-Loss 3.3884646836668256e-05   ###   Correct predictions 1.0\n",
      "Epoch 231  ###   Avg-Loss 3.234480197230975e-05   ###   Correct predictions 1.0\n",
      "Epoch 232  ###   Avg-Loss 3.662161761894822e-05   ###   Correct predictions 1.0\n",
      "Epoch 233  ###   Avg-Loss 3.697317248831193e-05   ###   Correct predictions 1.0\n",
      "Epoch 234  ###   Avg-Loss 2.878650751275321e-05   ###   Correct predictions 1.0\n",
      "Epoch 235  ###   Avg-Loss 3.556391845146815e-05   ###   Correct predictions 1.0\n",
      "Epoch 236  ###   Avg-Loss 2.7765481111903984e-05   ###   Correct predictions 1.0\n",
      "Epoch 237  ###   Avg-Loss 3.6760074241707726e-05   ###   Correct predictions 1.0\n",
      "Epoch 238  ###   Avg-Loss 3.500080201774835e-05   ###   Correct predictions 1.0\n",
      "Epoch 239  ###   Avg-Loss 3.9084527331093946e-05   ###   Correct predictions 1.0\n",
      "Epoch 240  ###   Avg-Loss 3.469903022050858e-05   ###   Correct predictions 1.0\n",
      "Epoch 241  ###   Avg-Loss 3.173545701429248e-05   ###   Correct predictions 1.0\n",
      "Epoch 242  ###   Avg-Loss 3.360264624158541e-05   ###   Correct predictions 1.0\n",
      "Epoch 243  ###   Avg-Loss 3.289517092828949e-05   ###   Correct predictions 1.0\n",
      "Epoch 244  ###   Avg-Loss 2.8689632502694926e-05   ###   Correct predictions 1.0\n",
      "Epoch 245  ###   Avg-Loss 3.048427946244677e-05   ###   Correct predictions 1.0\n",
      "Epoch 246  ###   Avg-Loss 3.113332592571775e-05   ###   Correct predictions 1.0\n",
      "Epoch 247  ###   Avg-Loss 2.8724937389294308e-05   ###   Correct predictions 1.0\n",
      "Epoch 248  ###   Avg-Loss 3.149114587965111e-05   ###   Correct predictions 1.0\n",
      "Epoch 249  ###   Avg-Loss 3.239394476016362e-05   ###   Correct predictions 1.0\n",
      "Epoch 250  ###   Avg-Loss 2.6755715953186155e-05   ###   Correct predictions 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('lin1',nn.Linear(2, 16, bias=True)),\n",
    "    ('relu1',nn.ReLU()),\n",
    "    ('lin2',nn.Linear(16, 32, bias=True)),\n",
    "    ('relu2',nn.ReLU()),\n",
    "    ('lin3',nn.Linear(32, 64, bias=True)),\n",
    "    ('relu3',nn.ReLU()),\n",
    "    ('lin4',nn.Linear(64, 2, bias=True))\n",
    "])\n",
    ")\n",
    "\n",
    "output_frequency = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(250):\n",
    "    loss_sum = 0\n",
    "    correct_pred = 0\n",
    "    for index, (data, target) in enumerate(train_loader):\n",
    "        #print(index)\n",
    "        output = model.forward(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss_sum = loss_sum + loss.data\n",
    "        for i in range(len(output)):\n",
    "            _, ind = torch.max(output[i],0)\n",
    "            label = target[i]\n",
    "            \n",
    "            if ind.data == label.data:\n",
    "                correct_pred +=1\n",
    "        #if index % (10*(output_frequency)) == 0:\n",
    "        #    print(\"#  Epoch  #  Batch  #  Avg-Loss ###############\")\n",
    "        #if index % (output_frequency) == 0 and index > 0:\n",
    "        #    print(\"#  %d  #  %d  #  %f  #\" % (epoch+1, index, loss_sum/output_frequency))\n",
    "        #    loss_sum = 0\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch '+str(epoch+1)+'  ###   Avg-Loss '+str(loss_sum.item()/480)+'   ###   Correct predictions '+str(correct_pred/480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  ###   Avg-Loss 0.019352535406748455   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 2  ###   Avg-Loss 0.017910422881444295   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 3  ###   Avg-Loss 0.01750417153040568   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 4  ###   Avg-Loss 0.017391308148701986   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 5  ###   Avg-Loss 0.017361233631769817   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 6  ###   Avg-Loss 0.017345829804738363   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 7  ###   Avg-Loss 0.017331127325693765   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 8  ###   Avg-Loss 0.01733660101890564   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 9  ###   Avg-Loss 0.01734957695007324   ###   Correct predictions 0.4895833333333333\n",
      "Epoch 10  ###   Avg-Loss 0.01734344959259033   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 11  ###   Avg-Loss 0.017335693041483562   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 12  ###   Avg-Loss 0.01733403404553731   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 13  ###   Avg-Loss 0.017345335086186728   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 14  ###   Avg-Loss 0.017336952686309814   ###   Correct predictions 0.50625\n",
      "Epoch 15  ###   Avg-Loss 0.017330414056777953   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 16  ###   Avg-Loss 0.01733735203742981   ###   Correct predictions 0.49375\n",
      "Epoch 17  ###   Avg-Loss 0.017340999841690064   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 18  ###   Avg-Loss 0.01733502944310506   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 19  ###   Avg-Loss 0.017336159944534302   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 20  ###   Avg-Loss 0.01733076572418213   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 21  ###   Avg-Loss 0.017335595687230428   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 22  ###   Avg-Loss 0.017331228653589884   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 23  ###   Avg-Loss 0.017345523834228514   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 24  ###   Avg-Loss 0.017328786849975585   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 25  ###   Avg-Loss 0.017338613669077556   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 26  ###   Avg-Loss 0.017339388529459637   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 27  ###   Avg-Loss 0.017334789037704468   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 28  ###   Avg-Loss 0.017332659165064494   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 29  ###   Avg-Loss 0.017332019408543904   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 30  ###   Avg-Loss 0.017343803246816   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 31  ###   Avg-Loss 0.017337658007939658   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 32  ###   Avg-Loss 0.017327052354812623   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 33  ###   Avg-Loss 0.017350995540618898   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 34  ###   Avg-Loss 0.017341856161753336   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 35  ###   Avg-Loss 0.01733092466990153   ###   Correct predictions 0.49375\n",
      "Epoch 36  ###   Avg-Loss 0.017343801259994508   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 37  ###   Avg-Loss 0.017331286271413168   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 38  ###   Avg-Loss 0.0173449436823527   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 39  ###   Avg-Loss 0.017336622873942057   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 40  ###   Avg-Loss 0.01733430624008179   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 41  ###   Avg-Loss 0.01733665664990743   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 42  ###   Avg-Loss 0.01732741594314575   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 43  ###   Avg-Loss 0.017335259914398195   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 44  ###   Avg-Loss 0.017344200611114503   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 45  ###   Avg-Loss 0.017335255940755207   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 46  ###   Avg-Loss 0.01732981006304423   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 47  ###   Avg-Loss 0.017340336243311563   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 48  ###   Avg-Loss 0.017337270577748618   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 49  ###   Avg-Loss 0.01734388470649719   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 50  ###   Avg-Loss 0.017336700359980264   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 51  ###   Avg-Loss 0.017336754004160564   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 52  ###   Avg-Loss 0.017329061031341554   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 53  ###   Avg-Loss 0.017343024412790935   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 54  ###   Avg-Loss 0.017335404952367146   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 55  ###   Avg-Loss 0.017333406209945678   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 56  ###   Avg-Loss 0.01734092434247335   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 57  ###   Avg-Loss 0.017334075768788655   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 58  ###   Avg-Loss 0.017333996295928956   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 59  ###   Avg-Loss 0.01734001040458679   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 60  ###   Avg-Loss 0.017343183358510334   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 61  ###   Avg-Loss 0.017334266503651937   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 62  ###   Avg-Loss 0.017326943079630532   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 63  ###   Avg-Loss 0.017331435283025106   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 64  ###   Avg-Loss 0.01734351714452108   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 65  ###   Avg-Loss 0.01733464002609253   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 66  ###   Avg-Loss 0.017335947354634604   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 67  ###   Avg-Loss 0.017339940865834555   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 68  ###   Avg-Loss 0.017335885763168336   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 69  ###   Avg-Loss 0.017330139875411987   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 70  ###   Avg-Loss 0.017336839437484743   ###   Correct predictions 0.48125\n",
      "Epoch 71  ###   Avg-Loss 0.01733196576436361   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 72  ###   Avg-Loss 0.01733160416285197   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 73  ###   Avg-Loss 0.017341158787409463   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 74  ###   Avg-Loss 0.01734090844790141   ###   Correct predictions 0.4895833333333333\n",
      "Epoch 75  ###   Avg-Loss 0.01733473539352417   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 76  ###   Avg-Loss 0.01733032464981079   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 77  ###   Avg-Loss 0.01734740932782491   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 78  ###   Avg-Loss 0.017335404952367146   ###   Correct predictions 0.49375\n",
      "Epoch 79  ###   Avg-Loss 0.017347671588261924   ###   Correct predictions 0.4895833333333333\n",
      "Epoch 80  ###   Avg-Loss 0.017328637838363647   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 81  ###   Avg-Loss 0.017335180441538492   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 82  ###   Avg-Loss 0.017335520188013712   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 83  ###   Avg-Loss 0.01733883221944173   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 84  ###   Avg-Loss 0.017346135775248208   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 85  ###   Avg-Loss 0.0173315425713857   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 86  ###   Avg-Loss 0.017339418331782024   ###   Correct predictions 0.48125\n",
      "Epoch 87  ###   Avg-Loss 0.01734487811724345   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 88  ###   Avg-Loss 0.01733728249867757   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 89  ###   Avg-Loss 0.01733392079671224   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 90  ###   Avg-Loss 0.017338260014851888   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 91  ###   Avg-Loss 0.017348573605219523   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 92  ###   Avg-Loss 0.017336924870808918   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 93  ###   Avg-Loss 0.017339259386062622   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 94  ###   Avg-Loss 0.017346447706222533   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 95  ###   Avg-Loss 0.01733930706977844   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 96  ###   Avg-Loss 0.01734011769294739   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 97  ###   Avg-Loss 0.01733176310857137   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 98  ###   Avg-Loss 0.017335659265518187   ###   Correct predictions 0.48125\n",
      "Epoch 99  ###   Avg-Loss 0.017331586281458537   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 100  ###   Avg-Loss 0.017337832848230997   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 101  ###   Avg-Loss 0.017333618799845376   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 102  ###   Avg-Loss 0.017344723145167034   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 103  ###   Avg-Loss 0.0173422376314799   ###   Correct predictions 0.48541666666666666\n",
      "Epoch 104  ###   Avg-Loss 0.017340991894404092   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 105  ###   Avg-Loss 0.01734160582224528   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 106  ###   Avg-Loss 0.017327898740768434   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 107  ###   Avg-Loss 0.01732935905456543   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 108  ###   Avg-Loss 0.017331963777542113   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 109  ###   Avg-Loss 0.01732992132504781   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 110  ###   Avg-Loss 0.017332820097605388   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 111  ###   Avg-Loss 0.017328357696533202   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 112  ###   Avg-Loss 0.017340288559595744   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 113  ###   Avg-Loss 0.017335466543833413   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 114  ###   Avg-Loss 0.017333012819290162   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 115  ###   Avg-Loss 0.017334548632303874   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 116  ###   Avg-Loss 0.017332889636357627   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 117  ###   Avg-Loss 0.017335166533788044   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 118  ###   Avg-Loss 0.01733022133509318   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 119  ###   Avg-Loss 0.017332543929417927   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 120  ###   Avg-Loss 0.017333388328552246   ###   Correct predictions 0.48541666666666666\n",
      "Epoch 121  ###   Avg-Loss 0.017331175009409585   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 122  ###   Avg-Loss 0.017329037189483643   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 123  ###   Avg-Loss 0.017340620358784992   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 124  ###   Avg-Loss 0.01733303666114807   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 125  ###   Avg-Loss 0.0173516849676768   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 126  ###   Avg-Loss 0.0173428475856781   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 127  ###   Avg-Loss 0.017335009574890137   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 128  ###   Avg-Loss 0.017333370447158814   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 129  ###   Avg-Loss 0.017340290546417236   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 130  ###   Avg-Loss 0.01732499400774638   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 131  ###   Avg-Loss 0.01733763813972473   ###   Correct predictions 0.48541666666666666\n",
      "Epoch 132  ###   Avg-Loss 0.017332053184509276   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 133  ###   Avg-Loss 0.017353246609369915   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 134  ###   Avg-Loss 0.017330745855967205   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 135  ###   Avg-Loss 0.01732763648033142   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 136  ###   Avg-Loss 0.017330714066823325   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 137  ###   Avg-Loss 0.017336773872375488   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 138  ###   Avg-Loss 0.017343024412790935   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 139  ###   Avg-Loss 0.01733009417851766   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 140  ###   Avg-Loss 0.017354045311609903   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 141  ###   Avg-Loss 0.017332154512405395   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 142  ###   Avg-Loss 0.017334461212158203   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 143  ###   Avg-Loss 0.017343628406524658   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 144  ###   Avg-Loss 0.017340620358784992   ###   Correct predictions 0.46875\n",
      "Epoch 145  ###   Avg-Loss 0.01733053723971049   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 146  ###   Avg-Loss 0.017334272464116413   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 147  ###   Avg-Loss 0.017340207099914552   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 148  ###   Avg-Loss 0.01733244260152181   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 149  ###   Avg-Loss 0.017334314187367757   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 150  ###   Avg-Loss 0.0173399825890859   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 151  ###   Avg-Loss 0.017332218090693154   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 152  ###   Avg-Loss 0.017343036333719888   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 153  ###   Avg-Loss 0.01734464367230733   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 154  ###   Avg-Loss 0.017339040835698444   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 155  ###   Avg-Loss 0.01733132799466451   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 156  ###   Avg-Loss 0.0173313041528066   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 157  ###   Avg-Loss 0.017345698674519856   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 158  ###   Avg-Loss 0.01734340190887451   ###   Correct predictions 0.47708333333333336\n",
      "Epoch 159  ###   Avg-Loss 0.017333271106084187   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 160  ###   Avg-Loss 0.01733576456705729   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 161  ###   Avg-Loss 0.01733560562133789   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 162  ###   Avg-Loss 0.017329166332880657   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 163  ###   Avg-Loss 0.017341987291971842   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 164  ###   Avg-Loss 0.017338371276855467   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 165  ###   Avg-Loss 0.017340058088302614   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 166  ###   Avg-Loss 0.017344150940577188   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 167  ###   Avg-Loss 0.017344385385513306   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 168  ###   Avg-Loss 0.017346356312433878   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 169  ###   Avg-Loss 0.017333459854125977   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 170  ###   Avg-Loss 0.01733231743176778   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 171  ###   Avg-Loss 0.01732916235923767   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 172  ###   Avg-Loss 0.017331536610921225   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 173  ###   Avg-Loss 0.01733509699503581   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 174  ###   Avg-Loss 0.01732465426127116   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 175  ###   Avg-Loss 0.017342160145441692   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 176  ###   Avg-Loss 0.017330139875411987   ###   Correct predictions 0.5104166666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177  ###   Avg-Loss 0.017332039276758828   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 178  ###   Avg-Loss 0.01733199159304301   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 179  ###   Avg-Loss 0.017338985204696657   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 180  ###   Avg-Loss 0.017342575391133628   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 181  ###   Avg-Loss 0.017335110902786256   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 182  ###   Avg-Loss 0.017331095536549886   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 183  ###   Avg-Loss 0.017341347535451253   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 184  ###   Avg-Loss 0.017332384983698528   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 185  ###   Avg-Loss 0.017330108086268108   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 186  ###   Avg-Loss 0.017338589827219645   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 187  ###   Avg-Loss 0.017340189218521117   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 188  ###   Avg-Loss 0.017331349849700927   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 189  ###   Avg-Loss 0.017359145482381187   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 190  ###   Avg-Loss 0.017331101497014365   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 191  ###   Avg-Loss 0.017330982287724814   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 192  ###   Avg-Loss 0.017326368888219198   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 193  ###   Avg-Loss 0.017343854904174803   ###   Correct predictions 0.47708333333333336\n",
      "Epoch 194  ###   Avg-Loss 0.017337441444396973   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 195  ###   Avg-Loss 0.017333298921585083   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 196  ###   Avg-Loss 0.017332341273625693   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 197  ###   Avg-Loss 0.017343161503473918   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 198  ###   Avg-Loss 0.017330855131149292   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 199  ###   Avg-Loss 0.01732876698176066   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 200  ###   Avg-Loss 0.017356745402018228   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 201  ###   Avg-Loss 0.017346465587615968   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 202  ###   Avg-Loss 0.017329835891723634   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 203  ###   Avg-Loss 0.01734454035758972   ###   Correct predictions 0.4895833333333333\n",
      "Epoch 204  ###   Avg-Loss 0.01733029286066691   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 205  ###   Avg-Loss 0.01733484069506327   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 206  ###   Avg-Loss 0.017335607608159383   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 207  ###   Avg-Loss 0.017337212959925335   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 208  ###   Avg-Loss 0.017347023884455363   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 209  ###   Avg-Loss 0.017332309484481813   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 210  ###   Avg-Loss 0.01733231743176778   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 211  ###   Avg-Loss 0.01734496553738912   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 212  ###   Avg-Loss 0.0173362930615743   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 213  ###   Avg-Loss 0.017337344090143838   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 214  ###   Avg-Loss 0.017353427410125733   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 215  ###   Avg-Loss 0.017331113417943318   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 216  ###   Avg-Loss 0.01733746329943339   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 217  ###   Avg-Loss 0.017332746585210165   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 218  ###   Avg-Loss 0.01733715335528056   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 219  ###   Avg-Loss 0.017337143421173096   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 220  ###   Avg-Loss 0.0173479954401652   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 221  ###   Avg-Loss 0.01733072797457377   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 222  ###   Avg-Loss 0.01734137535095215   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 223  ###   Avg-Loss 0.017333173751831056   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 224  ###   Avg-Loss 0.017326873540878297   ###   Correct predictions 0.50625\n",
      "Epoch 225  ###   Avg-Loss 0.01732667684555054   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 226  ###   Avg-Loss 0.017343149582544962   ###   Correct predictions 0.47708333333333336\n",
      "Epoch 227  ###   Avg-Loss 0.017348565657933555   ###   Correct predictions 0.49375\n",
      "Epoch 228  ###   Avg-Loss 0.017334616184234618   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 229  ###   Avg-Loss 0.017330666383107502   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 230  ###   Avg-Loss 0.017340850830078126   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 231  ###   Avg-Loss 0.01733234922091166   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 232  ###   Avg-Loss 0.01734009782473246   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 233  ###   Avg-Loss 0.017341488599777223   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 234  ###   Avg-Loss 0.017334944009780882   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 235  ###   Avg-Loss 0.017334401607513428   ###   Correct predictions 0.50625\n",
      "Epoch 236  ###   Avg-Loss 0.01733740170796712   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 237  ###   Avg-Loss 0.017334469159444175   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 238  ###   Avg-Loss 0.01733429233233134   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 239  ###   Avg-Loss 0.017332722743352253   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 240  ###   Avg-Loss 0.01733097235361735   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 241  ###   Avg-Loss 0.017334298292795817   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 242  ###   Avg-Loss 0.017329289515813192   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 243  ###   Avg-Loss 0.01733185847600301   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 244  ###   Avg-Loss 0.017335520188013712   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 245  ###   Avg-Loss 0.017344415187835693   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 246  ###   Avg-Loss 0.017336014906565347   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 247  ###   Avg-Loss 0.017329887549082438   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 248  ###   Avg-Loss 0.017341379324595133   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 249  ###   Avg-Loss 0.01733956535657247   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 250  ###   Avg-Loss 0.01733067234357198   ###   Correct predictions 0.5104166666666666\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "from Dataset.Dataset import makeMoonsDataset\n",
    "\n",
    "dataset_size = 600\n",
    "batch_size = 40\n",
    "train_size = dataset_size*0.8\n",
    "test_size = dataset_size*0.2\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset(dataset_size,batch_size)\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('lin1',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu1',nn.ReLU()),\n",
    "    ('lin2',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu2',nn.ReLU()),\n",
    "    ('lin3',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu3',nn.ReLU()),\n",
    "    ('lin4',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu4',nn.ReLU()),\n",
    "    ('lin5',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu5',nn.ReLU()),\n",
    "    ('lin6',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu6',nn.ReLU()),\n",
    "    ('lin7',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu7',nn.ReLU()),\n",
    "    ('lin8',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu8',nn.ReLU()),\n",
    "    ('lin9',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu9',nn.ReLU()),\n",
    "    ('lin10',nn.Linear(2, 2, bias=True))\n",
    "])\n",
    ")\n",
    "\n",
    "output_frequency = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(250):\n",
    "    loss_sum = 0\n",
    "    correct_pred = 0\n",
    "    for index, (data, target) in enumerate(train_loader):\n",
    "        #print(index)\n",
    "        output = model.forward(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss_sum = loss_sum + loss.data\n",
    "        for i in range(len(output)):\n",
    "            _, ind = torch.max(output[i],0)\n",
    "            label = target[i]\n",
    "            \n",
    "            if ind.data == label.data:\n",
    "                correct_pred +=1\n",
    "        #if index % (10*(output_frequency)) == 0:\n",
    "        #    print(\"#  Epoch  #  Batch  #  Avg-Loss ###############\")\n",
    "        #if index % (output_frequency) == 0 and index > 0:\n",
    "        #    print(\"#  %d  #  %d  #  %f  #\" % (epoch+1, index, loss_sum/output_frequency))\n",
    "        #    loss_sum = 0\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch '+str(epoch+1)+'  ###   Avg-Loss '+str(loss_sum.item()/train_size)+'   ###   Correct predictions '+str(correct_pred/train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7115,  1.4547]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.6434, -2.7388]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3886,  2.0311]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.9741, -3.1041]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-0.8089,  0.1275]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3526, -2.2834]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.8463, -2.8233]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.9055, -2.8244]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-1.7085,  1.4306]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.7206,  2.3536]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 0.7751, -1.6380]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.4437,  2.1093]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.1609,  1.7324]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-0.5344,  0.0122]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3842,  2.1159]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-1.5523,  1.2432]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.5676, -2.6339]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 2.0958, -3.2221]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 0.9023, -1.6219]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.7551,  2.3714]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3445, -2.3139]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.5533,  2.1857]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3841, -2.4257]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3780,  2.0977]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 0.0020, -0.6190]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 0.1119, -0.9363]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3832, -2.3543]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.5189, -2.5425]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-1.9774,  1.4456]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.2206, -2.2302]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-0.1666, -0.6191]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.3856,  2.0124]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.4732, -2.4705]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.4295,  2.1136]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-0.6206,  0.0895]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.7165, -2.7979]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.8907, -2.8468]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.8076, -2.8020]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3121,  1.9327]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.0975,  1.7790]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "for index, (data, target) in enumerate(test_loader):\n",
    "    print(model.forward(data))\n",
    "    print(target)\n",
    "    print('####################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 1.0\n"
     ]
    }
   ],
   "source": [
    "net.test(test_loader,120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,\n",
      "        1, 1])\n",
      "tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        1, 1])\n",
      "tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,\n",
      "        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
      "        1, 0])\n",
      "tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
      "        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "        0, 1])\n",
      "tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
      "        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,\n",
      "        0, 0])\n",
      "tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n",
      "        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        1, 1])\n",
      "tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n",
      "        1, 1])\n",
      "tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
      "        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,\n",
      "        1, 0])\n",
      "tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
      "        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1])\n",
      "tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 0])\n",
      "[1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "from pandas import DataFrame\n",
    "import torch\n",
    "\n",
    "x1 = []\n",
    "x2 = []\n",
    "y = []\n",
    "for (data,label) in train_loader:\n",
    "    res = torch.argmax(net.forward(data), dim=1)\n",
    "    print(res)\n",
    "    for point in range(len(data)):\n",
    "        x1.append(data[point][0].item())\n",
    "        x2.append(data[point][1].item())\n",
    "        y.append(res[point].item())\n",
    "#print(x1)\n",
    "df = DataFrame(dict(x=x1, y=x2, label=y))\n",
    "print(y)\n",
    "colors = {0:'red', 1:'blue'}\n",
    "fig, ax = pyplot.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 1.0\n"
     ]
    }
   ],
   "source": [
    "correct_pred = 0\n",
    "for _, (data, label) in enumerate(test_loader):\n",
    "    prediction = net.forward(data)\n",
    "    _, ind = torch.max(prediction,1)\n",
    "    if ind.data == label.data:\n",
    "        correct_pred +=1\n",
    "print('Correct predictions: '+str(correct_pred/120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:168: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  compared_weight_signs = torch.tensor(torch.ne(new_weight_signs, old_weight_signs).detach(), dtype=torch.float32)\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:173: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_weights = new_weight_signs*torch.tensor(torch.ge(torch.abs(self.m_accumulated),self.rho*max_weight_elem*torch.ones_like(self.m_accumulated)).detach(),dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import loadMNIST\n",
    "from Networks.ResNet import ConvNet, FCMSANet\n",
    "\n",
    "#net = ConvNet(3, [1], [3,6], num_fc=2, sizes_fc=[100,10])\n",
    "net = FCMSANet(num_fc=4,sizes_fc=[784,1024,1024,1024,10], bias=False, batchnorm=True, test=False)\n",
    "train_loader, test_loader = loadMNIST('Dataset/input/mnist_train.csv', 'Dataset/input/mnist_test.csv')\n",
    "#net.train_backprop(1,train_loader)\n",
    "#net.test(test_loader,10000)\n",
    "print(train_loader.batch_size)\n",
    "\n",
    "net.train_msa(1,train_loader)\n",
    "\n",
    "#for index, (data, target) in enumerate(train_loader):\n",
    "#    if index == 1:\n",
    "#        print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "60000\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.021136  #  0.286883  #\n",
      "#  2  #  0.013666  #  0.829800  #\n",
      "#  3  #  0.013053  #  0.897417  #\n",
      "#  4  #  0.012986  #  0.906250  #\n",
      "#  5  #  0.012988  #  0.906350  #\n",
      "#  6  #  0.012940  #  0.911467  #\n",
      "#  7  #  0.012933  #  0.911933  #\n",
      "#  8  #  0.012929  #  0.911917  #\n",
      "#  9  #  0.012904  #  0.915100  #\n",
      "#  10  #  0.012890  #  0.917233  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  11  #  0.012865  #  0.920600  #\n",
      "#  12  #  0.012873  #  0.919233  #\n",
      "#  13  #  0.012850  #  0.921300  #\n",
      "#  14  #  0.012837  #  0.922417  #\n",
      "#  15  #  0.012826  #  0.923083  #\n",
      "#  16  #  0.012819  #  0.925200  #\n",
      "#  17  #  0.012815  #  0.925233  #\n",
      "#  18  #  0.012803  #  0.924800  #\n",
      "#  19  #  0.012793  #  0.925500  #\n",
      "#  20  #  0.012786  #  0.927700  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  21  #  0.012767  #  0.928617  #\n",
      "#  22  #  0.012764  #  0.929817  #\n",
      "#  23  #  0.012761  #  0.929783  #\n",
      "#  24  #  0.012750  #  0.931117  #\n",
      "#  25  #  0.012755  #  0.930150  #\n",
      "#  26  #  0.012739  #  0.930950  #\n",
      "#  27  #  0.012737  #  0.932067  #\n",
      "#  28  #  0.012714  #  0.934367  #\n",
      "#  29  #  0.012710  #  0.933683  #\n",
      "#  30  #  0.012709  #  0.934067  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  31  #  0.012702  #  0.935467  #\n",
      "#  32  #  0.012690  #  0.937067  #\n",
      "#  33  #  0.012683  #  0.936800  #\n",
      "#  34  #  0.012679  #  0.937833  #\n",
      "#  35  #  0.012673  #  0.937717  #\n",
      "#  36  #  0.012665  #  0.939350  #\n",
      "#  37  #  0.012664  #  0.938350  #\n",
      "#  38  #  0.012651  #  0.939567  #\n",
      "#  39  #  0.012656  #  0.939817  #\n",
      "#  40  #  0.012646  #  0.941567  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  41  #  0.012644  #  0.940967  #\n",
      "#  42  #  0.012632  #  0.942317  #\n",
      "#  43  #  0.012623  #  0.944083  #\n",
      "#  44  #  0.012625  #  0.943300  #\n",
      "#  45  #  0.012613  #  0.944450  #\n",
      "#  46  #  0.012618  #  0.943500  #\n",
      "#  47  #  0.012606  #  0.944933  #\n",
      "#  48  #  0.012599  #  0.945317  #\n",
      "#  49  #  0.012598  #  0.946067  #\n",
      "#  50  #  0.012588  #  0.946917  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  51  #  0.012578  #  0.947050  #\n",
      "#  52  #  0.012578  #  0.947650  #\n",
      "#  53  #  0.012573  #  0.948067  #\n",
      "#  54  #  0.012572  #  0.948083  #\n",
      "#  55  #  0.012566  #  0.948533  #\n",
      "#  56  #  0.012561  #  0.948933  #\n",
      "#  57  #  0.012555  #  0.949333  #\n",
      "#  58  #  0.012557  #  0.949850  #\n",
      "#  59  #  0.012548  #  0.950500  #\n",
      "#  60  #  0.012550  #  0.950417  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  61  #  0.012548  #  0.950367  #\n",
      "#  62  #  0.012538  #  0.951200  #\n",
      "#  63  #  0.012539  #  0.951067  #\n",
      "#  64  #  0.012532  #  0.952583  #\n",
      "#  65  #  0.012539  #  0.951133  #\n",
      "#  66  #  0.012519  #  0.952433  #\n",
      "#  67  #  0.012516  #  0.953233  #\n",
      "#  68  #  0.012518  #  0.953417  #\n",
      "#  69  #  0.012520  #  0.952833  #\n",
      "#  70  #  0.012513  #  0.954000  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  71  #  0.012516  #  0.955067  #\n",
      "#  72  #  0.012513  #  0.955033  #\n",
      "#  73  #  0.012505  #  0.955567  #\n",
      "#  74  #  0.012507  #  0.954783  #\n",
      "#  75  #  0.012495  #  0.955800  #\n",
      "#  76  #  0.012499  #  0.955583  #\n",
      "#  77  #  0.012494  #  0.956133  #\n",
      "#  78  #  0.012495  #  0.955817  #\n",
      "#  79  #  0.012485  #  0.957150  #\n",
      "#  80  #  0.012484  #  0.956950  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  81  #  0.012489  #  0.956650  #\n",
      "#  82  #  0.012486  #  0.956533  #\n",
      "#  83  #  0.012485  #  0.956917  #\n",
      "#  84  #  0.012485  #  0.957567  #\n",
      "#  85  #  0.012472  #  0.958333  #\n",
      "#  86  #  0.012476  #  0.957617  #\n",
      "#  87  #  0.012479  #  0.957600  #\n",
      "#  88  #  0.012475  #  0.957917  #\n",
      "#  89  #  0.012474  #  0.957683  #\n",
      "#  90  #  0.012469  #  0.958500  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  91  #  0.012469  #  0.957800  #\n",
      "#  92  #  0.012466  #  0.958800  #\n",
      "#  93  #  0.012467  #  0.958267  #\n",
      "#  94  #  0.012462  #  0.959483  #\n",
      "#  95  #  0.012464  #  0.958783  #\n",
      "#  96  #  0.012465  #  0.959083  #\n",
      "#  97  #  0.012455  #  0.960100  #\n",
      "#  98  #  0.012458  #  0.959733  #\n",
      "#  99  #  0.012456  #  0.960383  #\n",
      "#  100  #  0.012450  #  0.960350  #\n",
      "Time elapsed:  42948.913511782994\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import loadMNIST\n",
    "from Networks.ResNet import ConvMSANet\n",
    "\n",
    "#net = ConvNet(3, [1], [3,6], num_fc=2, sizes_fc=[100,10])\n",
    "net = ConvMSANet(num_conv=5, num_channels=[1,3,6,6,12,12], subsample_points=[2,4], num_fc=2,sizes_fc=[588,900,10], batchnorm=True, test=False)\n",
    "train_loader, test_loader = loadMNIST('Dataset/input/mnist_train.csv', 'Dataset/input/mnist_test.csv')\n",
    "#net.train_backprop(1,train_loader)\n",
    "#net.test(test_loader,10000)\n",
    "print(train_loader.batch_size)\n",
    "print(len(train_loader.dataset))\n",
    "\n",
    "net.train_msa(100,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.017974  #  0.452367  #\n",
      "Test set accuracy:  0.9106\n",
      "#  2  #  0.005898  #  0.936783  #\n",
      "Test set accuracy:  0.942\n",
      "#  3  #  0.005187  #  0.950850  #\n",
      "Test set accuracy:  0.9385\n",
      "#  4  #  0.005175  #  0.951350  #\n",
      "Test set accuracy:  0.946\n",
      "#  5  #  0.005140  #  0.951917  #\n",
      "Test set accuracy:  0.9442\n",
      "#  6  #  0.005118  #  0.952883  #\n",
      "Test set accuracy:  0.9511\n",
      "#  7  #  0.005089  #  0.953283  #\n",
      "Test set accuracy:  0.9474\n",
      "#  8  #  0.005055  #  0.954533  #\n",
      "Test set accuracy:  0.9507\n",
      "#  9  #  0.005024  #  0.955300  #\n",
      "Test set accuracy:  0.9587\n",
      "#  10  #  0.005018  #  0.955067  #\n",
      "Test set accuracy:  0.9487\n",
      "Time elapsed:  804.156953663\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import loadMNIST\n",
    "from Networks.ResNet import ConvMSANet\n",
    "\n",
    "#net = ConvNet(3, [1], [3,6], num_fc=2, sizes_fc=[100,10])\n",
    "net = ConvMSANet(num_conv=3, num_channels=[1,3,6,12], subsample_points=[0,1], num_fc=2,sizes_fc=[588,900,10], batchnorm=True, test=False)\n",
    "train_loader, test_loader = loadMNIST('Dataset/input/mnist_train.csv', 'Dataset/input/mnist_test.csv')\n",
    "net.set_test_tracking(True)\n",
    "\n",
    "net.train_msa(10,train_loader, testloader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.004958  #  0.956550  #\n",
      "Test set accuracy:  0.9556\n",
      "Time elapsed:  65.56534038500013\n"
     ]
    }
   ],
   "source": [
    "net.train_msa(1,train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy:  0.9487\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9487"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.test(test_loader,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "60000\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.010105  #  0.672850  #\n",
      "#  2  #  0.001262  #  0.961583  #\n",
      "#  3  #  0.000785  #  0.976367  #\n",
      "#  4  #  0.000575  #  0.982483  #\n",
      "#  5  #  0.000435  #  0.986500  #\n",
      "#  6  #  0.000330  #  0.989950  #\n",
      "#  7  #  0.000251  #  0.992450  #\n",
      "#  8  #  0.000219  #  0.993333  #\n",
      "#  9  #  0.000174  #  0.994683  #\n",
      "#  10  #  0.000160  #  0.994983  #\n",
      "Time elapsed:  110.92861831799999\n"
     ]
    }
   ],
   "source": [
    "from Networks.ResNet import ConvNet\n",
    "from Dataset.Dataset import loadMNIST\n",
    "\n",
    "net = ConvNet(num_conv=3, num_channels=[1,3,6,12], subsample_points=[0,1], num_fc=2,sizes_fc=[588,900,10])\n",
    "train_loader, test_loader = loadMNIST('Dataset/input/mnist_train.csv', 'Dataset/input/mnist_test.csv')\n",
    "#net.train_backprop(1,train_loader)\n",
    "#net.test(test_loader,10000)\n",
    "print(train_loader.batch_size)\n",
    "print(len(train_loader.dataset))\n",
    "\n",
    "net.train(10,train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 0.9812\n"
     ]
    }
   ],
   "source": [
    "net.test(test_loader, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1574, -0.1913,  0.3191],\n",
      "        [ 0.4584,  0.3066, -0.3247],\n",
      "        [ 0.2495,  0.4553,  0.0500],\n",
      "        [ 0.3111, -0.3222,  0.2152]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.4820, -0.5667, -0.2615,  0.5324], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "test_layer = torch.nn.Linear(3, 4)\n",
    "print(test_layer.weight)\n",
    "print(test_layer.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6., 18., 36.], grad_fn=<SumBackward2>)\n",
      "tensor([[1., 1., 1.],\n",
      "        [2., 2., 2.],\n",
      "        [3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([[1,2,3],[2,3,4],[3,4,5]],dtype=torch.float32, requires_grad=True)\n",
    "y = torch.tensor([[1,1,1],[2,2,2],[3,3,3]],dtype=torch.float32)\n",
    "\n",
    "z = torch.sum(x*y, dim=1)\n",
    "print(z)\n",
    "#z.backward(torch.FloatTensor([0,0,1]), retain_graph=True)\n",
    "z.backward(torch.FloatTensor([1,1,1]), retain_graph=True)\n",
    "#z[0].backward(retain_graph=True)\n",
    "#z[1].backward(retain_graph=True)\n",
    "#z[2].backward(retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "tensor([[[[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]],\n",
      "\n",
      "         [[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]],\n",
      "\n",
      "         [[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]]],\n",
      "\n",
      "\n",
      "        [[[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]],\n",
      "\n",
      "         [[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]],\n",
      "\n",
      "         [[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]]],\n",
      "\n",
      "\n",
      "        [[[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]],\n",
      "\n",
      "         [[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]],\n",
      "\n",
      "         [[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]]]], dtype=torch.float64)\n",
      "tensor([120., 120., 120.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor((), dtype=torch.float64)\n",
    "y = torch.tensor((), dtype=torch.float64)\n",
    "x = x.new_ones((3,3,4,4))\n",
    "y = y.new_full((3,3,4,4), 2.5)\n",
    "print(x.dim())\n",
    "print(x*y)\n",
    "print(torch.sum(x*y, (1,2,3)))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 7, 9]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([[1,2,3]])\n",
    "y=torch.tensor([[4,5,6]])\n",
    "print(x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.218333333333334\n"
     ]
    }
   ],
   "source": [
    "time =0\n",
    "for i in range(61):\n",
    "    time += i*14.2\n",
    "print(time/60/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6)\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([[1,2,3,0,0,1,3,2]])\n",
    "y=torch.tensor([[1,2,2,1,0,1,3,2]])\n",
    "print(torch.sum(x==y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(120)\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0])\n",
    "b=torch.tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0])\n",
    "c=torch.tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1])\n",
    "d=torch.tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1])\n",
    "e=torch.tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0])\n",
    "f=torch.tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0])\n",
    "\n",
    "print(torch.sum(a==b)+torch.sum(c==d)+torch.sum(e==f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from Dataset.Dataset import makeMoonsDataset\n",
    "\n",
    "dataset_size = 600\n",
    "batch_size = 40\n",
    "test_set_size = dataset_size * 0.2\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset(dataset_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from pandas import DataFrame\n",
    "\n",
    "#x1 = []\n",
    "#x2 = []\n",
    "#y = []\n",
    "#for (data,label) in train_loader:\n",
    "#    res = label#torch.argmax(net.forward, dim=1)\n",
    "#    for point in range(len(data)):\n",
    "#        x1.append(data[point][0].item())\n",
    "#        x2.append(data[point][1].item())\n",
    "#        y.append(res[point].item())\n",
    "##print(x1)\n",
    "#df = DataFrame(dict(x=x1, y=x2, label=y))\n",
    "#colors = {0:'red', 1:'blue'}\n",
    "fig, ax2 = plt.subplots()\n",
    "#grouped = df.groupby('label')\n",
    "#for key, group in grouped:\n",
    "#    group.plot(ax=ax1, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "#ax1.set_title('Training Set')\n",
    "    \n",
    "x1 = []\n",
    "x2 = []\n",
    "y = []\n",
    "for (data,label) in test_loader:\n",
    "    res = label#torch.argmax(label, dim=1)\n",
    "    for point in range(len(data)):\n",
    "        x1.append(data[point][0].item())\n",
    "        x2.append(data[point][1].item())\n",
    "        y.append(res[point].item())\n",
    "#print(x1)\n",
    "df = DataFrame(dict(x=x1, y=x2, label=y))\n",
    "colors = {0:'red', 1:'blue'}\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax2, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "ax2.set_title('Test Set')\n",
    "#plt.tight_layout()\n",
    "plt.savefig(\"Plots/MoonsTest.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import loadMNIST\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "train_loader, test_loader = loadMNIST('Dataset/input/mnist_train.csv', 'Dataset/input/mnist_test.csv')\n",
    "arr_dict = {'arr0':[],'c0':0, 'arr1':[],'c1':0, 'arr2':[],'c2':0, 'arr3':[],'c3':0, 'arr4':[],'c4':0, 'arr5':[],'c5':0, 'arr6':[],'c6':0, 'arr7':[],'c7':0, 'arr8':[],'c8':0, 'arr9':[],'c9':0}\n",
    "train_loader.name = 'MNIST'\n",
    "\n",
    "for index, (data, label) in enumerate(train_loader):\n",
    "    if index < 200:\n",
    "        #print(data[0],label[0])\n",
    "        key = 'arr'+str(label[0].item())\n",
    "        key_c = 'c'+str(label[0].item())\n",
    "        arr = data[0][0].numpy()*255\n",
    "        if arr_dict[key] == []:\n",
    "            arr_dict[key] = arr\n",
    "            arr_dict[key_c] += 1\n",
    "        elif arr_dict[key_c] < 10:\n",
    "            arr_dict[key] = np.concatenate((arr_dict[key], arr), axis=1) \n",
    "            arr_dict[key_c] += 1\n",
    "            \n",
    "        #img = Image.fromarray(arr)\n",
    "        #if img.mode != 'RGB':\n",
    "        #    img = img.convert('RGB')\n",
    "        #img.save('Plots/'+str(label[0].item())+'/mnist'+str(index)+'.png')\n",
    "        #img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST\n"
     ]
    }
   ],
   "source": [
    "print(train_loader.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(10):\n",
    "#    print()\n",
    "arr = np.concatenate((arr_dict['arr0'],arr_dict['arr1'],arr_dict['arr2'],arr_dict['arr3'],arr_dict['arr4'],arr_dict['arr5'],arr_dict['arr6'],arr_dict['arr7'],arr_dict['arr8'],arr_dict['arr9']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.fromarray(arr)\n",
    "if img.mode != 'RGB':            \n",
    "    img = img.convert('RGB')\n",
    "    img.save('Plots/mnistall.png')\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "num_epochs = 100\n",
    "num_layers = 5\n",
    "layers = [2,16,32,64,128,2]\n",
    "bias = False\n",
    "test_set_size = 120\n",
    "\n",
    "basic_save_key = 'Convergence_results//FCNet//FCNet'#MSAMSA\n",
    "\n",
    "train_save_key = '_train_per_epoch_layers_'+str(num_layers)+'_max_hidden_size_'+str(max(layers))+'_bias_'+str(bias)\n",
    "test_save_key = '_test_per_epoch_layers_'+str(num_layers)+'_max_hidden_size_'+str(max(layers))+'_bias_'+str(bias)\n",
    "\n",
    "train_results_per_epoch = np.loadtxt(basic_save_key+train_save_key+'.csv')\n",
    "test_results_per_epoch = np.loadtxt(basic_save_key+test_save_key+'.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8822916746139526\n"
     ]
    }
   ],
   "source": [
    "print(train_results_per_epoch[15])#/test_set_sizemax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "layers = np.arange(2,max_layers+2,1)\n",
    "ax.plot(layers, train_results_per_epoch, label='train')\n",
    "ax.plot(layers, test_results_per_epoch/test_set_size, label='test')\n",
    "ax.set_ylim(0,1.1)\n",
    "ax.set_xlabel('Number of layers')\n",
    "ax.set_ylabel('Average Accuracy')\n",
    "ax.legend(loc='best')\n",
    "plt.savefig(\"Plots/\"+train_save_key+\".pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 16\n",
      "[0.71458334 0.8645833  0.87291664 0.84166664 0.89166665 0.88125\n",
      " 0.8666667  0.88958335 0.87083334 0.91041666 0.87291664 0.90833336\n",
      " 0.94166666 0.84375    0.93333334 0.97291666 0.9895833  0.9583333\n",
      " 0.9895833  0.99375    0.99583334 0.98541665 0.95208335 0.99375\n",
      " 0.99583334 0.99791664 0.96458334 0.9916667  0.99375    0.99375\n",
      " 0.99791664 1.         0.99583334 0.99375    0.99583334 0.99791664\n",
      " 0.99583334 1.         0.69375    0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[109. 110. 111. 112. 107. 112. 114. 114. 114. 115. 111. 116.  85. 113.\n",
      " 117. 120. 115. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.]\n",
      "[0.68125    0.84791666 0.86875    0.85625    0.8645833  0.8520833\n",
      " 0.8541667  0.86875    0.86875    0.86875    0.85625    0.86041665\n",
      " 0.8645833  0.87083334 0.87708336 0.8854167  0.8979167  0.84791666\n",
      " 0.9145833  0.9270833  0.9479167  0.96875    0.9291667  0.97083336\n",
      " 0.9916667  0.9791667  0.97083336 0.99375    0.99583334 0.99791664\n",
      " 0.97083336 1.         1.         1.         0.99791664 1.\n",
      " 1.         0.99375    0.99791664 0.93333334 0.99583334 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.99791664 0.99791664 1.         1.         0.99791664\n",
      " 0.99791664 1.         0.99791664 0.99791664 0.99791664 0.99791664\n",
      " 0.99583334 1.         0.99375    0.99791664 1.         0.99791664\n",
      " 1.         0.99791664 0.99583334 0.99791664 1.         0.99791664\n",
      " 1.         1.         0.99791664 1.         1.         1.\n",
      " 1.         0.99791664 1.         1.         1.         1.\n",
      " 1.         1.         0.99791664 1.         1.         0.99583334\n",
      " 1.         0.99583334 0.99791664 1.         1.         1.\n",
      " 1.         1.         1.         1.        ]\n",
      "[106. 109. 111. 106. 112. 111. 110. 106. 110. 111. 106. 112. 110. 111.\n",
      " 110. 111. 102. 111. 114. 114. 115. 118. 120. 120. 119. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.]\n",
      "[0.64375    0.8541667  0.86875    0.8520833  0.88125    0.8875\n",
      " 0.9270833  0.87291664 0.88125    0.9291667  0.96875    0.98333335\n",
      " 0.92083335 0.9270833  0.9916667  0.99375    0.99375    0.99375\n",
      " 0.99791664 0.94375    0.60833335 0.83958334 0.85625    0.8645833\n",
      " 0.86875    0.8645833  0.8666667  0.84583336 0.8520833  0.8541667\n",
      " 0.84583336 0.8666667  0.8645833  0.8666667  0.86041665 0.86041665\n",
      " 0.87083334 0.87083334 0.85833335 0.87291664 0.8541667  0.8625\n",
      " 0.87708336 0.8666667  0.8645833  0.86041665 0.87083334 0.87291664\n",
      " 0.85625    0.87083334 0.8666667  0.87291664 0.8666667  0.8625\n",
      " 0.86875    0.87291664 0.86875    0.875      0.87708336 0.86875\n",
      " 0.86875    0.8645833  0.87083334 0.87083334 0.87291664 0.87708336\n",
      " 0.86875    0.8625     0.86875    0.8645833  0.8645833  0.86041665\n",
      " 0.8625     0.86041665 0.87083334 0.875      0.8666667  0.85833335\n",
      " 0.8666667  0.8541667  0.8666667  0.8625     0.86041665 0.8645833\n",
      " 0.8645833  0.8625     0.8645833  0.86875    0.8666667  0.8625\n",
      " 0.86875    0.87291664 0.8666667  0.8666667  0.8645833  0.87083334\n",
      " 0.8666667  0.87083334 0.86041665 0.87708336]\n",
      "[111. 103. 112. 108. 111. 114. 117. 105. 111. 114. 120. 119. 110. 120.\n",
      " 120. 120. 120. 120. 120.  62. 109. 110. 111. 112. 110. 111. 106. 110.\n",
      " 110. 110. 111. 111. 111. 107. 110. 110. 111. 109. 110. 111. 112. 111.\n",
      " 110. 112. 107. 112. 111. 111. 111. 108. 112. 112. 112. 111. 110. 111.\n",
      " 112. 112. 110. 112. 112. 111. 112. 112. 109. 110. 106. 112. 109. 110.\n",
      " 108. 110. 110. 111. 112. 111. 110. 112. 110. 109. 111. 112. 111. 112.\n",
      " 111. 111. 111. 112. 110. 108. 112. 111. 108. 110. 110. 112. 112. 111.\n",
      " 111. 109.]\n",
      "[0.65833336 0.84583336 0.85625    0.86875    0.8833333  0.875\n",
      " 0.86875    0.8666667  0.88125    0.88125    0.8875     0.86875\n",
      " 0.8958333  0.89166665 0.8875     0.89375    0.9125     0.92083335\n",
      " 0.8958333  0.9291667  0.95       0.9604167  0.9604167  0.96875\n",
      " 0.85833335 0.875      0.94375    0.95625    0.95625    0.97291666\n",
      " 0.98541665 0.97291666 0.925      0.9875     0.9916667  0.99375\n",
      " 0.9916667  0.9916667  0.99375    0.8854167  0.9583333  0.9770833\n",
      " 0.9791667  0.98541665 0.98333335 0.99375    0.9875     0.99375\n",
      " 0.99583334 0.99791664 0.99791664 0.99583334 0.99791664 0.99583334\n",
      " 0.79375    0.90208334 0.97291666 0.9916667  0.9916667  0.99375\n",
      " 0.99583334 0.99375    0.99791664 0.99583334 0.99791664 0.99791664\n",
      " 0.99375    0.99791664 0.9916667  0.99791664 0.99791664 0.99791664\n",
      " 0.99791664 0.99791664 0.99791664 0.99791664 0.99583334 0.9916667\n",
      " 0.99583334 0.99791664 0.99791664 0.99583334 0.99583334 0.99375\n",
      " 1.         0.99791664 0.99583334 0.99791664 0.99583334 0.99791664\n",
      " 1.         0.99583334 0.99791664 0.99791664 0.99583334 0.99791664\n",
      " 0.9916667  0.99583334 0.99791664 0.99583334]\n",
      "[108. 111. 106. 110. 110. 111. 111. 111. 111. 111. 111. 111. 113. 113.\n",
      " 111. 115. 113. 106. 118. 117. 118. 118. 119. 118. 109. 116. 118. 118.\n",
      " 120. 120. 118. 120. 120. 120. 120. 120. 120. 120. 120. 115. 118. 118.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 107. 118.\n",
      " 120. 119. 120. 118. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.]\n",
      "[0.6354167  0.80625    0.81458336 0.875      0.8666667  0.86875\n",
      " 0.84791666 0.88125    0.8645833  0.8666667  0.89166665 0.87291664\n",
      " 0.87291664 0.8979167  0.8833333  0.88125    0.8854167  0.87708336\n",
      " 0.875      0.8625     0.8666667  0.8666667  0.8833333  0.8833333\n",
      " 0.8875     0.89375    0.86041665 0.87916666 0.87916666 0.87708336\n",
      " 0.875      0.8854167  0.87291664 0.87083334 0.8833333  0.88958335\n",
      " 0.89375    0.88125    0.87708336 0.8666667  0.8645833  0.88125\n",
      " 0.88958335 0.87916666 0.88125    0.8854167  0.87916666 0.8833333\n",
      " 0.8854167  0.8854167  0.8854167  0.86875    0.8979167  0.87083334\n",
      " 0.88125    0.87916666 0.87291664 0.88958335 0.87708336 0.875\n",
      " 0.88125    0.87708336 0.87916666 0.88125    0.8854167  0.8854167\n",
      " 0.87291664 0.8645833  0.8854167  0.87291664 0.87291664 0.87083334\n",
      " 0.89166665 0.89166665 0.9        0.8958333  0.8833333  0.87083334\n",
      " 0.875      0.88958335 0.8833333  0.88125    0.9        0.90625\n",
      " 0.9125     0.92083335 0.9479167  0.93958336 0.95625    0.975\n",
      " 0.98333335 0.9875     0.96666664 0.97291666 0.99583334 0.99583334\n",
      " 0.99375    1.         0.99791664 0.93541664]\n",
      "[100. 111. 109. 112. 110. 111. 110. 110. 109. 109. 109. 111. 111. 111.\n",
      " 111. 110. 109. 109. 111. 112. 112. 111. 110. 111. 111. 112. 110. 111.\n",
      " 106. 107. 110. 111. 111. 111. 112. 111. 111. 111. 110. 111. 110. 110.\n",
      " 111. 109. 112. 111. 111. 111. 111. 111. 112. 111. 111. 111. 111. 111.\n",
      " 110. 112. 111. 112. 111. 111. 112. 111. 112. 112. 111. 112. 111. 111.\n",
      " 111. 112. 112. 111. 109. 111. 112. 112. 111. 112. 111. 113. 113. 113.\n",
      " 114. 115. 118. 119. 119. 119. 120. 120. 112. 120. 120. 120. 120. 120.\n",
      " 114. 120.]\n",
      "[0.68333334 0.8020833  0.8645833  0.85833335 0.8645833  0.8541667\n",
      " 0.8666667  0.87708336 0.88958335 0.875      0.9        0.83958334\n",
      " 0.9125     0.9270833  0.94375    0.96458334 0.9916667  0.99375\n",
      " 0.99583334 0.97083336 0.9916667  0.99791664 0.99583334 0.99375\n",
      " 0.99583334 0.99583334 0.99791664 0.9916667  0.98125    0.99791664\n",
      " 1.         1.         1.         0.99791664 0.99583334 0.99791664\n",
      " 1.         0.99583334 1.         0.99583334 0.99791664 0.99791664\n",
      " 0.9916667  0.99583334 0.99791664 0.99791664 0.99791664 0.99791664\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.99791664 1.         1.         1.         0.99791664\n",
      " 1.         0.99583334 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1.         1.         1.         1.        ]\n",
      "[106. 107. 113. 112. 109. 113. 111. 111. 111. 114. 114. 113. 117. 116.\n",
      " 118. 120. 120. 120. 120. 120. 118. 120. 120. 120. 120. 120. 120. 117.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 116. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.]\n",
      "[0.51666665 0.82708335 0.84166664 0.86041665 0.93125    0.88125\n",
      " 0.93958336 0.83958334 0.96875    0.98333335 0.9791667  0.99791664\n",
      " 0.98125    0.99791664 0.99583334 0.99791664 0.98541665 0.99375\n",
      " 0.98541665 1.         1.         0.99791664 0.9875     0.99375\n",
      " 0.98333335 0.99791664 0.99791664 1.         0.99791664 1.\n",
      " 0.99791664 0.99583334 0.99791664 1.         1.         0.99375\n",
      " 0.8666667  0.9583333  0.97291666 0.98541665 0.99583334 0.9916667\n",
      " 0.99583334 1.         0.99583334 0.99791664 1.         0.99791664\n",
      " 0.99791664 0.99583334 1.         0.99791664 0.99583334 1.\n",
      " 1.         1.         0.99791664 0.99583334 0.9166667  0.9895833\n",
      " 0.99791664 0.99791664 0.99791664 1.         0.99791664 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.99791664 0.99791664 1.         1.         0.99791664 1.\n",
      " 1.         0.99791664 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         0.99791664 0.99791664 1.\n",
      " 1.         1.         1.         1.        ]\n",
      "[ 97. 110. 114. 112. 113. 114. 109. 116. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 117. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 115. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 119. 120. 120. 120. 120.\n",
      " 120. 107. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.]\n",
      "[0.625      0.84791666 0.85625    0.8666667  0.86875    0.8979167\n",
      " 0.90833336 0.92083335 0.94166666 0.975      0.98333335 0.99583334\n",
      " 0.99583334 1.         0.99791664 0.81875    0.90416664 0.92083335\n",
      " 0.9583333  0.98541665 0.99375    0.98541665 0.95       0.99583334\n",
      " 0.99791664 0.99791664 0.99791664 0.99583334 1.         0.99791664\n",
      " 1.         0.98125    0.90625    0.9875     0.99583334 0.99375\n",
      " 0.99791664 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         0.99791664 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.99583334\n",
      " 1.         1.         1.         1.         0.99791664 1.\n",
      " 1.         1.         1.         1.         1.         0.99791664\n",
      " 1.         1.         1.         1.         1.         0.99791664\n",
      " 0.99583334 0.99791664 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.99791664\n",
      " 1.         0.99375    0.99583334 0.99583334]\n",
      "[ 99. 110. 112. 112. 113. 109. 119. 120. 118. 120. 120. 120. 120. 120.\n",
      " 110. 113. 114. 117. 117. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120.  88. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.]\n",
      "[0.675      0.8333333  0.84583336 0.8645833  0.88958335 0.87291664\n",
      " 0.92291665 0.93541664 0.93958336 0.86041665 0.8333333  0.88125\n",
      " 0.90416664 0.9270833  0.93958336 0.97291666 0.84791666 0.96875\n",
      " 0.975      0.99375    0.93958336 0.9875     0.99791664 0.99791664\n",
      " 0.99375    0.99583334 1.         0.8208333  0.7875     0.8541667\n",
      " 0.86041665 0.8666667  0.87708336 0.88125    0.87916666 0.8854167\n",
      " 0.8854167  0.8854167  0.93333334 0.93333334 0.9479167  0.9625\n",
      " 0.9583333  0.9625     0.9770833  0.9875     0.97291666 0.99375\n",
      " 0.9895833  0.9625     0.75208336 0.8125     0.8666667  0.88125\n",
      " 0.88125    0.87916666 0.8625     0.875      0.87291664 0.87708336\n",
      " 0.87291664 0.8875     0.8979167  0.8833333  0.8854167  0.87916666\n",
      " 0.875      0.87916666 0.8645833  0.87708336 0.87916666 0.87291664\n",
      " 0.89166665 0.8666667  0.88958335 0.8854167  0.8833333  0.8833333\n",
      " 0.88125    0.87291664 0.8854167  0.875      0.87916666 0.87708336\n",
      " 0.8833333  0.875      0.88125    0.88958335 0.8854167  0.8833333\n",
      " 0.87916666 0.8875     0.88958335 0.8854167  0.87083334 0.87708336\n",
      " 0.8875     0.8854167  0.88125    0.8875    ]\n",
      "[104. 112. 108. 111. 112. 112. 115. 117. 118. 109. 111. 112. 116. 117.\n",
      " 116. 116. 117. 119. 120. 105. 120. 120. 120. 120. 120. 120. 120. 102.\n",
      " 109. 109. 110. 112. 112. 111. 111. 111. 113. 112. 117. 116. 116. 118.\n",
      " 118. 119. 120. 113. 120. 120. 120. 100. 106. 108. 109. 111. 111. 112.\n",
      " 111. 111. 111. 110. 112. 111. 110. 111. 111. 111. 112. 112. 111. 111.\n",
      " 112. 111. 111. 112. 111. 111. 111. 112. 111. 111. 110. 111. 110. 112.\n",
      " 111. 111. 111. 111. 111. 112. 111. 111. 112. 112. 111. 111. 110. 112.\n",
      " 111. 111.]\n",
      "[0.68541664 0.8645833  0.8645833  0.90416664 0.9166667  0.98125\n",
      " 0.92291665 0.99583334 0.8333333  0.8520833  0.87708336 0.8854167\n",
      " 0.8645833  0.8625     0.88125    0.85       0.88125    0.8854167\n",
      " 0.86041665 0.87916666 0.875      0.88125    0.8645833  0.87916666\n",
      " 0.87708336 0.88958335 0.8958333  0.87291664 0.87916666 0.87291664\n",
      " 0.8854167  0.87083334 0.88125    0.8854167  0.8833333  0.88958335\n",
      " 0.89166665 0.87708336 0.875      0.86875    0.88125    0.88125\n",
      " 0.8666667  0.875      0.87291664 0.86875    0.87916666 0.88125\n",
      " 0.87291664 0.8833333  0.86041665 0.88958335 0.8666667  0.8833333\n",
      " 0.86875    0.86875    0.87291664 0.8666667  0.875      0.8833333\n",
      " 0.87291664 0.8854167  0.8875     0.87291664 0.87916666 0.8645833\n",
      " 0.89166665 0.87291664 0.8666667  0.87083334 0.875      0.8833333\n",
      " 0.89166665 0.88125    0.87708336 0.87916666 0.8875     0.88958335\n",
      " 0.87708336 0.8833333  0.8875     0.8833333  0.88125    0.8645833\n",
      " 0.89375    0.8854167  0.8875     0.87291664 0.875      0.8854167\n",
      " 0.87916666 0.87916666 0.88125    0.88125    0.88125    0.8666667\n",
      " 0.875      0.8833333  0.8875     0.8875    ]\n",
      "[108. 108. 114. 110. 117. 118. 118. 120. 111. 109. 112. 111. 112. 112.\n",
      " 109. 112. 111. 111. 111. 112. 110. 110. 112. 110. 109. 111. 111. 110.\n",
      " 110. 111. 112. 111. 112. 110. 109. 111. 111. 111. 112. 110. 112. 111.\n",
      " 111. 111. 111. 111. 110. 112. 111. 111. 111. 111. 111. 111. 111. 111.\n",
      " 111. 111. 111. 111. 111. 111. 112. 112. 111. 112. 111. 111. 112. 110.\n",
      " 111. 112. 111. 111. 112. 111. 112. 111. 111. 111. 111. 111. 111. 112.\n",
      " 111. 111. 111. 111. 111. 111. 110. 111. 111. 111. 111. 111. 112. 111.\n",
      " 111. 111.]\n",
      "Number of layers: 17\n",
      "[0.60625    0.85       0.83125    0.88125    0.91041666 0.93958336\n",
      " 0.98541665 0.84166664 0.81041664 0.84375    0.8541667  0.87291664\n",
      " 0.92083335 0.93541664 0.9        0.94375    0.93333334 0.96875\n",
      " 0.9625     0.9791667  0.92083335 0.96666664 0.9770833  0.98333335\n",
      " 0.9895833  0.99583334 0.9875     0.99583334 0.99375    0.9875\n",
      " 0.99583334 0.9875     0.98541665 0.99583334 0.9458333  0.95625\n",
      " 0.9916667  0.98125    1.         0.99583334 0.9916667  0.9895833\n",
      " 0.99791664 0.99791664 0.99791664 0.99791664 0.99791664 1.\n",
      " 0.98333335 0.97083336 0.99583334 0.99375    1.         1.\n",
      " 0.99791664 0.99791664 0.99791664 0.99791664 0.99791664 0.99583334\n",
      " 1.         0.99791664 1.         1.         0.99791664 1.\n",
      " 0.99583334 1.         1.         0.99791664 0.99583334 0.99583334\n",
      " 0.99791664 0.99791664 1.         0.96458334 0.99583334 0.99791664\n",
      " 0.99791664 1.         0.99791664 1.         0.99791664 0.9916667\n",
      " 0.99791664 0.99583334 0.99375    0.99791664 0.99791664 1.\n",
      " 0.99583334 0.99791664 0.99791664 1.         1.         0.99791664\n",
      " 0.99375    0.99583334 0.99791664 1.        ]\n",
      "[ 97. 109. 108. 113. 117. 118. 120. 105. 110. 110.  97. 113. 113. 117.\n",
      " 116. 116. 117. 119. 119. 101. 114. 120. 120. 120. 120. 120. 120. 119.\n",
      " 120. 120. 117. 120. 118. 120.  93. 120. 120. 120. 120. 120. 120. 118.\n",
      " 120. 120. 120. 120. 120. 120. 116. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 120. 120.]\n",
      "[0.6125     0.8625     0.88125    0.8625     0.8666667  0.7875\n",
      " 0.8666667  0.87291664 0.8833333  0.8833333  0.8875     0.875\n",
      " 0.87291664 0.875      0.8833333  0.89166665 0.8833333  0.88125\n",
      " 0.87083334 0.87708336 0.87083334 0.86875    0.8833333  0.8645833\n",
      " 0.875      0.89375    0.875      0.88958335 0.87083334 0.8833333\n",
      " 0.89375    0.88958335 0.88125    0.9        0.90833336 0.9145833\n",
      " 0.93125    0.92083335 0.93958336 0.94375    0.95625    0.93125\n",
      " 0.96458334 0.97291666 0.96875    0.9875     0.96458334 0.97291666\n",
      " 0.98333335 0.87083334 0.95625    0.9770833  0.98541665 0.9895833\n",
      " 0.99583334 0.98333335 0.97291666 0.9916667  0.99791664 0.99583334\n",
      " 0.99583334 0.975      0.99375    0.9895833  0.99375    0.92291665\n",
      " 0.98333335 1.         0.99583334 0.99583334 0.99583334 1.\n",
      " 0.99791664 0.99375    0.99791664 0.99791664 0.99791664 0.99791664\n",
      " 0.95208335 0.9895833  0.99583334 0.99791664 0.99791664 1.\n",
      " 1.         1.         1.         1.         0.99791664 0.99583334\n",
      " 1.         1.         0.9916667  0.99583334 0.99791664 1.\n",
      " 0.99583334 0.99375    0.99583334 0.99791664]\n",
      "[109. 110. 112. 109. 112. 108. 111. 111. 110. 112. 110. 111. 111. 111.\n",
      " 112. 111. 111. 112. 111. 110. 111. 111. 111. 111. 112. 111. 111. 111.\n",
      " 110. 112. 112. 113. 112. 115. 115. 115. 115. 116. 116. 118. 117. 118.\n",
      " 119. 117. 118. 118. 120. 120. 107. 115. 120. 119. 119. 120. 118. 110.\n",
      " 120. 120. 120. 120. 112. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.]\n",
      "[0.62916666 0.85833335 0.84583336 0.84583336 0.8666667  0.85\n",
      " 0.87083334 0.87083334 0.8854167  0.87291664 0.8666667  0.8854167\n",
      " 0.86875    0.87291664 0.88125    0.8833333  0.88958335 0.87916666\n",
      " 0.8875     0.88125    0.8833333  0.86875    0.88125    0.8875\n",
      " 0.87083334 0.87708336 0.8666667  0.87708336 0.8854167  0.8833333\n",
      " 0.88125    0.87708336 0.8854167  0.875      0.875      0.87916666\n",
      " 0.86875    0.8833333  0.8666667  0.8833333  0.88125    0.87708336\n",
      " 0.88125    0.875      0.88125    0.875      0.8666667  0.8875\n",
      " 0.8854167  0.87916666 0.88125    0.8958333  0.8854167  0.87916666\n",
      " 0.8854167  0.8875     0.87708336 0.88125    0.87708336 0.8833333\n",
      " 0.87916666 0.87291664 0.87291664 0.89166665 0.87916666 0.89166665\n",
      " 0.875      0.89375    0.88958335 0.8854167  0.88125    0.88125\n",
      " 0.8854167  0.87708336 0.88958335 0.89166665 0.88125    0.8833333\n",
      " 0.875      0.88125    0.8833333  0.8854167  0.88125    0.87916666\n",
      " 0.87708336 0.8875     0.89166665 0.875      0.88958335 0.8833333\n",
      " 0.8958333  0.89375    0.88125    0.87916666 0.88125    0.8854167\n",
      " 0.875      0.8875     0.8833333  0.88125   ]\n",
      "[106. 112. 110. 104. 109. 112. 112. 112. 110. 110. 112. 112. 110. 110.\n",
      " 112. 110. 110. 111. 110. 111. 111. 111. 111. 110. 111. 110. 111. 111.\n",
      " 111. 110. 111. 111. 111. 111. 111. 111. 112. 111. 111. 109. 111. 111.\n",
      " 112. 111. 110. 111. 111. 110. 112. 111. 111. 110. 111. 111. 111. 111.\n",
      " 112. 111. 111. 111. 111. 112. 111. 111. 111. 111. 111. 111. 111. 111.\n",
      " 111. 112. 111. 112. 112. 111. 111. 112. 111. 109. 111. 112. 111. 111.\n",
      " 110. 111. 111. 111. 109. 111. 111. 110. 111. 112. 112. 111. 111. 111.\n",
      " 111. 111.]\n",
      "[0.46666667 0.48333332 0.4875     0.48333332 0.5208333  0.50416666\n",
      " 0.525      0.5125     0.49583334 0.49583334 0.48333332 0.49583334\n",
      " 0.47083333 0.48333332 0.46666667 0.49583334 0.5083333  0.50416666\n",
      " 0.5125     0.45       0.475      0.5        0.5083333  0.50416666\n",
      " 0.50416666 0.5125     0.50416666 0.49583334 0.4625     0.4625\n",
      " 0.475      0.50416666 0.49166667 0.50416666 0.475      0.4875\n",
      " 0.45416668 0.49166667 0.4875     0.5        0.45416668 0.48333332\n",
      " 0.4625     0.51666665 0.49583334 0.49166667 0.47916666 0.5\n",
      " 0.5125     0.49166667 0.5208333  0.4875     0.475      0.52916664\n",
      " 0.48333332 0.4875     0.5083333  0.45416668 0.5        0.49583334\n",
      " 0.48333332 0.47916666 0.49166667 0.49166667 0.49166667 0.47916666\n",
      " 0.44583333 0.48333332 0.48333332 0.5083333  0.475      0.46666667\n",
      " 0.51666665 0.49583334 0.475      0.475      0.48333332 0.475\n",
      " 0.5        0.5083333  0.5        0.48333332 0.5        0.47916666\n",
      " 0.5125     0.47916666 0.49166667 0.47083333 0.48333332 0.48333332\n",
      " 0.45833334 0.45       0.44166666 0.45416668 0.50416666 0.4625\n",
      " 0.5083333  0.4875     0.47083333 0.47083333]\n",
      "[62. 62. 58. 62. 58. 58. 62. 62. 58. 62. 62. 62. 58. 62. 62. 62. 58. 58.\n",
      " 62. 58. 62. 62. 62. 58. 58. 58. 58. 58. 58. 58. 58. 58. 58. 62. 62. 58.\n",
      " 58. 58. 62. 62. 58. 58. 58. 62. 58. 62. 62. 62. 58. 62. 58. 58. 58. 62.\n",
      " 62. 62. 62. 62. 58. 62. 58. 58. 62. 62. 58. 58. 58. 58. 58. 58. 58. 58.\n",
      " 62. 58. 58. 62. 62. 62. 62. 62. 58. 58. 58. 62. 58. 58. 62. 62. 62. 62.\n",
      " 62. 58. 62. 58. 58. 62. 58. 58. 58. 58.]\n",
      "[0.68333334 0.82708335 0.8625     0.8645833  0.875      0.875\n",
      " 0.8645833  0.8625     0.87708336 0.8666667  0.87708336 0.87708336\n",
      " 0.87291664 0.87916666 0.87291664 0.875      0.89375    0.87708336\n",
      " 0.875      0.8833333  0.88125    0.87708336 0.87083334 0.87083334\n",
      " 0.875      0.88125    0.87291664 0.86875    0.87708336 0.87916666\n",
      " 0.8875     0.86875    0.86875    0.875      0.8833333  0.87916666\n",
      " 0.87708336 0.88125    0.89166665 0.87916666 0.8875     0.87916666\n",
      " 0.87916666 0.8875     0.89375    0.88125    0.88958335 0.8833333\n",
      " 0.88125    0.875      0.89166665 0.87916666 0.8833333  0.88958335\n",
      " 0.8854167  0.87291664 0.8854167  0.87291664 0.8854167  0.88958335\n",
      " 0.88125    0.87708336 0.88958335 0.87083334 0.88125    0.89375\n",
      " 0.89166665 0.8958333  0.89166665 0.90625    0.8979167  0.8958333\n",
      " 0.90625    0.91875    0.92083335 0.93333334 0.9583333  0.9375\n",
      " 0.95       0.9604167  0.97083336 0.97291666 0.975      0.9458333\n",
      " 0.98541665 0.99375    0.98541665 0.98125    0.99375    0.96875\n",
      " 0.9895833  0.98541665 0.99583334 0.99583334 0.9791667  0.99375\n",
      " 0.6479167  0.49583334 0.49583334 0.49583334]\n",
      "[107. 112. 112. 107. 111. 110. 110. 110. 111. 111. 110. 112. 110. 111.\n",
      " 110. 112. 111. 110. 112. 111. 111. 111. 111. 111. 111. 111. 109. 111.\n",
      " 112. 111. 111. 111. 111. 111. 111. 112. 112. 111. 112. 111. 112. 111.\n",
      " 110. 111. 112. 111. 112. 111. 112. 110. 110. 111. 112. 112. 111. 111.\n",
      " 111. 110. 112. 110. 111. 112. 112. 111. 112. 109. 112. 113. 113. 113.\n",
      " 108. 114. 114. 115. 115. 116. 116. 117. 118. 119. 119. 118. 117. 119.\n",
      " 120. 118. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.  62.  62.\n",
      "  62.  62.]\n",
      "[0.50416666 0.82708335 0.8520833  0.86041665 0.86875    0.87291664\n",
      " 0.86875    0.85       0.8645833  0.85625    0.8833333  0.86875\n",
      " 0.86875    0.8520833  0.8625     0.8520833  0.85625    0.86041665\n",
      " 0.87291664 0.87083334 0.875      0.86041665 0.8625     0.86875\n",
      " 0.8375     0.8645833  0.8645833  0.8645833  0.87708336 0.86875\n",
      " 0.87916666 0.875      0.8645833  0.8875     0.875      0.88125\n",
      " 0.8833333  0.8875     0.88125    0.8833333  0.8875     0.86875\n",
      " 0.8833333  0.8854167  0.87291664 0.88958335 0.87708336 0.88958335\n",
      " 0.8854167  0.8833333  0.87916666 0.87708336 0.8854167  0.88958335\n",
      " 0.8854167  0.89166665 0.8833333  0.86875    0.88125    0.8854167\n",
      " 0.8875     0.8833333  0.8833333  0.8854167  0.8833333  0.87708336\n",
      " 0.87708336 0.89166665 0.8645833  0.87916666 0.88958335 0.87916666\n",
      " 0.88958335 0.88958335 0.8666667  0.89375    0.8854167  0.8875\n",
      " 0.87916666 0.8875     0.88958335 0.8833333  0.8979167  0.8854167\n",
      " 0.87916666 0.87708336 0.88958335 0.89166665 0.87291664 0.88958335\n",
      " 0.8875     0.89375    0.8833333  0.87083334 0.89166665 0.88958335\n",
      " 0.87916666 0.89375    0.88958335 0.87916666]\n",
      "[ 85. 109. 105. 111. 111. 111. 111. 109. 112. 110. 111. 111. 111. 110.\n",
      " 111. 109. 112. 111. 111. 112. 110. 112. 110. 110. 110. 112. 110. 110.\n",
      " 110. 110. 111. 111. 111. 110. 111. 111. 111. 110. 111. 110. 111. 111.\n",
      " 112. 110. 111. 109. 111. 111. 112. 110. 111. 111. 112. 112. 111. 109.\n",
      " 111. 112. 111. 111. 111. 111. 111. 111. 112. 111. 111. 110. 111. 111.\n",
      " 110. 111. 111. 111. 111. 111. 112. 112. 110. 111. 112. 112. 111. 111.\n",
      " 112. 110. 112. 111. 112. 111. 111. 111. 111. 112. 111. 112. 111. 111.\n",
      " 111. 111.]\n",
      "[0.68125    0.8520833  0.8541667  0.875      0.85625    0.87083334\n",
      " 0.88125    0.88958335 0.86875    0.86875    0.88125    0.86875\n",
      " 0.88958335 0.8875     0.87916666 0.89166665 0.8833333  0.8979167\n",
      " 0.9125     0.91041666 0.925      0.9583333  0.97083336 0.98541665\n",
      " 0.99375    0.99583334 0.99791664 1.         1.         0.99375\n",
      " 1.         0.99791664 0.99791664 0.99791664 0.99583334 0.99791664\n",
      " 0.99791664 0.5375     0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[112. 107. 111. 110. 110. 109. 111. 109. 110. 111. 110. 111. 109. 111.\n",
      " 111. 111. 111. 112. 114. 115. 115. 118. 120. 120. 120. 119. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.]\n",
      "[0.65208334 0.84166664 0.875      0.85833335 0.85833335 0.87083334\n",
      " 0.86041665 0.8833333  0.87083334 0.87708336 0.87083334 0.87708336\n",
      " 0.8833333  0.88958335 0.8854167  0.88958335 0.89166665 0.87916666\n",
      " 0.88125    0.87916666 0.8666667  0.8833333  0.88958335 0.8833333\n",
      " 0.8854167  0.8875     0.8875     0.87916666 0.8833333  0.89375\n",
      " 0.8958333  0.8854167  0.90416664 0.8979167  0.8854167  0.8958333\n",
      " 0.88958335 0.88125    0.88125    0.87916666 0.8854167  0.8875\n",
      " 0.89375    0.8833333  0.8979167  0.88958335 0.8833333  0.89375\n",
      " 0.89375    0.89166665 0.89375    0.8958333  0.8875     0.8854167\n",
      " 0.8875     0.8854167  0.89166665 0.8854167  0.90208334 0.8958333\n",
      " 0.90208334 0.89166665 0.9        0.89166665 0.87083334 0.8854167\n",
      " 0.90625    0.8958333  0.8979167  0.9        0.89375    0.8979167\n",
      " 0.90416664 0.89166665 0.90416664 0.90416664 0.90208334 0.89166665\n",
      " 0.89166665 0.89166665 0.90833336 0.90208334 0.90625    0.88125\n",
      " 0.88958335 0.8958333  0.8958333  0.9125     0.9166667  0.925\n",
      " 0.93958336 0.95416665 0.97291666 0.98125    0.98125    0.67291665\n",
      " 0.6        0.82916665 0.86041665 0.8625    ]\n",
      "[108. 111. 112. 111. 112. 109. 112. 111. 112. 113. 114. 110. 113. 112.\n",
      " 113. 112. 109. 113. 111. 113. 110. 113. 111. 111. 110. 110. 111. 111.\n",
      " 114. 113. 113. 112. 114. 113. 113. 113. 113. 112. 114. 114. 114. 113.\n",
      " 111. 112. 112. 111. 113. 113. 114. 113. 111. 110. 114. 114. 113. 113.\n",
      " 113. 109. 110. 114. 109. 114. 114. 110. 113. 113. 113. 112. 113. 114.\n",
      " 113. 111. 112. 112. 112. 113. 112. 111. 113. 113. 113. 111. 113. 112.\n",
      " 110. 110. 113. 113. 113. 116. 116. 116. 118. 120.  94.  68.  95. 110.\n",
      " 110. 110.]\n",
      "[0.675      0.8354167  0.87291664 0.9166667  0.95208335 0.9791667\n",
      " 0.93958336 0.9375     0.96458334 0.98125    1.         1.\n",
      " 0.99791664 0.99791664 0.99583334 0.99791664 0.9895833  0.99791664\n",
      " 1.         0.99791664 1.         0.99791664 0.99375    0.99583334\n",
      " 0.98541665 0.99375    0.99791664 0.99791664 1.         0.99791664\n",
      " 0.99791664 0.99791664 0.99791664 0.99791664 1.         0.99791664\n",
      " 1.         0.99791664 0.99375    1.         0.99791664 1.\n",
      " 1.         0.99791664 0.99791664 0.99791664 1.         0.99583334\n",
      " 0.99583334 0.99583334 0.99791664 0.99791664 0.99791664 1.\n",
      " 1.         0.99791664 0.99791664 0.99791664 0.99583334 0.73541665\n",
      " 0.49166667 0.4875     0.47916666 0.58125    0.7104167  0.73333335\n",
      " 0.78125    0.79791665 0.8333333  0.7895833  0.8625     0.86875\n",
      " 0.87083334 0.8625     0.8875     0.8625     0.87916666 0.8854167\n",
      " 0.8645833  0.88958335 0.8854167  0.875      0.8666667  0.88125\n",
      " 0.87916666 0.8875     0.86875    0.86875    0.88125    0.88958335\n",
      " 0.89166665 0.875      0.88125    0.87916666 0.8541667  0.87083334\n",
      " 0.8625     0.8833333  0.8875     0.8833333 ]\n",
      "[111. 109. 113. 114. 118. 120. 114. 113. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 118. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120.  62.  62.  62.  69.  76.  99. 101. 105.  97. 111. 111.\n",
      " 109. 111. 108. 111. 110. 110. 111. 110. 110. 111. 111. 110. 111. 111.\n",
      " 111. 110. 111. 112. 109. 111. 111. 111. 111. 111. 112. 107. 111. 111.\n",
      " 112. 111.]\n",
      "[0.55625    0.775      0.825      0.86875    0.85833335 0.86875\n",
      " 0.88958335 0.80625    0.8666667  0.89166665 0.9        0.8979167\n",
      " 0.94166666 0.89375    0.9145833  0.87916666 0.9458333  0.9125\n",
      " 0.96458334 0.9375     0.9583333  0.975      0.9604167  0.90833336\n",
      " 0.95       0.9875     0.975      0.96458334 0.9916667  0.975\n",
      " 0.98541665 0.9791667  0.9916667  0.99583334 0.9895833  0.99375\n",
      " 0.99791664 0.99583334 0.99583334 0.99583334 0.99375    0.6875\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[ 97. 108. 111. 112. 111. 109. 113. 110. 112. 113. 111. 114.  93. 117.\n",
      " 117. 117. 119. 117. 117. 120. 117. 120. 111. 119. 120. 120. 120. 120.\n",
      " 120. 112. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 117.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.]\n",
      "Number of layers: 18\n",
      "[0.5083333  0.76875    0.81666666 0.85833335 0.8541667  0.87291664\n",
      " 0.8520833  0.875      0.84791666 0.87291664 0.8666667  0.8666667\n",
      " 0.8645833  0.86041665 0.8625     0.8625     0.86875    0.8645833\n",
      " 0.85625    0.85833335 0.87083334 0.8520833  0.87083334 0.86041665\n",
      " 0.8666667  0.8645833  0.85833335 0.86875    0.8645833  0.87708336\n",
      " 0.86875    0.86041665 0.8520833  0.86875    0.8625     0.86875\n",
      " 0.85833335 0.85833335 0.87291664 0.87291664 0.8541667  0.86875\n",
      " 0.8666667  0.86875    0.8625     0.875      0.86041665 0.87291664\n",
      " 0.87083334 0.8666667  0.87083334 0.85625    0.8666667  0.8625\n",
      " 0.86875    0.86041665 0.8645833  0.87083334 0.8645833  0.875\n",
      " 0.86875    0.8625     0.86041665 0.8645833  0.87291664 0.8666667\n",
      " 0.8666667  0.85833335 0.8666667  0.85833335 0.8645833  0.86875\n",
      " 0.85833335 0.85625    0.8645833  0.8625     0.86875    0.87291664\n",
      " 0.8645833  0.85833335 0.8645833  0.87083334 0.8625     0.8625\n",
      " 0.86875    0.87291664 0.86041665 0.8645833  0.86041665 0.8645833\n",
      " 0.8625     0.86041665 0.86875    0.87916666 0.85625    0.86875\n",
      " 0.8625     0.87083334 0.8645833  0.87291664]\n",
      "[102. 105. 109. 112. 107. 107. 105. 109. 109. 111. 110. 108. 112. 106.\n",
      " 112. 112. 111. 111. 110. 110. 110. 110. 112. 112. 110. 106. 111. 110.\n",
      " 111. 111. 112. 112. 112. 111. 112. 111. 112. 111. 112. 111. 112. 111.\n",
      " 111. 107. 110. 109. 111. 111. 112. 112. 109. 111. 112. 111. 111. 110.\n",
      " 111. 110. 112. 106. 111. 113. 110. 110. 112. 112. 111. 113. 112. 111.\n",
      " 111. 112. 111. 112. 110. 112. 112. 112. 111. 112. 110. 108. 112. 112.\n",
      " 111. 111. 112. 112. 111. 112. 110. 112. 111. 110. 109. 112. 111. 110.\n",
      " 111. 111.]\n",
      "[0.62291664 0.84583336 0.8625     0.85833335 0.85833335 0.84375\n",
      " 0.85833335 0.8645833  0.87916666 0.87916666 0.89166665 0.87708336\n",
      " 0.8979167  0.90625    0.90208334 0.9166667  0.9291667  0.94375\n",
      " 0.9625     0.9479167  0.86875    0.9458333  0.96458334 0.98333335\n",
      " 0.9791667  0.9875     0.9895833  0.9583333  0.9895833  0.99375\n",
      " 0.9916667  0.99375    0.99583334 0.9895833  0.99375    0.99375\n",
      " 0.9875     0.99375    0.99583334 0.98541665 0.99583334 0.99791664\n",
      " 1.         0.99791664 0.99791664 1.         0.9895833  0.99583334\n",
      " 1.         0.99583334 0.99583334 0.99791664 1.         0.99791664\n",
      " 1.         1.         0.99791664 1.         0.99791664 0.99583334\n",
      " 0.6479167  0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[109. 109. 107. 111. 110. 110. 112. 111. 112. 110. 112. 111. 112. 114.\n",
      " 114. 115. 118. 119. 118.  76. 116. 117. 120. 120. 120. 120. 108. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 117. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  62.  62.]\n",
      "[0.70208335 0.8020833  0.8833333  0.8979167  0.90833336 0.90416664\n",
      " 0.9270833  0.975      0.9875     0.92291665 0.9875     0.99375\n",
      " 0.99375    0.99375    0.99583334 0.99791664 0.99791664 0.99583334\n",
      " 0.99791664 0.99791664 0.9791667  0.99791664 0.99791664 0.99791664\n",
      " 0.99791664 0.99583334 0.99791664 0.99791664 0.99791664 0.99791664\n",
      " 0.99791664 0.99791664 0.99791664 0.99375    0.9916667  0.99791664\n",
      " 0.99791664 0.99375    0.99375    0.99791664 0.99791664 0.99791664\n",
      " 0.99791664 1.         0.99791664 0.99583334 0.99791664 0.99791664\n",
      " 0.99791664 0.99791664 1.         0.99791664 0.99583334 1.\n",
      " 1.         1.         1.         1.         1.         0.9916667\n",
      " 0.99791664 0.99375    0.99791664 0.99583334 1.         1.\n",
      " 1.         1.         1.         0.99583334 0.99583334 0.99791664\n",
      " 1.         1.         1.         0.99583334 1.         1.\n",
      " 0.99791664 0.99791664 1.         1.         0.99791664 0.99375\n",
      " 1.         1.         1.         0.99791664 0.99583334 1.\n",
      " 1.         1.         1.         1.         1.         0.99583334\n",
      " 0.59375    0.49583334 0.49583334 0.49583334]\n",
      "[110. 109. 113. 114. 117. 117. 120. 120. 118. 120. 119. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.  62.  62.\n",
      "  62.  62.]\n",
      "[0.60625    0.79375    0.8625     0.8625     0.87291664 0.87291664\n",
      " 0.8520833  0.8875     0.86875    0.88958335 0.85       0.8875\n",
      " 0.8833333  0.8833333  0.87708336 0.87708336 0.875      0.87916666\n",
      " 0.8833333  0.87708336 0.8645833  0.88125    0.87291664 0.8833333\n",
      " 0.87291664 0.87708336 0.87916666 0.87083334 0.875      0.87916666\n",
      " 0.88125    0.89166665 0.88125    0.8833333  0.87708336 0.8666667\n",
      " 0.89166665 0.87291664 0.875      0.8875     0.89375    0.88125\n",
      " 0.875      0.87916666 0.88958335 0.8854167  0.9        0.875\n",
      " 0.875      0.86875    0.87916666 0.8875     0.8854167  0.8875\n",
      " 0.87708336 0.875      0.875      0.875      0.86875    0.8875\n",
      " 0.8666667  0.8854167  0.8875     0.87916666 0.87083334 0.88125\n",
      " 0.875      0.8875     0.8854167  0.8875     0.88958335 0.8875\n",
      " 0.8875     0.8875     0.875      0.875      0.87708336 0.875\n",
      " 0.8854167  0.8854167  0.8854167  0.87291664 0.8875     0.87291664\n",
      " 0.8625     0.8833333  0.87291664 0.89166665 0.875      0.8875\n",
      " 0.875      0.87916666 0.89375    0.8625     0.8875     0.87916666\n",
      " 0.87291664 0.8854167  0.87708336 0.8875    ]\n",
      "[ 94. 109. 102. 111. 111.  77. 111. 111. 111. 109. 110. 111. 110. 111.\n",
      " 110. 109. 110. 111. 111. 111. 111. 111. 112. 111. 112. 111. 109. 111.\n",
      " 111. 111. 111. 111. 111. 111. 112. 111. 108. 109. 110. 111. 110. 111.\n",
      " 112. 111. 111. 111. 111. 110. 110. 111. 110. 111. 111. 111. 111. 111.\n",
      " 107. 110. 110. 111. 110. 110. 111. 111. 110. 111. 111. 111. 111. 111.\n",
      " 111. 111. 111. 111. 111. 111. 111. 110. 111. 112. 111. 111. 110. 111.\n",
      " 110. 111. 111. 111. 111. 111. 110. 111. 108. 111. 111. 111. 111. 111.\n",
      " 111. 111.]\n",
      "[0.7        0.85       0.8375     0.85       0.8666667  0.8666667\n",
      " 0.88125    0.88125    0.88125    0.87916666 0.875      0.87083334\n",
      " 0.8875     0.9291667  0.96666664 0.94166666 0.99375    0.9895833\n",
      " 0.99791664 1.         0.99583334 0.99375    0.99791664 0.99583334\n",
      " 0.99791664 0.99791664 1.         1.         1.         1.\n",
      " 0.6145833  0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[107. 107. 109. 109. 111. 112. 112. 111. 110. 112. 110. 113. 113. 116.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.]\n",
      "[0.68333334 0.85       0.85       0.87083334 0.8833333  0.88125\n",
      " 0.8875     0.8854167  0.87916666 0.88125    0.8854167  0.87708336\n",
      " 0.86875    0.87708336 0.87916666 0.89166665 0.87708336 0.8666667\n",
      " 0.8833333  0.8854167  0.8854167  0.88125    0.88958335 0.8958333\n",
      " 0.875      0.87291664 0.87916666 0.87291664 0.88125    0.89375\n",
      " 0.8854167  0.88125    0.89166665 0.875      0.88125    0.87291664\n",
      " 0.86041665 0.875      0.88125    0.875      0.8833333  0.8875\n",
      " 0.875      0.88125    0.87708336 0.87291664 0.87916666 0.8666667\n",
      " 0.8854167  0.8854167  0.8875     0.9        0.87291664 0.8875\n",
      " 0.87083334 0.88125    0.8875     0.87708336 0.89166665 0.875\n",
      " 0.8854167  0.875      0.88125    0.87916666 0.87708336 0.8833333\n",
      " 0.8854167  0.8666667  0.8875     0.88958335 0.8833333  0.88125\n",
      " 0.8833333  0.88958335 0.875      0.88958335 0.88125    0.88125\n",
      " 0.8875     0.875      0.87916666 0.87708336 0.87708336 0.88958335\n",
      " 0.8833333  0.8833333  0.88125    0.8854167  0.8833333  0.8854167\n",
      " 0.8833333  0.87083334 0.875      0.87916666 0.8854167  0.8833333\n",
      " 0.87916666 0.87708336 0.87083334 0.8875    ]\n",
      "[111. 103. 110. 111. 111. 111. 110. 110. 110. 110. 110. 111. 111. 111.\n",
      " 111. 111. 111. 111. 111. 112. 111. 111. 111. 111. 110. 111. 108. 111.\n",
      " 111. 111. 111. 111. 112. 111. 110. 111. 111. 111. 111. 108. 112. 111.\n",
      " 110. 112. 111. 110. 109. 111. 111. 111. 111. 111. 109. 111. 112. 111.\n",
      " 111. 111. 111. 110. 111. 111. 111. 111. 112. 110. 111. 111. 111. 111.\n",
      " 109. 111. 111. 111. 111. 111. 111. 111. 111. 111. 111. 111. 111. 110.\n",
      " 111. 111. 111. 111. 111. 110. 110. 110. 110. 111. 111. 111. 111. 110.\n",
      " 111. 111.]\n",
      "[0.52916664 0.78333336 0.8645833  0.84166664 0.8541667  0.8875\n",
      " 0.88958335 0.90416664 0.92291665 0.94166666 0.76458335 0.85625\n",
      " 0.87083334 0.8625     0.86875    0.8645833  0.87708336 0.86875\n",
      " 0.875      0.8666667  0.88125    0.8875     0.8645833  0.88958335\n",
      " 0.86041665 0.87708336 0.8625     0.88125    0.8833333  0.88125\n",
      " 0.875      0.87083334 0.87916666 0.8833333  0.87291664 0.8833333\n",
      " 0.88125    0.88958335 0.9        0.89166665 0.86875    0.875\n",
      " 0.8645833  0.8833333  0.8875     0.88958335 0.87291664 0.87916666\n",
      " 0.875      0.87916666 0.875      0.8875     0.87708336 0.8958333\n",
      " 0.8645833  0.88958335 0.8833333  0.87083334 0.8854167  0.875\n",
      " 0.87916666 0.87708336 0.8854167  0.86875    0.87708336 0.87291664\n",
      " 0.89166665 0.8875     0.87291664 0.87291664 0.8833333  0.8875\n",
      " 0.8833333  0.8875     0.8875     0.87916666 0.88125    0.87291664\n",
      " 0.8958333  0.8833333  0.8833333  0.8854167  0.8875     0.88125\n",
      " 0.87083334 0.86041665 0.89166665 0.8979167  0.875      0.87916666\n",
      " 0.8875     0.8854167  0.8833333  0.89166665 0.86875    0.8875\n",
      " 0.8833333  0.87916666 0.8833333  0.88125   ]\n",
      "[104. 112. 110. 109. 111. 112. 114. 113. 116. 119. 108. 110. 112. 111.\n",
      " 111. 111. 112. 110. 112. 110. 110. 110. 110. 111. 110. 111. 111. 111.\n",
      " 111. 108. 111. 110. 111. 111. 111. 111. 111. 112. 111. 111. 111. 111.\n",
      " 111. 111. 111. 109. 112. 110. 111. 111. 112. 110. 111. 111. 111. 110.\n",
      " 111. 111. 109. 111. 111. 112. 111. 109. 109. 111. 111. 109. 110. 111.\n",
      " 111. 111. 111. 111. 111. 112. 111. 111. 111. 111. 111. 111. 110. 111.\n",
      " 111. 111. 111. 111. 110. 112. 111. 111. 111. 110. 111. 111. 111. 111.\n",
      " 111. 111.]\n",
      "[0.66875    0.8354167  0.8041667  0.8875     0.9270833  0.9625\n",
      " 0.9145833  0.98333335 0.98541665 0.9625     0.9895833  0.99583334\n",
      " 0.99791664 0.99375    0.99375    0.99375    0.99791664 0.99791664\n",
      " 0.99791664 0.99583334 0.84583336 0.51458335 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[111. 111. 109. 112. 114. 118. 120. 120. 119. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120.  78.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.]\n",
      "[0.69375    0.8354167  0.84375    0.8666667  0.89166665 0.85625\n",
      " 0.9291667  0.96458334 0.96875    0.99583334 0.99375    0.99583334\n",
      " 0.62083334 0.7375     0.75625    0.77708334 0.825      0.87083334\n",
      " 0.8854167  0.88958335 0.8666667  0.89166665 0.875      0.8854167\n",
      " 0.8833333  0.8625     0.87083334 0.8854167  0.8854167  0.8645833\n",
      " 0.88125    0.87291664 0.8833333  0.87708336 0.8666667  0.875\n",
      " 0.8854167  0.8666667  0.8625     0.88125    0.87708336 0.87916666\n",
      " 0.875      0.86041665 0.875      0.89166665 0.8854167  0.8875\n",
      " 0.88958335 0.87083334 0.86875    0.89166665 0.88125    0.8833333\n",
      " 0.87291664 0.875      0.88958335 0.87708336 0.87083334 0.8854167\n",
      " 0.8854167  0.8854167  0.87916666 0.86875    0.8875     0.87916666\n",
      " 0.8854167  0.8854167  0.87083334 0.86875    0.8833333  0.88125\n",
      " 0.875      0.875      0.88125    0.87708336 0.875      0.88958335\n",
      " 0.87708336 0.8625     0.8833333  0.8854167  0.88958335 0.88958335\n",
      " 0.8854167  0.8833333  0.87916666 0.88125    0.87916666 0.89375\n",
      " 0.87083334 0.88958335 0.87916666 0.88958335 0.875      0.88125\n",
      " 0.87916666 0.8875     0.8833333  0.8666667 ]\n",
      "[107. 107. 112. 113. 113. 113. 116. 111. 120. 120. 120. 117.  96.  98.\n",
      " 103. 107. 107. 111. 110. 111. 109. 110. 111. 112. 109. 111. 111. 111.\n",
      " 110. 111. 111. 110. 112. 111. 110. 112. 111. 103. 111. 112. 111. 111.\n",
      " 111. 111. 112. 111. 111. 111. 111. 111. 110. 111. 111. 111. 111. 112.\n",
      " 111. 110. 111. 111. 111. 112. 110. 112. 110. 111. 111. 111. 111. 111.\n",
      " 111. 111. 109. 111. 112. 109. 110. 111. 112. 111. 111. 111. 111. 111.\n",
      " 112. 111. 110. 112. 111. 111. 111. 111. 112. 112. 111. 111. 112. 111.\n",
      " 111. 111.]\n",
      "[0.47916666 0.48541668 0.54375    0.59583336 0.7270833  0.76458335\n",
      " 0.78541666 0.8625     0.8645833  0.85625    0.8354167  0.85833335\n",
      " 0.86041665 0.87083334 0.86875    0.8625     0.85833335 0.84791666\n",
      " 0.8520833  0.8645833  0.8666667  0.87083334 0.8645833  0.8541667\n",
      " 0.8645833  0.8625     0.8645833  0.8666667  0.8666667  0.85\n",
      " 0.8645833  0.8541667  0.84375    0.86875    0.87083334 0.87083334\n",
      " 0.8625     0.8625     0.84791666 0.8666667  0.86041665 0.8625\n",
      " 0.8666667  0.8666667  0.87083334 0.86041665 0.87291664 0.86041665\n",
      " 0.8666667  0.87291664 0.8625     0.8645833  0.85625    0.875\n",
      " 0.8645833  0.8625     0.8520833  0.87083334 0.8520833  0.8645833\n",
      " 0.8625     0.8666667  0.8541667  0.85833335 0.84375    0.86041665\n",
      " 0.86041665 0.86875    0.87291664 0.8625     0.8666667  0.8666667\n",
      " 0.8666667  0.86875    0.875      0.85       0.8645833  0.8666667\n",
      " 0.87291664 0.86875    0.85625    0.85625    0.86041665 0.8541667\n",
      " 0.8645833  0.8645833  0.86875    0.8645833  0.86041665 0.875\n",
      " 0.86041665 0.87291664 0.86041665 0.87083334 0.8625     0.86041665\n",
      " 0.8666667  0.85625    0.87083334 0.87083334]\n",
      "[ 62.  62.  59.  92. 100. 103. 107. 111. 109. 106. 106. 111. 110. 112.\n",
      " 110. 111. 108. 108. 111. 112. 110. 108. 111. 106. 112. 111. 112. 112.\n",
      " 112. 111. 111. 111. 111. 111. 110. 112. 104. 109. 107. 111. 111. 111.\n",
      " 110. 110. 113. 109. 111. 110. 110. 111. 107. 110. 112. 111. 111. 110.\n",
      " 112. 110. 111. 112. 112. 111. 111. 108. 110. 111. 110. 113. 110. 111.\n",
      " 110. 110. 112. 111. 109. 111. 110. 111. 113. 110. 112. 106. 111. 110.\n",
      " 112. 111. 111. 112. 110. 112. 112. 110. 112. 112. 112. 109. 111. 112.\n",
      " 111. 108.]\n",
      "Number of layers: 19\n",
      "[0.7104167  0.8666667  0.8625     0.8833333  0.87708336 0.87291664\n",
      " 0.8625     0.8958333  0.8854167  0.86041665 0.87291664 0.8833333\n",
      " 0.86041665 0.8625     0.8645833  0.875      0.875      0.8645833\n",
      " 0.87708336 0.87916666 0.8645833  0.89166665 0.87708336 0.86875\n",
      " 0.87083334 0.87083334 0.8854167  0.87708336 0.8958333  0.8854167\n",
      " 0.85       0.87291664 0.8958333  0.87916666 0.87291664 0.87708336\n",
      " 0.87083334 0.86875    0.8666667  0.8854167  0.8875     0.87291664\n",
      " 0.8833333  0.87083334 0.8645833  0.87291664 0.88125    0.87916666\n",
      " 0.88125    0.875      0.87916666 0.88125    0.87916666 0.8666667\n",
      " 0.87708336 0.8854167  0.89375    0.86041665 0.87708336 0.88958335\n",
      " 0.87916666 0.88125    0.88125    0.8854167  0.8854167  0.8833333\n",
      " 0.875      0.8833333  0.8833333  0.8833333  0.8645833  0.87291664\n",
      " 0.8854167  0.8645833  0.8833333  0.8875     0.8854167  0.88125\n",
      " 0.875      0.875      0.87916666 0.89375    0.86875    0.875\n",
      " 0.8875     0.8666667  0.87916666 0.87291664 0.8875     0.87916666\n",
      " 0.87916666 0.87708336 0.8875     0.8833333  0.8833333  0.88125\n",
      " 0.8833333  0.85833335 0.87291664 0.87916666]\n",
      "[109. 111. 111. 112. 111. 111. 112. 112. 112. 110. 111. 109. 111. 111.\n",
      " 111. 111. 110. 111. 111. 111. 111. 112. 109. 110. 112. 111. 111. 111.\n",
      " 111. 112. 111. 112. 111. 111. 111. 111. 111. 112. 111. 112. 111. 111.\n",
      " 111. 111. 111. 111. 111. 112. 112. 111. 111. 112. 111. 110. 112. 111.\n",
      " 111. 112. 111. 112. 111. 111. 112. 112. 112. 111. 111. 112. 112. 112.\n",
      " 110. 111. 111. 108. 110. 112. 110. 111. 111. 111. 111. 112. 111. 111.\n",
      " 112. 112. 112. 112. 112. 112. 112. 112. 112. 112. 112. 111. 112. 111.\n",
      " 111. 112.]\n",
      "[0.7395833  0.88125    0.8375     0.87083334 0.8833333  0.89375\n",
      " 0.8875     0.875      0.875      0.8625     0.8875     0.88958335\n",
      " 0.8979167  0.88125    0.88958335 0.9        0.8854167  0.875\n",
      " 0.87291664 0.89166665 0.87916666 0.8979167  0.88958335 0.8854167\n",
      " 0.9        0.89375    0.90625    0.88125    0.88958335 0.89166665\n",
      " 0.89375    0.89375    0.90208334 0.89166665 0.87916666 0.9\n",
      " 0.8854167  0.8958333  0.8854167  0.8854167  0.89375    0.8979167\n",
      " 0.9        0.8979167  0.8979167  0.9        0.8833333  0.9\n",
      " 0.90208334 0.8958333  0.89375    0.8875     0.89375    0.90208334\n",
      " 0.9        0.9        0.88958335 0.8875     0.9        0.8958333\n",
      " 0.8875     0.8854167  0.89166665 0.90625    0.89375    0.89166665\n",
      " 0.88958335 0.89166665 0.8958333  0.8979167  0.8979167  0.8854167\n",
      " 0.8979167  0.90416664 0.90208334 0.90208334 0.9        0.90208334\n",
      " 0.88958335 0.88958335 0.8958333  0.89375    0.8958333  0.89166665\n",
      " 0.8854167  0.8979167  0.88125    0.90208334 0.8979167  0.8979167\n",
      " 0.89166665 0.90416664 0.90208334 0.89166665 0.8958333  0.90208334\n",
      " 0.89166665 0.89375    0.8979167  0.89375   ]\n",
      "[113. 111. 109. 114. 113. 112. 113. 114. 109. 112. 113. 112. 111. 113.\n",
      " 114. 113. 114. 113. 113. 113. 110. 110. 114. 110. 113. 110. 111. 112.\n",
      " 109. 113. 112. 113. 110. 113. 114. 113. 114. 110. 112. 111. 111. 112.\n",
      " 113. 113. 113. 110. 113. 113. 111. 113. 112. 113. 112. 109. 114. 113.\n",
      " 112. 113. 112. 113. 113. 113. 114. 113. 113. 112. 113. 112. 113. 111.\n",
      " 112. 114. 112. 111. 110. 113. 113. 112. 113. 113. 113. 111. 109. 111.\n",
      " 112. 112. 111. 111. 111. 113. 114. 113. 114. 113. 112. 113. 111. 113.\n",
      " 111. 113.]\n",
      "[0.6666667  0.84583336 0.85625    0.87708336 0.8520833  0.87708336\n",
      " 0.89166665 0.9145833  0.90833336 0.93958336 0.94375    0.975\n",
      " 0.95625    0.5083333  0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[111. 105. 109. 111. 112. 112. 113. 108. 111. 118. 116. 120.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  62.  62.]\n",
      "[0.50416666 0.5395833  0.50416666 0.525      0.5083333  0.47916666\n",
      " 0.51458335 0.475      0.4875     0.48333332 0.5125     0.7270833\n",
      " 0.8520833  0.86041665 0.85       0.8520833  0.8645833  0.85625\n",
      " 0.87083334 0.8520833  0.87291664 0.87083334 0.8625     0.8645833\n",
      " 0.85833335 0.85833335 0.86041665 0.8625     0.84791666 0.86875\n",
      " 0.85625    0.8875     0.85833335 0.8666667  0.85833335 0.8520833\n",
      " 0.8645833  0.87291664 0.8666667  0.8645833  0.8520833  0.86875\n",
      " 0.87083334 0.8645833  0.8625     0.8625     0.86875    0.8645833\n",
      " 0.84791666 0.8645833  0.85625    0.8625     0.87708336 0.8625\n",
      " 0.8625     0.87916666 0.8666667  0.85833335 0.87291664 0.87083334\n",
      " 0.8625     0.8666667  0.875      0.85625    0.86041665 0.8666667\n",
      " 0.87708336 0.86041665 0.8666667  0.87291664 0.8645833  0.85625\n",
      " 0.86875    0.86875    0.86875    0.8666667  0.8541667  0.86875\n",
      " 0.86041665 0.86041665 0.86041665 0.8625     0.87083334 0.87708336\n",
      " 0.8666667  0.86875    0.85833335 0.85833335 0.87291664 0.85833335\n",
      " 0.85833335 0.87083334 0.87291664 0.87291664 0.875      0.8645833\n",
      " 0.8625     0.8666667  0.87708336 0.86875   ]\n",
      "[ 62.  58.  58.  62.  58.  58.  62.  58.  58.  58.  62. 107. 109. 111.\n",
      " 111. 108. 112. 110. 110. 112. 108. 112. 111. 111. 111. 111. 106. 113.\n",
      " 111. 110. 111. 110. 110. 110. 108. 112. 111. 111. 112. 112. 110. 112.\n",
      " 109. 113. 111. 112. 112. 111. 109. 110. 112. 110. 113. 111. 110. 110.\n",
      " 112. 110. 112. 112. 111. 110. 110. 110. 111. 111. 113. 112. 111. 111.\n",
      " 110. 112. 109. 110. 110. 110. 111. 112. 111. 111. 111. 111. 110. 111.\n",
      " 112. 110. 113. 111. 110. 111. 111. 113. 111. 110. 112. 109. 110. 111.\n",
      " 112. 112.]\n",
      "[0.5375     0.80625    0.85625    0.86041665 0.8520833  0.8333333\n",
      " 0.8541667  0.8541667  0.85833335 0.8625     0.8541667  0.8520833\n",
      " 0.87083334 0.8645833  0.87083334 0.8645833  0.85833335 0.85833335\n",
      " 0.8666667  0.8520833  0.8625     0.85625    0.86875    0.85833335\n",
      " 0.87083334 0.8625     0.86041665 0.86875    0.87083334 0.86875\n",
      " 0.8666667  0.8645833  0.8625     0.8625     0.85833335 0.87083334\n",
      " 0.8666667  0.8625     0.86875    0.825      0.8666667  0.8666667\n",
      " 0.8625     0.8625     0.8625     0.8625     0.8625     0.875\n",
      " 0.86875    0.875      0.8645833  0.8666667  0.87083334 0.8645833\n",
      " 0.8541667  0.8645833  0.86875    0.875      0.8541667  0.86875\n",
      " 0.8645833  0.8666667  0.8645833  0.8541667  0.8645833  0.87916666\n",
      " 0.8645833  0.86041665 0.87708336 0.86875    0.8666667  0.87083334\n",
      " 0.8645833  0.86875    0.86875    0.8666667  0.86875    0.87291664\n",
      " 0.8625     0.8645833  0.8625     0.8666667  0.8625     0.8645833\n",
      " 0.8645833  0.85833335 0.85833335 0.8645833  0.875      0.87083334\n",
      " 0.8666667  0.87916666 0.85625    0.8645833  0.87083334 0.86875\n",
      " 0.8625     0.8625     0.8666667  0.85833335]\n",
      "[101. 109. 108. 110. 110. 103. 110. 110. 111. 108. 110. 111. 111. 111.\n",
      " 112. 111. 111. 112. 112. 110. 110. 111. 110. 112. 112. 111. 111. 110.\n",
      " 113. 109. 111. 106. 111. 111. 110. 111. 112. 111. 111. 109. 111. 109.\n",
      " 112. 112. 111. 110. 111. 111. 110. 112. 112. 111. 109. 109. 112. 111.\n",
      " 111. 112. 111. 111. 112. 112. 111. 110. 112. 112. 112. 109. 113. 112.\n",
      " 112. 112. 110. 112. 111. 110. 110. 110. 111. 113. 112. 112. 112. 111.\n",
      " 112. 112. 111. 112. 111. 112. 111. 109. 112. 111. 111. 111. 111. 111.\n",
      " 111. 109.]\n",
      "[0.62083334 0.84166664 0.8541667  0.86041665 0.85833335 0.85833335\n",
      " 0.84583336 0.8520833  0.87708336 0.87083334 0.8666667  0.86041665\n",
      " 0.87291664 0.875      0.87916666 0.8833333  0.8979167  0.90208334\n",
      " 0.9166667  0.91875    0.65625    0.83958334 0.875      0.87291664\n",
      " 0.8645833  0.8645833  0.8625     0.8645833  0.86041665 0.87083334\n",
      " 0.87083334 0.86875    0.85625    0.88125    0.8875     0.8645833\n",
      " 0.8854167  0.87916666 0.86875    0.87916666 0.8666667  0.89166665\n",
      " 0.87708336 0.87291664 0.87083334 0.87708336 0.88958335 0.87916666\n",
      " 0.8854167  0.8833333  0.8645833  0.8666667  0.8854167  0.85833335\n",
      " 0.88958335 0.87708336 0.88125    0.87916666 0.87708336 0.8958333\n",
      " 0.87291664 0.87083334 0.86875    0.8875     0.89375    0.87708336\n",
      " 0.87916666 0.875      0.85625    0.8833333  0.8854167  0.8645833\n",
      " 0.88125    0.8666667  0.89166665 0.8875     0.88958335 0.88958335\n",
      " 0.8833333  0.8833333  0.8875     0.87916666 0.85833335 0.8875\n",
      " 0.875      0.87708336 0.88958335 0.8833333  0.89375    0.87083334\n",
      " 0.8854167  0.8833333  0.87291664 0.8854167  0.8875     0.8833333\n",
      " 0.87708336 0.8854167  0.8833333  0.87708336]\n",
      "[111. 112. 103. 110. 112. 110. 112. 111. 109. 111. 111. 110. 113. 110.\n",
      " 111. 112. 108. 116. 117.  93. 104. 111. 111. 110. 110. 110. 106. 111.\n",
      " 112. 110. 111. 111. 112. 110. 111. 108. 110. 109. 112. 112. 110. 110.\n",
      " 111. 111. 111. 110. 111. 111. 111. 109. 111. 111. 111. 111. 111. 112.\n",
      " 111. 110. 112. 111. 111. 111. 112. 111. 111. 111. 112. 110. 111. 111.\n",
      " 111. 110. 111. 111. 110. 111. 112. 110. 111. 110. 110. 111. 112. 111.\n",
      " 111. 111. 110. 111. 112. 111. 111. 111. 111. 111. 112. 111. 111. 109.\n",
      " 111. 111.]\n",
      "[0.6354167  0.85       0.84166664 0.86875    0.86041665 0.8541667\n",
      " 0.85625    0.875      0.8520833  0.8645833  0.87916666 0.8625\n",
      " 0.85       0.85833335 0.8666667  0.85833335 0.8541667  0.8666667\n",
      " 0.8625     0.8645833  0.86041665 0.875      0.8645833  0.875\n",
      " 0.88125    0.875      0.87708336 0.86875    0.89166665 0.89375\n",
      " 0.87708336 0.89166665 0.8833333  0.93541664 0.91875    0.9125\n",
      " 0.93333334 0.9375     0.86875    0.86875    0.9375     0.92083335\n",
      " 0.89166665 0.9604167  0.9375     0.9145833  0.9770833  0.9895833\n",
      " 0.9916667  0.9875     0.9916667  0.9916667  1.         0.99791664\n",
      " 0.99375    0.99375    0.99791664 0.99791664 0.99791664 0.99583334\n",
      " 1.         1.         0.99583334 0.99791664 0.99583334 0.99375\n",
      " 0.99375    0.99791664 0.99583334 0.99791664 0.99583334 0.99791664\n",
      " 0.99583334 0.99791664 0.99583334 0.99583334 0.99583334 0.99791664\n",
      " 1.         0.99583334 0.99583334 0.9916667  0.99583334 0.99791664\n",
      " 0.99791664 0.9895833  0.99375    1.         1.         0.99791664\n",
      " 0.99791664 1.         0.99791664 1.         1.         0.99375\n",
      " 0.99791664 0.99375    0.99791664 1.        ]\n",
      "[108. 108. 103. 110. 108. 113. 112. 107. 111. 111. 108. 107. 111. 111.\n",
      " 112. 112. 109. 106. 112. 112. 111. 109. 112. 112. 111. 109. 111. 112.\n",
      " 110. 112. 112. 113. 112. 112. 118. 114. 112. 116.  95. 112. 113.  76.\n",
      " 116. 120.  97. 119. 118. 120. 120. 120. 120. 120. 120. 120. 119. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 119. 120. 120. 120. 120. 120. 120. 117. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.]\n",
      "[0.6333333  0.76875    0.82916665 0.83125    0.8645833  0.875\n",
      " 0.84583336 0.8625     0.8541667  0.88958335 0.875      0.85\n",
      " 0.89375    0.89375    0.89166665 0.9145833  0.9625     0.96666664\n",
      " 0.9916667  0.8354167  0.84583336 0.86875    0.8625     0.85625\n",
      " 0.85833335 0.84791666 0.8666667  0.85       0.85833335 0.8645833\n",
      " 0.8541667  0.8625     0.8666667  0.86875    0.8645833  0.8625\n",
      " 0.85625    0.86875    0.85       0.87083334 0.85       0.86875\n",
      " 0.8666667  0.86041665 0.8666667  0.8645833  0.8666667  0.8541667\n",
      " 0.8625     0.8645833  0.875      0.8666667  0.8666667  0.8625\n",
      " 0.85625    0.87083334 0.87083334 0.8625     0.8645833  0.8645833\n",
      " 0.8666667  0.85625    0.85625    0.86875    0.8645833  0.85833335\n",
      " 0.8645833  0.85833335 0.8541667  0.8625     0.8666667  0.8520833\n",
      " 0.8666667  0.86041665 0.87083334 0.87291664 0.8625     0.8645833\n",
      " 0.8625     0.8625     0.87083334 0.86875    0.8645833  0.86875\n",
      " 0.8625     0.8625     0.8541667  0.8645833  0.87291664 0.87083334\n",
      " 0.8625     0.87083334 0.8666667  0.85833335 0.85833335 0.8625\n",
      " 0.86875    0.8645833  0.8666667  0.86041665]\n",
      "[ 58. 106. 109. 109. 110. 112. 106. 110. 110. 107. 106. 110. 114. 109.\n",
      " 111. 118. 114. 120. 120.  92. 111. 109. 112. 110. 112. 110. 112. 111.\n",
      " 110. 111. 111. 110. 106. 110. 112. 113. 111. 108. 111. 110. 106. 112.\n",
      " 112. 112. 111. 112. 111. 111. 112. 111. 112. 112. 111. 110. 109. 111.\n",
      " 111. 106. 111. 112. 110. 112. 111. 112. 112. 112. 111. 110. 111. 111.\n",
      " 110. 111. 112. 110. 111. 110. 111. 110. 112. 111. 111. 110. 110. 113.\n",
      " 112. 106. 110. 109. 112. 110. 111. 110. 109. 111. 112. 109. 112. 112.\n",
      " 111. 112.]\n",
      "[0.6166667  0.84166664 0.8520833  0.85       0.8375     0.87083334\n",
      " 0.8520833  0.8625     0.87083334 0.86875    0.8625     0.85\n",
      " 0.85833335 0.8666667  0.87083334 0.86041665 0.8666667  0.8645833\n",
      " 0.84375    0.8625     0.8645833  0.86875    0.8541667  0.8645833\n",
      " 0.87916666 0.87708336 0.86875    0.84375    0.86875    0.87083334\n",
      " 0.86041665 0.87708336 0.87291664 0.87291664 0.8625     0.8645833\n",
      " 0.8541667  0.85625    0.8645833  0.88125    0.87083334 0.88125\n",
      " 0.9291667  0.93958336 0.95208335 0.99583334 0.99375    0.99375\n",
      " 0.90416664 0.6666667  0.8229167  0.8541667  0.8666667  0.8645833\n",
      " 0.87083334 0.87708336 0.8833333  0.87916666 0.88958335 0.87916666\n",
      " 0.8666667  0.88958335 0.87083334 0.8875     0.8875     0.88125\n",
      " 0.875      0.875      0.875      0.87083334 0.87916666 0.88125\n",
      " 0.88958335 0.875      0.87083334 0.875      0.875      0.8625\n",
      " 0.8833333  0.87916666 0.8854167  0.875      0.8833333  0.8833333\n",
      " 0.86875    0.8625     0.875      0.875      0.88125    0.86875\n",
      " 0.87708336 0.8854167  0.8854167  0.8666667  0.87708336 0.87083334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.8645833  0.85833335 0.875      0.875     ]\n",
      "[110. 108. 112. 107. 110. 111. 108. 112. 112. 109. 110. 111. 111. 110.\n",
      " 110. 112. 112. 111. 111. 112. 107. 111. 111. 110. 111. 110. 111. 111.\n",
      " 109. 110. 109. 110. 112. 111. 106. 111. 110. 110. 111. 112. 104. 112.\n",
      " 113. 116. 120. 120. 120. 120.  75.  86. 111. 112. 111. 110. 110. 111.\n",
      " 110. 110. 112. 110. 111. 110. 112. 111. 110. 106. 111. 111. 111. 111.\n",
      " 111. 111. 107. 111. 111. 111. 110. 111. 111. 110. 111. 111. 112. 112.\n",
      " 111. 112. 109. 111. 111. 111. 112. 112. 108. 109. 111. 112. 111. 111.\n",
      " 110. 112.]\n",
      "[0.675      0.84791666 0.83125    0.85625    0.8625     0.86041665\n",
      " 0.8666667  0.86875    0.84583336 0.8666667  0.84583336 0.87291664\n",
      " 0.86041665 0.86875    0.8666667  0.85833335 0.8520833  0.86041665\n",
      " 0.875      0.8645833  0.87291664 0.8625     0.84375    0.86875\n",
      " 0.87291664 0.86875    0.86875    0.8666667  0.86041665 0.87916666\n",
      " 0.8666667  0.875      0.875      0.87916666 0.87916666 0.8875\n",
      " 0.85       0.88958335 0.8958333  0.88125    0.8875     0.89166665\n",
      " 0.89166665 0.8875     0.8875     0.88958335 0.89166665 0.89375\n",
      " 0.8979167  0.88125    0.8958333  0.88958335 0.88958335 0.86875\n",
      " 0.8979167  0.8875     0.87291664 0.8875     0.89375    0.89375\n",
      " 0.88958335 0.87916666 0.8958333  0.88958335 0.8833333  0.8833333\n",
      " 0.87916666 0.86875    0.8854167  0.8833333  0.8854167  0.88958335\n",
      " 0.8875     0.8875     0.8833333  0.89166665 0.88958335 0.8875\n",
      " 0.8854167  0.89375    0.89375    0.89166665 0.89375    0.875\n",
      " 0.89166665 0.87916666 0.89166665 0.8958333  0.89166665 0.8958333\n",
      " 0.8979167  0.88958335 0.89375    0.8854167  0.89375    0.87916666\n",
      " 0.90208334 0.89375    0.89375    0.8833333 ]\n",
      "[ 89. 111. 111. 110. 111. 111. 110. 111. 108. 111. 111. 108. 112. 112.\n",
      " 109. 111. 111. 108. 112. 112. 110. 111. 110. 111. 112. 109. 108. 110.\n",
      " 113. 112. 111. 112. 112. 107. 112. 113. 112. 112. 112. 112. 112. 113.\n",
      " 109. 112. 113. 112. 113. 113. 113. 112. 112. 111. 111. 114. 113. 110.\n",
      " 113. 113. 109. 111. 112. 112. 112. 111. 112. 114. 109. 111. 111. 111.\n",
      " 113. 112. 113. 113. 111. 113. 113. 113. 113. 113. 114. 112. 111. 113.\n",
      " 112. 113. 112. 111. 113. 113. 112. 113. 110. 114. 112. 113. 111. 112.\n",
      " 114. 113.]\n",
      "Number of layers: 20\n",
      "[0.49166667 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.]\n",
      "[0.74375    0.85       0.8645833  0.8645833  0.84375    0.83958334\n",
      " 0.8520833  0.86041665 0.8625     0.84791666 0.86875    0.8666667\n",
      " 0.8541667  0.87708336 0.8666667  0.8666667  0.86041665 0.8666667\n",
      " 0.8625     0.8666667  0.8666667  0.87708336 0.8645833  0.8666667\n",
      " 0.87291664 0.87291664 0.8666667  0.8541667  0.8625     0.8645833\n",
      " 0.85625    0.87291664 0.8666667  0.8645833  0.87291664 0.8625\n",
      " 0.85833335 0.8645833  0.8625     0.8666667  0.86875    0.86875\n",
      " 0.87708336 0.8645833  0.8645833  0.8625     0.8625     0.8625\n",
      " 0.8666667  0.87708336 0.8645833  0.85625    0.8645833  0.8625\n",
      " 0.875      0.8645833  0.85833335 0.85625    0.8645833  0.86041665\n",
      " 0.8645833  0.8645833  0.87083334 0.85833335 0.85625    0.8625\n",
      " 0.8666667  0.87291664 0.875      0.8625     0.8666667  0.86875\n",
      " 0.87083334 0.8666667  0.87916666 0.8625     0.85833335 0.85625\n",
      " 0.87291664 0.87916666 0.875      0.85625    0.86875    0.86875\n",
      " 0.85625    0.8625     0.8625     0.86875    0.8666667  0.8625\n",
      " 0.8645833  0.8645833  0.8625     0.85833335 0.8666667  0.875\n",
      " 0.8645833  0.86041665 0.8666667  0.86875   ]\n",
      "[108. 111. 110. 110. 111. 111. 111. 111. 108. 111. 111. 111. 110. 111.\n",
      " 109. 112. 112. 112. 109. 112. 110. 111. 111. 111. 112. 112. 112. 111.\n",
      " 112. 111. 111. 112. 111. 107. 112. 111. 110. 112. 112. 111. 110. 112.\n",
      " 113. 112. 111. 111. 112. 112. 110. 112. 111. 111. 110. 111. 112. 112.\n",
      " 110. 111. 110. 109. 111. 113. 110. 109. 112. 111. 111. 112. 112. 110.\n",
      " 111. 111. 110. 111. 112. 110. 112. 111. 113. 111. 110. 112. 112. 111.\n",
      " 110. 112. 112. 110. 111. 112. 112. 112. 112. 111. 112. 111. 112. 111.\n",
      " 109. 111.]\n",
      "[0.65833336 0.8208333  0.86041665 0.875      0.86041665 0.8833333\n",
      " 0.84791666 0.89166665 0.90625    0.9166667  0.9291667  0.9\n",
      " 0.90416664 0.95       0.93125    0.92291665 0.97291666 0.98333335\n",
      " 0.92291665 0.9791667  0.94375    0.9875     0.9895833  0.97291666\n",
      " 1.         0.99583334 0.99583334 0.99791664 0.99583334 1.\n",
      " 1.         1.         0.975      0.99791664 1.         1.\n",
      " 0.99791664 0.99791664 0.99791664 0.99791664 0.99583334 0.99375\n",
      " 0.98541665 0.99791664 1.         1.         0.97083336 0.90833336\n",
      " 0.9916667  1.         1.         0.99791664 0.99791664 0.99583334\n",
      " 0.99583334 0.99791664 0.99791664 1.         0.99791664 1.\n",
      " 0.99583334 1.         1.         1.         1.         0.99791664\n",
      " 0.99791664 1.         1.         1.         1.         1.\n",
      " 1.         0.99791664 1.         0.99791664 1.         1.\n",
      " 0.775      0.7875     0.87291664 0.89166665 0.93958336 0.9625\n",
      " 0.95625    0.9625     0.9916667  0.95       1.         1.\n",
      " 0.99583334 0.97291666 0.71666664 0.82708335 0.88958335 0.96666664\n",
      " 0.98541665 0.9875     0.9916667  0.98333335]\n",
      "[107. 108. 110. 109. 109. 112. 112. 113. 115. 100. 118. 114. 118. 111.\n",
      " 110. 118. 118. 118. 120. 116. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 107. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 100. 107. 112. 116. 118. 113.\n",
      " 120. 118. 115. 116. 120. 120. 120.  87. 104. 110. 115. 119. 120. 120.\n",
      " 120. 120.]\n",
      "[0.5416667  0.8125     0.82916665 0.85625    0.8854167  0.8875\n",
      " 0.8833333  0.89166665 0.87083334 0.8854167  0.88958335 0.89375\n",
      " 0.89166665 0.87291664 0.8979167  0.8958333  0.8833333  0.8854167\n",
      " 0.8875     0.87916666 0.8854167  0.8958333  0.87708336 0.8854167\n",
      " 0.8854167  0.8833333  0.89375    0.88958335 0.87083334 0.87916666\n",
      " 0.89166665 0.86875    0.89375    0.8854167  0.87916666 0.88125\n",
      " 0.87916666 0.8854167  0.8875     0.8875     0.8875     0.9\n",
      " 0.89166665 0.88958335 0.88958335 0.8875     0.8854167  0.89375\n",
      " 0.88958335 0.8833333  0.8875     0.8854167  0.89166665 0.875\n",
      " 0.8833333  0.89375    0.8833333  0.8854167  0.8833333  0.87708336\n",
      " 0.87916666 0.89166665 0.8854167  0.87708336 0.87916666 0.88125\n",
      " 0.8833333  0.8854167  0.8833333  0.88125    0.87916666 0.88125\n",
      " 0.8854167  0.90208334 0.89166665 0.9        0.88958335 0.89375\n",
      " 0.88958335 0.89166665 0.88958335 0.88958335 0.87708336 0.89166665\n",
      " 0.8854167  0.8854167  0.8854167  0.88958335 0.8958333  0.8833333\n",
      " 0.89166665 0.88125    0.8875     0.89166665 0.8854167  0.9\n",
      " 0.8854167  0.8958333  0.88958335 0.89166665]\n",
      "[105.  85. 110. 113. 113. 112. 111. 112. 113. 113. 114. 111. 112. 113.\n",
      " 111. 112. 111. 113. 112. 113. 113. 109. 109. 112. 111. 111. 112. 113.\n",
      " 112. 113. 111. 112. 113. 111. 109. 110. 110. 111. 112. 112. 110. 112.\n",
      " 112. 113. 112. 112. 112. 111. 111. 112. 111. 112. 113. 114. 111. 112.\n",
      " 113. 110. 108. 112. 113. 112. 111. 111. 112. 112. 114. 113. 112. 113.\n",
      " 112. 112. 112. 112. 112. 112. 112. 109. 113. 112. 112. 111. 113. 112.\n",
      " 114. 110. 113. 110. 112. 113. 113. 111. 112. 111. 112. 109. 114. 110.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 114. 112.]\n",
      "[0.65833336 0.84375    0.90416664 0.87916666 0.94375    0.95208335\n",
      " 0.99375    0.9916667  0.99583334 0.99583334 0.99583334 1.\n",
      " 1.         0.99583334 1.         0.99791664 0.99791664 0.57916665\n",
      " 0.53125    0.5541667  0.56875    0.575      0.5833333  0.62083334\n",
      " 0.6333333  0.6354167  0.6354167  0.6458333  0.6479167  0.6479167\n",
      " 0.65       0.65       0.65       0.65       0.65       0.65\n",
      " 0.65       0.65       0.65       0.65       0.65       0.65\n",
      " 0.65       0.65       0.65       0.65       0.65       0.65\n",
      " 0.65       0.65       0.65       0.65       0.65       0.65\n",
      " 0.65       0.65       0.65       0.65       0.65       0.65\n",
      " 0.65       0.65       0.65       0.65       0.65       0.65\n",
      " 0.65       0.65       0.65       0.65       0.65       0.65\n",
      " 0.65       0.6625     0.6770833  0.68125    0.68125    0.68125\n",
      " 0.68125    0.68125    0.68333334 0.6979167  0.68958336 0.70416665\n",
      " 0.71458334 0.71875    0.71875    0.72083336 0.72083336 0.72083336\n",
      " 0.72083336 0.72083336 0.72083336 0.73541665 0.7395833  0.7416667\n",
      " 0.7375     0.7375     0.74375    0.74791664]\n",
      "[106. 110. 115. 113. 119. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 119.  59.  61.  62.  64.  65.  71.  75.  76.  76.  77.  79.\n",
      "  79.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.\n",
      "  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.\n",
      "  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.\n",
      "  82.  82.  82.  86.  86.  86.  86.  86.  86.  86.  87.  88.  87.  88.\n",
      "  90.  90.  90.  91.  91.  91.  92.  92.  93.  93.  93.  93.  94.  94.\n",
      "  94.  94.]\n",
      "[0.5125     0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.]\n",
      "[0.71666664 0.8375     0.86875    0.86875    0.87291664 0.83958334\n",
      " 0.8645833  0.8666667  0.87083334 0.8833333  0.83125    0.85833335\n",
      " 0.87291664 0.8666667  0.85833335 0.8666667  0.88125    0.89375\n",
      " 0.89375    0.95       0.9        0.93541664 0.94375    0.97291666\n",
      " 0.9479167  0.89166665 0.92083335 0.95       0.9604167  0.9583333\n",
      " 0.96666664 0.9291667  0.95416665 0.9791667  0.9604167  0.95\n",
      " 0.95208335 0.98541665 0.92291665 0.9791667  0.975      0.86875\n",
      " 0.95416665 0.9916667  0.9791667  1.         0.925      0.9791667\n",
      " 0.9895833  0.99791664 0.9916667  0.99375    0.9895833  0.74375\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[111. 109. 111. 112. 105. 112. 111. 112. 110. 104. 112. 107. 113. 112.\n",
      " 111. 113. 115. 113. 116. 117. 113. 111. 111. 119. 117. 116. 118. 119.\n",
      " 120. 115. 120. 118. 118. 114. 112. 120. 120.  90. 120. 120. 116. 117.\n",
      " 118. 112. 120. 120. 118. 120. 120. 120. 120. 120. 120.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.]\n",
      "[0.4875     0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.]\n",
      "[0.48333332 0.67083335 0.87083334 0.84583336 0.87916666 0.8854167\n",
      " 0.87083334 0.87916666 0.88958335 0.86875    0.87916666 0.88125\n",
      " 0.87083334 0.8979167  0.88125    0.88125    0.8854167  0.88125\n",
      " 0.88125    0.88958335 0.86875    0.87708336 0.8875     0.88125\n",
      " 0.8875     0.89166665 0.8979167  0.88958335 0.88125    0.8854167\n",
      " 0.89375    0.89375    0.89375    0.9        0.88958335 0.8958333\n",
      " 0.89166665 0.8875     0.88958335 0.88958335 0.90625    0.8979167\n",
      " 0.8979167  0.8958333  0.8958333  0.8958333  0.89166665 0.89375\n",
      " 0.8979167  0.8958333  0.8958333  0.8958333  0.88958335 0.8958333\n",
      " 0.8875     0.90625    0.89166665 0.89166665 0.89375    0.8958333\n",
      " 0.89375    0.9        0.89375    0.8958333  0.9        0.8875\n",
      " 0.9        0.88958335 0.9        0.88958335 0.89166665 0.90208334\n",
      " 0.8979167  0.89375    0.9        0.88958335 0.90208334 0.8875\n",
      " 0.89375    0.9        0.89166665 0.9        0.8979167  0.90208334\n",
      " 0.89375    0.8979167  0.8833333  0.8958333  0.90208334 0.9\n",
      " 0.8979167  0.90208334 0.9        0.8979167  0.90208334 0.9\n",
      " 0.8958333  0.8958333  0.90625    0.9       ]\n",
      "[ 58. 109. 107. 109. 111. 111. 113. 111. 111. 114. 108. 111. 114. 113.\n",
      " 114. 113. 111. 114. 114. 111. 111. 111. 112. 112. 112. 114. 111. 111.\n",
      " 113. 113. 113. 114. 114. 111. 114. 113. 112. 113. 111. 111. 111. 110.\n",
      " 113. 112. 112. 113. 112. 113. 113. 112. 112. 113. 112. 111. 114. 110.\n",
      " 112. 114. 113. 112. 112. 113. 114. 111. 113. 113. 110. 113. 112. 111.\n",
      " 114. 113. 114. 111. 112. 112. 112. 113. 111. 113. 111. 111. 113. 111.\n",
      " 113. 113. 111. 113. 113. 113. 112. 113. 113. 112. 112. 112. 113. 112.\n",
      " 113. 112.]\n",
      "[0.53125    0.7625     0.83958334 0.88125    0.89166665 0.87083334\n",
      " 0.85833335 0.90625    0.94375    0.97083336 0.89375    0.97291666\n",
      " 0.95       0.9875     0.9916667  0.99791664 0.9916667  0.8354167\n",
      " 0.96458334 0.97291666 0.9895833  0.99375    0.99375    0.99791664\n",
      " 0.99583334 0.99583334 0.99791664 0.99375    0.99791664 0.99791664\n",
      " 0.99791664 0.99375    1.         0.99583334 0.99583334 1.\n",
      " 0.99791664 1.         1.         0.99791664 0.9895833  0.99791664\n",
      " 0.99583334 0.99583334 0.9916667  1.         1.         0.99791664\n",
      " 0.99583334 0.99791664 1.         1.         1.         0.99791664\n",
      " 1.         0.99791664 1.         0.99583334 0.8541667  0.9458333\n",
      " 0.98125    0.99583334 0.99583334 1.         0.9916667  0.99791664\n",
      " 0.99583334 1.         1.         1.         1.         0.99791664\n",
      " 0.99791664 1.         0.99791664 1.         1.         0.99791664\n",
      " 0.99791664 0.99791664 1.         1.         1.         0.99791664\n",
      " 1.         1.         1.         0.99791664 0.99791664 1.\n",
      " 1.         1.         1.         0.99791664 1.         1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1.         1.         0.99791664 0.99791664]\n",
      "[106. 111. 111. 112. 108. 114. 113. 115. 118. 102. 120. 120. 120. 120.\n",
      " 120. 120. 120. 117. 118. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 119. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 114. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.]\n",
      "Number of layers: 21\n",
      "[0.5208333  0.51875    0.68125    0.73333335 0.76458335 0.81041664\n",
      " 0.82708335 0.84791666 0.86041665 0.8520833  0.80625    0.8666667\n",
      " 0.85625    0.85625    0.8625     0.85       0.8645833  0.8625\n",
      " 0.86875    0.84791666 0.8645833  0.85833335 0.86875    0.85833335\n",
      " 0.875      0.8666667  0.875      0.87916666 0.8833333  0.88125\n",
      " 0.88958335 0.88125    0.88958335 0.8854167  0.87708336 0.87916666\n",
      " 0.8833333  0.88125    0.8854167  0.875      0.87291664 0.88958335\n",
      " 0.8833333  0.87708336 0.8854167  0.88125    0.87083334 0.87708336\n",
      " 0.87291664 0.875      0.89375    0.85833335 0.87708336 0.87291664\n",
      " 0.87916666 0.87708336 0.8875     0.88125    0.8854167  0.8666667\n",
      " 0.88958335 0.88125    0.89375    0.8854167  0.87708336 0.8875\n",
      " 0.89166665 0.87083334 0.87916666 0.86875    0.875      0.87708336\n",
      " 0.88125    0.87916666 0.88958335 0.8854167  0.87708336 0.88125\n",
      " 0.8854167  0.86875    0.875      0.8875     0.8833333  0.87291664\n",
      " 0.8666667  0.86875    0.87291664 0.8833333  0.8875     0.88958335\n",
      " 0.88125    0.87916666 0.86875    0.88958335 0.8875     0.87291664\n",
      " 0.86875    0.87291664 0.88125    0.875     ]\n",
      "[ 62.  80.  92. 100. 102. 108. 108. 106. 106. 111. 111. 111. 107. 112.\n",
      " 112. 111. 105. 111. 110. 111. 112. 112. 112. 109. 111. 109. 111. 110.\n",
      " 110. 112. 110. 111. 111. 109. 111. 110. 111. 111. 112. 111. 111. 111.\n",
      " 112. 112. 112. 110. 110. 111. 111. 110. 111. 111. 110. 112. 110. 110.\n",
      " 111. 111. 112. 111. 112. 111. 112. 111. 111. 111. 110. 111. 111. 111.\n",
      " 111. 112. 112. 112. 112. 111. 112. 111. 109. 111. 111. 111. 111. 111.\n",
      " 111. 110. 111. 111. 111. 110. 108. 111. 112. 111. 112. 112. 111. 111.\n",
      " 112. 111.]\n",
      "[0.66875    0.84166664 0.8645833  0.87291664 0.8854167  0.8854167\n",
      " 0.86875    0.86875    0.86875    0.8645833  0.87291664 0.8833333\n",
      " 0.875      0.87083334 0.8854167  0.88125    0.8833333  0.8833333\n",
      " 0.87708336 0.86875    0.87291664 0.87708336 0.88125    0.89375\n",
      " 0.87083334 0.87083334 0.8854167  0.87291664 0.875      0.87708336\n",
      " 0.87916666 0.8833333  0.86875    0.87083334 0.87291664 0.88125\n",
      " 0.87083334 0.86875    0.8833333  0.89166665 0.8833333  0.89375\n",
      " 0.875      0.87083334 0.87708336 0.8645833  0.875      0.88125\n",
      " 0.87708336 0.88125    0.8854167  0.88125    0.88125    0.8854167\n",
      " 0.8833333  0.8645833  0.88958335 0.8833333  0.87708336 0.8645833\n",
      " 0.89375    0.8666667  0.88958335 0.8854167  0.8875     0.8854167\n",
      " 0.88125    0.8833333  0.8833333  0.875      0.86875    0.8875\n",
      " 0.875      0.8833333  0.87916666 0.88125    0.87916666 0.89375\n",
      " 0.87708336 0.88125    0.88125    0.8666667  0.88125    0.8833333\n",
      " 0.88125    0.8833333  0.8875     0.87916666 0.87708336 0.87291664\n",
      " 0.88125    0.87291664 0.86875    0.87708336 0.87916666 0.87916666\n",
      " 0.8854167  0.88125    0.88125    0.8875    ]\n",
      "[104. 110. 111. 112. 109. 110. 111. 111. 109. 110. 111. 111. 111. 111.\n",
      " 111. 112. 110. 110. 109. 111. 110. 111. 111. 111. 112. 111. 111. 111.\n",
      " 110. 112. 110. 110. 112. 111. 111. 111. 110. 111. 111. 111. 111. 111.\n",
      " 111. 111. 110. 111. 110. 111. 112. 112. 111. 112. 112. 111. 112. 112.\n",
      " 111. 111. 109. 111. 111. 111. 112. 111. 111. 111. 112. 111. 112. 111.\n",
      " 110. 112. 111. 112. 112. 110. 111. 112. 112. 111. 112. 111. 111. 111.\n",
      " 112. 112. 111. 111. 109. 111. 109. 111. 111. 111. 111. 111. 112. 111.\n",
      " 111. 111.]\n",
      "[0.5        0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.]\n",
      "[0.66041666 0.84166664 0.84791666 0.8625     0.88125    0.8645833\n",
      " 0.875      0.875      0.88125    0.8854167  0.87708336 0.87916666\n",
      " 0.89375    0.8875     0.87708336 0.87708336 0.875      0.88958335\n",
      " 0.87083334 0.88958335 0.89166665 0.89375    0.925      0.88958335\n",
      " 0.9270833  0.925      0.95208335 0.93125    0.9375     0.95416665\n",
      " 0.95       0.9625     0.91875    0.95416665 0.97291666 0.95625\n",
      " 0.98333335 0.95416665 0.98125    0.98541665 0.99583334 0.9770833\n",
      " 0.975      0.9916667  0.99791664 0.99375    0.9791667  0.99375\n",
      " 0.99583334 0.99375    0.9875     0.99583334 0.99583334 0.99791664\n",
      " 1.         0.99791664 0.99791664 0.99375    0.9916667  0.9895833\n",
      " 0.99583334 0.99375    0.99791664 0.99375    1.         0.99583334\n",
      " 1.         0.99583334 0.99375    0.99791664 0.99583334 0.99583334\n",
      " 0.98125    0.9375     1.         1.         0.9895833  0.99583334\n",
      " 0.99791664 0.99791664 0.99791664 0.99583334 0.99583334 0.99375\n",
      " 0.99791664 0.99791664 0.99583334 0.99791664 0.99583334 0.99375\n",
      " 0.99375    0.99791664 0.99375    0.99791664 0.99583334 0.99791664\n",
      " 0.99791664 0.99583334 0.99583334 0.99791664]\n",
      "[103.  58. 108. 111. 111. 109. 111. 112. 110. 110. 111. 112. 113. 109.\n",
      " 112. 112. 111. 112. 113. 113. 113. 115. 115. 115. 117. 118. 116. 114.\n",
      " 118. 119. 120. 118. 115. 116. 120. 120. 119. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 118. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 117. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.  96. 120. 120. 120. 120. 120. 120. 120. 120. 120. 118. 120.\n",
      " 120. 120. 120. 120. 117. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.]\n",
      "[0.6125     0.81041664 0.85625    0.87291664 0.9        0.94375\n",
      " 0.8979167  0.98125    0.98125    0.9604167  0.81041664 0.86875\n",
      " 0.87083334 0.8645833  0.8833333  0.8875     0.8875     0.8854167\n",
      " 0.90208334 0.8833333  0.8958333  0.89166665 0.87916666 0.8833333\n",
      " 0.8854167  0.88125    0.8875     0.89375    0.89375    0.87291664\n",
      " 0.89166665 0.8958333  0.8875     0.8854167  0.8833333  0.89375\n",
      " 0.89166665 0.8875     0.89166665 0.9        0.8833333  0.8625\n",
      " 0.87916666 0.8875     0.88125    0.87083334 0.88958335 0.89375\n",
      " 0.87916666 0.8833333  0.89166665 0.87708336 0.89166665 0.8958333\n",
      " 0.8979167  0.89166665 0.88958335 0.8854167  0.88958335 0.89166665\n",
      " 0.90416664 0.8958333  0.8979167  0.90208334 0.8875     0.89375\n",
      " 0.8979167  0.89166665 0.8958333  0.8979167  0.90416664 0.8979167\n",
      " 0.8979167  0.8875     0.89375    0.89166665 0.9        0.90208334\n",
      " 0.8875     0.8958333  0.9        0.8958333  0.8979167  0.8958333\n",
      " 0.89375    0.8979167  0.9        0.90208334 0.8979167  0.89375\n",
      " 0.8833333  0.90625    0.8979167  0.8979167  0.9        0.90208334\n",
      " 0.8854167  0.90208334 0.90416664 0.90416664]\n",
      "[ 62. 106. 106. 110. 117.  81. 118. 120. 120.  86. 111. 113. 112. 107.\n",
      " 111. 113. 114. 114. 109. 113. 109. 110. 113. 113. 109. 109. 114. 112.\n",
      " 112. 110. 109. 112. 111. 110. 106. 113. 112. 113. 111. 113. 114. 110.\n",
      " 112. 112. 109. 110. 111. 111. 113. 110. 114. 112. 113. 111. 112. 109.\n",
      " 113. 113. 111. 113. 114. 111. 113. 111. 111. 114. 112. 111. 113. 112.\n",
      " 111. 112. 111. 113. 113. 112. 111. 114. 111. 113. 114. 112. 113. 113.\n",
      " 111. 111. 112. 111. 111. 111. 114. 110. 111. 112. 111. 112. 110. 113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 110. 109.]\n",
      "[0.53333336 0.8125     0.86041665 0.8645833  0.87708336 0.88125\n",
      " 0.8645833  0.87083334 0.85833335 0.8833333  0.88958335 0.86875\n",
      " 0.87291664 0.86875    0.8645833  0.88125    0.875      0.87916666\n",
      " 0.875      0.875      0.8833333  0.87291664 0.875      0.87708336\n",
      " 0.88958335 0.87291664 0.8875     0.8833333  0.87916666 0.89166665\n",
      " 0.88958335 0.87291664 0.87708336 0.86875    0.875      0.87916666\n",
      " 0.8833333  0.87083334 0.88125    0.86875    0.87291664 0.8625\n",
      " 0.87083334 0.89166665 0.88125    0.8875     0.8875     0.87916666\n",
      " 0.87083334 0.8833333  0.87916666 0.89166665 0.8833333  0.87708336\n",
      " 0.8958333  0.87291664 0.875      0.8833333  0.875      0.87916666\n",
      " 0.89166665 0.86875    0.87708336 0.88125    0.87083334 0.87916666\n",
      " 0.8875     0.8854167  0.88958335 0.875      0.8875     0.87916666\n",
      " 0.88125    0.88958335 0.87916666 0.87291664 0.8833333  0.8645833\n",
      " 0.8854167  0.85625    0.87916666 0.8854167  0.87083334 0.8854167\n",
      " 0.86875    0.8854167  0.88125    0.88958335 0.88958335 0.8625\n",
      " 0.875      0.8645833  0.8833333  0.875      0.8854167  0.88958335\n",
      " 0.89375    0.8854167  0.87708336 0.8833333 ]\n",
      "[106. 111. 109. 111. 111. 112. 109. 111. 111. 111. 112. 112. 109. 111.\n",
      " 111. 111. 110. 111. 111. 111. 110. 110. 111. 110. 111. 111. 110. 111.\n",
      " 111. 108. 110. 109. 110. 110. 111. 111. 111. 111. 111. 111. 109. 111.\n",
      " 110. 110. 110. 111. 110. 111. 111. 111. 112. 111. 112. 111. 112. 111.\n",
      " 111. 111. 111. 111. 110. 111. 112. 112. 111. 111. 111. 111. 109. 111.\n",
      " 111. 112. 111. 111. 111. 112. 111. 111. 111. 111. 112. 112. 111. 112.\n",
      " 111. 111. 111. 112. 111. 112. 110. 112. 111. 111. 111. 111. 111. 111.\n",
      " 111. 112.]\n",
      "[0.60625    0.8354167  0.8541667  0.8333333  0.8520833  0.87291664\n",
      " 0.86875    0.8625     0.87291664 0.8666667  0.86041665 0.86875\n",
      " 0.8645833  0.8645833  0.8645833  0.85833335 0.85625    0.8625\n",
      " 0.84791666 0.86875    0.8666667  0.8666667  0.87916666 0.8666667\n",
      " 0.8666667  0.8520833  0.87708336 0.86041665 0.8541667  0.86875\n",
      " 0.86875    0.8625     0.85833335 0.8520833  0.85833335 0.8625\n",
      " 0.86875    0.86875    0.8625     0.875      0.86041665 0.8541667\n",
      " 0.86041665 0.86041665 0.8625     0.8645833  0.8625     0.8625\n",
      " 0.86041665 0.86041665 0.85625    0.85625    0.85833335 0.8666667\n",
      " 0.8520833  0.85833335 0.8666667  0.86875    0.8666667  0.8666667\n",
      " 0.8666667  0.86875    0.8666667  0.85625    0.8625     0.8625\n",
      " 0.87083334 0.85833335 0.8666667  0.87291664 0.8875     0.8875\n",
      " 0.9458333  0.9        0.925      0.9458333  0.94375    0.95208335\n",
      " 0.9291667  0.74791664 0.8645833  0.8666667  0.8541667  0.8645833\n",
      " 0.84791666 0.8520833  0.86041665 0.86875    0.8541667  0.86875\n",
      " 0.87916666 0.85625    0.86875    0.87708336 0.86041665 0.86875\n",
      " 0.87291664 0.8625     0.86041665 0.8666667 ]\n",
      "[112. 108. 108. 112. 112. 112. 110. 112. 111. 111. 111. 111. 112. 109.\n",
      " 110. 111. 112. 106. 111. 110. 112. 110. 110. 112. 111. 111. 111. 112.\n",
      " 111. 110. 107. 109. 108. 112. 111. 112. 112. 111. 111. 111. 111. 108.\n",
      " 112. 109. 106. 112. 111. 111. 112. 109. 106. 110. 111. 108. 109. 112.\n",
      " 112. 108. 111. 110. 110. 111. 108. 110. 110. 111. 112. 112. 112. 112.\n",
      " 111. 113. 110. 116. 115. 113. 117. 117.  58. 112. 111. 111. 110. 112.\n",
      " 111. 109. 111. 112. 112. 111. 107. 111. 109. 112. 110. 108. 110. 111.\n",
      " 110. 112.]\n",
      "[0.48333332 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.]\n",
      "[0.5208333  0.5729167  0.8020833  0.82916665 0.84583336 0.87708336\n",
      " 0.87916666 0.87291664 0.87708336 0.87916666 0.87916666 0.87083334\n",
      " 0.8854167  0.8875     0.88125    0.8875     0.88125    0.8854167\n",
      " 0.8875     0.8958333  0.875      0.87916666 0.875      0.8854167\n",
      " 0.87916666 0.86041665 0.875      0.8854167  0.88125    0.88958335\n",
      " 0.87291664 0.88958335 0.8666667  0.875      0.8875     0.88958335\n",
      " 0.88125    0.88125    0.8875     0.89375    0.8854167  0.8833333\n",
      " 0.87291664 0.8979167  0.90833336 0.8979167  0.8958333  0.8833333\n",
      " 0.88958335 0.89375    0.8854167  0.89166665 0.88958335 0.91041666\n",
      " 0.88958335 0.8854167  0.87708336 0.87708336 0.89375    0.88958335\n",
      " 0.8833333  0.89375    0.89166665 0.87916666 0.8979167  0.87083334\n",
      " 0.8833333  0.89166665 0.87916666 0.89166665 0.88125    0.87708336\n",
      " 0.89375    0.8958333  0.8854167  0.8875     0.88958335 0.90625\n",
      " 0.9        0.89166665 0.89166665 0.88958335 0.88958335 0.8854167\n",
      " 0.88125    0.88958335 0.8875     0.8875     0.8958333  0.8979167\n",
      " 0.89166665 0.8979167  0.8854167  0.8979167  0.88958335 0.87916666\n",
      " 0.8958333  0.87916666 0.88125    0.8979167 ]\n",
      "[ 92.  97. 109. 110. 111. 110. 110. 106. 113. 113. 113. 112. 107. 110.\n",
      " 113. 111. 113. 113. 109. 111. 109. 112. 111. 113. 112. 112. 112. 111.\n",
      " 111. 112. 111. 112. 111. 112. 110. 111. 110. 112. 110. 113. 113. 110.\n",
      " 111. 112. 112. 112. 112. 114. 113. 112. 113. 111. 112. 111. 113. 112.\n",
      " 114. 111. 111. 111. 112. 114. 113. 113. 114. 112. 113. 113. 113. 110.\n",
      " 113. 110. 113. 113. 112. 111. 113. 110. 109. 113. 112. 111. 111. 112.\n",
      " 113. 111. 114. 112. 113. 114. 113. 114. 114. 109. 112. 113. 114. 113.\n",
      " 111. 113.]\n",
      "[0.54583335 0.6875     0.75625    0.80625    0.85625    0.8666667\n",
      " 0.86875    0.875      0.8854167  0.875      0.8833333  0.87291664\n",
      " 0.89375    0.87916666 0.87916666 0.89166665 0.88125    0.87708336\n",
      " 0.8854167  0.8833333  0.8875     0.8666667  0.87083334 0.88958335\n",
      " 0.88125    0.8875     0.87708336 0.89166665 0.88958335 0.8833333\n",
      " 0.8958333  0.9125     0.9145833  0.8354167  0.84583336 0.8833333\n",
      " 0.8875     0.8875     0.88958335 0.89166665 0.87916666 0.8875\n",
      " 0.87708336 0.8833333  0.89375    0.88958335 0.87291664 0.8833333\n",
      " 0.875      0.87708336 0.87083334 0.8833333  0.87708336 0.88125\n",
      " 0.8875     0.87291664 0.875      0.8625     0.8854167  0.8875\n",
      " 0.8645833  0.87708336 0.88958335 0.8833333  0.87083334 0.8875\n",
      " 0.89375    0.8854167  0.87708336 0.86875    0.87708336 0.8833333\n",
      " 0.8625     0.87291664 0.875      0.875      0.88125    0.875\n",
      " 0.87916666 0.88125    0.8854167  0.87291664 0.87708336 0.8979167\n",
      " 0.87708336 0.87916666 0.89375    0.86875    0.87708336 0.88125\n",
      " 0.87916666 0.88125    0.8875     0.8666667  0.88958335 0.8875\n",
      " 0.875      0.88958335 0.88125    0.87916666]\n",
      "[ 79.  99. 105. 107. 109. 110. 112. 112. 112. 110. 112. 110. 112. 112.\n",
      " 112. 111. 111. 112. 111. 111. 111. 111. 111. 111. 111. 112. 112. 111.\n",
      " 112. 112. 113. 113. 116. 107. 109. 111. 111. 110. 111. 111. 111. 112.\n",
      " 112. 111. 112. 112. 109. 111. 112. 111. 111. 111. 111. 110. 112. 111.\n",
      " 111. 111. 111. 111. 112. 112. 111. 112. 111. 112. 111. 111. 111. 111.\n",
      " 112. 111. 110. 110. 111. 111. 111. 111. 110. 112. 111. 111. 112. 109.\n",
      " 111. 111. 111. 111. 111. 111. 111. 111. 109. 111. 112. 111. 111. 111.\n",
      " 111. 112.]\n"
     ]
    }
   ],
   "source": [
    "from Networks.ResNet import ResAntiSymNet\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "num_epochs = 100\n",
    "num_features = 2\n",
    "num_classes = 2\n",
    "size_hidden = 8\n",
    "gamma = 0.3\n",
    "h = 1\n",
    "lr = 0.5\n",
    "\n",
    "max_layers = 20\n",
    "train_results_per_layer = np.zeros(max_layers)\n",
    "test_results_per_layer = np.zeros(max_layers)\n",
    "basic_save_key = 'AntiSym_Comparison_results//AntiSymNet'\n",
    "\n",
    "for num_hidden_layers in range(15,max_layers+1):\n",
    "    tic = timeit.default_timer()\n",
    "    num_layers = num_hidden_layers + 1\n",
    "    print('Number of layers: '+str(num_layers))\n",
    "    train_results_per_epoch = np.zeros(num_epochs)\n",
    "    test_results_per_epoch = np.zeros(num_epochs)\n",
    "    for i in range(10):\n",
    "        net = ResAntiSymNet(features=num_features, classes=num_classes, num_layers=num_layers, gamma=gamma, h=h, bias=True, hidden_size=size_hidden)\n",
    "        net.set_test_tracking(True)\n",
    "        net.train(num_epochs, train_loader, test_loader, print_output=False)\n",
    "        print(net.avg_correct_pred.numpy())\n",
    "        print(net.test_results.numpy())\n",
    "        del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x215af6b9240>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from Networks.ResNet import ResAntiSymNet\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "num_epochs = 100\n",
    "num_features = 2\n",
    "num_classes = 2\n",
    "size_hidden = 8\n",
    "gamma = 0.3\n",
    "h = 0.1\n",
    "lr = 0.5\n",
    "test_set_size = 120\n",
    "\n",
    "\n",
    "num_layers = 50\n",
    "basic_save_key = 'AntiSym_Comparison_results//AntiSymNet'\n",
    "\n",
    "train_save_key = basic_save_key+'_train_per_epoch_layers_'+str(num_layers)+'_h_'+str(h)+'_lr_'+str(lr)+'.csv'\n",
    "test_save_key = basic_save_key+'_test_per_epoch_layers_'+str(num_layers)+'_h_'+str(h)+'_lr_'+str(lr)+'.csv'\n",
    "\n",
    "train_results_per_epoch = np.loadtxt(train_save_key)\n",
    "test_results_per_epoch = np.loadtxt(test_save_key)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "epochs = np.arange(1,101,1)\n",
    "ax.plot(epochs, train_results_per_epoch, label='train')\n",
    "ax.plot(epochs, test_results_per_epoch/test_set_size, label='test')\n",
    "ax.set_ylim(0,1.1)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Average Accuracy')\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "basic_save_key = 'Convolutional_results//ConvMSANet//'\n",
    "\n",
    "test_set_size = 10000\n",
    "train_save_key = 'ConvMSANettrain'\n",
    "test_save_key = 'ConvMSANettest'\n",
    "\n",
    "train_results_per_epoch = np.loadtxt(basic_save_key+train_save_key+'.csv')\n",
    "test_results_per_epoch = np.loadtxt(basic_save_key+test_save_key+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "layers = np.arange(1,11,1)\n",
    "ax.plot(layers, train_results_per_epoch, label='train')\n",
    "ax.plot(layers, test_results_per_epoch/test_set_size, label='test')\n",
    "ax.set_ylim(0,1.1)\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Average Accuracy')\n",
    "ax.legend(loc='best')\n",
    "plt.savefig(\"Plots/\"+train_save_key+\".pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
