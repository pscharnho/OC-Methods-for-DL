{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.732142  #  0.529167  #\n",
      "#  2  #  0.708847  #  0.508333  #\n",
      "#  3  #  0.696802  #  0.587500  #\n",
      "#  4  #  0.676409  #  0.593750  #\n",
      "#  5  #  0.680870  #  0.585417  #\n",
      "#  6  #  0.628762  #  0.643750  #\n",
      "#  7  #  0.628904  #  0.681250  #\n",
      "#  8  #  0.643702  #  0.668750  #\n",
      "#  9  #  0.633571  #  0.625000  #\n",
      "#  10  #  0.628553  #  0.708333  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  11  #  0.646496  #  0.658333  #\n",
      "#  12  #  0.653374  #  0.639583  #\n",
      "#  13  #  0.640968  #  0.662500  #\n",
      "#  14  #  0.625854  #  0.697917  #\n",
      "#  15  #  0.631267  #  0.672917  #\n",
      "#  16  #  0.635552  #  0.683333  #\n",
      "#  17  #  0.608842  #  0.716667  #\n",
      "#  18  #  0.631524  #  0.689583  #\n",
      "#  19  #  0.609664  #  0.725000  #\n",
      "#  20  #  0.583097  #  0.783333  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  21  #  0.595362  #  0.739583  #\n",
      "#  22  #  0.587086  #  0.756250  #\n",
      "#  23  #  0.582291  #  0.764583  #\n",
      "#  24  #  0.601011  #  0.722917  #\n",
      "#  25  #  0.595249  #  0.731250  #\n",
      "#  26  #  0.591052  #  0.733333  #\n",
      "#  27  #  0.614392  #  0.718750  #\n",
      "#  28  #  0.610011  #  0.727083  #\n",
      "#  29  #  0.598729  #  0.735417  #\n",
      "#  30  #  0.606819  #  0.731250  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  31  #  0.611400  #  0.704167  #\n",
      "#  32  #  0.585835  #  0.766667  #\n",
      "#  33  #  0.596569  #  0.739583  #\n",
      "#  34  #  0.601993  #  0.733333  #\n",
      "#  35  #  0.606648  #  0.737500  #\n",
      "#  36  #  0.596208  #  0.743750  #\n",
      "#  37  #  0.605188  #  0.722917  #\n",
      "#  38  #  0.602758  #  0.722917  #\n",
      "#  39  #  0.584131  #  0.764583  #\n",
      "#  40  #  0.595232  #  0.743750  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  41  #  0.608973  #  0.712500  #\n",
      "#  42  #  0.619266  #  0.702083  #\n",
      "#  43  #  0.614502  #  0.700000  #\n",
      "#  44  #  0.618406  #  0.708333  #\n",
      "#  45  #  0.612661  #  0.720833  #\n",
      "#  46  #  0.606037  #  0.720833  #\n",
      "#  47  #  0.589780  #  0.733333  #\n",
      "#  48  #  0.605248  #  0.727083  #\n",
      "#  49  #  0.595805  #  0.752083  #\n",
      "#  50  #  0.592441  #  0.750000  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  51  #  0.591331  #  0.754167  #\n",
      "#  52  #  0.587033  #  0.756250  #\n",
      "#  53  #  0.577223  #  0.764583  #\n",
      "#  54  #  0.584600  #  0.758333  #\n",
      "#  55  #  0.592844  #  0.754167  #\n",
      "#  56  #  0.609264  #  0.735417  #\n",
      "#  57  #  0.609112  #  0.722917  #\n",
      "#  58  #  0.589049  #  0.758333  #\n",
      "#  59  #  0.588335  #  0.760417  #\n",
      "#  60  #  0.593482  #  0.743750  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  61  #  0.598648  #  0.733333  #\n",
      "#  62  #  0.591604  #  0.735417  #\n",
      "#  63  #  0.586666  #  0.754167  #\n",
      "#  64  #  0.593258  #  0.741667  #\n",
      "#  65  #  0.587862  #  0.741667  #\n",
      "#  66  #  0.589941  #  0.752083  #\n",
      "#  67  #  0.602658  #  0.725000  #\n",
      "#  68  #  0.588006  #  0.752083  #\n",
      "#  69  #  0.585346  #  0.750000  #\n",
      "#  70  #  0.582606  #  0.750000  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  71  #  0.580665  #  0.739583  #\n",
      "#  72  #  0.587616  #  0.743750  #\n",
      "#  73  #  0.584899  #  0.750000  #\n",
      "#  74  #  0.594439  #  0.737500  #\n",
      "#  75  #  0.579451  #  0.752083  #\n",
      "#  76  #  0.584045  #  0.747917  #\n",
      "#  77  #  0.582626  #  0.747917  #\n",
      "#  78  #  0.590679  #  0.745833  #\n",
      "#  79  #  0.590071  #  0.739583  #\n",
      "#  80  #  0.588210  #  0.747917  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  81  #  0.582745  #  0.766667  #\n",
      "#  82  #  0.583351  #  0.750000  #\n",
      "#  83  #  0.594585  #  0.745833  #\n",
      "#  84  #  0.584197  #  0.733333  #\n",
      "#  85  #  0.574833  #  0.756250  #\n",
      "#  86  #  0.589866  #  0.731250  #\n",
      "#  87  #  0.581095  #  0.766667  #\n",
      "#  88  #  0.587169  #  0.743750  #\n",
      "#  89  #  0.586631  #  0.750000  #\n",
      "#  90  #  0.597392  #  0.735417  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  91  #  0.584671  #  0.747917  #\n",
      "#  92  #  0.568630  #  0.783333  #\n",
      "#  93  #  0.580178  #  0.752083  #\n",
      "#  94  #  0.598290  #  0.731250  #\n",
      "#  95  #  0.596279  #  0.710417  #\n",
      "#  96  #  0.588656  #  0.733333  #\n",
      "#  97  #  0.564699  #  0.770833  #\n",
      "#  98  #  0.570371  #  0.756250  #\n",
      "#  99  #  0.580683  #  0.754167  #\n",
      "#  100  #  0.580783  #  0.735417  #\n",
      "Time elapsed:  190.18332964899992\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import FCMSANet\n",
    "import torch\n",
    "\n",
    "\n",
    "def make_uniform_layer_list(layers, num_features):\n",
    "    return [num_features] * (layers+1)\n",
    "    \n",
    "def make_uniform_hidden_layer_list(layers, num_features, num_classes, size_hidden):\n",
    "    res = [num_features]\n",
    "    res.extend([size_hidden]*(layers-1))\n",
    "    res.extend([num_classes])\n",
    "    return res\n",
    "\n",
    "\n",
    "dataset_size = 600\n",
    "batch_size = 40\n",
    "test_set_size = dataset_size * 0.2\n",
    "\n",
    "\n",
    "#torch.manual_seed(0)\n",
    "train_loader, test_loader = makeMoonsDataset(dataset_size, batch_size)\n",
    "#torch.manual_seed(3)\n",
    "\n",
    "num_layers = 60\n",
    "#layers = make_uniform_layer_list(num_layers, 2)\n",
    "layers = make_uniform_hidden_layer_list(num_layers, 2, 2, 32)\n",
    "#print(layers)\n",
    "net = FCMSANet(num_fc=num_layers, sizes_fc=layers, bias=True, batchnorm=True, test=False)   \n",
    "#net = FCMSANet(num_fc=25,sizes_fc=[2,8,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,2], bias=False, batchnorm=True, test=False)  \n",
    "net.set_rho(0.5)\n",
    "net.set_ema_alpha(0.99)\n",
    "net.train_msa(100,train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.017179  #  0.602083  #\n",
      "#  2  #  0.016375  #  0.570833  #\n",
      "#  3  #  0.016177  #  0.600000  #\n",
      "#  4  #  0.016091  #  0.581250  #\n",
      "#  5  #  0.015821  #  0.597917  #\n",
      "#  6  #  0.015631  #  0.604167  #\n",
      "#  7  #  0.015214  #  0.633333  #\n",
      "#  8  #  0.014879  #  0.643750  #\n",
      "#  9  #  0.014284  #  0.681250  #\n",
      "#  10  #  0.013627  #  0.691667  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  11  #  0.013143  #  0.697917  #\n",
      "#  12  #  0.012654  #  0.718750  #\n",
      "#  13  #  0.012137  #  0.743750  #\n",
      "#  14  #  0.011413  #  0.766667  #\n",
      "#  15  #  0.010518  #  0.789583  #\n",
      "#  16  #  0.009862  #  0.814583  #\n",
      "#  17  #  0.009275  #  0.833333  #\n",
      "#  18  #  0.008768  #  0.841667  #\n",
      "#  19  #  0.008331  #  0.854167  #\n",
      "#  20  #  0.007912  #  0.862500  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  21  #  0.007587  #  0.879167  #\n",
      "#  22  #  0.007282  #  0.881250  #\n",
      "#  23  #  0.006972  #  0.891667  #\n",
      "#  24  #  0.006641  #  0.895833  #\n",
      "#  25  #  0.006390  #  0.897917  #\n",
      "#  26  #  0.006167  #  0.902083  #\n",
      "#  27  #  0.005939  #  0.914583  #\n",
      "#  28  #  0.005666  #  0.912500  #\n",
      "#  29  #  0.005483  #  0.920833  #\n",
      "#  30  #  0.005334  #  0.927083  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  31  #  0.005086  #  0.925000  #\n",
      "#  32  #  0.004922  #  0.933333  #\n",
      "#  33  #  0.004807  #  0.931250  #\n",
      "#  34  #  0.004630  #  0.937500  #\n",
      "#  35  #  0.004484  #  0.939583  #\n",
      "#  36  #  0.004364  #  0.935417  #\n",
      "#  37  #  0.004205  #  0.939583  #\n",
      "#  38  #  0.004123  #  0.939583  #\n",
      "#  39  #  0.003986  #  0.945833  #\n",
      "#  40  #  0.003871  #  0.945833  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  41  #  0.003784  #  0.947917  #\n",
      "#  42  #  0.003697  #  0.950000  #\n",
      "#  43  #  0.003584  #  0.952083  #\n",
      "#  44  #  0.003509  #  0.952083  #\n",
      "#  45  #  0.003429  #  0.956250  #\n",
      "#  46  #  0.003369  #  0.958333  #\n",
      "#  47  #  0.003311  #  0.958333  #\n",
      "#  48  #  0.003226  #  0.956250  #\n",
      "#  49  #  0.003188  #  0.956250  #\n",
      "#  50  #  0.003075  #  0.958333  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  51  #  0.003083  #  0.962500  #\n",
      "#  52  #  0.003002  #  0.958333  #\n",
      "#  53  #  0.002976  #  0.958333  #\n",
      "#  54  #  0.002894  #  0.958333  #\n",
      "#  55  #  0.002858  #  0.962500  #\n",
      "#  56  #  0.002795  #  0.960417  #\n",
      "#  57  #  0.002774  #  0.968750  #\n",
      "#  58  #  0.002716  #  0.964583  #\n",
      "#  59  #  0.002686  #  0.964583  #\n",
      "#  60  #  0.002625  #  0.972917  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  61  #  0.002524  #  0.970833  #\n",
      "#  62  #  0.002475  #  0.975000  #\n",
      "#  63  #  0.002512  #  0.972917  #\n",
      "#  64  #  0.002462  #  0.972917  #\n",
      "#  65  #  0.002374  #  0.972917  #\n",
      "#  66  #  0.002312  #  0.975000  #\n",
      "#  67  #  0.002310  #  0.972917  #\n",
      "#  68  #  0.002298  #  0.970833  #\n",
      "#  69  #  0.002220  #  0.979167  #\n",
      "#  70  #  0.002159  #  0.979167  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  71  #  0.002200  #  0.979167  #\n",
      "#  72  #  0.002137  #  0.979167  #\n",
      "#  73  #  0.002067  #  0.981250  #\n",
      "#  74  #  0.002131  #  0.975000  #\n",
      "#  75  #  0.002108  #  0.977083  #\n",
      "#  76  #  0.002077  #  0.977083  #\n",
      "#  77  #  0.002040  #  0.979167  #\n",
      "#  78  #  0.001999  #  0.979167  #\n",
      "#  79  #  0.001973  #  0.981250  #\n",
      "#  80  #  0.001983  #  0.977083  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  81  #  0.001919  #  0.975000  #\n",
      "#  82  #  0.001909  #  0.981250  #\n",
      "#  83  #  0.001887  #  0.983333  #\n",
      "#  84  #  0.001850  #  0.983333  #\n",
      "#  85  #  0.001848  #  0.983333  #\n",
      "#  86  #  0.001832  #  0.979167  #\n",
      "#  87  #  0.001793  #  0.981250  #\n",
      "#  88  #  0.001795  #  0.981250  #\n",
      "#  89  #  0.001754  #  0.981250  #\n",
      "#  90  #  0.001796  #  0.983333  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  91  #  0.001773  #  0.985417  #\n",
      "#  92  #  0.001749  #  0.981250  #\n",
      "#  93  #  0.001730  #  0.981250  #\n",
      "#  94  #  0.001693  #  0.983333  #\n",
      "#  95  #  0.001697  #  0.983333  #\n",
      "#  96  #  0.001711  #  0.983333  #\n",
      "#  97  #  0.001737  #  0.985417  #\n",
      "#  98  #  0.001635  #  0.987500  #\n",
      "#  99  #  0.001789  #  0.985417  #\n",
      "#  100  #  0.001664  #  0.985417  #\n",
      "Time elapsed:  4.823962247000054\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import ResAntiSymNet\n",
    "import torch\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset(600,40)\n",
    "gamma = 0.3\n",
    "h = 1\n",
    "net = ResAntiSymNet(features=2, classes=2, num_layers=10, gamma=gamma, h=h, bias=True, hidden_size=2)\n",
    "net.train(train_loader, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.017350  #  0.489583  #\n",
      "#  2  #  0.017379  #  0.497917  #\n",
      "#  3  #  0.017338  #  0.485417  #\n",
      "#  4  #  0.017331  #  0.510417  #\n",
      "#  5  #  0.017339  #  0.489583  #\n",
      "#  6  #  0.017356  #  0.510417  #\n",
      "#  7  #  0.017340  #  0.489583  #\n",
      "#  8  #  0.017343  #  0.502083  #\n",
      "#  9  #  0.017357  #  0.510417  #\n",
      "#  10  #  0.017334  #  0.510417  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  11  #  0.017342  #  0.485417  #\n",
      "#  12  #  0.017345  #  0.510417  #\n",
      "#  13  #  0.017339  #  0.510417  #\n",
      "#  14  #  0.017353  #  0.489583  #\n",
      "#  15  #  0.017360  #  0.464583  #\n",
      "#  16  #  0.017340  #  0.489583  #\n",
      "#  17  #  0.017350  #  0.510417  #\n",
      "#  18  #  0.017344  #  0.472917  #\n",
      "#  19  #  0.017347  #  0.510417  #\n",
      "#  20  #  0.017340  #  0.510417  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  21  #  0.017348  #  0.502083  #\n",
      "#  22  #  0.017337  #  0.510417  #\n",
      "#  23  #  0.017354  #  0.510417  #\n",
      "#  24  #  0.017344  #  0.485417  #\n",
      "#  25  #  0.017345  #  0.510417  #\n",
      "#  26  #  0.017346  #  0.506250  #\n",
      "#  27  #  0.017343  #  0.510417  #\n",
      "#  28  #  0.017343  #  0.510417  #\n",
      "#  29  #  0.017359  #  0.502083  #\n",
      "#  30  #  0.017368  #  0.485417  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  31  #  0.017364  #  0.493750  #\n",
      "#  32  #  0.017352  #  0.510417  #\n",
      "#  33  #  0.017333  #  0.510417  #\n",
      "#  34  #  0.017333  #  0.510417  #\n",
      "#  35  #  0.017355  #  0.510417  #\n",
      "#  36  #  0.017346  #  0.481250  #\n",
      "#  37  #  0.017343  #  0.510417  #\n",
      "#  38  #  0.017342  #  0.468750  #\n",
      "#  39  #  0.017344  #  0.510417  #\n",
      "#  40  #  0.017359  #  0.510417  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  41  #  0.017345  #  0.510417  #\n",
      "#  42  #  0.017339  #  0.510417  #\n",
      "#  43  #  0.017342  #  0.510417  #\n",
      "#  44  #  0.017336  #  0.510417  #\n",
      "#  45  #  0.017347  #  0.485417  #\n",
      "#  46  #  0.017342  #  0.497917  #\n",
      "#  47  #  0.017344  #  0.497917  #\n",
      "#  48  #  0.017346  #  0.516667  #\n",
      "#  49  #  0.017338  #  0.510417  #\n",
      "#  50  #  0.017342  #  0.510417  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  51  #  0.017350  #  0.485417  #\n",
      "#  52  #  0.017336  #  0.510417  #\n",
      "#  53  #  0.017338  #  0.477083  #\n",
      "#  54  #  0.017363  #  0.510417  #\n",
      "#  55  #  0.017335  #  0.510417  #\n",
      "#  56  #  0.017334  #  0.497917  #\n",
      "#  57  #  0.017336  #  0.510417  #\n",
      "#  58  #  0.017338  #  0.506250  #\n",
      "#  59  #  0.017335  #  0.510417  #\n",
      "#  60  #  0.017337  #  0.510417  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  61  #  0.017334  #  0.510417  #\n",
      "#  62  #  0.017338  #  0.506250  #\n",
      "#  63  #  0.017332  #  0.497917  #\n",
      "#  64  #  0.017353  #  0.510417  #\n",
      "#  65  #  0.017345  #  0.472917  #\n",
      "#  66  #  0.017345  #  0.510417  #\n",
      "#  67  #  0.017334  #  0.510417  #\n",
      "#  68  #  0.017361  #  0.493750  #\n",
      "#  69  #  0.017331  #  0.510417  #\n",
      "#  70  #  0.017348  #  0.493750  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  71  #  0.017351  #  0.497917  #\n",
      "#  72  #  0.017342  #  0.506250  #\n",
      "#  73  #  0.017366  #  0.489583  #\n",
      "#  74  #  0.017342  #  0.510417  #\n",
      "#  75  #  0.017336  #  0.510417  #\n",
      "#  76  #  0.017340  #  0.510417  #\n",
      "#  77  #  0.017339  #  0.497917  #\n",
      "#  78  #  0.017334  #  0.506250  #\n",
      "#  79  #  0.017340  #  0.493750  #\n",
      "#  80  #  0.017343  #  0.510417  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  81  #  0.017339  #  0.510417  #\n",
      "#  82  #  0.017343  #  0.485417  #\n",
      "#  83  #  0.017348  #  0.510417  #\n",
      "#  84  #  0.017353  #  0.502083  #\n",
      "#  85  #  0.017345  #  0.502083  #\n",
      "#  86  #  0.017358  #  0.472917  #\n",
      "#  87  #  0.017340  #  0.510417  #\n",
      "#  88  #  0.017341  #  0.477083  #\n",
      "#  89  #  0.017340  #  0.510417  #\n",
      "#  90  #  0.017338  #  0.481250  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  91  #  0.017346  #  0.510417  #\n",
      "#  92  #  0.017359  #  0.489583  #\n",
      "#  93  #  0.017335  #  0.493750  #\n",
      "#  94  #  0.017341  #  0.510417  #\n",
      "#  95  #  0.017351  #  0.489583  #\n",
      "#  96  #  0.017341  #  0.493750  #\n",
      "#  97  #  0.017342  #  0.522917  #\n",
      "#  98  #  0.017353  #  0.502083  #\n",
      "#  99  #  0.017342  #  0.510417  #\n",
      "#  100  #  0.017339  #  0.510417  #\n",
      "Time elapsed:  3.5178064170000027\n",
      "Correct predictions: 0.4583333333333333\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import FCNet\n",
    "import torch\n",
    "\n",
    "dataset_size = 600\n",
    "batch_size = 40\n",
    "test_set_size = dataset_size * 0.2\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset(dataset_size,batch_size)\n",
    "\n",
    "net = FCNet(num_layers=10, layers=[2,4,8,8,8,8,8,8,8,8,2], bias=True)\n",
    "net.train(train_loader, 100)\n",
    "\n",
    "net.test(test_loader, test_set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  5.947391  #  0.510417  #\n",
      "#  2  #  0.418802  #  0.620833  #\n",
      "#  3  #  0.045253  #  0.787500  #\n",
      "#  4  #  0.031671  #  0.837500  #\n",
      "#  5  #  0.039474  #  0.822917  #\n",
      "#  6  #  0.028580  #  0.839583  #\n",
      "#  7  #  0.887916  #  0.731250  #\n",
      "#  8  #  0.092333  #  0.791667  #\n",
      "#  9  #  0.081724  #  0.843750  #\n",
      "#  10  #  0.073902  #  0.845833  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  11  #  0.067656  #  0.837500  #\n",
      "#  12  #  0.061948  #  0.843750  #\n",
      "#  13  #  0.054905  #  0.843750  #\n",
      "#  14  #  0.059348  #  0.850000  #\n",
      "#  15  #  0.067474  #  0.831250  #\n",
      "#  16  #  0.058724  #  0.825000  #\n",
      "#  17  #  0.053314  #  0.770833  #\n",
      "#  18  #  0.047713  #  0.812500  #\n",
      "#  19  #  0.043622  #  0.800000  #\n",
      "#  20  #  0.040249  #  0.787500  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  21  #  0.037845  #  0.779167  #\n",
      "#  22  #  0.035598  #  0.777083  #\n",
      "#  23  #  0.033933  #  0.789583  #\n",
      "#  24  #  0.032295  #  0.785417  #\n",
      "#  25  #  0.030474  #  0.768750  #\n",
      "#  26  #  0.028449  #  0.764583  #\n",
      "#  27  #  0.027521  #  0.768750  #\n",
      "#  28  #  0.026605  #  0.785417  #\n",
      "#  29  #  0.025311  #  0.822917  #\n",
      "#  30  #  0.025094  #  0.785417  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  31  #  0.024118  #  0.814583  #\n",
      "#  32  #  0.023716  #  0.802083  #\n",
      "#  33  #  0.023024  #  0.814583  #\n",
      "#  34  #  0.022484  #  0.812500  #\n",
      "#  35  #  0.022230  #  0.820833  #\n",
      "#  36  #  0.021449  #  0.837500  #\n",
      "#  37  #  0.021444  #  0.791667  #\n",
      "#  38  #  0.020775  #  0.843750  #\n",
      "#  39  #  0.020483  #  0.825000  #\n",
      "#  40  #  0.019767  #  0.837500  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  41  #  0.019540  #  0.841667  #\n",
      "#  42  #  0.019483  #  0.833333  #\n",
      "#  43  #  0.019027  #  0.843750  #\n",
      "#  44  #  0.018847  #  0.843750  #\n",
      "#  45  #  0.018770  #  0.835417  #\n",
      "#  46  #  0.018048  #  0.837500  #\n",
      "#  47  #  0.017967  #  0.854167  #\n",
      "#  48  #  0.023551  #  0.793750  #\n",
      "#  49  #  0.023666  #  0.814583  #\n",
      "#  50  #  0.020542  #  0.845833  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  51  #  0.019236  #  0.843750  #\n",
      "#  52  #  0.018274  #  0.845833  #\n",
      "#  53  #  0.017650  #  0.843750  #\n",
      "#  54  #  0.016963  #  0.858333  #\n",
      "#  55  #  0.017019  #  0.843750  #\n",
      "#  56  #  0.016453  #  0.850000  #\n",
      "#  57  #  0.016382  #  0.843750  #\n",
      "#  58  #  0.015981  #  0.856250  #\n",
      "#  59  #  0.015719  #  0.856250  #\n",
      "#  60  #  0.015404  #  0.860417  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  61  #  0.015118  #  0.866667  #\n",
      "#  62  #  0.014963  #  0.847917  #\n",
      "#  63  #  0.014542  #  0.852083  #\n",
      "#  64  #  0.015147  #  0.837500  #\n",
      "#  65  #  0.014425  #  0.854167  #\n",
      "#  66  #  0.014342  #  0.854167  #\n",
      "#  67  #  0.014420  #  0.856250  #\n",
      "#  68  #  0.014059  #  0.854167  #\n",
      "#  69  #  0.013553  #  0.862500  #\n",
      "#  70  #  0.011635  #  0.862500  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  71  #  0.011854  #  0.866667  #\n",
      "#  72  #  0.011359  #  0.862500  #\n",
      "#  73  #  0.011397  #  0.868750  #\n",
      "#  74  #  0.011258  #  0.872917  #\n",
      "#  75  #  0.011236  #  0.864583  #\n",
      "#  76  #  0.011139  #  0.868750  #\n",
      "#  77  #  0.011371  #  0.858333  #\n",
      "#  78  #  0.011168  #  0.858333  #\n",
      "#  79  #  0.011033  #  0.864583  #\n",
      "#  80  #  0.010787  #  0.872917  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  81  #  0.011013  #  0.870833  #\n",
      "#  82  #  0.011075  #  0.875000  #\n",
      "#  83  #  0.011249  #  0.860417  #\n",
      "#  84  #  0.011161  #  0.860417  #\n",
      "#  85  #  0.010735  #  0.875000  #\n",
      "#  86  #  0.011047  #  0.872917  #\n",
      "#  87  #  0.010808  #  0.872917  #\n",
      "#  88  #  0.010778  #  0.868750  #\n",
      "#  89  #  0.010695  #  0.870833  #\n",
      "#  90  #  0.010749  #  0.872917  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  91  #  0.010695  #  0.875000  #\n",
      "#  92  #  0.010567  #  0.875000  #\n",
      "#  93  #  0.010962  #  0.872917  #\n",
      "#  94  #  0.010529  #  0.866667  #\n",
      "#  95  #  0.010477  #  0.879167  #\n",
      "#  96  #  0.010454  #  0.868750  #\n",
      "#  97  #  0.010844  #  0.866667  #\n",
      "#  98  #  0.010432  #  0.870833  #\n",
      "#  99  #  0.010364  #  0.872917  #\n",
      "#  100  #  0.010349  #  0.883333  #\n",
      "Time elapsed:  4.947027466000009\n",
      "Correct predictions: 0.875\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import ResFCNet\n",
    "import torch\n",
    "\n",
    "def make_uniform_layer_list(layers, num_features):\n",
    "        return [num_features] * (layers+1)\n",
    "\n",
    "def make_uniform_hidden_layer_list(layers, num_features, num_classes, size_hidden):\n",
    "    res = [num_features]\n",
    "    res.extend([size_hidden]*(layers-1))\n",
    "    res.extend([num_classes])\n",
    "    return res\n",
    "\n",
    "\n",
    "dataset_size = 600\n",
    "batch_size = 40\n",
    "test_set_size = dataset_size * 0.2\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset(dataset_size,batch_size)\n",
    "\n",
    "num_layers = 20\n",
    "#layers = make_uniform_layer_list(num_layers, 2)\n",
    "layers = make_uniform_hidden_layer_list(num_layers, 2, 2, 2)\n",
    "\n",
    "net = ResFCNet(num_layers=num_layers, layers=layers, bias=True)\n",
    "\n",
    "#net = ResFCNet(num_layers=30, layers=[2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2], bias=True)\n",
    "net.train(train_loader, 100)\n",
    "\n",
    "net.test(test_loader, test_set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:340: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:343: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  compared_weight_signs = torch.tensor(torch.ne(new_weight_signs, old_weight_signs).detach(), dtype=torch.float32)\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:148: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_weights = new_weight_signs*torch.tensor(torch.ge(torch.abs(self.m_accumulated),self.rho*max_weight_elem*torch.ones_like(self.m_accumulated)).detach(),dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed:  10.146643088\n",
      "Seed 0   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.60604992\n",
      "Seed 1   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.699145381999998\n",
      "Seed 2   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.273395311000002\n",
      "Seed 3   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.354167834000002\n",
      "Seed 4   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.854717102000002\n",
      "Seed 5   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.360114429000006\n",
      "Seed 6   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.212407759000001\n",
      "Seed 7   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.896629595000007\n",
      "Seed 8   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.603672412999998\n",
      "Seed 9   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.097800613000004\n",
      "Seed 10   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.248547408000007\n",
      "Seed 11   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.193569678999978\n",
      "Seed 12   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.238679597000015\n",
      "Seed 13   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.202587755999986\n",
      "Seed 14   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.563680431999984\n",
      "Seed 15   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.800362409000002\n",
      "Seed 16   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.172943199000002\n",
      "Seed 17   ###   Best-Avg 0.8\n",
      "Time elapsed:  11.664164929999998\n",
      "Seed 18   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.64493668099999\n",
      "Seed 19   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.808216150000021\n",
      "Seed 20   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.073397882000023\n",
      "Seed 21   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.22652783800001\n",
      "Seed 22   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.820626478999998\n",
      "Seed 23   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.476716109999984\n",
      "Seed 24   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.439701024999977\n",
      "Seed 25   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.500485010999967\n",
      "Seed 26   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.450659146999953\n",
      "Seed 27   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.663561942000001\n",
      "Seed 28   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.897770284000046\n",
      "Seed 29   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.072501369000008\n",
      "Seed 30   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.462130296999987\n",
      "Seed 31   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.181674433000012\n",
      "Seed 32   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.720457097999997\n",
      "Seed 33   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.911982378000005\n",
      "Seed 34   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.12446210600001\n",
      "Seed 35   ###   Best-Avg 0.8\n",
      "Time elapsed:  11.751844816999949\n",
      "Seed 36   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.914494053999988\n",
      "Seed 37   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.721946830999968\n",
      "Seed 38   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.854742291999969\n",
      "Seed 39   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.348190395000017\n",
      "Seed 40   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.857938689000036\n",
      "Seed 41   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.154126195999993\n",
      "Seed 42   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.182050207999964\n",
      "Seed 43   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.263982440000007\n",
      "Seed 44   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.456949645000066\n",
      "Seed 45   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.78960939500007\n",
      "Seed 46   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.985499520000076\n",
      "Seed 47   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.699192676000052\n",
      "Seed 48   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.425626183999952\n",
      "Seed 49   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.54046722499993\n",
      "Seed 50   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.030507655000065\n",
      "Seed 51   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.125098506000086\n",
      "Seed 52   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.919794993999972\n",
      "Seed 53   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.273062202999995\n",
      "Seed 54   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.648992066000005\n",
      "Seed 55   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.420492824999997\n",
      "Seed 56   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.824312771999985\n",
      "Seed 57   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.768213887999991\n",
      "Seed 58   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.478139016\n",
      "Seed 59   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.264060061999999\n",
      "Seed 60   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.428517747\n",
      "Seed 61   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.804717488000051\n",
      "Seed 62   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.830327735999958\n",
      "Seed 63   ###   Best-Avg 0.84375\n",
      "Time elapsed:  13.149567551000018\n",
      "Seed 64   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.077774036999926\n",
      "Seed 65   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.304072090999966\n",
      "Seed 66   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.241903238999953\n",
      "Seed 67   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.745793354000057\n",
      "Seed 68   ###   Best-Avg 0.84375\n",
      "Time elapsed:  13.113821154999982\n",
      "Seed 69   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.56083924699999\n",
      "Seed 70   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.023275922000039\n",
      "Seed 71   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.34733552099999\n",
      "Seed 72   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.205158547999986\n",
      "Seed 73   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.781024666000008\n",
      "Seed 74   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.818582595000066\n",
      "Seed 75   ###   Best-Avg 0.8\n",
      "Time elapsed:  12.775833219999981\n",
      "Seed 76   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.596671489999949\n",
      "Seed 77   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.738887018000014\n",
      "Seed 78   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.079107496999995\n",
      "Seed 79   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.085618524999973\n",
      "Seed 80   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.447313674000043\n",
      "Seed 81   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.547355056000015\n",
      "Seed 82   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.684601208999993\n",
      "Seed 83   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.680478483000002\n",
      "Seed 84   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.97397130999991\n",
      "Seed 85   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.147157143999948\n",
      "Seed 86   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.294491636999965\n",
      "Seed 87   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.387910954999938\n",
      "Seed 88   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.786750217999952\n",
      "Seed 89   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.385938010000018\n",
      "Seed 90   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.857001564999791\n",
      "Seed 91   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.818907478000028\n",
      "Seed 92   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.13523568200003\n",
      "Seed 93   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.777255096999852\n",
      "Seed 94   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.35629602400013\n",
      "Seed 95   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.564049009999962\n",
      "Seed 96   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.523687422999956\n",
      "Seed 97   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.850773782000033\n",
      "Seed 98   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.6953454840002\n",
      "Seed 99   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.015339932000188\n",
      "Seed 100   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.39123638000001\n",
      "Seed 101   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.594417870000143\n",
      "Seed 102   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.783619105999833\n",
      "Seed 103   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.876350617000071\n",
      "Seed 104   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.439500544000111\n",
      "Seed 105   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.181604522000043\n",
      "Seed 106   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.076896543999965\n",
      "Seed 107   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.832593179000014\n",
      "Seed 108   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.307717772999922\n",
      "Seed 109   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.43920239099998\n",
      "Seed 110   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.498897093999858\n",
      "Seed 111   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.657795781000004\n",
      "Seed 112   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.814789377999887\n",
      "Seed 113   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.931495412999993\n",
      "Seed 114   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.288754776999895\n",
      "Seed 115   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.244204666000087\n",
      "Seed 116   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.550813107000067\n",
      "Seed 117   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.388158215999965\n",
      "Seed 118   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.750045622000016\n",
      "Seed 119   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.235819392000167\n",
      "Seed 120   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.996203699000034\n",
      "Seed 121   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.989480880999963\n",
      "Seed 122   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.056708553999897\n",
      "Seed 123   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.285523938999859\n",
      "Seed 124   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.525031678000005\n",
      "Seed 125   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.853770211999972\n",
      "Seed 126   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.874691244999894\n",
      "Seed 127   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.638378874000182\n",
      "Seed 128   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.042981213999838\n",
      "Seed 129   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.147106766999968\n",
      "Seed 130   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.252185378999911\n",
      "Seed 131   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.136590218000038\n",
      "Seed 132   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.631353791000038\n",
      "Seed 133   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.084455215999924\n",
      "Seed 134   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.278537409000137\n",
      "Seed 135   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.179119578000154\n",
      "Seed 136   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.569564312000011\n",
      "Seed 137   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.627897280999832\n",
      "Seed 138   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.784591183999964\n",
      "Seed 139   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.15707995899993\n",
      "Seed 140   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.181525357000055\n",
      "Seed 141   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.311887792000107\n",
      "Seed 142   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.489734566999914\n",
      "Seed 143   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.879751607999879\n",
      "Seed 144   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.325272771000073\n",
      "Seed 145   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.095041162999905\n",
      "Seed 146   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.463122424999938\n",
      "Seed 147   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.492982883999957\n",
      "Seed 148   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.48772615300004\n",
      "Seed 149   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.673118235000175\n",
      "Seed 150   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.028434469000103\n",
      "Seed 151   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.975339212000108\n",
      "Seed 152   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.189310215000205\n",
      "Seed 153   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.902211723999926\n",
      "Seed 154   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.252953891999823\n",
      "Seed 155   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.101709491000065\n",
      "Seed 156   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.322991391999949\n",
      "Seed 157   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.231441695000058\n",
      "Seed 158   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.062093285999936\n",
      "Seed 159   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.669411380999918\n",
      "Seed 160   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.712783790999993\n",
      "Seed 161   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.289970005000214\n",
      "Seed 162   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.054718644999866\n",
      "Seed 163   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.402621168999985\n",
      "Seed 164   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.74273369599996\n",
      "Seed 165   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.994736071000034\n",
      "Seed 166   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.28945646399984\n",
      "Seed 167   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.997000486000161\n",
      "Seed 168   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.741073809999989\n",
      "Seed 169   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.930781389999993\n",
      "Seed 170   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.058178237999982\n",
      "Seed 171   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.299073929000087\n",
      "Seed 172   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.528790453000056\n",
      "Seed 173   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.644680681000182\n",
      "Seed 174   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.82024248000016\n",
      "Seed 175   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.990435996000087\n",
      "Seed 176   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.8661229679999\n",
      "Seed 177   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.123140467999974\n",
      "Seed 178   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.590671946999919\n",
      "Seed 179   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.689152142000012\n",
      "Seed 180   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.67604732399991\n",
      "Seed 181   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.032750995000015\n",
      "Seed 182   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.450169252000023\n",
      "Seed 183   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.201859338999839\n",
      "Seed 184   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.017197729000145\n",
      "Seed 185   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.891306548999637\n",
      "Seed 186   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.39308081199988\n",
      "Seed 187   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.244145035999736\n",
      "Seed 188   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.604870163000214\n",
      "Seed 189   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.960879856999782\n",
      "Seed 190   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.77327322300016\n",
      "Seed 191   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.03098315699981\n",
      "Seed 192   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.10578955100027\n",
      "Seed 193   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.368450354000288\n",
      "Seed 194   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.467802901000141\n",
      "Seed 195   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.05645203999984\n",
      "Seed 196   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.349908370000321\n",
      "Seed 197   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.354014645000007\n",
      "Seed 198   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.494870496999738\n",
      "Seed 199   ###   Best-Avg 0.825\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import ConvNet, FCMSANet\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset()\n",
    "\n",
    "for seed in range(200):\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    net = FCMSANet(num_fc=2,sizes_fc=[2,4,2], bias=False, test=False)\n",
    "    \n",
    "    net.train_msa(60,train_loader)\n",
    "    print('Seed '+str(seed)+'   ###   Best-Avg '+str(net.best_avg))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  ###   Avg-Loss 0.016693341732025146   ###   Correct predictions 0.7354166666666667\n",
      "Epoch 2  ###   Avg-Loss 0.01539247731367747   ###   Correct predictions 0.7895833333333333\n",
      "Epoch 3  ###   Avg-Loss 0.013269580403963725   ###   Correct predictions 0.7958333333333333\n",
      "Epoch 4  ###   Avg-Loss 0.010812340180079143   ###   Correct predictions 0.8125\n",
      "Epoch 5  ###   Avg-Loss 0.009092676639556884   ###   Correct predictions 0.8375\n",
      "Epoch 6  ###   Avg-Loss 0.007869457205136618   ###   Correct predictions 0.8583333333333333\n",
      "Epoch 7  ###   Avg-Loss 0.007090519865353902   ###   Correct predictions 0.8625\n",
      "Epoch 8  ###   Avg-Loss 0.00648987740278244   ###   Correct predictions 0.8916666666666667\n",
      "Epoch 9  ###   Avg-Loss 0.006088708837827046   ###   Correct predictions 0.8979166666666667\n",
      "Epoch 10  ###   Avg-Loss 0.005733463664849599   ###   Correct predictions 0.9\n",
      "Epoch 11  ###   Avg-Loss 0.005439147353172302   ###   Correct predictions 0.9041666666666667\n",
      "Epoch 12  ###   Avg-Loss 0.005182546377182007   ###   Correct predictions 0.9125\n",
      "Epoch 13  ###   Avg-Loss 0.0049934898813565574   ###   Correct predictions 0.9104166666666667\n",
      "Epoch 14  ###   Avg-Loss 0.004753189285596212   ###   Correct predictions 0.91875\n",
      "Epoch 15  ###   Avg-Loss 0.004416432480017344   ###   Correct predictions 0.9229166666666667\n",
      "Epoch 16  ###   Avg-Loss 0.004125663638114929   ###   Correct predictions 0.93125\n",
      "Epoch 17  ###   Avg-Loss 0.003898448000351588   ###   Correct predictions 0.93125\n",
      "Epoch 18  ###   Avg-Loss 0.003615226596593857   ###   Correct predictions 0.9375\n",
      "Epoch 19  ###   Avg-Loss 0.0030299144486586253   ###   Correct predictions 0.95\n",
      "Epoch 20  ###   Avg-Loss 0.0029660664498806   ###   Correct predictions 0.95625\n",
      "Epoch 21  ###   Avg-Loss 0.00286577045917511   ###   Correct predictions 0.9479166666666666\n",
      "Epoch 22  ###   Avg-Loss 0.002026676634947459   ###   Correct predictions 0.9770833333333333\n",
      "Epoch 23  ###   Avg-Loss 0.00176669346789519   ###   Correct predictions 0.9791666666666666\n",
      "Epoch 24  ###   Avg-Loss 0.0016901899129152299   ###   Correct predictions 0.98125\n",
      "Epoch 25  ###   Avg-Loss 0.001406506821513176   ###   Correct predictions 0.9854166666666667\n",
      "Epoch 26  ###   Avg-Loss 0.0012301721920569737   ###   Correct predictions 0.9895833333333334\n",
      "Epoch 27  ###   Avg-Loss 0.0012656405568122863   ###   Correct predictions 0.9895833333333334\n",
      "Epoch 28  ###   Avg-Loss 0.0009948708117008208   ###   Correct predictions 0.9916666666666667\n",
      "Epoch 29  ###   Avg-Loss 0.0008995652198791504   ###   Correct predictions 0.99375\n",
      "Epoch 30  ###   Avg-Loss 0.0008839219808578491   ###   Correct predictions 0.99375\n",
      "Epoch 31  ###   Avg-Loss 0.0007204620788494746   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 32  ###   Avg-Loss 0.0006353583186864853   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 33  ###   Avg-Loss 0.0006963463500142097   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 34  ###   Avg-Loss 0.0006097466374437014   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 35  ###   Avg-Loss 0.0006425640856226285   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 36  ###   Avg-Loss 0.0004952810704708099   ###   Correct predictions 1.0\n",
      "Epoch 37  ###   Avg-Loss 0.00046397863576809565   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 38  ###   Avg-Loss 0.00047715206940968834   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 39  ###   Avg-Loss 0.0004436716747780641   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 40  ###   Avg-Loss 0.00044246241450309756   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 41  ###   Avg-Loss 0.0003799566999077797   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 42  ###   Avg-Loss 0.00037729634592930473   ###   Correct predictions 1.0\n",
      "Epoch 43  ###   Avg-Loss 0.00042406826590498287   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 44  ###   Avg-Loss 0.0003210838573674361   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 45  ###   Avg-Loss 0.00035742980738480884   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 46  ###   Avg-Loss 0.0003395354375243187   ###   Correct predictions 1.0\n",
      "Epoch 47  ###   Avg-Loss 0.0003530730493366718   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 48  ###   Avg-Loss 0.0003453268048663934   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 49  ###   Avg-Loss 0.0003007656273742517   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 50  ###   Avg-Loss 0.0002979370454947154   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 51  ###   Avg-Loss 0.0002575600054115057   ###   Correct predictions 1.0\n",
      "Epoch 52  ###   Avg-Loss 0.0003250787034630775   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 53  ###   Avg-Loss 0.00023143263533711433   ###   Correct predictions 1.0\n",
      "Epoch 54  ###   Avg-Loss 0.0002476739386717478   ###   Correct predictions 1.0\n",
      "Epoch 55  ###   Avg-Loss 0.0002808337099850178   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 56  ###   Avg-Loss 0.00022636189435919126   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 57  ###   Avg-Loss 0.00020725494250655173   ###   Correct predictions 1.0\n",
      "Epoch 58  ###   Avg-Loss 0.0002392620158692201   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 59  ###   Avg-Loss 0.0002071976816902558   ###   Correct predictions 1.0\n",
      "Epoch 60  ###   Avg-Loss 0.00020230164130528768   ###   Correct predictions 1.0\n",
      "Epoch 61  ###   Avg-Loss 0.00018292929356296858   ###   Correct predictions 1.0\n",
      "Epoch 62  ###   Avg-Loss 0.00022179240671296914   ###   Correct predictions 1.0\n",
      "Epoch 63  ###   Avg-Loss 0.00019362818760176498   ###   Correct predictions 1.0\n",
      "Epoch 64  ###   Avg-Loss 0.00022219737681249778   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 65  ###   Avg-Loss 0.00018464084714651108   ###   Correct predictions 1.0\n",
      "Epoch 66  ###   Avg-Loss 0.00021055651207764944   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 67  ###   Avg-Loss 0.00020628821415205796   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 68  ###   Avg-Loss 0.00019506152408818405   ###   Correct predictions 1.0\n",
      "Epoch 69  ###   Avg-Loss 0.0001632713247090578   ###   Correct predictions 1.0\n",
      "Epoch 70  ###   Avg-Loss 0.00017592293831209343   ###   Correct predictions 1.0\n",
      "Epoch 71  ###   Avg-Loss 0.00016277733569343886   ###   Correct predictions 1.0\n",
      "Epoch 72  ###   Avg-Loss 0.0001429748721420765   ###   Correct predictions 1.0\n",
      "Epoch 73  ###   Avg-Loss 0.0001682426780462265   ###   Correct predictions 1.0\n",
      "Epoch 74  ###   Avg-Loss 0.00015255645848810673   ###   Correct predictions 1.0\n",
      "Epoch 75  ###   Avg-Loss 0.00013346169143915176   ###   Correct predictions 1.0\n",
      "Epoch 76  ###   Avg-Loss 0.00015274204003314177   ###   Correct predictions 1.0\n",
      "Epoch 77  ###   Avg-Loss 0.0001632209246357282   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 78  ###   Avg-Loss 0.00016444246284663677   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 79  ###   Avg-Loss 0.0001559902292986711   ###   Correct predictions 1.0\n",
      "Epoch 80  ###   Avg-Loss 0.00012979219512393076   ###   Correct predictions 1.0\n",
      "Epoch 81  ###   Avg-Loss 0.00014563739920655887   ###   Correct predictions 1.0\n",
      "Epoch 82  ###   Avg-Loss 0.00012268397646645704   ###   Correct predictions 1.0\n",
      "Epoch 83  ###   Avg-Loss 0.00014513544738292694   ###   Correct predictions 1.0\n",
      "Epoch 84  ###   Avg-Loss 0.00012508279954393705   ###   Correct predictions 1.0\n",
      "Epoch 85  ###   Avg-Loss 0.00013901602166394394   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 86  ###   Avg-Loss 0.0001333181746304035   ###   Correct predictions 1.0\n",
      "Epoch 87  ###   Avg-Loss 0.00012044358688096205   ###   Correct predictions 1.0\n",
      "Epoch 88  ###   Avg-Loss 0.00014618256439765295   ###   Correct predictions 1.0\n",
      "Epoch 89  ###   Avg-Loss 0.00012452843754241864   ###   Correct predictions 1.0\n",
      "Epoch 90  ###   Avg-Loss 0.00011009617398182551   ###   Correct predictions 1.0\n",
      "Epoch 91  ###   Avg-Loss 0.00012323612657686074   ###   Correct predictions 1.0\n",
      "Epoch 92  ###   Avg-Loss 0.00011581191793084145   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 93  ###   Avg-Loss 0.0001325206986318032   ###   Correct predictions 1.0\n",
      "Epoch 94  ###   Avg-Loss 0.00010437506716698408   ###   Correct predictions 1.0\n",
      "Epoch 95  ###   Avg-Loss 0.00010432829149067401   ###   Correct predictions 1.0\n",
      "Epoch 96  ###   Avg-Loss 0.00011434933015455803   ###   Correct predictions 1.0\n",
      "Epoch 97  ###   Avg-Loss 0.00010554267403980096   ###   Correct predictions 1.0\n",
      "Epoch 98  ###   Avg-Loss 0.00013531527171532312   ###   Correct predictions 1.0\n",
      "Epoch 99  ###   Avg-Loss 9.843817291160425e-05   ###   Correct predictions 1.0\n",
      "Epoch 100  ###   Avg-Loss 9.126464525858562e-05   ###   Correct predictions 1.0\n",
      "Epoch 101  ###   Avg-Loss 8.715117195000251e-05   ###   Correct predictions 1.0\n",
      "Epoch 102  ###   Avg-Loss 9.70254031320413e-05   ###   Correct predictions 1.0\n",
      "Epoch 103  ###   Avg-Loss 0.00010043517686426639   ###   Correct predictions 1.0\n",
      "Epoch 104  ###   Avg-Loss 8.699353784322739e-05   ###   Correct predictions 1.0\n",
      "Epoch 105  ###   Avg-Loss 9.705725436409315e-05   ###   Correct predictions 1.0\n",
      "Epoch 106  ###   Avg-Loss 8.577681922664246e-05   ###   Correct predictions 1.0\n",
      "Epoch 107  ###   Avg-Loss 0.00010882464703172445   ###   Correct predictions 1.0\n",
      "Epoch 108  ###   Avg-Loss 0.00010686044115573167   ###   Correct predictions 1.0\n",
      "Epoch 109  ###   Avg-Loss 0.00010626811999827623   ###   Correct predictions 1.0\n",
      "Epoch 110  ###   Avg-Loss 0.00010530042927712203   ###   Correct predictions 1.0\n",
      "Epoch 111  ###   Avg-Loss 0.00010232297548403342   ###   Correct predictions 1.0\n",
      "Epoch 112  ###   Avg-Loss 8.531966401884954e-05   ###   Correct predictions 1.0\n",
      "Epoch 113  ###   Avg-Loss 8.699455453703801e-05   ###   Correct predictions 1.0\n",
      "Epoch 114  ###   Avg-Loss 8.856581213573615e-05   ###   Correct predictions 1.0\n",
      "Epoch 115  ###   Avg-Loss 7.67768050233523e-05   ###   Correct predictions 1.0\n",
      "Epoch 116  ###   Avg-Loss 8.174949325621129e-05   ###   Correct predictions 1.0\n",
      "Epoch 117  ###   Avg-Loss 7.995864531646172e-05   ###   Correct predictions 1.0\n",
      "Epoch 118  ###   Avg-Loss 7.204515859484672e-05   ###   Correct predictions 1.0\n",
      "Epoch 119  ###   Avg-Loss 8.846645553906758e-05   ###   Correct predictions 1.0\n",
      "Epoch 120  ###   Avg-Loss 8.003233621517817e-05   ###   Correct predictions 1.0\n",
      "Epoch 121  ###   Avg-Loss 7.330861408263444e-05   ###   Correct predictions 1.0\n",
      "Epoch 122  ###   Avg-Loss 6.652111187577248e-05   ###   Correct predictions 1.0\n",
      "Epoch 123  ###   Avg-Loss 7.577384822070599e-05   ###   Correct predictions 1.0\n",
      "Epoch 124  ###   Avg-Loss 6.504503932471076e-05   ###   Correct predictions 1.0\n",
      "Epoch 125  ###   Avg-Loss 9.361816725383202e-05   ###   Correct predictions 1.0\n",
      "Epoch 126  ###   Avg-Loss 6.556252483278513e-05   ###   Correct predictions 1.0\n",
      "Epoch 127  ###   Avg-Loss 6.442156542713443e-05   ###   Correct predictions 1.0\n",
      "Epoch 128  ###   Avg-Loss 7.758416080226501e-05   ###   Correct predictions 1.0\n",
      "Epoch 129  ###   Avg-Loss 7.533366636683544e-05   ###   Correct predictions 1.0\n",
      "Epoch 130  ###   Avg-Loss 6.837865803390741e-05   ###   Correct predictions 1.0\n",
      "Epoch 131  ###   Avg-Loss 7.118641709287961e-05   ###   Correct predictions 1.0\n",
      "Epoch 132  ###   Avg-Loss 8.623798688252766e-05   ###   Correct predictions 1.0\n",
      "Epoch 133  ###   Avg-Loss 6.864879590769608e-05   ###   Correct predictions 1.0\n",
      "Epoch 134  ###   Avg-Loss 7.340631758173307e-05   ###   Correct predictions 1.0\n",
      "Epoch 135  ###   Avg-Loss 5.6213835099091135e-05   ###   Correct predictions 1.0\n",
      "Epoch 136  ###   Avg-Loss 5.9551175218075515e-05   ###   Correct predictions 1.0\n",
      "Epoch 137  ###   Avg-Loss 7.160236903776725e-05   ###   Correct predictions 1.0\n",
      "Epoch 138  ###   Avg-Loss 6.109048845246435e-05   ###   Correct predictions 1.0\n",
      "Epoch 139  ###   Avg-Loss 7.094782777130603e-05   ###   Correct predictions 1.0\n",
      "Epoch 140  ###   Avg-Loss 7.791101622084776e-05   ###   Correct predictions 1.0\n",
      "Epoch 141  ###   Avg-Loss 6.113395793363451e-05   ###   Correct predictions 1.0\n",
      "Epoch 142  ###   Avg-Loss 6.176692356045048e-05   ###   Correct predictions 1.0\n",
      "Epoch 143  ###   Avg-Loss 7.38037284463644e-05   ###   Correct predictions 1.0\n",
      "Epoch 144  ###   Avg-Loss 6.984003509084384e-05   ###   Correct predictions 1.0\n",
      "Epoch 145  ###   Avg-Loss 6.579244509339333e-05   ###   Correct predictions 1.0\n",
      "Epoch 146  ###   Avg-Loss 5.51832839846611e-05   ###   Correct predictions 1.0\n",
      "Epoch 147  ###   Avg-Loss 5.711079575121403e-05   ###   Correct predictions 1.0\n",
      "Epoch 148  ###   Avg-Loss 5.0937927638490996e-05   ###   Correct predictions 1.0\n",
      "Epoch 149  ###   Avg-Loss 5.608038045465946e-05   ###   Correct predictions 1.0\n",
      "Epoch 150  ###   Avg-Loss 6.162119098007679e-05   ###   Correct predictions 1.0\n",
      "Epoch 151  ###   Avg-Loss 5.776884887988369e-05   ###   Correct predictions 1.0\n",
      "Epoch 152  ###   Avg-Loss 6.204020076741776e-05   ###   Correct predictions 1.0\n",
      "Epoch 153  ###   Avg-Loss 5.5288333290567e-05   ###   Correct predictions 1.0\n",
      "Epoch 154  ###   Avg-Loss 4.973360725368063e-05   ###   Correct predictions 1.0\n",
      "Epoch 155  ###   Avg-Loss 5.3552817553281785e-05   ###   Correct predictions 1.0\n",
      "Epoch 156  ###   Avg-Loss 4.901509576787551e-05   ###   Correct predictions 1.0\n",
      "Epoch 157  ###   Avg-Loss 5.7395625238617264e-05   ###   Correct predictions 1.0\n",
      "Epoch 158  ###   Avg-Loss 4.948605395232638e-05   ###   Correct predictions 1.0\n",
      "Epoch 159  ###   Avg-Loss 4.963681955511371e-05   ###   Correct predictions 1.0\n",
      "Epoch 160  ###   Avg-Loss 5.433883440370361e-05   ###   Correct predictions 1.0\n",
      "Epoch 161  ###   Avg-Loss 5.3806684445589784e-05   ###   Correct predictions 1.0\n",
      "Epoch 162  ###   Avg-Loss 4.746093569944302e-05   ###   Correct predictions 1.0\n",
      "Epoch 163  ###   Avg-Loss 7.585322794814905e-05   ###   Correct predictions 1.0\n",
      "Epoch 164  ###   Avg-Loss 4.612901248037815e-05   ###   Correct predictions 1.0\n",
      "Epoch 165  ###   Avg-Loss 4.46078289921085e-05   ###   Correct predictions 1.0\n",
      "Epoch 166  ###   Avg-Loss 4.7858059406280515e-05   ###   Correct predictions 1.0\n",
      "Epoch 167  ###   Avg-Loss 5.252285239597161e-05   ###   Correct predictions 1.0\n",
      "Epoch 168  ###   Avg-Loss 6.123439331228535e-05   ###   Correct predictions 1.0\n",
      "Epoch 169  ###   Avg-Loss 5.4478478462745744e-05   ###   Correct predictions 1.0\n",
      "Epoch 170  ###   Avg-Loss 5.318818924327691e-05   ###   Correct predictions 1.0\n",
      "Epoch 171  ###   Avg-Loss 4.756053676828742e-05   ###   Correct predictions 1.0\n",
      "Epoch 172  ###   Avg-Loss 4.761051774645845e-05   ###   Correct predictions 1.0\n",
      "Epoch 173  ###   Avg-Loss 5.6479397850732006e-05   ###   Correct predictions 1.0\n",
      "Epoch 174  ###   Avg-Loss 4.788977870096763e-05   ###   Correct predictions 1.0\n",
      "Epoch 175  ###   Avg-Loss 6.446146095792452e-05   ###   Correct predictions 1.0\n",
      "Epoch 176  ###   Avg-Loss 4.3726297250638405e-05   ###   Correct predictions 1.0\n",
      "Epoch 177  ###   Avg-Loss 5.430232267826796e-05   ###   Correct predictions 1.0\n",
      "Epoch 178  ###   Avg-Loss 5.736845002199213e-05   ###   Correct predictions 1.0\n",
      "Epoch 179  ###   Avg-Loss 5.051289529850086e-05   ###   Correct predictions 1.0\n",
      "Epoch 180  ###   Avg-Loss 5.14243069725732e-05   ###   Correct predictions 1.0\n",
      "Epoch 181  ###   Avg-Loss 3.913740317026774e-05   ###   Correct predictions 1.0\n",
      "Epoch 182  ###   Avg-Loss 5.9283819670478506e-05   ###   Correct predictions 1.0\n",
      "Epoch 183  ###   Avg-Loss 4.4025426420072714e-05   ###   Correct predictions 1.0\n",
      "Epoch 184  ###   Avg-Loss 4.923796902100245e-05   ###   Correct predictions 1.0\n",
      "Epoch 185  ###   Avg-Loss 4.7920217427114645e-05   ###   Correct predictions 1.0\n",
      "Epoch 186  ###   Avg-Loss 4.310249350965023e-05   ###   Correct predictions 1.0\n",
      "Epoch 187  ###   Avg-Loss 4.640198312699795e-05   ###   Correct predictions 1.0\n",
      "Epoch 188  ###   Avg-Loss 4.455156158655882e-05   ###   Correct predictions 1.0\n",
      "Epoch 189  ###   Avg-Loss 4.795944939057032e-05   ###   Correct predictions 1.0\n",
      "Epoch 190  ###   Avg-Loss 3.5239538798729576e-05   ###   Correct predictions 1.0\n",
      "Epoch 191  ###   Avg-Loss 4.360287372643749e-05   ###   Correct predictions 1.0\n",
      "Epoch 192  ###   Avg-Loss 4.576954136913021e-05   ###   Correct predictions 1.0\n",
      "Epoch 193  ###   Avg-Loss 3.716207187001904e-05   ###   Correct predictions 1.0\n",
      "Epoch 194  ###   Avg-Loss 4.478615010157227e-05   ###   Correct predictions 1.0\n",
      "Epoch 195  ###   Avg-Loss 4.2566253493229546e-05   ###   Correct predictions 1.0\n",
      "Epoch 196  ###   Avg-Loss 4.4678539658586185e-05   ###   Correct predictions 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197  ###   Avg-Loss 3.961453524728616e-05   ###   Correct predictions 1.0\n",
      "Epoch 198  ###   Avg-Loss 4.014573448027174e-05   ###   Correct predictions 1.0\n",
      "Epoch 199  ###   Avg-Loss 4.041891467447082e-05   ###   Correct predictions 1.0\n",
      "Epoch 200  ###   Avg-Loss 3.8891894898066916e-05   ###   Correct predictions 1.0\n",
      "Epoch 201  ###   Avg-Loss 3.913562589635452e-05   ###   Correct predictions 1.0\n",
      "Epoch 202  ###   Avg-Loss 4.7289044596254824e-05   ###   Correct predictions 1.0\n",
      "Epoch 203  ###   Avg-Loss 3.683992739145954e-05   ###   Correct predictions 1.0\n",
      "Epoch 204  ###   Avg-Loss 3.6930983575681846e-05   ###   Correct predictions 1.0\n",
      "Epoch 205  ###   Avg-Loss 3.837041634445389e-05   ###   Correct predictions 1.0\n",
      "Epoch 206  ###   Avg-Loss 3.6403711419552565e-05   ###   Correct predictions 1.0\n",
      "Epoch 207  ###   Avg-Loss 3.761501672367255e-05   ###   Correct predictions 1.0\n",
      "Epoch 208  ###   Avg-Loss 3.68371062601606e-05   ###   Correct predictions 1.0\n",
      "Epoch 209  ###   Avg-Loss 4.079292993992567e-05   ###   Correct predictions 1.0\n",
      "Epoch 210  ###   Avg-Loss 3.946068463847041e-05   ###   Correct predictions 1.0\n",
      "Epoch 211  ###   Avg-Loss 4.385570452238123e-05   ###   Correct predictions 1.0\n",
      "Epoch 212  ###   Avg-Loss 4.163978931804498e-05   ###   Correct predictions 1.0\n",
      "Epoch 213  ###   Avg-Loss 3.332675745089849e-05   ###   Correct predictions 1.0\n",
      "Epoch 214  ###   Avg-Loss 4.257323065151771e-05   ###   Correct predictions 1.0\n",
      "Epoch 215  ###   Avg-Loss 4.2936753015965226e-05   ###   Correct predictions 1.0\n",
      "Epoch 216  ###   Avg-Loss 3.636674955487251e-05   ###   Correct predictions 1.0\n",
      "Epoch 217  ###   Avg-Loss 3.94138313519458e-05   ###   Correct predictions 1.0\n",
      "Epoch 218  ###   Avg-Loss 3.3846831259628135e-05   ###   Correct predictions 1.0\n",
      "Epoch 219  ###   Avg-Loss 3.627579426392913e-05   ###   Correct predictions 1.0\n",
      "Epoch 220  ###   Avg-Loss 4.044672629485528e-05   ###   Correct predictions 1.0\n",
      "Epoch 221  ###   Avg-Loss 3.899487977226575e-05   ###   Correct predictions 1.0\n",
      "Epoch 222  ###   Avg-Loss 3.2388655624041956e-05   ###   Correct predictions 1.0\n",
      "Epoch 223  ###   Avg-Loss 3.148379425207774e-05   ###   Correct predictions 1.0\n",
      "Epoch 224  ###   Avg-Loss 4.479126849522193e-05   ###   Correct predictions 1.0\n",
      "Epoch 225  ###   Avg-Loss 3.618796666463216e-05   ###   Correct predictions 1.0\n",
      "Epoch 226  ###   Avg-Loss 3.777043893933296e-05   ###   Correct predictions 1.0\n",
      "Epoch 227  ###   Avg-Loss 3.232196516667803e-05   ###   Correct predictions 1.0\n",
      "Epoch 228  ###   Avg-Loss 3.010243138608833e-05   ###   Correct predictions 1.0\n",
      "Epoch 229  ###   Avg-Loss 3.525512292981148e-05   ###   Correct predictions 1.0\n",
      "Epoch 230  ###   Avg-Loss 3.3884646836668256e-05   ###   Correct predictions 1.0\n",
      "Epoch 231  ###   Avg-Loss 3.234480197230975e-05   ###   Correct predictions 1.0\n",
      "Epoch 232  ###   Avg-Loss 3.662161761894822e-05   ###   Correct predictions 1.0\n",
      "Epoch 233  ###   Avg-Loss 3.697317248831193e-05   ###   Correct predictions 1.0\n",
      "Epoch 234  ###   Avg-Loss 2.878650751275321e-05   ###   Correct predictions 1.0\n",
      "Epoch 235  ###   Avg-Loss 3.556391845146815e-05   ###   Correct predictions 1.0\n",
      "Epoch 236  ###   Avg-Loss 2.7765481111903984e-05   ###   Correct predictions 1.0\n",
      "Epoch 237  ###   Avg-Loss 3.6760074241707726e-05   ###   Correct predictions 1.0\n",
      "Epoch 238  ###   Avg-Loss 3.500080201774835e-05   ###   Correct predictions 1.0\n",
      "Epoch 239  ###   Avg-Loss 3.9084527331093946e-05   ###   Correct predictions 1.0\n",
      "Epoch 240  ###   Avg-Loss 3.469903022050858e-05   ###   Correct predictions 1.0\n",
      "Epoch 241  ###   Avg-Loss 3.173545701429248e-05   ###   Correct predictions 1.0\n",
      "Epoch 242  ###   Avg-Loss 3.360264624158541e-05   ###   Correct predictions 1.0\n",
      "Epoch 243  ###   Avg-Loss 3.289517092828949e-05   ###   Correct predictions 1.0\n",
      "Epoch 244  ###   Avg-Loss 2.8689632502694926e-05   ###   Correct predictions 1.0\n",
      "Epoch 245  ###   Avg-Loss 3.048427946244677e-05   ###   Correct predictions 1.0\n",
      "Epoch 246  ###   Avg-Loss 3.113332592571775e-05   ###   Correct predictions 1.0\n",
      "Epoch 247  ###   Avg-Loss 2.8724937389294308e-05   ###   Correct predictions 1.0\n",
      "Epoch 248  ###   Avg-Loss 3.149114587965111e-05   ###   Correct predictions 1.0\n",
      "Epoch 249  ###   Avg-Loss 3.239394476016362e-05   ###   Correct predictions 1.0\n",
      "Epoch 250  ###   Avg-Loss 2.6755715953186155e-05   ###   Correct predictions 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('lin1',nn.Linear(2, 16, bias=True)),\n",
    "    ('relu1',nn.ReLU()),\n",
    "    ('lin2',nn.Linear(16, 32, bias=True)),\n",
    "    ('relu2',nn.ReLU()),\n",
    "    ('lin3',nn.Linear(32, 64, bias=True)),\n",
    "    ('relu3',nn.ReLU()),\n",
    "    ('lin4',nn.Linear(64, 2, bias=True))\n",
    "])\n",
    ")\n",
    "\n",
    "output_frequency = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(250):\n",
    "    loss_sum = 0\n",
    "    correct_pred = 0\n",
    "    for index, (data, target) in enumerate(train_loader):\n",
    "        #print(index)\n",
    "        output = model.forward(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss_sum = loss_sum + loss.data\n",
    "        for i in range(len(output)):\n",
    "            _, ind = torch.max(output[i],0)\n",
    "            label = target[i]\n",
    "            \n",
    "            if ind.data == label.data:\n",
    "                correct_pred +=1\n",
    "        #if index % (10*(output_frequency)) == 0:\n",
    "        #    print(\"#  Epoch  #  Batch  #  Avg-Loss ###############\")\n",
    "        #if index % (output_frequency) == 0 and index > 0:\n",
    "        #    print(\"#  %d  #  %d  #  %f  #\" % (epoch+1, index, loss_sum/output_frequency))\n",
    "        #    loss_sum = 0\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch '+str(epoch+1)+'  ###   Avg-Loss '+str(loss_sum.item()/480)+'   ###   Correct predictions '+str(correct_pred/480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  ###   Avg-Loss 0.019352535406748455   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 2  ###   Avg-Loss 0.017910422881444295   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 3  ###   Avg-Loss 0.01750417153040568   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 4  ###   Avg-Loss 0.017391308148701986   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 5  ###   Avg-Loss 0.017361233631769817   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 6  ###   Avg-Loss 0.017345829804738363   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 7  ###   Avg-Loss 0.017331127325693765   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 8  ###   Avg-Loss 0.01733660101890564   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 9  ###   Avg-Loss 0.01734957695007324   ###   Correct predictions 0.4895833333333333\n",
      "Epoch 10  ###   Avg-Loss 0.01734344959259033   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 11  ###   Avg-Loss 0.017335693041483562   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 12  ###   Avg-Loss 0.01733403404553731   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 13  ###   Avg-Loss 0.017345335086186728   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 14  ###   Avg-Loss 0.017336952686309814   ###   Correct predictions 0.50625\n",
      "Epoch 15  ###   Avg-Loss 0.017330414056777953   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 16  ###   Avg-Loss 0.01733735203742981   ###   Correct predictions 0.49375\n",
      "Epoch 17  ###   Avg-Loss 0.017340999841690064   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 18  ###   Avg-Loss 0.01733502944310506   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 19  ###   Avg-Loss 0.017336159944534302   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 20  ###   Avg-Loss 0.01733076572418213   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 21  ###   Avg-Loss 0.017335595687230428   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 22  ###   Avg-Loss 0.017331228653589884   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 23  ###   Avg-Loss 0.017345523834228514   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 24  ###   Avg-Loss 0.017328786849975585   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 25  ###   Avg-Loss 0.017338613669077556   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 26  ###   Avg-Loss 0.017339388529459637   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 27  ###   Avg-Loss 0.017334789037704468   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 28  ###   Avg-Loss 0.017332659165064494   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 29  ###   Avg-Loss 0.017332019408543904   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 30  ###   Avg-Loss 0.017343803246816   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 31  ###   Avg-Loss 0.017337658007939658   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 32  ###   Avg-Loss 0.017327052354812623   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 33  ###   Avg-Loss 0.017350995540618898   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 34  ###   Avg-Loss 0.017341856161753336   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 35  ###   Avg-Loss 0.01733092466990153   ###   Correct predictions 0.49375\n",
      "Epoch 36  ###   Avg-Loss 0.017343801259994508   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 37  ###   Avg-Loss 0.017331286271413168   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 38  ###   Avg-Loss 0.0173449436823527   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 39  ###   Avg-Loss 0.017336622873942057   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 40  ###   Avg-Loss 0.01733430624008179   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 41  ###   Avg-Loss 0.01733665664990743   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 42  ###   Avg-Loss 0.01732741594314575   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 43  ###   Avg-Loss 0.017335259914398195   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 44  ###   Avg-Loss 0.017344200611114503   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 45  ###   Avg-Loss 0.017335255940755207   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 46  ###   Avg-Loss 0.01732981006304423   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 47  ###   Avg-Loss 0.017340336243311563   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 48  ###   Avg-Loss 0.017337270577748618   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 49  ###   Avg-Loss 0.01734388470649719   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 50  ###   Avg-Loss 0.017336700359980264   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 51  ###   Avg-Loss 0.017336754004160564   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 52  ###   Avg-Loss 0.017329061031341554   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 53  ###   Avg-Loss 0.017343024412790935   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 54  ###   Avg-Loss 0.017335404952367146   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 55  ###   Avg-Loss 0.017333406209945678   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 56  ###   Avg-Loss 0.01734092434247335   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 57  ###   Avg-Loss 0.017334075768788655   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 58  ###   Avg-Loss 0.017333996295928956   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 59  ###   Avg-Loss 0.01734001040458679   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 60  ###   Avg-Loss 0.017343183358510334   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 61  ###   Avg-Loss 0.017334266503651937   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 62  ###   Avg-Loss 0.017326943079630532   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 63  ###   Avg-Loss 0.017331435283025106   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 64  ###   Avg-Loss 0.01734351714452108   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 65  ###   Avg-Loss 0.01733464002609253   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 66  ###   Avg-Loss 0.017335947354634604   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 67  ###   Avg-Loss 0.017339940865834555   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 68  ###   Avg-Loss 0.017335885763168336   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 69  ###   Avg-Loss 0.017330139875411987   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 70  ###   Avg-Loss 0.017336839437484743   ###   Correct predictions 0.48125\n",
      "Epoch 71  ###   Avg-Loss 0.01733196576436361   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 72  ###   Avg-Loss 0.01733160416285197   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 73  ###   Avg-Loss 0.017341158787409463   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 74  ###   Avg-Loss 0.01734090844790141   ###   Correct predictions 0.4895833333333333\n",
      "Epoch 75  ###   Avg-Loss 0.01733473539352417   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 76  ###   Avg-Loss 0.01733032464981079   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 77  ###   Avg-Loss 0.01734740932782491   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 78  ###   Avg-Loss 0.017335404952367146   ###   Correct predictions 0.49375\n",
      "Epoch 79  ###   Avg-Loss 0.017347671588261924   ###   Correct predictions 0.4895833333333333\n",
      "Epoch 80  ###   Avg-Loss 0.017328637838363647   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 81  ###   Avg-Loss 0.017335180441538492   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 82  ###   Avg-Loss 0.017335520188013712   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 83  ###   Avg-Loss 0.01733883221944173   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 84  ###   Avg-Loss 0.017346135775248208   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 85  ###   Avg-Loss 0.0173315425713857   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 86  ###   Avg-Loss 0.017339418331782024   ###   Correct predictions 0.48125\n",
      "Epoch 87  ###   Avg-Loss 0.01734487811724345   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 88  ###   Avg-Loss 0.01733728249867757   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 89  ###   Avg-Loss 0.01733392079671224   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 90  ###   Avg-Loss 0.017338260014851888   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 91  ###   Avg-Loss 0.017348573605219523   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 92  ###   Avg-Loss 0.017336924870808918   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 93  ###   Avg-Loss 0.017339259386062622   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 94  ###   Avg-Loss 0.017346447706222533   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 95  ###   Avg-Loss 0.01733930706977844   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 96  ###   Avg-Loss 0.01734011769294739   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 97  ###   Avg-Loss 0.01733176310857137   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 98  ###   Avg-Loss 0.017335659265518187   ###   Correct predictions 0.48125\n",
      "Epoch 99  ###   Avg-Loss 0.017331586281458537   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 100  ###   Avg-Loss 0.017337832848230997   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 101  ###   Avg-Loss 0.017333618799845376   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 102  ###   Avg-Loss 0.017344723145167034   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 103  ###   Avg-Loss 0.0173422376314799   ###   Correct predictions 0.48541666666666666\n",
      "Epoch 104  ###   Avg-Loss 0.017340991894404092   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 105  ###   Avg-Loss 0.01734160582224528   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 106  ###   Avg-Loss 0.017327898740768434   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 107  ###   Avg-Loss 0.01732935905456543   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 108  ###   Avg-Loss 0.017331963777542113   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 109  ###   Avg-Loss 0.01732992132504781   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 110  ###   Avg-Loss 0.017332820097605388   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 111  ###   Avg-Loss 0.017328357696533202   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 112  ###   Avg-Loss 0.017340288559595744   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 113  ###   Avg-Loss 0.017335466543833413   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 114  ###   Avg-Loss 0.017333012819290162   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 115  ###   Avg-Loss 0.017334548632303874   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 116  ###   Avg-Loss 0.017332889636357627   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 117  ###   Avg-Loss 0.017335166533788044   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 118  ###   Avg-Loss 0.01733022133509318   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 119  ###   Avg-Loss 0.017332543929417927   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 120  ###   Avg-Loss 0.017333388328552246   ###   Correct predictions 0.48541666666666666\n",
      "Epoch 121  ###   Avg-Loss 0.017331175009409585   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 122  ###   Avg-Loss 0.017329037189483643   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 123  ###   Avg-Loss 0.017340620358784992   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 124  ###   Avg-Loss 0.01733303666114807   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 125  ###   Avg-Loss 0.0173516849676768   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 126  ###   Avg-Loss 0.0173428475856781   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 127  ###   Avg-Loss 0.017335009574890137   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 128  ###   Avg-Loss 0.017333370447158814   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 129  ###   Avg-Loss 0.017340290546417236   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 130  ###   Avg-Loss 0.01732499400774638   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 131  ###   Avg-Loss 0.01733763813972473   ###   Correct predictions 0.48541666666666666\n",
      "Epoch 132  ###   Avg-Loss 0.017332053184509276   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 133  ###   Avg-Loss 0.017353246609369915   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 134  ###   Avg-Loss 0.017330745855967205   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 135  ###   Avg-Loss 0.01732763648033142   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 136  ###   Avg-Loss 0.017330714066823325   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 137  ###   Avg-Loss 0.017336773872375488   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 138  ###   Avg-Loss 0.017343024412790935   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 139  ###   Avg-Loss 0.01733009417851766   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 140  ###   Avg-Loss 0.017354045311609903   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 141  ###   Avg-Loss 0.017332154512405395   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 142  ###   Avg-Loss 0.017334461212158203   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 143  ###   Avg-Loss 0.017343628406524658   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 144  ###   Avg-Loss 0.017340620358784992   ###   Correct predictions 0.46875\n",
      "Epoch 145  ###   Avg-Loss 0.01733053723971049   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 146  ###   Avg-Loss 0.017334272464116413   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 147  ###   Avg-Loss 0.017340207099914552   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 148  ###   Avg-Loss 0.01733244260152181   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 149  ###   Avg-Loss 0.017334314187367757   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 150  ###   Avg-Loss 0.0173399825890859   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 151  ###   Avg-Loss 0.017332218090693154   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 152  ###   Avg-Loss 0.017343036333719888   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 153  ###   Avg-Loss 0.01734464367230733   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 154  ###   Avg-Loss 0.017339040835698444   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 155  ###   Avg-Loss 0.01733132799466451   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 156  ###   Avg-Loss 0.0173313041528066   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 157  ###   Avg-Loss 0.017345698674519856   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 158  ###   Avg-Loss 0.01734340190887451   ###   Correct predictions 0.47708333333333336\n",
      "Epoch 159  ###   Avg-Loss 0.017333271106084187   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 160  ###   Avg-Loss 0.01733576456705729   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 161  ###   Avg-Loss 0.01733560562133789   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 162  ###   Avg-Loss 0.017329166332880657   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 163  ###   Avg-Loss 0.017341987291971842   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 164  ###   Avg-Loss 0.017338371276855467   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 165  ###   Avg-Loss 0.017340058088302614   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 166  ###   Avg-Loss 0.017344150940577188   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 167  ###   Avg-Loss 0.017344385385513306   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 168  ###   Avg-Loss 0.017346356312433878   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 169  ###   Avg-Loss 0.017333459854125977   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 170  ###   Avg-Loss 0.01733231743176778   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 171  ###   Avg-Loss 0.01732916235923767   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 172  ###   Avg-Loss 0.017331536610921225   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 173  ###   Avg-Loss 0.01733509699503581   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 174  ###   Avg-Loss 0.01732465426127116   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 175  ###   Avg-Loss 0.017342160145441692   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 176  ###   Avg-Loss 0.017330139875411987   ###   Correct predictions 0.5104166666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177  ###   Avg-Loss 0.017332039276758828   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 178  ###   Avg-Loss 0.01733199159304301   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 179  ###   Avg-Loss 0.017338985204696657   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 180  ###   Avg-Loss 0.017342575391133628   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 181  ###   Avg-Loss 0.017335110902786256   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 182  ###   Avg-Loss 0.017331095536549886   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 183  ###   Avg-Loss 0.017341347535451253   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 184  ###   Avg-Loss 0.017332384983698528   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 185  ###   Avg-Loss 0.017330108086268108   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 186  ###   Avg-Loss 0.017338589827219645   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 187  ###   Avg-Loss 0.017340189218521117   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 188  ###   Avg-Loss 0.017331349849700927   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 189  ###   Avg-Loss 0.017359145482381187   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 190  ###   Avg-Loss 0.017331101497014365   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 191  ###   Avg-Loss 0.017330982287724814   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 192  ###   Avg-Loss 0.017326368888219198   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 193  ###   Avg-Loss 0.017343854904174803   ###   Correct predictions 0.47708333333333336\n",
      "Epoch 194  ###   Avg-Loss 0.017337441444396973   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 195  ###   Avg-Loss 0.017333298921585083   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 196  ###   Avg-Loss 0.017332341273625693   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 197  ###   Avg-Loss 0.017343161503473918   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 198  ###   Avg-Loss 0.017330855131149292   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 199  ###   Avg-Loss 0.01732876698176066   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 200  ###   Avg-Loss 0.017356745402018228   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 201  ###   Avg-Loss 0.017346465587615968   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 202  ###   Avg-Loss 0.017329835891723634   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 203  ###   Avg-Loss 0.01734454035758972   ###   Correct predictions 0.4895833333333333\n",
      "Epoch 204  ###   Avg-Loss 0.01733029286066691   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 205  ###   Avg-Loss 0.01733484069506327   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 206  ###   Avg-Loss 0.017335607608159383   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 207  ###   Avg-Loss 0.017337212959925335   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 208  ###   Avg-Loss 0.017347023884455363   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 209  ###   Avg-Loss 0.017332309484481813   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 210  ###   Avg-Loss 0.01733231743176778   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 211  ###   Avg-Loss 0.01734496553738912   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 212  ###   Avg-Loss 0.0173362930615743   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 213  ###   Avg-Loss 0.017337344090143838   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 214  ###   Avg-Loss 0.017353427410125733   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 215  ###   Avg-Loss 0.017331113417943318   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 216  ###   Avg-Loss 0.01733746329943339   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 217  ###   Avg-Loss 0.017332746585210165   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 218  ###   Avg-Loss 0.01733715335528056   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 219  ###   Avg-Loss 0.017337143421173096   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 220  ###   Avg-Loss 0.0173479954401652   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 221  ###   Avg-Loss 0.01733072797457377   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 222  ###   Avg-Loss 0.01734137535095215   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 223  ###   Avg-Loss 0.017333173751831056   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 224  ###   Avg-Loss 0.017326873540878297   ###   Correct predictions 0.50625\n",
      "Epoch 225  ###   Avg-Loss 0.01732667684555054   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 226  ###   Avg-Loss 0.017343149582544962   ###   Correct predictions 0.47708333333333336\n",
      "Epoch 227  ###   Avg-Loss 0.017348565657933555   ###   Correct predictions 0.49375\n",
      "Epoch 228  ###   Avg-Loss 0.017334616184234618   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 229  ###   Avg-Loss 0.017330666383107502   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 230  ###   Avg-Loss 0.017340850830078126   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 231  ###   Avg-Loss 0.01733234922091166   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 232  ###   Avg-Loss 0.01734009782473246   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 233  ###   Avg-Loss 0.017341488599777223   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 234  ###   Avg-Loss 0.017334944009780882   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 235  ###   Avg-Loss 0.017334401607513428   ###   Correct predictions 0.50625\n",
      "Epoch 236  ###   Avg-Loss 0.01733740170796712   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 237  ###   Avg-Loss 0.017334469159444175   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 238  ###   Avg-Loss 0.01733429233233134   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 239  ###   Avg-Loss 0.017332722743352253   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 240  ###   Avg-Loss 0.01733097235361735   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 241  ###   Avg-Loss 0.017334298292795817   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 242  ###   Avg-Loss 0.017329289515813192   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 243  ###   Avg-Loss 0.01733185847600301   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 244  ###   Avg-Loss 0.017335520188013712   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 245  ###   Avg-Loss 0.017344415187835693   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 246  ###   Avg-Loss 0.017336014906565347   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 247  ###   Avg-Loss 0.017329887549082438   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 248  ###   Avg-Loss 0.017341379324595133   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 249  ###   Avg-Loss 0.01733956535657247   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 250  ###   Avg-Loss 0.01733067234357198   ###   Correct predictions 0.5104166666666666\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "from Dataset.Dataset import makeMoonsDataset\n",
    "\n",
    "dataset_size = 600\n",
    "batch_size = 40\n",
    "train_size = dataset_size*0.8\n",
    "test_size = dataset_size*0.2\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset(dataset_size,batch_size)\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('lin1',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu1',nn.ReLU()),\n",
    "    ('lin2',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu2',nn.ReLU()),\n",
    "    ('lin3',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu3',nn.ReLU()),\n",
    "    ('lin4',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu4',nn.ReLU()),\n",
    "    ('lin5',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu5',nn.ReLU()),\n",
    "    ('lin6',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu6',nn.ReLU()),\n",
    "    ('lin7',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu7',nn.ReLU()),\n",
    "    ('lin8',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu8',nn.ReLU()),\n",
    "    ('lin9',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu9',nn.ReLU()),\n",
    "    ('lin10',nn.Linear(2, 2, bias=True))\n",
    "])\n",
    ")\n",
    "\n",
    "output_frequency = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(250):\n",
    "    loss_sum = 0\n",
    "    correct_pred = 0\n",
    "    for index, (data, target) in enumerate(train_loader):\n",
    "        #print(index)\n",
    "        output = model.forward(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss_sum = loss_sum + loss.data\n",
    "        for i in range(len(output)):\n",
    "            _, ind = torch.max(output[i],0)\n",
    "            label = target[i]\n",
    "            \n",
    "            if ind.data == label.data:\n",
    "                correct_pred +=1\n",
    "        #if index % (10*(output_frequency)) == 0:\n",
    "        #    print(\"#  Epoch  #  Batch  #  Avg-Loss ###############\")\n",
    "        #if index % (output_frequency) == 0 and index > 0:\n",
    "        #    print(\"#  %d  #  %d  #  %f  #\" % (epoch+1, index, loss_sum/output_frequency))\n",
    "        #    loss_sum = 0\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch '+str(epoch+1)+'  ###   Avg-Loss '+str(loss_sum.item()/train_size)+'   ###   Correct predictions '+str(correct_pred/train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7115,  1.4547]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.6434, -2.7388]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3886,  2.0311]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.9741, -3.1041]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-0.8089,  0.1275]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3526, -2.2834]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.8463, -2.8233]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.9055, -2.8244]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-1.7085,  1.4306]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.7206,  2.3536]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 0.7751, -1.6380]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.4437,  2.1093]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.1609,  1.7324]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-0.5344,  0.0122]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3842,  2.1159]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-1.5523,  1.2432]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.5676, -2.6339]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 2.0958, -3.2221]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 0.9023, -1.6219]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.7551,  2.3714]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3445, -2.3139]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.5533,  2.1857]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3841, -2.4257]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3780,  2.0977]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 0.0020, -0.6190]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 0.1119, -0.9363]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3832, -2.3543]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.5189, -2.5425]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-1.9774,  1.4456]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.2206, -2.2302]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-0.1666, -0.6191]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.3856,  2.0124]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.4732, -2.4705]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.4295,  2.1136]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-0.6206,  0.0895]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.7165, -2.7979]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.8907, -2.8468]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.8076, -2.8020]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3121,  1.9327]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.0975,  1.7790]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "for index, (data, target) in enumerate(test_loader):\n",
    "    print(model.forward(data))\n",
    "    print(target)\n",
    "    print('####################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 0.9833333333333333\n"
     ]
    }
   ],
   "source": [
    "net.test(test_loader,120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0])\n",
      "tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1])\n",
      "tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1])\n",
      "tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n",
      "        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1])\n",
      "tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,\n",
      "        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1])\n",
      "tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1])\n",
      "tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
      "        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1])\n",
      "tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,\n",
      "        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
      "        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0])\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,\n",
      "        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0])\n",
      "[0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "from pandas import DataFrame\n",
    "import torch\n",
    "\n",
    "x1 = []\n",
    "x2 = []\n",
    "y = []\n",
    "for (data,label) in train_loader:\n",
    "    res = torch.argmax(net.forward(data), dim=1)\n",
    "    print(res)\n",
    "    for point in range(len(data)):\n",
    "        x1.append(data[point][0].item())\n",
    "        x2.append(data[point][1].item())\n",
    "        y.append(res[point].item())\n",
    "#print(x1)\n",
    "df = DataFrame(dict(x=x1, y=x2, label=y))\n",
    "print(y)\n",
    "colors = {0:'red', 1:'blue'}\n",
    "fig, ax = pyplot.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 1.0\n"
     ]
    }
   ],
   "source": [
    "correct_pred = 0\n",
    "for _, (data, label) in enumerate(test_loader):\n",
    "    prediction = net.forward(data)\n",
    "    _, ind = torch.max(prediction,1)\n",
    "    if ind.data == label.data:\n",
    "        correct_pred +=1\n",
    "print('Correct predictions: '+str(correct_pred/120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:168: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  compared_weight_signs = torch.tensor(torch.ne(new_weight_signs, old_weight_signs).detach(), dtype=torch.float32)\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:173: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_weights = new_weight_signs*torch.tensor(torch.ge(torch.abs(self.m_accumulated),self.rho*max_weight_elem*torch.ones_like(self.m_accumulated)).detach(),dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import loadMNIST\n",
    "from Networks.ResNet import ConvNet, FCMSANet\n",
    "\n",
    "#net = ConvNet(3, [1], [3,6], num_fc=2, sizes_fc=[100,10])\n",
    "net = FCMSANet(num_fc=4,sizes_fc=[784,1024,1024,1024,10], bias=False, batchnorm=True, test=False)\n",
    "train_loader, test_loader = loadMNIST('Dataset/input/mnist_train.csv', 'Dataset/input/mnist_test.csv')\n",
    "#net.train_backprop(1,train_loader)\n",
    "#net.test(test_loader,10000)\n",
    "print(train_loader.batch_size)\n",
    "\n",
    "net.train_msa(1,train_loader)\n",
    "\n",
    "#for index, (data, target) in enumerate(train_loader):\n",
    "#    if index == 1:\n",
    "#        print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.test(test_loader,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1574, -0.1913,  0.3191],\n",
      "        [ 0.4584,  0.3066, -0.3247],\n",
      "        [ 0.2495,  0.4553,  0.0500],\n",
      "        [ 0.3111, -0.3222,  0.2152]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.4820, -0.5667, -0.2615,  0.5324], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "test_layer = torch.nn.Linear(3, 4)\n",
    "print(test_layer.weight)\n",
    "print(test_layer.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6., 18., 36.], grad_fn=<SumBackward2>)\n",
      "tensor([[1., 1., 1.],\n",
      "        [2., 2., 2.],\n",
      "        [3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([[1,2,3],[2,3,4],[3,4,5]],dtype=torch.float32, requires_grad=True)\n",
    "y = torch.tensor([[1,1,1],[2,2,2],[3,3,3]],dtype=torch.float32)\n",
    "\n",
    "z = torch.sum(x*y, dim=1)\n",
    "print(z)\n",
    "#z.backward(torch.FloatTensor([0,0,1]), retain_graph=True)\n",
    "z.backward(torch.FloatTensor([1,1,1]), retain_graph=True)\n",
    "#z[0].backward(retain_graph=True)\n",
    "#z[1].backward(retain_graph=True)\n",
    "#z[2].backward(retain_graph=True)\n",
    "print(x.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
