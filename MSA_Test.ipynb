{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.808340  #  0.552083  #\n",
      "Correct predictions: 0.7833333333333333\n",
      "#  2  #  0.411628  #  0.831250  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  3  #  0.285570  #  0.945833  #\n",
      "Correct predictions: 0.9083333333333333\n",
      "#  4  #  0.200868  #  0.981250  #\n",
      "Correct predictions: 0.9666666666666667\n",
      "#  5  #  0.242199  #  0.945833  #\n",
      "Correct predictions: 0.975\n",
      "#  6  #  0.210706  #  0.981250  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  7  #  0.196120  #  0.972917  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  8  #  0.181256  #  0.983333  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  9  #  0.163337  #  0.989583  #\n",
      "Correct predictions: 1.0\n",
      "#  10  #  0.165084  #  0.989583  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  11  #  0.162165  #  0.989583  #\n",
      "Correct predictions: 0.9833333333333333\n",
      "#  12  #  0.162648  #  0.991667  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  13  #  0.172137  #  0.991667  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  14  #  0.166049  #  0.987500  #\n",
      "Correct predictions: 1.0\n",
      "#  15  #  0.202394  #  0.962500  #\n",
      "Correct predictions: 1.0\n",
      "#  16  #  0.156602  #  0.991667  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  17  #  0.178343  #  0.968750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  18  #  0.158333  #  0.987500  #\n",
      "Correct predictions: 1.0\n",
      "#  19  #  0.148993  #  0.989583  #\n",
      "Correct predictions: 1.0\n",
      "#  20  #  0.159728  #  0.989583  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  21  #  0.162459  #  0.987500  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  22  #  0.155231  #  0.989583  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  23  #  0.142529  #  1.000000  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  24  #  0.144426  #  0.995833  #\n",
      "Correct predictions: 0.975\n",
      "#  25  #  0.153088  #  0.991667  #\n",
      "Correct predictions: 0.975\n",
      "#  26  #  0.153835  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  27  #  0.151801  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  28  #  0.168344  #  0.977083  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  29  #  0.143604  #  0.995833  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  30  #  0.151432  #  0.991667  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  31  #  0.156119  #  0.985417  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  32  #  0.170007  #  0.979167  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  33  #  0.163274  #  0.983333  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  34  #  0.149747  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  35  #  0.155871  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  36  #  0.153484  #  0.989583  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  37  #  0.163589  #  0.985417  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  38  #  0.146085  #  0.995833  #\n",
      "Correct predictions: 0.9583333333333334\n",
      "#  39  #  0.162134  #  0.987500  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  40  #  0.144824  #  0.995833  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  41  #  0.147468  #  0.995833  #\n",
      "Correct predictions: 0.9833333333333333\n",
      "#  42  #  0.147761  #  0.989583  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  43  #  0.157261  #  0.985417  #\n",
      "Correct predictions: 1.0\n",
      "#  44  #  0.145169  #  0.991667  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  45  #  0.148623  #  0.991667  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  46  #  0.146692  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  47  #  0.146538  #  0.989583  #\n",
      "Correct predictions: 1.0\n",
      "#  48  #  0.162144  #  0.985417  #\n",
      "Correct predictions: 0.9833333333333333\n",
      "#  49  #  0.141463  #  0.995833  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  50  #  0.155941  #  0.985417  #\n",
      "Correct predictions: 0.925\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  51  #  0.162672  #  0.987500  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  52  #  0.178063  #  0.975000  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  53  #  0.156318  #  0.991667  #\n",
      "Correct predictions: 1.0\n",
      "#  54  #  0.154580  #  0.985417  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  55  #  0.156357  #  0.985417  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  56  #  0.149931  #  0.989583  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  57  #  0.144883  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  58  #  0.151132  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  59  #  0.161829  #  0.985417  #\n",
      "Correct predictions: 1.0\n",
      "#  60  #  0.153336  #  0.987500  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  61  #  0.150468  #  0.989583  #\n",
      "Correct predictions: 1.0\n",
      "#  62  #  0.148907  #  0.991667  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  63  #  0.147932  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  64  #  0.149734  #  0.989583  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  65  #  0.142000  #  0.997917  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  66  #  0.148420  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  67  #  0.149443  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  68  #  0.144668  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  69  #  0.147300  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  70  #  0.144443  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  71  #  0.152695  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  72  #  0.142211  #  0.997917  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  73  #  0.143431  #  0.995833  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  74  #  0.143691  #  0.995833  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  75  #  0.144267  #  0.995833  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  76  #  0.149862  #  0.991667  #\n",
      "Correct predictions: 1.0\n",
      "#  77  #  0.150395  #  0.995833  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  78  #  0.141215  #  0.995833  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  79  #  0.139038  #  0.997917  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  80  #  0.163974  #  0.983333  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  81  #  0.172182  #  0.975000  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  82  #  0.146684  #  0.995833  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  83  #  0.153879  #  0.987500  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  84  #  0.150215  #  0.987500  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  85  #  0.151643  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  86  #  0.150531  #  0.991667  #\n",
      "Correct predictions: 1.0\n",
      "#  87  #  0.142617  #  0.995833  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  88  #  0.148605  #  0.989583  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  89  #  0.154771  #  0.989583  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  90  #  0.139640  #  0.997917  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  91  #  0.147058  #  0.987500  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  92  #  0.144845  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  93  #  0.157150  #  0.989583  #\n",
      "Correct predictions: 1.0\n",
      "#  94  #  0.156387  #  0.987500  #\n",
      "Correct predictions: 1.0\n",
      "#  95  #  0.154718  #  0.989583  #\n",
      "Correct predictions: 1.0\n",
      "#  96  #  0.139922  #  0.995833  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  97  #  0.150784  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  98  #  0.139827  #  0.995833  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  99  #  0.142193  #  0.997917  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  100  #  0.139270  #  1.000000  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "Time elapsed:  31.295864621999996\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import FCMSANet\n",
    "import torch\n",
    "\n",
    "\n",
    "def make_uniform_layer_list(layers, num_features):\n",
    "    return [num_features] * (layers+1)\n",
    "    \n",
    "def make_uniform_hidden_layer_list(layers, num_features, num_classes, size_hidden):\n",
    "    res = [num_features]\n",
    "    res.extend([size_hidden]*(layers-1))\n",
    "    res.extend([num_classes])\n",
    "    return res\n",
    "\n",
    "\n",
    "dataset_size = 600\n",
    "batch_size = 50\n",
    "test_set_size = dataset_size * 0.2\n",
    "\n",
    "\n",
    "#torch.manual_seed(0)\n",
    "train_loader, test_loader = makeMoonsDataset(dataset_size, batch_size)\n",
    "#torch.manual_seed(3)\n",
    "\n",
    "num_layers = 10\n",
    "#layers = make_uniform_layer_list(num_layers, 2)\n",
    "layers = make_uniform_hidden_layer_list(num_layers, 2, 2, 64)\n",
    "#print(layers)\n",
    "net = FCMSANet(num_fc=num_layers, sizes_fc=layers, bias=False, batchnorm=True, test=False)   \n",
    "#net = FCMSANet(num_fc=3,sizes_fc=[2,2,4,2], bias=False, batchnorm=True, test=False)  \n",
    "net.set_rho(0.5)\n",
    "net.set_ema_alpha(0.99)\n",
    "net.set_test_tracking(True)\n",
    "net.train_msa(100,train_loader, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  nan  #  0.514583  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  2  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  3  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  4  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  5  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  6  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  7  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  8  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  9  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  10  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  11  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  12  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  13  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  14  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  15  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  16  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  17  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  18  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  19  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  20  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  21  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  22  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  23  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  24  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  25  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  26  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  27  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  28  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  29  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  30  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  31  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  32  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  33  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  34  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  35  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  36  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  37  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  38  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  39  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  40  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  41  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  42  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  43  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  44  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  45  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  46  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  47  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  48  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  49  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  50  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  51  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  52  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  53  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  54  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  55  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  56  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  57  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  58  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  59  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  60  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  61  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  62  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  63  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  64  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  65  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  66  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  67  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  68  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  69  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  70  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  71  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  72  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  73  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  74  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  75  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  76  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  77  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  78  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  79  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  80  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  81  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  82  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  83  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  84  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  85  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  86  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  87  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  88  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  89  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  90  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  91  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  92  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  93  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  94  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  95  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  96  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  97  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  98  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  99  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  100  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "Time elapsed:  8.439392241000178\n",
      "tensor([0.5146, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104,\n",
      "        0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104,\n",
      "        0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104,\n",
      "        0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104,\n",
      "        0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104,\n",
      "        0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104,\n",
      "        0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104,\n",
      "        0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104,\n",
      "        0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104,\n",
      "        0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104,\n",
      "        0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104, 0.5104,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0.5104])\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import ResAntiSymNet\n",
    "import torch\n",
    "#import gc\n",
    "#gc.collect()\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset(600,40)\n",
    "gamma = 0.3\n",
    "h = 1\n",
    "net = ResAntiSymNet(features=2, classes=2, num_layers=21, gamma=gamma, h=h, bias=True, hidden_size=8)\n",
    "net.set_test_tracking(True)\n",
    "net.train(num_epochs=100, dataloader=train_loader, testloader=test_loader)\n",
    "print(net.avg_correct_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.029316  #  0.464583  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  2  #  0.017411  #  0.497917  #\n",
      "Correct predictions: 0.5416666666666666\n",
      "#  3  #  0.017303  #  0.531250  #\n",
      "Correct predictions: 0.6833333333333333\n",
      "#  4  #  0.016103  #  0.572917  #\n",
      "Correct predictions: 0.825\n",
      "#  5  #  0.009681  #  0.837500  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  6  #  0.009520  #  0.841667  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  7  #  0.007282  #  0.881250  #\n",
      "Correct predictions: 0.875\n",
      "#  8  #  0.007659  #  0.862500  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  9  #  0.007233  #  0.870833  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  10  #  0.007105  #  0.885417  #\n",
      "Correct predictions: 0.85\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  11  #  0.007785  #  0.862500  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  12  #  0.007215  #  0.870833  #\n",
      "Correct predictions: 0.85\n",
      "#  13  #  0.007417  #  0.864583  #\n",
      "Correct predictions: 0.85\n",
      "#  14  #  0.007197  #  0.864583  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  15  #  0.006995  #  0.877083  #\n",
      "Correct predictions: 0.875\n",
      "#  16  #  0.007821  #  0.866667  #\n",
      "Correct predictions: 0.875\n",
      "#  17  #  0.007211  #  0.872917  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  18  #  0.006832  #  0.875000  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  19  #  0.007244  #  0.879167  #\n",
      "Correct predictions: 0.875\n",
      "#  20  #  0.006833  #  0.868750  #\n",
      "Correct predictions: 0.875\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  21  #  0.006654  #  0.887500  #\n",
      "Correct predictions: 0.875\n",
      "#  22  #  0.006578  #  0.877083  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  23  #  0.006915  #  0.875000  #\n",
      "Correct predictions: 0.85\n",
      "#  24  #  0.007495  #  0.868750  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  25  #  0.007306  #  0.877083  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  26  #  0.007292  #  0.870833  #\n",
      "Correct predictions: 0.875\n",
      "#  27  #  0.006754  #  0.877083  #\n",
      "Correct predictions: 0.875\n",
      "#  28  #  0.006647  #  0.877083  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  29  #  0.007025  #  0.870833  #\n",
      "Correct predictions: 0.85\n",
      "#  30  #  0.007614  #  0.866667  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  31  #  0.006785  #  0.868750  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  32  #  0.006839  #  0.881250  #\n",
      "Correct predictions: 0.875\n",
      "#  33  #  0.007159  #  0.868750  #\n",
      "Correct predictions: 0.875\n",
      "#  34  #  0.006778  #  0.877083  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  35  #  0.007097  #  0.875000  #\n",
      "Correct predictions: 0.85\n",
      "#  36  #  0.006668  #  0.879167  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  37  #  0.006747  #  0.877083  #\n",
      "Correct predictions: 0.875\n",
      "#  38  #  0.007414  #  0.870833  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  39  #  0.006785  #  0.881250  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  40  #  0.007066  #  0.866667  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  41  #  0.006759  #  0.872917  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  42  #  0.006536  #  0.877083  #\n",
      "Correct predictions: 0.875\n",
      "#  43  #  0.006759  #  0.872917  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  44  #  0.006835  #  0.875000  #\n",
      "Correct predictions: 0.8833333333333333\n",
      "#  45  #  0.006507  #  0.870833  #\n",
      "Correct predictions: 0.875\n",
      "#  46  #  0.006652  #  0.883333  #\n",
      "Correct predictions: 0.875\n",
      "#  47  #  0.006843  #  0.879167  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  48  #  0.006360  #  0.879167  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  49  #  0.006938  #  0.872917  #\n",
      "Correct predictions: 0.875\n",
      "#  50  #  0.006670  #  0.872917  #\n",
      "Correct predictions: 0.875\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  51  #  0.006691  #  0.868750  #\n",
      "Correct predictions: 0.875\n",
      "#  52  #  0.006696  #  0.866667  #\n",
      "Correct predictions: 0.875\n",
      "#  53  #  0.006857  #  0.868750  #\n",
      "Correct predictions: 0.875\n",
      "#  54  #  0.006796  #  0.875000  #\n",
      "Correct predictions: 0.875\n",
      "#  55  #  0.006683  #  0.860417  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  56  #  0.006782  #  0.872917  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  57  #  0.006996  #  0.864583  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  58  #  0.006986  #  0.860417  #\n",
      "Correct predictions: 0.875\n",
      "#  59  #  0.006545  #  0.868750  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  60  #  0.006572  #  0.879167  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  61  #  0.006718  #  0.872917  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  62  #  0.006520  #  0.870833  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  63  #  0.006743  #  0.866667  #\n",
      "Correct predictions: 0.875\n",
      "#  64  #  0.006683  #  0.870833  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  65  #  0.006950  #  0.862500  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  66  #  0.006552  #  0.879167  #\n",
      "Correct predictions: 0.85\n",
      "#  67  #  0.006942  #  0.872917  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  68  #  0.006742  #  0.866667  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  69  #  0.006486  #  0.875000  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  70  #  0.006788  #  0.879167  #\n",
      "Correct predictions: 0.875\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  71  #  0.006659  #  0.872917  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  72  #  0.006510  #  0.875000  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  73  #  0.006604  #  0.870833  #\n",
      "Correct predictions: 0.875\n",
      "#  74  #  0.006660  #  0.872917  #\n",
      "Correct predictions: 0.85\n",
      "#  75  #  0.006817  #  0.866667  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  76  #  0.006496  #  0.866667  #\n",
      "Correct predictions: 0.875\n",
      "#  77  #  0.006636  #  0.864583  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  78  #  0.006638  #  0.881250  #\n",
      "Correct predictions: 0.875\n",
      "#  79  #  0.006558  #  0.877083  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  80  #  0.006843  #  0.868750  #\n",
      "Correct predictions: 0.875\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  81  #  0.006438  #  0.872917  #\n",
      "Correct predictions: 0.85\n",
      "#  82  #  0.006646  #  0.879167  #\n",
      "Correct predictions: 0.875\n",
      "#  83  #  0.006586  #  0.868750  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  84  #  0.006706  #  0.870833  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  85  #  0.006533  #  0.872917  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  86  #  0.006533  #  0.870833  #\n",
      "Correct predictions: 0.875\n",
      "#  87  #  0.006815  #  0.870833  #\n",
      "Correct predictions: 0.875\n",
      "#  88  #  0.006755  #  0.872917  #\n",
      "Correct predictions: 0.875\n",
      "#  89  #  0.006579  #  0.877083  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  90  #  0.006477  #  0.877083  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  91  #  0.006645  #  0.875000  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  92  #  0.006589  #  0.870833  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  93  #  0.006683  #  0.872917  #\n",
      "Correct predictions: 0.85\n",
      "#  94  #  0.006831  #  0.868750  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  95  #  0.006837  #  0.854167  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  96  #  0.006501  #  0.877083  #\n",
      "Correct predictions: 0.875\n",
      "#  97  #  0.006604  #  0.870833  #\n",
      "Correct predictions: 0.875\n",
      "#  98  #  0.006433  #  0.877083  #\n",
      "Correct predictions: 0.875\n",
      "#  99  #  0.006663  #  0.877083  #\n",
      "Correct predictions: 0.8666666666666667\n",
      "#  100  #  0.006481  #  0.879167  #\n",
      "Correct predictions: 0.85\n",
      "Time elapsed:  8.941094796000016\n",
      "tensor([0.4646, 0.4979, 0.5312, 0.5729, 0.8375, 0.8417, 0.8813, 0.8625, 0.8708,\n",
      "        0.8854, 0.8625, 0.8708, 0.8646, 0.8646, 0.8771, 0.8667, 0.8729, 0.8750,\n",
      "        0.8792, 0.8687, 0.8875, 0.8771, 0.8750, 0.8687, 0.8771, 0.8708, 0.8771,\n",
      "        0.8771, 0.8708, 0.8667, 0.8687, 0.8813, 0.8687, 0.8771, 0.8750, 0.8792,\n",
      "        0.8771, 0.8708, 0.8813, 0.8667, 0.8729, 0.8771, 0.8729, 0.8750, 0.8708,\n",
      "        0.8833, 0.8792, 0.8792, 0.8729, 0.8729, 0.8687, 0.8667, 0.8687, 0.8750,\n",
      "        0.8604, 0.8729, 0.8646, 0.8604, 0.8687, 0.8792, 0.8729, 0.8708, 0.8667,\n",
      "        0.8708, 0.8625, 0.8792, 0.8729, 0.8667, 0.8750, 0.8792, 0.8729, 0.8750,\n",
      "        0.8708, 0.8729, 0.8667, 0.8667, 0.8646, 0.8813, 0.8771, 0.8687, 0.8729,\n",
      "        0.8792, 0.8687, 0.8708, 0.8729, 0.8708, 0.8708, 0.8729, 0.8771, 0.8771,\n",
      "        0.8750, 0.8708, 0.8729, 0.8687, 0.8542, 0.8771, 0.8708, 0.8771, 0.8771,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0.8792])\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.3\n",
    "h = 1\n",
    "net = None\n",
    "net = ResAntiSymNet(features=2, classes=2, num_layers=21, gamma=gamma, h=h, bias=True, hidden_size=8)\n",
    "net.set_test_tracking(True)\n",
    "net.train(num_epochs=100, dataloader=train_loader, testloader=test_loader)\n",
    "print(net.avg_correct_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.017344  #  0.506250  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  2  #  0.017221  #  0.543750  #\n",
      "Correct predictions: 0.7416666666666667\n",
      "#  3  #  0.017083  #  0.758333  #\n",
      "Correct predictions: 0.725\n",
      "#  4  #  0.016913  #  0.727083  #\n",
      "Correct predictions: 0.7416666666666667\n",
      "#  5  #  0.016583  #  0.750000  #\n",
      "Correct predictions: 0.725\n",
      "#  6  #  0.015957  #  0.750000  #\n",
      "Correct predictions: 0.7583333333333333\n",
      "#  7  #  0.014648  #  0.770833  #\n",
      "Correct predictions: 0.775\n",
      "#  8  #  0.012616  #  0.783333  #\n",
      "Correct predictions: 0.7833333333333333\n",
      "#  9  #  0.010690  #  0.810417  #\n",
      "Correct predictions: 0.8083333333333333\n",
      "#  10  #  0.009303  #  0.839583  #\n",
      "Correct predictions: 0.8166666666666667\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  11  #  0.008267  #  0.854167  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  12  #  0.007619  #  0.875000  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  13  #  0.007144  #  0.887500  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  14  #  0.007082  #  0.872917  #\n",
      "Correct predictions: 0.875\n",
      "#  15  #  0.006887  #  0.883333  #\n",
      "Correct predictions: 0.875\n",
      "#  16  #  0.006328  #  0.891667  #\n",
      "Correct predictions: 0.8833333333333333\n",
      "#  17  #  0.006506  #  0.889583  #\n",
      "Correct predictions: 0.8916666666666667\n",
      "#  18  #  0.005510  #  0.906250  #\n",
      "Correct predictions: 0.9166666666666666\n",
      "#  19  #  0.005588  #  0.910417  #\n",
      "Correct predictions: 0.8583333333333333\n",
      "#  20  #  0.005392  #  0.897917  #\n",
      "Correct predictions: 0.9\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  21  #  0.004840  #  0.912500  #\n",
      "Correct predictions: 0.9333333333333333\n",
      "#  22  #  0.009136  #  0.854167  #\n",
      "Correct predictions: 0.925\n",
      "#  23  #  0.004225  #  0.935417  #\n",
      "Correct predictions: 0.9416666666666667\n",
      "#  24  #  0.003940  #  0.943750  #\n",
      "Correct predictions: 0.9416666666666667\n",
      "#  25  #  0.003696  #  0.950000  #\n",
      "Correct predictions: 0.95\n",
      "#  26  #  0.002610  #  0.962500  #\n",
      "Correct predictions: 0.95\n",
      "#  27  #  0.001950  #  0.979167  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  28  #  0.001676  #  0.977083  #\n",
      "Correct predictions: 0.9833333333333333\n",
      "#  29  #  0.001806  #  0.972917  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  30  #  0.000944  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  31  #  0.001162  #  0.989583  #\n",
      "Correct predictions: 1.0\n",
      "#  32  #  0.000738  #  0.991667  #\n",
      "Correct predictions: 1.0\n",
      "#  33  #  0.000679  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  34  #  0.000604  #  0.991667  #\n",
      "Correct predictions: 1.0\n",
      "#  35  #  0.000508  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  36  #  0.000421  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  37  #  0.010973  #  0.916667  #\n",
      "Correct predictions: 0.85\n",
      "#  38  #  0.004246  #  0.941667  #\n",
      "Correct predictions: 1.0\n",
      "#  39  #  0.001160  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  40  #  0.000843  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  41  #  0.000613  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  42  #  0.000616  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  43  #  0.000382  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  44  #  0.000414  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  45  #  0.000318  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  46  #  0.000278  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  47  #  0.000257  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  48  #  0.000217  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  49  #  0.000250  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  50  #  0.000203  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  51  #  0.000221  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  52  #  0.000165  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  53  #  0.000153  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  54  #  0.000189  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  55  #  0.000143  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  56  #  0.000138  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  57  #  0.000133  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  58  #  0.000194  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  59  #  0.000188  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  60  #  0.000139  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  61  #  0.000125  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  62  #  0.000165  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  63  #  0.000149  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  64  #  0.000108  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  65  #  0.000215  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  66  #  0.000100  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  67  #  0.000079  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  68  #  0.000134  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  69  #  0.000129  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  70  #  0.000067  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  71  #  0.000115  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  72  #  0.000141  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  73  #  0.000085  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  74  #  0.000079  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  75  #  0.000100  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  76  #  0.000092  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  77  #  0.000053  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  78  #  0.000077  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  79  #  0.000098  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  80  #  0.000071  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  81  #  0.000079  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  82  #  0.000053  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  83  #  0.000093  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  84  #  0.000060  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  85  #  0.000101  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  86  #  0.000095  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  87  #  0.000108  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  88  #  0.000078  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  89  #  0.000092  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  90  #  0.000062  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  91  #  0.000261  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  92  #  0.000047  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  93  #  0.000040  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  94  #  0.000057  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  95  #  0.000046  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  96  #  0.000077  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  97  #  0.000044  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  98  #  0.000041  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  99  #  0.000043  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  100  #  0.000064  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "Time elapsed:  4.904766947000098\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import FCNet\n",
    "import torch\n",
    "\n",
    "dataset_size = 600\n",
    "batch_size = 40\n",
    "test_set_size = dataset_size * 0.2\n",
    "num_layers = 11\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset(dataset_size,batch_size)\n",
    "layers = make_uniform_hidden_layer_list(num_layers, 2, 2, 64)\n",
    "# net = FCNet(num_layers=num_layers, layers=layers, bias=True)\n",
    "net = FCNet(num_layers=6, layers=[2,16,32,64,128,128,2], bias=True)\n",
    "net.set_test_tracking(True)\n",
    "net.train(100, train_loader, test_loader)\n",
    "\n",
    "#net.test(test_loader, test_set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.472702  #  0.641667  #\n",
      "#  2  #  0.005676  #  0.914583  #\n",
      "#  3  #  0.003665  #  0.952083  #\n",
      "#  4  #  0.002344  #  0.975000  #\n",
      "#  5  #  0.001609  #  0.987500  #\n",
      "#  6  #  0.001037  #  0.991667  #\n",
      "#  7  #  0.000751  #  0.995833  #\n",
      "#  8  #  0.000607  #  0.995833  #\n",
      "#  9  #  0.000501  #  0.995833  #\n",
      "#  10  #  0.000413  #  0.997917  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  11  #  0.000426  #  0.993750  #\n",
      "#  12  #  0.000313  #  0.997917  #\n",
      "#  13  #  0.000319  #  0.995833  #\n",
      "#  14  #  0.000263  #  0.995833  #\n",
      "#  15  #  0.000228  #  1.000000  #\n",
      "#  16  #  0.000187  #  0.997917  #\n",
      "#  17  #  0.000175  #  0.997917  #\n",
      "#  18  #  0.000161  #  1.000000  #\n",
      "#  19  #  0.000157  #  1.000000  #\n",
      "#  20  #  0.000161  #  0.997917  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  21  #  0.000129  #  1.000000  #\n",
      "#  22  #  0.000172  #  1.000000  #\n",
      "#  23  #  0.000179  #  0.997917  #\n",
      "#  24  #  0.000146  #  0.997917  #\n",
      "#  25  #  0.000090  #  1.000000  #\n",
      "#  26  #  0.000087  #  1.000000  #\n",
      "#  27  #  0.000106  #  0.997917  #\n",
      "#  28  #  0.000091  #  1.000000  #\n",
      "#  29  #  0.000135  #  0.997917  #\n",
      "#  30  #  0.000126  #  0.997917  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  31  #  0.000100  #  1.000000  #\n",
      "#  32  #  0.000090  #  1.000000  #\n",
      "#  33  #  0.000109  #  0.997917  #\n",
      "#  34  #  0.000074  #  1.000000  #\n",
      "#  35  #  0.000074  #  1.000000  #\n",
      "#  36  #  0.000158  #  0.997917  #\n",
      "#  37  #  0.000055  #  1.000000  #\n",
      "#  38  #  0.000051  #  1.000000  #\n",
      "#  39  #  0.000048  #  1.000000  #\n",
      "#  40  #  0.000041  #  1.000000  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  41  #  0.000037  #  1.000000  #\n",
      "#  42  #  0.000050  #  1.000000  #\n",
      "#  43  #  0.000072  #  0.997917  #\n",
      "#  44  #  0.000137  #  0.995833  #\n",
      "#  45  #  0.000062  #  1.000000  #\n",
      "#  46  #  0.000165  #  0.995833  #\n",
      "#  47  #  0.000074  #  1.000000  #\n",
      "#  48  #  0.000147  #  0.995833  #\n",
      "#  49  #  0.000153  #  0.995833  #\n",
      "#  50  #  0.000083  #  0.997917  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  51  #  0.000075  #  1.000000  #\n",
      "#  52  #  0.000029  #  1.000000  #\n",
      "#  53  #  0.000053  #  1.000000  #\n",
      "#  54  #  0.000063  #  0.997917  #\n",
      "#  55  #  0.000051  #  1.000000  #\n",
      "#  56  #  0.000056  #  1.000000  #\n",
      "#  57  #  0.000041  #  1.000000  #\n",
      "#  58  #  0.000039  #  1.000000  #\n",
      "#  59  #  0.000030  #  1.000000  #\n",
      "#  60  #  0.000038  #  1.000000  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  61  #  0.000067  #  1.000000  #\n",
      "#  62  #  0.000058  #  0.997917  #\n",
      "#  63  #  0.000214  #  0.993750  #\n",
      "#  64  #  0.000050  #  1.000000  #\n",
      "#  65  #  0.000022  #  1.000000  #\n",
      "#  66  #  0.000025  #  1.000000  #\n",
      "#  67  #  0.000024  #  1.000000  #\n",
      "#  68  #  0.000030  #  1.000000  #\n",
      "#  69  #  0.000020  #  1.000000  #\n",
      "#  70  #  0.000021  #  1.000000  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  71  #  0.000028  #  1.000000  #\n",
      "#  72  #  0.000018  #  1.000000  #\n",
      "#  73  #  0.000019  #  1.000000  #\n",
      "#  74  #  0.000018  #  1.000000  #\n",
      "#  75  #  0.000019  #  1.000000  #\n",
      "#  76  #  0.000014  #  1.000000  #\n",
      "#  77  #  0.000015  #  1.000000  #\n",
      "#  78  #  0.000015  #  1.000000  #\n",
      "#  79  #  0.000020  #  1.000000  #\n",
      "#  80  #  0.000014  #  1.000000  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  81  #  0.000014  #  1.000000  #\n",
      "#  82  #  0.000013  #  1.000000  #\n",
      "#  83  #  0.000018  #  1.000000  #\n",
      "#  84  #  0.000015  #  1.000000  #\n",
      "#  85  #  0.000014  #  1.000000  #\n",
      "#  86  #  0.000012  #  1.000000  #\n",
      "#  87  #  0.000012  #  1.000000  #\n",
      "#  88  #  0.000013  #  1.000000  #\n",
      "#  89  #  0.000011  #  1.000000  #\n",
      "#  90  #  0.000012  #  1.000000  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  91  #  0.000011  #  1.000000  #\n",
      "#  92  #  0.000012  #  1.000000  #\n",
      "#  93  #  0.000012  #  1.000000  #\n",
      "#  94  #  0.000011  #  1.000000  #\n",
      "#  95  #  0.000010  #  1.000000  #\n",
      "#  96  #  0.000012  #  1.000000  #\n",
      "#  97  #  0.000011  #  1.000000  #\n",
      "#  98  #  0.000010  #  1.000000  #\n",
      "#  99  #  0.000010  #  1.000000  #\n",
      "#  100  #  0.000009  #  1.000000  #\n",
      "Time elapsed:  6.256591480996576\n",
      "Correct predictions: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import ResFCNet\n",
    "import torch\n",
    "\n",
    "def make_uniform_layer_list(layers, num_features):\n",
    "        return [num_features] * (layers+1)\n",
    "\n",
    "def make_uniform_hidden_layer_list(layers, num_features, num_classes, size_hidden):\n",
    "    res = [num_features]\n",
    "    res.extend([size_hidden]*(layers-1))\n",
    "    res.extend([num_classes])\n",
    "    return res\n",
    "\n",
    "\n",
    "dataset_size = 600\n",
    "batch_size = 40\n",
    "test_set_size = dataset_size * 0.2\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset(dataset_size,batch_size)\n",
    "\n",
    "num_layers = 14\n",
    "#layers = make_uniform_layer_list(num_layers, 2)\n",
    "layers = make_uniform_hidden_layer_list(num_layers, 2, 2, 64)\n",
    "print(len(layers))\n",
    "net = ResFCNet(num_layers=num_layers, layers=layers, bias=True)\n",
    "\n",
    "#net = ResFCNet(num_layers=30, layers=[2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2], bias=True)\n",
    "net.train(100, train_loader)\n",
    "\n",
    "net.test(test_loader, test_set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:340: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:343: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  compared_weight_signs = torch.tensor(torch.ne(new_weight_signs, old_weight_signs).detach(), dtype=torch.float32)\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:148: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_weights = new_weight_signs*torch.tensor(torch.ge(torch.abs(self.m_accumulated),self.rho*max_weight_elem*torch.ones_like(self.m_accumulated)).detach(),dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed:  10.146643088\n",
      "Seed 0   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.60604992\n",
      "Seed 1   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.699145381999998\n",
      "Seed 2   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.273395311000002\n",
      "Seed 3   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.354167834000002\n",
      "Seed 4   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.854717102000002\n",
      "Seed 5   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.360114429000006\n",
      "Seed 6   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.212407759000001\n",
      "Seed 7   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.896629595000007\n",
      "Seed 8   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.603672412999998\n",
      "Seed 9   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.097800613000004\n",
      "Seed 10   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.248547408000007\n",
      "Seed 11   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.193569678999978\n",
      "Seed 12   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.238679597000015\n",
      "Seed 13   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.202587755999986\n",
      "Seed 14   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.563680431999984\n",
      "Seed 15   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.800362409000002\n",
      "Seed 16   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.172943199000002\n",
      "Seed 17   ###   Best-Avg 0.8\n",
      "Time elapsed:  11.664164929999998\n",
      "Seed 18   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.64493668099999\n",
      "Seed 19   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.808216150000021\n",
      "Seed 20   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.073397882000023\n",
      "Seed 21   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.22652783800001\n",
      "Seed 22   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.820626478999998\n",
      "Seed 23   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.476716109999984\n",
      "Seed 24   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.439701024999977\n",
      "Seed 25   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.500485010999967\n",
      "Seed 26   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.450659146999953\n",
      "Seed 27   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.663561942000001\n",
      "Seed 28   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.897770284000046\n",
      "Seed 29   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.072501369000008\n",
      "Seed 30   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.462130296999987\n",
      "Seed 31   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.181674433000012\n",
      "Seed 32   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.720457097999997\n",
      "Seed 33   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.911982378000005\n",
      "Seed 34   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.12446210600001\n",
      "Seed 35   ###   Best-Avg 0.8\n",
      "Time elapsed:  11.751844816999949\n",
      "Seed 36   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.914494053999988\n",
      "Seed 37   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.721946830999968\n",
      "Seed 38   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.854742291999969\n",
      "Seed 39   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.348190395000017\n",
      "Seed 40   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.857938689000036\n",
      "Seed 41   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.154126195999993\n",
      "Seed 42   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.182050207999964\n",
      "Seed 43   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.263982440000007\n",
      "Seed 44   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.456949645000066\n",
      "Seed 45   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.78960939500007\n",
      "Seed 46   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.985499520000076\n",
      "Seed 47   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.699192676000052\n",
      "Seed 48   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.425626183999952\n",
      "Seed 49   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.54046722499993\n",
      "Seed 50   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.030507655000065\n",
      "Seed 51   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.125098506000086\n",
      "Seed 52   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.919794993999972\n",
      "Seed 53   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.273062202999995\n",
      "Seed 54   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.648992066000005\n",
      "Seed 55   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.420492824999997\n",
      "Seed 56   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.824312771999985\n",
      "Seed 57   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.768213887999991\n",
      "Seed 58   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.478139016\n",
      "Seed 59   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.264060061999999\n",
      "Seed 60   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.428517747\n",
      "Seed 61   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.804717488000051\n",
      "Seed 62   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.830327735999958\n",
      "Seed 63   ###   Best-Avg 0.84375\n",
      "Time elapsed:  13.149567551000018\n",
      "Seed 64   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.077774036999926\n",
      "Seed 65   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.304072090999966\n",
      "Seed 66   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.241903238999953\n",
      "Seed 67   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.745793354000057\n",
      "Seed 68   ###   Best-Avg 0.84375\n",
      "Time elapsed:  13.113821154999982\n",
      "Seed 69   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.56083924699999\n",
      "Seed 70   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.023275922000039\n",
      "Seed 71   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.34733552099999\n",
      "Seed 72   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.205158547999986\n",
      "Seed 73   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.781024666000008\n",
      "Seed 74   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.818582595000066\n",
      "Seed 75   ###   Best-Avg 0.8\n",
      "Time elapsed:  12.775833219999981\n",
      "Seed 76   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.596671489999949\n",
      "Seed 77   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.738887018000014\n",
      "Seed 78   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.079107496999995\n",
      "Seed 79   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.085618524999973\n",
      "Seed 80   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.447313674000043\n",
      "Seed 81   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.547355056000015\n",
      "Seed 82   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.684601208999993\n",
      "Seed 83   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.680478483000002\n",
      "Seed 84   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.97397130999991\n",
      "Seed 85   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.147157143999948\n",
      "Seed 86   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.294491636999965\n",
      "Seed 87   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.387910954999938\n",
      "Seed 88   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.786750217999952\n",
      "Seed 89   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.385938010000018\n",
      "Seed 90   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.857001564999791\n",
      "Seed 91   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.818907478000028\n",
      "Seed 92   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.13523568200003\n",
      "Seed 93   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.777255096999852\n",
      "Seed 94   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.35629602400013\n",
      "Seed 95   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.564049009999962\n",
      "Seed 96   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.523687422999956\n",
      "Seed 97   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.850773782000033\n",
      "Seed 98   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.6953454840002\n",
      "Seed 99   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.015339932000188\n",
      "Seed 100   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.39123638000001\n",
      "Seed 101   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.594417870000143\n",
      "Seed 102   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.783619105999833\n",
      "Seed 103   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.876350617000071\n",
      "Seed 104   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.439500544000111\n",
      "Seed 105   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.181604522000043\n",
      "Seed 106   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.076896543999965\n",
      "Seed 107   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.832593179000014\n",
      "Seed 108   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.307717772999922\n",
      "Seed 109   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.43920239099998\n",
      "Seed 110   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.498897093999858\n",
      "Seed 111   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.657795781000004\n",
      "Seed 112   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.814789377999887\n",
      "Seed 113   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.931495412999993\n",
      "Seed 114   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.288754776999895\n",
      "Seed 115   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.244204666000087\n",
      "Seed 116   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.550813107000067\n",
      "Seed 117   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.388158215999965\n",
      "Seed 118   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.750045622000016\n",
      "Seed 119   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.235819392000167\n",
      "Seed 120   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.996203699000034\n",
      "Seed 121   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.989480880999963\n",
      "Seed 122   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.056708553999897\n",
      "Seed 123   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.285523938999859\n",
      "Seed 124   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.525031678000005\n",
      "Seed 125   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.853770211999972\n",
      "Seed 126   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.874691244999894\n",
      "Seed 127   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.638378874000182\n",
      "Seed 128   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.042981213999838\n",
      "Seed 129   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.147106766999968\n",
      "Seed 130   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.252185378999911\n",
      "Seed 131   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.136590218000038\n",
      "Seed 132   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.631353791000038\n",
      "Seed 133   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.084455215999924\n",
      "Seed 134   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.278537409000137\n",
      "Seed 135   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.179119578000154\n",
      "Seed 136   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.569564312000011\n",
      "Seed 137   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.627897280999832\n",
      "Seed 138   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.784591183999964\n",
      "Seed 139   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.15707995899993\n",
      "Seed 140   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.181525357000055\n",
      "Seed 141   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.311887792000107\n",
      "Seed 142   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.489734566999914\n",
      "Seed 143   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.879751607999879\n",
      "Seed 144   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.325272771000073\n",
      "Seed 145   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.095041162999905\n",
      "Seed 146   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.463122424999938\n",
      "Seed 147   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.492982883999957\n",
      "Seed 148   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.48772615300004\n",
      "Seed 149   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.673118235000175\n",
      "Seed 150   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.028434469000103\n",
      "Seed 151   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.975339212000108\n",
      "Seed 152   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.189310215000205\n",
      "Seed 153   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.902211723999926\n",
      "Seed 154   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.252953891999823\n",
      "Seed 155   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.101709491000065\n",
      "Seed 156   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.322991391999949\n",
      "Seed 157   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.231441695000058\n",
      "Seed 158   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.062093285999936\n",
      "Seed 159   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.669411380999918\n",
      "Seed 160   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.712783790999993\n",
      "Seed 161   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.289970005000214\n",
      "Seed 162   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.054718644999866\n",
      "Seed 163   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.402621168999985\n",
      "Seed 164   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.74273369599996\n",
      "Seed 165   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.994736071000034\n",
      "Seed 166   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.28945646399984\n",
      "Seed 167   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.997000486000161\n",
      "Seed 168   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.741073809999989\n",
      "Seed 169   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.930781389999993\n",
      "Seed 170   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.058178237999982\n",
      "Seed 171   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.299073929000087\n",
      "Seed 172   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.528790453000056\n",
      "Seed 173   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.644680681000182\n",
      "Seed 174   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.82024248000016\n",
      "Seed 175   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.990435996000087\n",
      "Seed 176   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.8661229679999\n",
      "Seed 177   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.123140467999974\n",
      "Seed 178   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.590671946999919\n",
      "Seed 179   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.689152142000012\n",
      "Seed 180   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.67604732399991\n",
      "Seed 181   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.032750995000015\n",
      "Seed 182   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.450169252000023\n",
      "Seed 183   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.201859338999839\n",
      "Seed 184   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.017197729000145\n",
      "Seed 185   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.891306548999637\n",
      "Seed 186   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.39308081199988\n",
      "Seed 187   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.244145035999736\n",
      "Seed 188   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.604870163000214\n",
      "Seed 189   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.960879856999782\n",
      "Seed 190   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.77327322300016\n",
      "Seed 191   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.03098315699981\n",
      "Seed 192   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.10578955100027\n",
      "Seed 193   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.368450354000288\n",
      "Seed 194   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.467802901000141\n",
      "Seed 195   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.05645203999984\n",
      "Seed 196   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.349908370000321\n",
      "Seed 197   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.354014645000007\n",
      "Seed 198   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.494870496999738\n",
      "Seed 199   ###   Best-Avg 0.825\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import ConvNet, FCMSANet\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset()\n",
    "\n",
    "for seed in range(200):\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    net = FCMSANet(num_fc=2,sizes_fc=[2,4,2], bias=False, test=False)\n",
    "    \n",
    "    net.train_msa(60,train_loader)\n",
    "    print('Seed '+str(seed)+'   ###   Best-Avg '+str(net.best_avg))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  ###   Avg-Loss 0.016693341732025146   ###   Correct predictions 0.7354166666666667\n",
      "Epoch 2  ###   Avg-Loss 0.01539247731367747   ###   Correct predictions 0.7895833333333333\n",
      "Epoch 3  ###   Avg-Loss 0.013269580403963725   ###   Correct predictions 0.7958333333333333\n",
      "Epoch 4  ###   Avg-Loss 0.010812340180079143   ###   Correct predictions 0.8125\n",
      "Epoch 5  ###   Avg-Loss 0.009092676639556884   ###   Correct predictions 0.8375\n",
      "Epoch 6  ###   Avg-Loss 0.007869457205136618   ###   Correct predictions 0.8583333333333333\n",
      "Epoch 7  ###   Avg-Loss 0.007090519865353902   ###   Correct predictions 0.8625\n",
      "Epoch 8  ###   Avg-Loss 0.00648987740278244   ###   Correct predictions 0.8916666666666667\n",
      "Epoch 9  ###   Avg-Loss 0.006088708837827046   ###   Correct predictions 0.8979166666666667\n",
      "Epoch 10  ###   Avg-Loss 0.005733463664849599   ###   Correct predictions 0.9\n",
      "Epoch 11  ###   Avg-Loss 0.005439147353172302   ###   Correct predictions 0.9041666666666667\n",
      "Epoch 12  ###   Avg-Loss 0.005182546377182007   ###   Correct predictions 0.9125\n",
      "Epoch 13  ###   Avg-Loss 0.0049934898813565574   ###   Correct predictions 0.9104166666666667\n",
      "Epoch 14  ###   Avg-Loss 0.004753189285596212   ###   Correct predictions 0.91875\n",
      "Epoch 15  ###   Avg-Loss 0.004416432480017344   ###   Correct predictions 0.9229166666666667\n",
      "Epoch 16  ###   Avg-Loss 0.004125663638114929   ###   Correct predictions 0.93125\n",
      "Epoch 17  ###   Avg-Loss 0.003898448000351588   ###   Correct predictions 0.93125\n",
      "Epoch 18  ###   Avg-Loss 0.003615226596593857   ###   Correct predictions 0.9375\n",
      "Epoch 19  ###   Avg-Loss 0.0030299144486586253   ###   Correct predictions 0.95\n",
      "Epoch 20  ###   Avg-Loss 0.0029660664498806   ###   Correct predictions 0.95625\n",
      "Epoch 21  ###   Avg-Loss 0.00286577045917511   ###   Correct predictions 0.9479166666666666\n",
      "Epoch 22  ###   Avg-Loss 0.002026676634947459   ###   Correct predictions 0.9770833333333333\n",
      "Epoch 23  ###   Avg-Loss 0.00176669346789519   ###   Correct predictions 0.9791666666666666\n",
      "Epoch 24  ###   Avg-Loss 0.0016901899129152299   ###   Correct predictions 0.98125\n",
      "Epoch 25  ###   Avg-Loss 0.001406506821513176   ###   Correct predictions 0.9854166666666667\n",
      "Epoch 26  ###   Avg-Loss 0.0012301721920569737   ###   Correct predictions 0.9895833333333334\n",
      "Epoch 27  ###   Avg-Loss 0.0012656405568122863   ###   Correct predictions 0.9895833333333334\n",
      "Epoch 28  ###   Avg-Loss 0.0009948708117008208   ###   Correct predictions 0.9916666666666667\n",
      "Epoch 29  ###   Avg-Loss 0.0008995652198791504   ###   Correct predictions 0.99375\n",
      "Epoch 30  ###   Avg-Loss 0.0008839219808578491   ###   Correct predictions 0.99375\n",
      "Epoch 31  ###   Avg-Loss 0.0007204620788494746   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 32  ###   Avg-Loss 0.0006353583186864853   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 33  ###   Avg-Loss 0.0006963463500142097   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 34  ###   Avg-Loss 0.0006097466374437014   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 35  ###   Avg-Loss 0.0006425640856226285   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 36  ###   Avg-Loss 0.0004952810704708099   ###   Correct predictions 1.0\n",
      "Epoch 37  ###   Avg-Loss 0.00046397863576809565   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 38  ###   Avg-Loss 0.00047715206940968834   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 39  ###   Avg-Loss 0.0004436716747780641   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 40  ###   Avg-Loss 0.00044246241450309756   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 41  ###   Avg-Loss 0.0003799566999077797   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 42  ###   Avg-Loss 0.00037729634592930473   ###   Correct predictions 1.0\n",
      "Epoch 43  ###   Avg-Loss 0.00042406826590498287   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 44  ###   Avg-Loss 0.0003210838573674361   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 45  ###   Avg-Loss 0.00035742980738480884   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 46  ###   Avg-Loss 0.0003395354375243187   ###   Correct predictions 1.0\n",
      "Epoch 47  ###   Avg-Loss 0.0003530730493366718   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 48  ###   Avg-Loss 0.0003453268048663934   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 49  ###   Avg-Loss 0.0003007656273742517   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 50  ###   Avg-Loss 0.0002979370454947154   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 51  ###   Avg-Loss 0.0002575600054115057   ###   Correct predictions 1.0\n",
      "Epoch 52  ###   Avg-Loss 0.0003250787034630775   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 53  ###   Avg-Loss 0.00023143263533711433   ###   Correct predictions 1.0\n",
      "Epoch 54  ###   Avg-Loss 0.0002476739386717478   ###   Correct predictions 1.0\n",
      "Epoch 55  ###   Avg-Loss 0.0002808337099850178   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 56  ###   Avg-Loss 0.00022636189435919126   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 57  ###   Avg-Loss 0.00020725494250655173   ###   Correct predictions 1.0\n",
      "Epoch 58  ###   Avg-Loss 0.0002392620158692201   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 59  ###   Avg-Loss 0.0002071976816902558   ###   Correct predictions 1.0\n",
      "Epoch 60  ###   Avg-Loss 0.00020230164130528768   ###   Correct predictions 1.0\n",
      "Epoch 61  ###   Avg-Loss 0.00018292929356296858   ###   Correct predictions 1.0\n",
      "Epoch 62  ###   Avg-Loss 0.00022179240671296914   ###   Correct predictions 1.0\n",
      "Epoch 63  ###   Avg-Loss 0.00019362818760176498   ###   Correct predictions 1.0\n",
      "Epoch 64  ###   Avg-Loss 0.00022219737681249778   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 65  ###   Avg-Loss 0.00018464084714651108   ###   Correct predictions 1.0\n",
      "Epoch 66  ###   Avg-Loss 0.00021055651207764944   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 67  ###   Avg-Loss 0.00020628821415205796   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 68  ###   Avg-Loss 0.00019506152408818405   ###   Correct predictions 1.0\n",
      "Epoch 69  ###   Avg-Loss 0.0001632713247090578   ###   Correct predictions 1.0\n",
      "Epoch 70  ###   Avg-Loss 0.00017592293831209343   ###   Correct predictions 1.0\n",
      "Epoch 71  ###   Avg-Loss 0.00016277733569343886   ###   Correct predictions 1.0\n",
      "Epoch 72  ###   Avg-Loss 0.0001429748721420765   ###   Correct predictions 1.0\n",
      "Epoch 73  ###   Avg-Loss 0.0001682426780462265   ###   Correct predictions 1.0\n",
      "Epoch 74  ###   Avg-Loss 0.00015255645848810673   ###   Correct predictions 1.0\n",
      "Epoch 75  ###   Avg-Loss 0.00013346169143915176   ###   Correct predictions 1.0\n",
      "Epoch 76  ###   Avg-Loss 0.00015274204003314177   ###   Correct predictions 1.0\n",
      "Epoch 77  ###   Avg-Loss 0.0001632209246357282   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 78  ###   Avg-Loss 0.00016444246284663677   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 79  ###   Avg-Loss 0.0001559902292986711   ###   Correct predictions 1.0\n",
      "Epoch 80  ###   Avg-Loss 0.00012979219512393076   ###   Correct predictions 1.0\n",
      "Epoch 81  ###   Avg-Loss 0.00014563739920655887   ###   Correct predictions 1.0\n",
      "Epoch 82  ###   Avg-Loss 0.00012268397646645704   ###   Correct predictions 1.0\n",
      "Epoch 83  ###   Avg-Loss 0.00014513544738292694   ###   Correct predictions 1.0\n",
      "Epoch 84  ###   Avg-Loss 0.00012508279954393705   ###   Correct predictions 1.0\n",
      "Epoch 85  ###   Avg-Loss 0.00013901602166394394   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 86  ###   Avg-Loss 0.0001333181746304035   ###   Correct predictions 1.0\n",
      "Epoch 87  ###   Avg-Loss 0.00012044358688096205   ###   Correct predictions 1.0\n",
      "Epoch 88  ###   Avg-Loss 0.00014618256439765295   ###   Correct predictions 1.0\n",
      "Epoch 89  ###   Avg-Loss 0.00012452843754241864   ###   Correct predictions 1.0\n",
      "Epoch 90  ###   Avg-Loss 0.00011009617398182551   ###   Correct predictions 1.0\n",
      "Epoch 91  ###   Avg-Loss 0.00012323612657686074   ###   Correct predictions 1.0\n",
      "Epoch 92  ###   Avg-Loss 0.00011581191793084145   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 93  ###   Avg-Loss 0.0001325206986318032   ###   Correct predictions 1.0\n",
      "Epoch 94  ###   Avg-Loss 0.00010437506716698408   ###   Correct predictions 1.0\n",
      "Epoch 95  ###   Avg-Loss 0.00010432829149067401   ###   Correct predictions 1.0\n",
      "Epoch 96  ###   Avg-Loss 0.00011434933015455803   ###   Correct predictions 1.0\n",
      "Epoch 97  ###   Avg-Loss 0.00010554267403980096   ###   Correct predictions 1.0\n",
      "Epoch 98  ###   Avg-Loss 0.00013531527171532312   ###   Correct predictions 1.0\n",
      "Epoch 99  ###   Avg-Loss 9.843817291160425e-05   ###   Correct predictions 1.0\n",
      "Epoch 100  ###   Avg-Loss 9.126464525858562e-05   ###   Correct predictions 1.0\n",
      "Epoch 101  ###   Avg-Loss 8.715117195000251e-05   ###   Correct predictions 1.0\n",
      "Epoch 102  ###   Avg-Loss 9.70254031320413e-05   ###   Correct predictions 1.0\n",
      "Epoch 103  ###   Avg-Loss 0.00010043517686426639   ###   Correct predictions 1.0\n",
      "Epoch 104  ###   Avg-Loss 8.699353784322739e-05   ###   Correct predictions 1.0\n",
      "Epoch 105  ###   Avg-Loss 9.705725436409315e-05   ###   Correct predictions 1.0\n",
      "Epoch 106  ###   Avg-Loss 8.577681922664246e-05   ###   Correct predictions 1.0\n",
      "Epoch 107  ###   Avg-Loss 0.00010882464703172445   ###   Correct predictions 1.0\n",
      "Epoch 108  ###   Avg-Loss 0.00010686044115573167   ###   Correct predictions 1.0\n",
      "Epoch 109  ###   Avg-Loss 0.00010626811999827623   ###   Correct predictions 1.0\n",
      "Epoch 110  ###   Avg-Loss 0.00010530042927712203   ###   Correct predictions 1.0\n",
      "Epoch 111  ###   Avg-Loss 0.00010232297548403342   ###   Correct predictions 1.0\n",
      "Epoch 112  ###   Avg-Loss 8.531966401884954e-05   ###   Correct predictions 1.0\n",
      "Epoch 113  ###   Avg-Loss 8.699455453703801e-05   ###   Correct predictions 1.0\n",
      "Epoch 114  ###   Avg-Loss 8.856581213573615e-05   ###   Correct predictions 1.0\n",
      "Epoch 115  ###   Avg-Loss 7.67768050233523e-05   ###   Correct predictions 1.0\n",
      "Epoch 116  ###   Avg-Loss 8.174949325621129e-05   ###   Correct predictions 1.0\n",
      "Epoch 117  ###   Avg-Loss 7.995864531646172e-05   ###   Correct predictions 1.0\n",
      "Epoch 118  ###   Avg-Loss 7.204515859484672e-05   ###   Correct predictions 1.0\n",
      "Epoch 119  ###   Avg-Loss 8.846645553906758e-05   ###   Correct predictions 1.0\n",
      "Epoch 120  ###   Avg-Loss 8.003233621517817e-05   ###   Correct predictions 1.0\n",
      "Epoch 121  ###   Avg-Loss 7.330861408263444e-05   ###   Correct predictions 1.0\n",
      "Epoch 122  ###   Avg-Loss 6.652111187577248e-05   ###   Correct predictions 1.0\n",
      "Epoch 123  ###   Avg-Loss 7.577384822070599e-05   ###   Correct predictions 1.0\n",
      "Epoch 124  ###   Avg-Loss 6.504503932471076e-05   ###   Correct predictions 1.0\n",
      "Epoch 125  ###   Avg-Loss 9.361816725383202e-05   ###   Correct predictions 1.0\n",
      "Epoch 126  ###   Avg-Loss 6.556252483278513e-05   ###   Correct predictions 1.0\n",
      "Epoch 127  ###   Avg-Loss 6.442156542713443e-05   ###   Correct predictions 1.0\n",
      "Epoch 128  ###   Avg-Loss 7.758416080226501e-05   ###   Correct predictions 1.0\n",
      "Epoch 129  ###   Avg-Loss 7.533366636683544e-05   ###   Correct predictions 1.0\n",
      "Epoch 130  ###   Avg-Loss 6.837865803390741e-05   ###   Correct predictions 1.0\n",
      "Epoch 131  ###   Avg-Loss 7.118641709287961e-05   ###   Correct predictions 1.0\n",
      "Epoch 132  ###   Avg-Loss 8.623798688252766e-05   ###   Correct predictions 1.0\n",
      "Epoch 133  ###   Avg-Loss 6.864879590769608e-05   ###   Correct predictions 1.0\n",
      "Epoch 134  ###   Avg-Loss 7.340631758173307e-05   ###   Correct predictions 1.0\n",
      "Epoch 135  ###   Avg-Loss 5.6213835099091135e-05   ###   Correct predictions 1.0\n",
      "Epoch 136  ###   Avg-Loss 5.9551175218075515e-05   ###   Correct predictions 1.0\n",
      "Epoch 137  ###   Avg-Loss 7.160236903776725e-05   ###   Correct predictions 1.0\n",
      "Epoch 138  ###   Avg-Loss 6.109048845246435e-05   ###   Correct predictions 1.0\n",
      "Epoch 139  ###   Avg-Loss 7.094782777130603e-05   ###   Correct predictions 1.0\n",
      "Epoch 140  ###   Avg-Loss 7.791101622084776e-05   ###   Correct predictions 1.0\n",
      "Epoch 141  ###   Avg-Loss 6.113395793363451e-05   ###   Correct predictions 1.0\n",
      "Epoch 142  ###   Avg-Loss 6.176692356045048e-05   ###   Correct predictions 1.0\n",
      "Epoch 143  ###   Avg-Loss 7.38037284463644e-05   ###   Correct predictions 1.0\n",
      "Epoch 144  ###   Avg-Loss 6.984003509084384e-05   ###   Correct predictions 1.0\n",
      "Epoch 145  ###   Avg-Loss 6.579244509339333e-05   ###   Correct predictions 1.0\n",
      "Epoch 146  ###   Avg-Loss 5.51832839846611e-05   ###   Correct predictions 1.0\n",
      "Epoch 147  ###   Avg-Loss 5.711079575121403e-05   ###   Correct predictions 1.0\n",
      "Epoch 148  ###   Avg-Loss 5.0937927638490996e-05   ###   Correct predictions 1.0\n",
      "Epoch 149  ###   Avg-Loss 5.608038045465946e-05   ###   Correct predictions 1.0\n",
      "Epoch 150  ###   Avg-Loss 6.162119098007679e-05   ###   Correct predictions 1.0\n",
      "Epoch 151  ###   Avg-Loss 5.776884887988369e-05   ###   Correct predictions 1.0\n",
      "Epoch 152  ###   Avg-Loss 6.204020076741776e-05   ###   Correct predictions 1.0\n",
      "Epoch 153  ###   Avg-Loss 5.5288333290567e-05   ###   Correct predictions 1.0\n",
      "Epoch 154  ###   Avg-Loss 4.973360725368063e-05   ###   Correct predictions 1.0\n",
      "Epoch 155  ###   Avg-Loss 5.3552817553281785e-05   ###   Correct predictions 1.0\n",
      "Epoch 156  ###   Avg-Loss 4.901509576787551e-05   ###   Correct predictions 1.0\n",
      "Epoch 157  ###   Avg-Loss 5.7395625238617264e-05   ###   Correct predictions 1.0\n",
      "Epoch 158  ###   Avg-Loss 4.948605395232638e-05   ###   Correct predictions 1.0\n",
      "Epoch 159  ###   Avg-Loss 4.963681955511371e-05   ###   Correct predictions 1.0\n",
      "Epoch 160  ###   Avg-Loss 5.433883440370361e-05   ###   Correct predictions 1.0\n",
      "Epoch 161  ###   Avg-Loss 5.3806684445589784e-05   ###   Correct predictions 1.0\n",
      "Epoch 162  ###   Avg-Loss 4.746093569944302e-05   ###   Correct predictions 1.0\n",
      "Epoch 163  ###   Avg-Loss 7.585322794814905e-05   ###   Correct predictions 1.0\n",
      "Epoch 164  ###   Avg-Loss 4.612901248037815e-05   ###   Correct predictions 1.0\n",
      "Epoch 165  ###   Avg-Loss 4.46078289921085e-05   ###   Correct predictions 1.0\n",
      "Epoch 166  ###   Avg-Loss 4.7858059406280515e-05   ###   Correct predictions 1.0\n",
      "Epoch 167  ###   Avg-Loss 5.252285239597161e-05   ###   Correct predictions 1.0\n",
      "Epoch 168  ###   Avg-Loss 6.123439331228535e-05   ###   Correct predictions 1.0\n",
      "Epoch 169  ###   Avg-Loss 5.4478478462745744e-05   ###   Correct predictions 1.0\n",
      "Epoch 170  ###   Avg-Loss 5.318818924327691e-05   ###   Correct predictions 1.0\n",
      "Epoch 171  ###   Avg-Loss 4.756053676828742e-05   ###   Correct predictions 1.0\n",
      "Epoch 172  ###   Avg-Loss 4.761051774645845e-05   ###   Correct predictions 1.0\n",
      "Epoch 173  ###   Avg-Loss 5.6479397850732006e-05   ###   Correct predictions 1.0\n",
      "Epoch 174  ###   Avg-Loss 4.788977870096763e-05   ###   Correct predictions 1.0\n",
      "Epoch 175  ###   Avg-Loss 6.446146095792452e-05   ###   Correct predictions 1.0\n",
      "Epoch 176  ###   Avg-Loss 4.3726297250638405e-05   ###   Correct predictions 1.0\n",
      "Epoch 177  ###   Avg-Loss 5.430232267826796e-05   ###   Correct predictions 1.0\n",
      "Epoch 178  ###   Avg-Loss 5.736845002199213e-05   ###   Correct predictions 1.0\n",
      "Epoch 179  ###   Avg-Loss 5.051289529850086e-05   ###   Correct predictions 1.0\n",
      "Epoch 180  ###   Avg-Loss 5.14243069725732e-05   ###   Correct predictions 1.0\n",
      "Epoch 181  ###   Avg-Loss 3.913740317026774e-05   ###   Correct predictions 1.0\n",
      "Epoch 182  ###   Avg-Loss 5.9283819670478506e-05   ###   Correct predictions 1.0\n",
      "Epoch 183  ###   Avg-Loss 4.4025426420072714e-05   ###   Correct predictions 1.0\n",
      "Epoch 184  ###   Avg-Loss 4.923796902100245e-05   ###   Correct predictions 1.0\n",
      "Epoch 185  ###   Avg-Loss 4.7920217427114645e-05   ###   Correct predictions 1.0\n",
      "Epoch 186  ###   Avg-Loss 4.310249350965023e-05   ###   Correct predictions 1.0\n",
      "Epoch 187  ###   Avg-Loss 4.640198312699795e-05   ###   Correct predictions 1.0\n",
      "Epoch 188  ###   Avg-Loss 4.455156158655882e-05   ###   Correct predictions 1.0\n",
      "Epoch 189  ###   Avg-Loss 4.795944939057032e-05   ###   Correct predictions 1.0\n",
      "Epoch 190  ###   Avg-Loss 3.5239538798729576e-05   ###   Correct predictions 1.0\n",
      "Epoch 191  ###   Avg-Loss 4.360287372643749e-05   ###   Correct predictions 1.0\n",
      "Epoch 192  ###   Avg-Loss 4.576954136913021e-05   ###   Correct predictions 1.0\n",
      "Epoch 193  ###   Avg-Loss 3.716207187001904e-05   ###   Correct predictions 1.0\n",
      "Epoch 194  ###   Avg-Loss 4.478615010157227e-05   ###   Correct predictions 1.0\n",
      "Epoch 195  ###   Avg-Loss 4.2566253493229546e-05   ###   Correct predictions 1.0\n",
      "Epoch 196  ###   Avg-Loss 4.4678539658586185e-05   ###   Correct predictions 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197  ###   Avg-Loss 3.961453524728616e-05   ###   Correct predictions 1.0\n",
      "Epoch 198  ###   Avg-Loss 4.014573448027174e-05   ###   Correct predictions 1.0\n",
      "Epoch 199  ###   Avg-Loss 4.041891467447082e-05   ###   Correct predictions 1.0\n",
      "Epoch 200  ###   Avg-Loss 3.8891894898066916e-05   ###   Correct predictions 1.0\n",
      "Epoch 201  ###   Avg-Loss 3.913562589635452e-05   ###   Correct predictions 1.0\n",
      "Epoch 202  ###   Avg-Loss 4.7289044596254824e-05   ###   Correct predictions 1.0\n",
      "Epoch 203  ###   Avg-Loss 3.683992739145954e-05   ###   Correct predictions 1.0\n",
      "Epoch 204  ###   Avg-Loss 3.6930983575681846e-05   ###   Correct predictions 1.0\n",
      "Epoch 205  ###   Avg-Loss 3.837041634445389e-05   ###   Correct predictions 1.0\n",
      "Epoch 206  ###   Avg-Loss 3.6403711419552565e-05   ###   Correct predictions 1.0\n",
      "Epoch 207  ###   Avg-Loss 3.761501672367255e-05   ###   Correct predictions 1.0\n",
      "Epoch 208  ###   Avg-Loss 3.68371062601606e-05   ###   Correct predictions 1.0\n",
      "Epoch 209  ###   Avg-Loss 4.079292993992567e-05   ###   Correct predictions 1.0\n",
      "Epoch 210  ###   Avg-Loss 3.946068463847041e-05   ###   Correct predictions 1.0\n",
      "Epoch 211  ###   Avg-Loss 4.385570452238123e-05   ###   Correct predictions 1.0\n",
      "Epoch 212  ###   Avg-Loss 4.163978931804498e-05   ###   Correct predictions 1.0\n",
      "Epoch 213  ###   Avg-Loss 3.332675745089849e-05   ###   Correct predictions 1.0\n",
      "Epoch 214  ###   Avg-Loss 4.257323065151771e-05   ###   Correct predictions 1.0\n",
      "Epoch 215  ###   Avg-Loss 4.2936753015965226e-05   ###   Correct predictions 1.0\n",
      "Epoch 216  ###   Avg-Loss 3.636674955487251e-05   ###   Correct predictions 1.0\n",
      "Epoch 217  ###   Avg-Loss 3.94138313519458e-05   ###   Correct predictions 1.0\n",
      "Epoch 218  ###   Avg-Loss 3.3846831259628135e-05   ###   Correct predictions 1.0\n",
      "Epoch 219  ###   Avg-Loss 3.627579426392913e-05   ###   Correct predictions 1.0\n",
      "Epoch 220  ###   Avg-Loss 4.044672629485528e-05   ###   Correct predictions 1.0\n",
      "Epoch 221  ###   Avg-Loss 3.899487977226575e-05   ###   Correct predictions 1.0\n",
      "Epoch 222  ###   Avg-Loss 3.2388655624041956e-05   ###   Correct predictions 1.0\n",
      "Epoch 223  ###   Avg-Loss 3.148379425207774e-05   ###   Correct predictions 1.0\n",
      "Epoch 224  ###   Avg-Loss 4.479126849522193e-05   ###   Correct predictions 1.0\n",
      "Epoch 225  ###   Avg-Loss 3.618796666463216e-05   ###   Correct predictions 1.0\n",
      "Epoch 226  ###   Avg-Loss 3.777043893933296e-05   ###   Correct predictions 1.0\n",
      "Epoch 227  ###   Avg-Loss 3.232196516667803e-05   ###   Correct predictions 1.0\n",
      "Epoch 228  ###   Avg-Loss 3.010243138608833e-05   ###   Correct predictions 1.0\n",
      "Epoch 229  ###   Avg-Loss 3.525512292981148e-05   ###   Correct predictions 1.0\n",
      "Epoch 230  ###   Avg-Loss 3.3884646836668256e-05   ###   Correct predictions 1.0\n",
      "Epoch 231  ###   Avg-Loss 3.234480197230975e-05   ###   Correct predictions 1.0\n",
      "Epoch 232  ###   Avg-Loss 3.662161761894822e-05   ###   Correct predictions 1.0\n",
      "Epoch 233  ###   Avg-Loss 3.697317248831193e-05   ###   Correct predictions 1.0\n",
      "Epoch 234  ###   Avg-Loss 2.878650751275321e-05   ###   Correct predictions 1.0\n",
      "Epoch 235  ###   Avg-Loss 3.556391845146815e-05   ###   Correct predictions 1.0\n",
      "Epoch 236  ###   Avg-Loss 2.7765481111903984e-05   ###   Correct predictions 1.0\n",
      "Epoch 237  ###   Avg-Loss 3.6760074241707726e-05   ###   Correct predictions 1.0\n",
      "Epoch 238  ###   Avg-Loss 3.500080201774835e-05   ###   Correct predictions 1.0\n",
      "Epoch 239  ###   Avg-Loss 3.9084527331093946e-05   ###   Correct predictions 1.0\n",
      "Epoch 240  ###   Avg-Loss 3.469903022050858e-05   ###   Correct predictions 1.0\n",
      "Epoch 241  ###   Avg-Loss 3.173545701429248e-05   ###   Correct predictions 1.0\n",
      "Epoch 242  ###   Avg-Loss 3.360264624158541e-05   ###   Correct predictions 1.0\n",
      "Epoch 243  ###   Avg-Loss 3.289517092828949e-05   ###   Correct predictions 1.0\n",
      "Epoch 244  ###   Avg-Loss 2.8689632502694926e-05   ###   Correct predictions 1.0\n",
      "Epoch 245  ###   Avg-Loss 3.048427946244677e-05   ###   Correct predictions 1.0\n",
      "Epoch 246  ###   Avg-Loss 3.113332592571775e-05   ###   Correct predictions 1.0\n",
      "Epoch 247  ###   Avg-Loss 2.8724937389294308e-05   ###   Correct predictions 1.0\n",
      "Epoch 248  ###   Avg-Loss 3.149114587965111e-05   ###   Correct predictions 1.0\n",
      "Epoch 249  ###   Avg-Loss 3.239394476016362e-05   ###   Correct predictions 1.0\n",
      "Epoch 250  ###   Avg-Loss 2.6755715953186155e-05   ###   Correct predictions 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('lin1',nn.Linear(2, 16, bias=True)),\n",
    "    ('relu1',nn.ReLU()),\n",
    "    ('lin2',nn.Linear(16, 32, bias=True)),\n",
    "    ('relu2',nn.ReLU()),\n",
    "    ('lin3',nn.Linear(32, 64, bias=True)),\n",
    "    ('relu3',nn.ReLU()),\n",
    "    ('lin4',nn.Linear(64, 2, bias=True))\n",
    "])\n",
    ")\n",
    "\n",
    "output_frequency = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(250):\n",
    "    loss_sum = 0\n",
    "    correct_pred = 0\n",
    "    for index, (data, target) in enumerate(train_loader):\n",
    "        #print(index)\n",
    "        output = model.forward(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss_sum = loss_sum + loss.data\n",
    "        for i in range(len(output)):\n",
    "            _, ind = torch.max(output[i],0)\n",
    "            label = target[i]\n",
    "            \n",
    "            if ind.data == label.data:\n",
    "                correct_pred +=1\n",
    "        #if index % (10*(output_frequency)) == 0:\n",
    "        #    print(\"#  Epoch  #  Batch  #  Avg-Loss ###############\")\n",
    "        #if index % (output_frequency) == 0 and index > 0:\n",
    "        #    print(\"#  %d  #  %d  #  %f  #\" % (epoch+1, index, loss_sum/output_frequency))\n",
    "        #    loss_sum = 0\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch '+str(epoch+1)+'  ###   Avg-Loss '+str(loss_sum.item()/480)+'   ###   Correct predictions '+str(correct_pred/480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  ###   Avg-Loss 0.019352535406748455   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 2  ###   Avg-Loss 0.017910422881444295   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 3  ###   Avg-Loss 0.01750417153040568   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 4  ###   Avg-Loss 0.017391308148701986   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 5  ###   Avg-Loss 0.017361233631769817   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 6  ###   Avg-Loss 0.017345829804738363   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 7  ###   Avg-Loss 0.017331127325693765   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 8  ###   Avg-Loss 0.01733660101890564   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 9  ###   Avg-Loss 0.01734957695007324   ###   Correct predictions 0.4895833333333333\n",
      "Epoch 10  ###   Avg-Loss 0.01734344959259033   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 11  ###   Avg-Loss 0.017335693041483562   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 12  ###   Avg-Loss 0.01733403404553731   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 13  ###   Avg-Loss 0.017345335086186728   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 14  ###   Avg-Loss 0.017336952686309814   ###   Correct predictions 0.50625\n",
      "Epoch 15  ###   Avg-Loss 0.017330414056777953   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 16  ###   Avg-Loss 0.01733735203742981   ###   Correct predictions 0.49375\n",
      "Epoch 17  ###   Avg-Loss 0.017340999841690064   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 18  ###   Avg-Loss 0.01733502944310506   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 19  ###   Avg-Loss 0.017336159944534302   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 20  ###   Avg-Loss 0.01733076572418213   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 21  ###   Avg-Loss 0.017335595687230428   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 22  ###   Avg-Loss 0.017331228653589884   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 23  ###   Avg-Loss 0.017345523834228514   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 24  ###   Avg-Loss 0.017328786849975585   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 25  ###   Avg-Loss 0.017338613669077556   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 26  ###   Avg-Loss 0.017339388529459637   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 27  ###   Avg-Loss 0.017334789037704468   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 28  ###   Avg-Loss 0.017332659165064494   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 29  ###   Avg-Loss 0.017332019408543904   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 30  ###   Avg-Loss 0.017343803246816   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 31  ###   Avg-Loss 0.017337658007939658   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 32  ###   Avg-Loss 0.017327052354812623   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 33  ###   Avg-Loss 0.017350995540618898   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 34  ###   Avg-Loss 0.017341856161753336   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 35  ###   Avg-Loss 0.01733092466990153   ###   Correct predictions 0.49375\n",
      "Epoch 36  ###   Avg-Loss 0.017343801259994508   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 37  ###   Avg-Loss 0.017331286271413168   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 38  ###   Avg-Loss 0.0173449436823527   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 39  ###   Avg-Loss 0.017336622873942057   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 40  ###   Avg-Loss 0.01733430624008179   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 41  ###   Avg-Loss 0.01733665664990743   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 42  ###   Avg-Loss 0.01732741594314575   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 43  ###   Avg-Loss 0.017335259914398195   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 44  ###   Avg-Loss 0.017344200611114503   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 45  ###   Avg-Loss 0.017335255940755207   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 46  ###   Avg-Loss 0.01732981006304423   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 47  ###   Avg-Loss 0.017340336243311563   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 48  ###   Avg-Loss 0.017337270577748618   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 49  ###   Avg-Loss 0.01734388470649719   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 50  ###   Avg-Loss 0.017336700359980264   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 51  ###   Avg-Loss 0.017336754004160564   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 52  ###   Avg-Loss 0.017329061031341554   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 53  ###   Avg-Loss 0.017343024412790935   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 54  ###   Avg-Loss 0.017335404952367146   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 55  ###   Avg-Loss 0.017333406209945678   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 56  ###   Avg-Loss 0.01734092434247335   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 57  ###   Avg-Loss 0.017334075768788655   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 58  ###   Avg-Loss 0.017333996295928956   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 59  ###   Avg-Loss 0.01734001040458679   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 60  ###   Avg-Loss 0.017343183358510334   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 61  ###   Avg-Loss 0.017334266503651937   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 62  ###   Avg-Loss 0.017326943079630532   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 63  ###   Avg-Loss 0.017331435283025106   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 64  ###   Avg-Loss 0.01734351714452108   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 65  ###   Avg-Loss 0.01733464002609253   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 66  ###   Avg-Loss 0.017335947354634604   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 67  ###   Avg-Loss 0.017339940865834555   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 68  ###   Avg-Loss 0.017335885763168336   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 69  ###   Avg-Loss 0.017330139875411987   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 70  ###   Avg-Loss 0.017336839437484743   ###   Correct predictions 0.48125\n",
      "Epoch 71  ###   Avg-Loss 0.01733196576436361   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 72  ###   Avg-Loss 0.01733160416285197   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 73  ###   Avg-Loss 0.017341158787409463   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 74  ###   Avg-Loss 0.01734090844790141   ###   Correct predictions 0.4895833333333333\n",
      "Epoch 75  ###   Avg-Loss 0.01733473539352417   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 76  ###   Avg-Loss 0.01733032464981079   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 77  ###   Avg-Loss 0.01734740932782491   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 78  ###   Avg-Loss 0.017335404952367146   ###   Correct predictions 0.49375\n",
      "Epoch 79  ###   Avg-Loss 0.017347671588261924   ###   Correct predictions 0.4895833333333333\n",
      "Epoch 80  ###   Avg-Loss 0.017328637838363647   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 81  ###   Avg-Loss 0.017335180441538492   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 82  ###   Avg-Loss 0.017335520188013712   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 83  ###   Avg-Loss 0.01733883221944173   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 84  ###   Avg-Loss 0.017346135775248208   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 85  ###   Avg-Loss 0.0173315425713857   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 86  ###   Avg-Loss 0.017339418331782024   ###   Correct predictions 0.48125\n",
      "Epoch 87  ###   Avg-Loss 0.01734487811724345   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 88  ###   Avg-Loss 0.01733728249867757   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 89  ###   Avg-Loss 0.01733392079671224   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 90  ###   Avg-Loss 0.017338260014851888   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 91  ###   Avg-Loss 0.017348573605219523   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 92  ###   Avg-Loss 0.017336924870808918   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 93  ###   Avg-Loss 0.017339259386062622   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 94  ###   Avg-Loss 0.017346447706222533   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 95  ###   Avg-Loss 0.01733930706977844   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 96  ###   Avg-Loss 0.01734011769294739   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 97  ###   Avg-Loss 0.01733176310857137   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 98  ###   Avg-Loss 0.017335659265518187   ###   Correct predictions 0.48125\n",
      "Epoch 99  ###   Avg-Loss 0.017331586281458537   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 100  ###   Avg-Loss 0.017337832848230997   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 101  ###   Avg-Loss 0.017333618799845376   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 102  ###   Avg-Loss 0.017344723145167034   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 103  ###   Avg-Loss 0.0173422376314799   ###   Correct predictions 0.48541666666666666\n",
      "Epoch 104  ###   Avg-Loss 0.017340991894404092   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 105  ###   Avg-Loss 0.01734160582224528   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 106  ###   Avg-Loss 0.017327898740768434   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 107  ###   Avg-Loss 0.01732935905456543   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 108  ###   Avg-Loss 0.017331963777542113   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 109  ###   Avg-Loss 0.01732992132504781   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 110  ###   Avg-Loss 0.017332820097605388   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 111  ###   Avg-Loss 0.017328357696533202   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 112  ###   Avg-Loss 0.017340288559595744   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 113  ###   Avg-Loss 0.017335466543833413   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 114  ###   Avg-Loss 0.017333012819290162   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 115  ###   Avg-Loss 0.017334548632303874   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 116  ###   Avg-Loss 0.017332889636357627   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 117  ###   Avg-Loss 0.017335166533788044   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 118  ###   Avg-Loss 0.01733022133509318   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 119  ###   Avg-Loss 0.017332543929417927   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 120  ###   Avg-Loss 0.017333388328552246   ###   Correct predictions 0.48541666666666666\n",
      "Epoch 121  ###   Avg-Loss 0.017331175009409585   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 122  ###   Avg-Loss 0.017329037189483643   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 123  ###   Avg-Loss 0.017340620358784992   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 124  ###   Avg-Loss 0.01733303666114807   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 125  ###   Avg-Loss 0.0173516849676768   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 126  ###   Avg-Loss 0.0173428475856781   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 127  ###   Avg-Loss 0.017335009574890137   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 128  ###   Avg-Loss 0.017333370447158814   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 129  ###   Avg-Loss 0.017340290546417236   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 130  ###   Avg-Loss 0.01732499400774638   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 131  ###   Avg-Loss 0.01733763813972473   ###   Correct predictions 0.48541666666666666\n",
      "Epoch 132  ###   Avg-Loss 0.017332053184509276   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 133  ###   Avg-Loss 0.017353246609369915   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 134  ###   Avg-Loss 0.017330745855967205   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 135  ###   Avg-Loss 0.01732763648033142   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 136  ###   Avg-Loss 0.017330714066823325   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 137  ###   Avg-Loss 0.017336773872375488   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 138  ###   Avg-Loss 0.017343024412790935   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 139  ###   Avg-Loss 0.01733009417851766   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 140  ###   Avg-Loss 0.017354045311609903   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 141  ###   Avg-Loss 0.017332154512405395   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 142  ###   Avg-Loss 0.017334461212158203   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 143  ###   Avg-Loss 0.017343628406524658   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 144  ###   Avg-Loss 0.017340620358784992   ###   Correct predictions 0.46875\n",
      "Epoch 145  ###   Avg-Loss 0.01733053723971049   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 146  ###   Avg-Loss 0.017334272464116413   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 147  ###   Avg-Loss 0.017340207099914552   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 148  ###   Avg-Loss 0.01733244260152181   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 149  ###   Avg-Loss 0.017334314187367757   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 150  ###   Avg-Loss 0.0173399825890859   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 151  ###   Avg-Loss 0.017332218090693154   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 152  ###   Avg-Loss 0.017343036333719888   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 153  ###   Avg-Loss 0.01734464367230733   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 154  ###   Avg-Loss 0.017339040835698444   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 155  ###   Avg-Loss 0.01733132799466451   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 156  ###   Avg-Loss 0.0173313041528066   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 157  ###   Avg-Loss 0.017345698674519856   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 158  ###   Avg-Loss 0.01734340190887451   ###   Correct predictions 0.47708333333333336\n",
      "Epoch 159  ###   Avg-Loss 0.017333271106084187   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 160  ###   Avg-Loss 0.01733576456705729   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 161  ###   Avg-Loss 0.01733560562133789   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 162  ###   Avg-Loss 0.017329166332880657   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 163  ###   Avg-Loss 0.017341987291971842   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 164  ###   Avg-Loss 0.017338371276855467   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 165  ###   Avg-Loss 0.017340058088302614   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 166  ###   Avg-Loss 0.017344150940577188   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 167  ###   Avg-Loss 0.017344385385513306   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 168  ###   Avg-Loss 0.017346356312433878   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 169  ###   Avg-Loss 0.017333459854125977   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 170  ###   Avg-Loss 0.01733231743176778   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 171  ###   Avg-Loss 0.01732916235923767   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 172  ###   Avg-Loss 0.017331536610921225   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 173  ###   Avg-Loss 0.01733509699503581   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 174  ###   Avg-Loss 0.01732465426127116   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 175  ###   Avg-Loss 0.017342160145441692   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 176  ###   Avg-Loss 0.017330139875411987   ###   Correct predictions 0.5104166666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177  ###   Avg-Loss 0.017332039276758828   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 178  ###   Avg-Loss 0.01733199159304301   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 179  ###   Avg-Loss 0.017338985204696657   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 180  ###   Avg-Loss 0.017342575391133628   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 181  ###   Avg-Loss 0.017335110902786256   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 182  ###   Avg-Loss 0.017331095536549886   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 183  ###   Avg-Loss 0.017341347535451253   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 184  ###   Avg-Loss 0.017332384983698528   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 185  ###   Avg-Loss 0.017330108086268108   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 186  ###   Avg-Loss 0.017338589827219645   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 187  ###   Avg-Loss 0.017340189218521117   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 188  ###   Avg-Loss 0.017331349849700927   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 189  ###   Avg-Loss 0.017359145482381187   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 190  ###   Avg-Loss 0.017331101497014365   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 191  ###   Avg-Loss 0.017330982287724814   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 192  ###   Avg-Loss 0.017326368888219198   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 193  ###   Avg-Loss 0.017343854904174803   ###   Correct predictions 0.47708333333333336\n",
      "Epoch 194  ###   Avg-Loss 0.017337441444396973   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 195  ###   Avg-Loss 0.017333298921585083   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 196  ###   Avg-Loss 0.017332341273625693   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 197  ###   Avg-Loss 0.017343161503473918   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 198  ###   Avg-Loss 0.017330855131149292   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 199  ###   Avg-Loss 0.01732876698176066   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 200  ###   Avg-Loss 0.017356745402018228   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 201  ###   Avg-Loss 0.017346465587615968   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 202  ###   Avg-Loss 0.017329835891723634   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 203  ###   Avg-Loss 0.01734454035758972   ###   Correct predictions 0.4895833333333333\n",
      "Epoch 204  ###   Avg-Loss 0.01733029286066691   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 205  ###   Avg-Loss 0.01733484069506327   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 206  ###   Avg-Loss 0.017335607608159383   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 207  ###   Avg-Loss 0.017337212959925335   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 208  ###   Avg-Loss 0.017347023884455363   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 209  ###   Avg-Loss 0.017332309484481813   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 210  ###   Avg-Loss 0.01733231743176778   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 211  ###   Avg-Loss 0.01734496553738912   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 212  ###   Avg-Loss 0.0173362930615743   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 213  ###   Avg-Loss 0.017337344090143838   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 214  ###   Avg-Loss 0.017353427410125733   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 215  ###   Avg-Loss 0.017331113417943318   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 216  ###   Avg-Loss 0.01733746329943339   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 217  ###   Avg-Loss 0.017332746585210165   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 218  ###   Avg-Loss 0.01733715335528056   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 219  ###   Avg-Loss 0.017337143421173096   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 220  ###   Avg-Loss 0.0173479954401652   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 221  ###   Avg-Loss 0.01733072797457377   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 222  ###   Avg-Loss 0.01734137535095215   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 223  ###   Avg-Loss 0.017333173751831056   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 224  ###   Avg-Loss 0.017326873540878297   ###   Correct predictions 0.50625\n",
      "Epoch 225  ###   Avg-Loss 0.01732667684555054   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 226  ###   Avg-Loss 0.017343149582544962   ###   Correct predictions 0.47708333333333336\n",
      "Epoch 227  ###   Avg-Loss 0.017348565657933555   ###   Correct predictions 0.49375\n",
      "Epoch 228  ###   Avg-Loss 0.017334616184234618   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 229  ###   Avg-Loss 0.017330666383107502   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 230  ###   Avg-Loss 0.017340850830078126   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 231  ###   Avg-Loss 0.01733234922091166   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 232  ###   Avg-Loss 0.01734009782473246   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 233  ###   Avg-Loss 0.017341488599777223   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 234  ###   Avg-Loss 0.017334944009780882   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 235  ###   Avg-Loss 0.017334401607513428   ###   Correct predictions 0.50625\n",
      "Epoch 236  ###   Avg-Loss 0.01733740170796712   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 237  ###   Avg-Loss 0.017334469159444175   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 238  ###   Avg-Loss 0.01733429233233134   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 239  ###   Avg-Loss 0.017332722743352253   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 240  ###   Avg-Loss 0.01733097235361735   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 241  ###   Avg-Loss 0.017334298292795817   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 242  ###   Avg-Loss 0.017329289515813192   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 243  ###   Avg-Loss 0.01733185847600301   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 244  ###   Avg-Loss 0.017335520188013712   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 245  ###   Avg-Loss 0.017344415187835693   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 246  ###   Avg-Loss 0.017336014906565347   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 247  ###   Avg-Loss 0.017329887549082438   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 248  ###   Avg-Loss 0.017341379324595133   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 249  ###   Avg-Loss 0.01733956535657247   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 250  ###   Avg-Loss 0.01733067234357198   ###   Correct predictions 0.5104166666666666\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "from Dataset.Dataset import makeMoonsDataset\n",
    "\n",
    "dataset_size = 600\n",
    "batch_size = 40\n",
    "train_size = dataset_size*0.8\n",
    "test_size = dataset_size*0.2\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset(dataset_size,batch_size)\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('lin1',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu1',nn.ReLU()),\n",
    "    ('lin2',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu2',nn.ReLU()),\n",
    "    ('lin3',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu3',nn.ReLU()),\n",
    "    ('lin4',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu4',nn.ReLU()),\n",
    "    ('lin5',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu5',nn.ReLU()),\n",
    "    ('lin6',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu6',nn.ReLU()),\n",
    "    ('lin7',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu7',nn.ReLU()),\n",
    "    ('lin8',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu8',nn.ReLU()),\n",
    "    ('lin9',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu9',nn.ReLU()),\n",
    "    ('lin10',nn.Linear(2, 2, bias=True))\n",
    "])\n",
    ")\n",
    "\n",
    "output_frequency = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(250):\n",
    "    loss_sum = 0\n",
    "    correct_pred = 0\n",
    "    for index, (data, target) in enumerate(train_loader):\n",
    "        #print(index)\n",
    "        output = model.forward(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss_sum = loss_sum + loss.data\n",
    "        for i in range(len(output)):\n",
    "            _, ind = torch.max(output[i],0)\n",
    "            label = target[i]\n",
    "            \n",
    "            if ind.data == label.data:\n",
    "                correct_pred +=1\n",
    "        #if index % (10*(output_frequency)) == 0:\n",
    "        #    print(\"#  Epoch  #  Batch  #  Avg-Loss ###############\")\n",
    "        #if index % (output_frequency) == 0 and index > 0:\n",
    "        #    print(\"#  %d  #  %d  #  %f  #\" % (epoch+1, index, loss_sum/output_frequency))\n",
    "        #    loss_sum = 0\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch '+str(epoch+1)+'  ###   Avg-Loss '+str(loss_sum.item()/train_size)+'   ###   Correct predictions '+str(correct_pred/train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7115,  1.4547]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.6434, -2.7388]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3886,  2.0311]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.9741, -3.1041]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-0.8089,  0.1275]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3526, -2.2834]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.8463, -2.8233]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.9055, -2.8244]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-1.7085,  1.4306]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.7206,  2.3536]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 0.7751, -1.6380]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.4437,  2.1093]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.1609,  1.7324]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-0.5344,  0.0122]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3842,  2.1159]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-1.5523,  1.2432]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.5676, -2.6339]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 2.0958, -3.2221]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 0.9023, -1.6219]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.7551,  2.3714]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3445, -2.3139]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.5533,  2.1857]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3841, -2.4257]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3780,  2.0977]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 0.0020, -0.6190]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 0.1119, -0.9363]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3832, -2.3543]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.5189, -2.5425]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-1.9774,  1.4456]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.2206, -2.2302]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-0.1666, -0.6191]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.3856,  2.0124]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.4732, -2.4705]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.4295,  2.1136]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-0.6206,  0.0895]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.7165, -2.7979]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.8907, -2.8468]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.8076, -2.8020]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3121,  1.9327]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.0975,  1.7790]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "for index, (data, target) in enumerate(test_loader):\n",
    "    print(model.forward(data))\n",
    "    print(target)\n",
    "    print('####################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 1.0\n"
     ]
    }
   ],
   "source": [
    "net.test(test_loader,120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
      "        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1])\n",
      "tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,\n",
      "        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1])\n",
      "tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n",
      "        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1])\n",
      "tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1])\n",
      "tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1])\n",
      "tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n",
      "        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0])\n",
      "tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,\n",
      "        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1])\n",
      "tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
      "        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,\n",
      "        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0])\n",
      "tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
      "        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0])\n",
      "tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0])\n",
      "tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1])\n",
      "[0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "from pandas import DataFrame\n",
    "import torch\n",
    "\n",
    "x1 = []\n",
    "x2 = []\n",
    "y = []\n",
    "for (data,label) in train_loader:\n",
    "    res = torch.argmax(net.forward(data), dim=1)\n",
    "    print(res)\n",
    "    for point in range(len(data)):\n",
    "        x1.append(data[point][0].item())\n",
    "        x2.append(data[point][1].item())\n",
    "        y.append(res[point].item())\n",
    "#print(x1)\n",
    "df = DataFrame(dict(x=x1, y=x2, label=y))\n",
    "print(y)\n",
    "colors = {0:'red', 1:'blue'}\n",
    "fig, ax = pyplot.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 1.0\n"
     ]
    }
   ],
   "source": [
    "correct_pred = 0\n",
    "for _, (data, label) in enumerate(test_loader):\n",
    "    prediction = net.forward(data)\n",
    "    _, ind = torch.max(prediction,1)\n",
    "    if ind.data == label.data:\n",
    "        correct_pred +=1\n",
    "print('Correct predictions: '+str(correct_pred/120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:168: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  compared_weight_signs = torch.tensor(torch.ne(new_weight_signs, old_weight_signs).detach(), dtype=torch.float32)\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:173: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_weights = new_weight_signs*torch.tensor(torch.ge(torch.abs(self.m_accumulated),self.rho*max_weight_elem*torch.ones_like(self.m_accumulated)).detach(),dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import loadMNIST\n",
    "from Networks.ResNet import ConvNet, FCMSANet\n",
    "\n",
    "#net = ConvNet(3, [1], [3,6], num_fc=2, sizes_fc=[100,10])\n",
    "net = FCMSANet(num_fc=4,sizes_fc=[784,1024,1024,1024,10], bias=False, batchnorm=True, test=False)\n",
    "train_loader, test_loader = loadMNIST('Dataset/input/mnist_train.csv', 'Dataset/input/mnist_test.csv')\n",
    "#net.train_backprop(1,train_loader)\n",
    "#net.test(test_loader,10000)\n",
    "print(train_loader.batch_size)\n",
    "\n",
    "net.train_msa(1,train_loader)\n",
    "\n",
    "#for index, (data, target) in enumerate(train_loader):\n",
    "#    if index == 1:\n",
    "#        print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "60000\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.021136  #  0.286883  #\n",
      "#  2  #  0.013666  #  0.829800  #\n",
      "#  3  #  0.013053  #  0.897417  #\n",
      "#  4  #  0.012986  #  0.906250  #\n",
      "#  5  #  0.012988  #  0.906350  #\n",
      "#  6  #  0.012940  #  0.911467  #\n",
      "#  7  #  0.012933  #  0.911933  #\n",
      "#  8  #  0.012929  #  0.911917  #\n",
      "#  9  #  0.012904  #  0.915100  #\n",
      "#  10  #  0.012890  #  0.917233  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  11  #  0.012865  #  0.920600  #\n",
      "#  12  #  0.012873  #  0.919233  #\n",
      "#  13  #  0.012850  #  0.921300  #\n",
      "#  14  #  0.012837  #  0.922417  #\n",
      "#  15  #  0.012826  #  0.923083  #\n",
      "#  16  #  0.012819  #  0.925200  #\n",
      "#  17  #  0.012815  #  0.925233  #\n",
      "#  18  #  0.012803  #  0.924800  #\n",
      "#  19  #  0.012793  #  0.925500  #\n",
      "#  20  #  0.012786  #  0.927700  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  21  #  0.012767  #  0.928617  #\n",
      "#  22  #  0.012764  #  0.929817  #\n",
      "#  23  #  0.012761  #  0.929783  #\n",
      "#  24  #  0.012750  #  0.931117  #\n",
      "#  25  #  0.012755  #  0.930150  #\n",
      "#  26  #  0.012739  #  0.930950  #\n",
      "#  27  #  0.012737  #  0.932067  #\n",
      "#  28  #  0.012714  #  0.934367  #\n",
      "#  29  #  0.012710  #  0.933683  #\n",
      "#  30  #  0.012709  #  0.934067  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  31  #  0.012702  #  0.935467  #\n",
      "#  32  #  0.012690  #  0.937067  #\n",
      "#  33  #  0.012683  #  0.936800  #\n",
      "#  34  #  0.012679  #  0.937833  #\n",
      "#  35  #  0.012673  #  0.937717  #\n",
      "#  36  #  0.012665  #  0.939350  #\n",
      "#  37  #  0.012664  #  0.938350  #\n",
      "#  38  #  0.012651  #  0.939567  #\n",
      "#  39  #  0.012656  #  0.939817  #\n",
      "#  40  #  0.012646  #  0.941567  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  41  #  0.012644  #  0.940967  #\n",
      "#  42  #  0.012632  #  0.942317  #\n",
      "#  43  #  0.012623  #  0.944083  #\n",
      "#  44  #  0.012625  #  0.943300  #\n",
      "#  45  #  0.012613  #  0.944450  #\n",
      "#  46  #  0.012618  #  0.943500  #\n",
      "#  47  #  0.012606  #  0.944933  #\n",
      "#  48  #  0.012599  #  0.945317  #\n",
      "#  49  #  0.012598  #  0.946067  #\n",
      "#  50  #  0.012588  #  0.946917  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  51  #  0.012578  #  0.947050  #\n",
      "#  52  #  0.012578  #  0.947650  #\n",
      "#  53  #  0.012573  #  0.948067  #\n",
      "#  54  #  0.012572  #  0.948083  #\n",
      "#  55  #  0.012566  #  0.948533  #\n",
      "#  56  #  0.012561  #  0.948933  #\n",
      "#  57  #  0.012555  #  0.949333  #\n",
      "#  58  #  0.012557  #  0.949850  #\n",
      "#  59  #  0.012548  #  0.950500  #\n",
      "#  60  #  0.012550  #  0.950417  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  61  #  0.012548  #  0.950367  #\n",
      "#  62  #  0.012538  #  0.951200  #\n",
      "#  63  #  0.012539  #  0.951067  #\n",
      "#  64  #  0.012532  #  0.952583  #\n",
      "#  65  #  0.012539  #  0.951133  #\n",
      "#  66  #  0.012519  #  0.952433  #\n",
      "#  67  #  0.012516  #  0.953233  #\n",
      "#  68  #  0.012518  #  0.953417  #\n",
      "#  69  #  0.012520  #  0.952833  #\n",
      "#  70  #  0.012513  #  0.954000  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  71  #  0.012516  #  0.955067  #\n",
      "#  72  #  0.012513  #  0.955033  #\n",
      "#  73  #  0.012505  #  0.955567  #\n",
      "#  74  #  0.012507  #  0.954783  #\n",
      "#  75  #  0.012495  #  0.955800  #\n",
      "#  76  #  0.012499  #  0.955583  #\n",
      "#  77  #  0.012494  #  0.956133  #\n",
      "#  78  #  0.012495  #  0.955817  #\n",
      "#  79  #  0.012485  #  0.957150  #\n",
      "#  80  #  0.012484  #  0.956950  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  81  #  0.012489  #  0.956650  #\n",
      "#  82  #  0.012486  #  0.956533  #\n",
      "#  83  #  0.012485  #  0.956917  #\n",
      "#  84  #  0.012485  #  0.957567  #\n",
      "#  85  #  0.012472  #  0.958333  #\n",
      "#  86  #  0.012476  #  0.957617  #\n",
      "#  87  #  0.012479  #  0.957600  #\n",
      "#  88  #  0.012475  #  0.957917  #\n",
      "#  89  #  0.012474  #  0.957683  #\n",
      "#  90  #  0.012469  #  0.958500  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  91  #  0.012469  #  0.957800  #\n",
      "#  92  #  0.012466  #  0.958800  #\n",
      "#  93  #  0.012467  #  0.958267  #\n",
      "#  94  #  0.012462  #  0.959483  #\n",
      "#  95  #  0.012464  #  0.958783  #\n",
      "#  96  #  0.012465  #  0.959083  #\n",
      "#  97  #  0.012455  #  0.960100  #\n",
      "#  98  #  0.012458  #  0.959733  #\n",
      "#  99  #  0.012456  #  0.960383  #\n",
      "#  100  #  0.012450  #  0.960350  #\n",
      "Time elapsed:  42948.913511782994\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import loadMNIST\n",
    "from Networks.ResNet import ConvMSANet\n",
    "\n",
    "#net = ConvNet(3, [1], [3,6], num_fc=2, sizes_fc=[100,10])\n",
    "net = ConvMSANet(num_conv=5, num_channels=[1,3,6,6,12,12], subsample_points=[2,4], num_fc=2,sizes_fc=[588,900,10], batchnorm=True, test=False)\n",
    "train_loader, test_loader = loadMNIST('Dataset/input/mnist_train.csv', 'Dataset/input/mnist_test.csv')\n",
    "#net.train_backprop(1,train_loader)\n",
    "#net.test(test_loader,10000)\n",
    "print(train_loader.batch_size)\n",
    "print(len(train_loader.dataset))\n",
    "\n",
    "net.train_msa(100,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "60000\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.021318  #  0.282983  #\n",
      "#  2  #  0.013563  #  0.806183  #\n",
      "#  3  #  0.012885  #  0.862233  #\n",
      "#  4  #  0.012822  #  0.867867  #\n",
      "#  5  #  0.012795  #  0.870683  #\n",
      "#  6  #  0.012774  #  0.871517  #\n",
      "#  7  #  0.012752  #  0.873517  #\n",
      "#  8  #  0.012751  #  0.873633  #\n",
      "#  9  #  0.012717  #  0.875800  #\n",
      "#  10  #  0.012708  #  0.876800  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  11  #  0.012702  #  0.877467  #\n",
      "#  12  #  0.012699  #  0.876667  #\n",
      "#  13  #  0.012677  #  0.878633  #\n",
      "#  14  #  0.012669  #  0.880550  #\n",
      "#  15  #  0.012644  #  0.881283  #\n",
      "#  16  #  0.012645  #  0.882050  #\n",
      "#  17  #  0.012624  #  0.883467  #\n",
      "#  18  #  0.012623  #  0.882917  #\n",
      "#  19  #  0.012607  #  0.884183  #\n",
      "#  20  #  0.012616  #  0.884483  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  21  #  0.012576  #  0.887333  #\n",
      "#  22  #  0.012588  #  0.887333  #\n",
      "#  23  #  0.012569  #  0.886800  #\n",
      "#  24  #  0.012565  #  0.888267  #\n",
      "#  25  #  0.012573  #  0.889050  #\n",
      "#  26  #  0.012534  #  0.890700  #\n",
      "#  27  #  0.012530  #  0.890050  #\n",
      "#  28  #  0.012518  #  0.892150  #\n",
      "#  29  #  0.012511  #  0.893217  #\n",
      "#  30  #  0.012497  #  0.894183  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  31  #  0.012494  #  0.893133  #\n",
      "#  32  #  0.012490  #  0.892850  #\n",
      "#  33  #  0.012474  #  0.895450  #\n",
      "#  34  #  0.012465  #  0.896083  #\n",
      "#  35  #  0.012455  #  0.897367  #\n",
      "#  36  #  0.012441  #  0.898100  #\n",
      "#  37  #  0.012436  #  0.897667  #\n",
      "#  38  #  0.012432  #  0.899317  #\n",
      "#  39  #  0.012422  #  0.899717  #\n",
      "#  40  #  0.012419  #  0.900050  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  41  #  0.012415  #  0.899800  #\n",
      "#  42  #  0.012398  #  0.901533  #\n",
      "#  43  #  0.012395  #  0.902367  #\n",
      "#  44  #  0.012379  #  0.903100  #\n",
      "#  45  #  0.012380  #  0.903933  #\n",
      "#  46  #  0.012363  #  0.904383  #\n",
      "#  47  #  0.012352  #  0.904850  #\n",
      "#  48  #  0.012355  #  0.905150  #\n",
      "#  49  #  0.012348  #  0.905417  #\n",
      "#  50  #  0.012349  #  0.905767  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  51  #  0.012332  #  0.906833  #\n",
      "#  52  #  0.012335  #  0.906883  #\n",
      "#  53  #  0.012310  #  0.908233  #\n",
      "#  54  #  0.012308  #  0.908583  #\n",
      "#  55  #  0.012312  #  0.908317  #\n",
      "#  56  #  0.012301  #  0.909717  #\n",
      "#  57  #  0.012289  #  0.909850  #\n",
      "#  58  #  0.012293  #  0.910850  #\n",
      "#  59  #  0.012282  #  0.912300  #\n",
      "#  60  #  0.012275  #  0.910617  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  61  #  0.012274  #  0.912250  #\n",
      "#  62  #  0.012258  #  0.914300  #\n",
      "#  63  #  0.012264  #  0.912167  #\n",
      "#  64  #  0.012248  #  0.913900  #\n",
      "#  65  #  0.012246  #  0.914233  #\n",
      "#  66  #  0.012240  #  0.914600  #\n",
      "#  67  #  0.012226  #  0.915383  #\n",
      "#  68  #  0.012230  #  0.914733  #\n",
      "#  69  #  0.012225  #  0.915783  #\n",
      "#  70  #  0.012228  #  0.915350  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  71  #  0.012222  #  0.915467  #\n",
      "#  72  #  0.012210  #  0.916050  #\n",
      "#  73  #  0.012193  #  0.917767  #\n",
      "#  74  #  0.012201  #  0.917683  #\n",
      "#  75  #  0.012192  #  0.919400  #\n",
      "#  76  #  0.012180  #  0.919383  #\n",
      "#  77  #  0.012197  #  0.919333  #\n",
      "#  78  #  0.012183  #  0.920050  #\n",
      "#  79  #  0.012173  #  0.920567  #\n",
      "#  80  #  0.012173  #  0.920167  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  81  #  0.012166  #  0.920733  #\n",
      "#  82  #  0.012170  #  0.920367  #\n",
      "#  83  #  0.012152  #  0.922383  #\n",
      "#  84  #  0.012162  #  0.920633  #\n",
      "#  85  #  0.012157  #  0.920083  #\n",
      "#  86  #  0.012157  #  0.921667  #\n",
      "#  87  #  0.012143  #  0.921100  #\n",
      "#  88  #  0.012150  #  0.921067  #\n",
      "#  89  #  0.012139  #  0.922300  #\n",
      "#  90  #  0.012126  #  0.922500  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  91  #  0.012136  #  0.922817  #\n",
      "#  92  #  0.012129  #  0.922833  #\n",
      "#  93  #  0.012123  #  0.924333  #\n",
      "#  94  #  0.012123  #  0.924483  #\n",
      "#  95  #  0.012112  #  0.926050  #\n",
      "#  96  #  0.012118  #  0.924417  #\n",
      "#  97  #  0.012111  #  0.924783  #\n",
      "#  98  #  0.012117  #  0.925517  #\n",
      "#  99  #  0.012104  #  0.926117  #\n",
      "#  100  #  0.012094  #  0.926283  #\n",
      "Time elapsed:  26136.839193304\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import loadMNIST\n",
    "from Networks.ResNet import ConvMSANet\n",
    "\n",
    "#net = ConvNet(3, [1], [3,6], num_fc=2, sizes_fc=[100,10])\n",
    "net = ConvMSANet(num_conv=3, num_channels=[1,3,6,12], subsample_points=[0,1], num_fc=2,sizes_fc=[588,900,10], batchnorm=True, test=False)\n",
    "train_loader, test_loader = loadMNIST('Dataset/input/mnist_train.csv', 'Dataset/input/mnist_test.csv')\n",
    "#net.train_backprop(1,train_loader)\n",
    "#net.test(test_loader,10000)\n",
    "print(train_loader.batch_size)\n",
    "print(len(train_loader.dataset))\n",
    "\n",
    "net.train_msa(100,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n"
     ]
    }
   ],
   "source": [
    "net.train_msa(1,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy:  0.9253\n"
     ]
    }
   ],
   "source": [
    "net.test(test_loader,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "60000\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.010105  #  0.672850  #\n",
      "#  2  #  0.001262  #  0.961583  #\n",
      "#  3  #  0.000785  #  0.976367  #\n",
      "#  4  #  0.000575  #  0.982483  #\n",
      "#  5  #  0.000435  #  0.986500  #\n",
      "#  6  #  0.000330  #  0.989950  #\n",
      "#  7  #  0.000251  #  0.992450  #\n",
      "#  8  #  0.000219  #  0.993333  #\n",
      "#  9  #  0.000174  #  0.994683  #\n",
      "#  10  #  0.000160  #  0.994983  #\n",
      "Time elapsed:  110.92861831799999\n"
     ]
    }
   ],
   "source": [
    "from Networks.ResNet import ConvNet\n",
    "from Dataset.Dataset import loadMNIST\n",
    "\n",
    "net = ConvNet(num_conv=3, num_channels=[1,3,6,12], subsample_points=[0,1], num_fc=2,sizes_fc=[588,900,10])\n",
    "train_loader, test_loader = loadMNIST('Dataset/input/mnist_train.csv', 'Dataset/input/mnist_test.csv')\n",
    "#net.train_backprop(1,train_loader)\n",
    "#net.test(test_loader,10000)\n",
    "print(train_loader.batch_size)\n",
    "print(len(train_loader.dataset))\n",
    "\n",
    "net.train(10,train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 0.9812\n"
     ]
    }
   ],
   "source": [
    "net.test(test_loader, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1574, -0.1913,  0.3191],\n",
      "        [ 0.4584,  0.3066, -0.3247],\n",
      "        [ 0.2495,  0.4553,  0.0500],\n",
      "        [ 0.3111, -0.3222,  0.2152]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.4820, -0.5667, -0.2615,  0.5324], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "test_layer = torch.nn.Linear(3, 4)\n",
    "print(test_layer.weight)\n",
    "print(test_layer.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6., 18., 36.], grad_fn=<SumBackward2>)\n",
      "tensor([[1., 1., 1.],\n",
      "        [2., 2., 2.],\n",
      "        [3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([[1,2,3],[2,3,4],[3,4,5]],dtype=torch.float32, requires_grad=True)\n",
    "y = torch.tensor([[1,1,1],[2,2,2],[3,3,3]],dtype=torch.float32)\n",
    "\n",
    "z = torch.sum(x*y, dim=1)\n",
    "print(z)\n",
    "#z.backward(torch.FloatTensor([0,0,1]), retain_graph=True)\n",
    "z.backward(torch.FloatTensor([1,1,1]), retain_graph=True)\n",
    "#z[0].backward(retain_graph=True)\n",
    "#z[1].backward(retain_graph=True)\n",
    "#z[2].backward(retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "tensor([[[[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]],\n",
      "\n",
      "         [[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]],\n",
      "\n",
      "         [[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]]],\n",
      "\n",
      "\n",
      "        [[[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]],\n",
      "\n",
      "         [[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]],\n",
      "\n",
      "         [[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]]],\n",
      "\n",
      "\n",
      "        [[[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]],\n",
      "\n",
      "         [[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]],\n",
      "\n",
      "         [[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]]]], dtype=torch.float64)\n",
      "tensor([120., 120., 120.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor((), dtype=torch.float64)\n",
    "y = torch.tensor((), dtype=torch.float64)\n",
    "x = x.new_ones((3,3,4,4))\n",
    "y = y.new_full((3,3,4,4), 2.5)\n",
    "print(x.dim())\n",
    "print(x*y)\n",
    "print(torch.sum(x*y, (1,2,3)))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 7, 9]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([[1,2,3]])\n",
    "y=torch.tensor([[4,5,6]])\n",
    "print(x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.218333333333334\n"
     ]
    }
   ],
   "source": [
    "time =0\n",
    "for i in range(61):\n",
    "    time += i*14.2\n",
    "print(time/60/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6)\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([[1,2,3,0,0,1,3,2]])\n",
    "y=torch.tensor([[1,2,2,1,0,1,3,2]])\n",
    "print(torch.sum(x==y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(120)\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0])\n",
    "b=torch.tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0])\n",
    "c=torch.tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1])\n",
    "d=torch.tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1])\n",
    "e=torch.tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0])\n",
    "f=torch.tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0])\n",
    "\n",
    "print(torch.sum(a==b)+torch.sum(c==d)+torch.sum(e==f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from Dataset.Dataset import makeMoonsDataset\n",
    "\n",
    "dataset_size = 600\n",
    "batch_size = 40\n",
    "test_set_size = dataset_size * 0.2\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset(dataset_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from pandas import DataFrame\n",
    "\n",
    "#x1 = []\n",
    "#x2 = []\n",
    "#y = []\n",
    "#for (data,label) in train_loader:\n",
    "#    res = label#torch.argmax(net.forward, dim=1)\n",
    "#    for point in range(len(data)):\n",
    "#        x1.append(data[point][0].item())\n",
    "#        x2.append(data[point][1].item())\n",
    "#        y.append(res[point].item())\n",
    "##print(x1)\n",
    "#df = DataFrame(dict(x=x1, y=x2, label=y))\n",
    "#colors = {0:'red', 1:'blue'}\n",
    "fig, ax2 = plt.subplots()\n",
    "#grouped = df.groupby('label')\n",
    "#for key, group in grouped:\n",
    "#    group.plot(ax=ax1, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "#ax1.set_title('Training Set')\n",
    "    \n",
    "x1 = []\n",
    "x2 = []\n",
    "y = []\n",
    "for (data,label) in test_loader:\n",
    "    res = label#torch.argmax(label, dim=1)\n",
    "    for point in range(len(data)):\n",
    "        x1.append(data[point][0].item())\n",
    "        x2.append(data[point][1].item())\n",
    "        y.append(res[point].item())\n",
    "#print(x1)\n",
    "df = DataFrame(dict(x=x1, y=x2, label=y))\n",
    "colors = {0:'red', 1:'blue'}\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax2, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "ax2.set_title('Test Set')\n",
    "#plt.tight_layout()\n",
    "plt.savefig(\"Plots/MoonsTest.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import loadMNIST\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "train_loader, test_loader = loadMNIST('Dataset/input/mnist_train.csv', 'Dataset/input/mnist_test.csv')\n",
    "arr_dict = {'arr0':[],'c0':0, 'arr1':[],'c1':0, 'arr2':[],'c2':0, 'arr3':[],'c3':0, 'arr4':[],'c4':0, 'arr5':[],'c5':0, 'arr6':[],'c6':0, 'arr7':[],'c7':0, 'arr8':[],'c8':0, 'arr9':[],'c9':0}\n",
    "\n",
    "\n",
    "for index, (data, label) in enumerate(train_loader):\n",
    "    if index < 200:\n",
    "        #print(data[0],label[0])\n",
    "        key = 'arr'+str(label[0].item())\n",
    "        key_c = 'c'+str(label[0].item())\n",
    "        arr = data[0][0].numpy()*255\n",
    "        if arr_dict[key] == []:\n",
    "            arr_dict[key] = arr\n",
    "            arr_dict[key_c] += 1\n",
    "        elif arr_dict[key_c] < 10:\n",
    "            arr_dict[key] = np.concatenate((arr_dict[key], arr), axis=1) \n",
    "            arr_dict[key_c] += 1\n",
    "            \n",
    "        #img = Image.fromarray(arr)\n",
    "        #if img.mode != 'RGB':\n",
    "        #    img = img.convert('RGB')\n",
    "        #img.save('Plots/'+str(label[0].item())+'/mnist'+str(index)+'.png')\n",
    "        #img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(10):\n",
    "#    print()\n",
    "arr = np.concatenate((arr_dict['arr0'],arr_dict['arr1'],arr_dict['arr2'],arr_dict['arr3'],arr_dict['arr4'],arr_dict['arr5'],arr_dict['arr6'],arr_dict['arr7'],arr_dict['arr8'],arr_dict['arr9']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.fromarray(arr)\n",
    "if img.mode != 'RGB':            \n",
    "    img = img.convert('RGB')\n",
    "    img.save('Plots/mnistall.png')\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "num_epochs = 100\n",
    "num_layers = 4\n",
    "layers = [2,16,32,64,2]\n",
    "bias = True\n",
    "test_set_size = 120\n",
    "\n",
    "basic_save_key = 'MSA_Comparison_results//FCMSANet'\n",
    "\n",
    "train_save_key = '_train_per_epoch_layers_'+str(num_layers)+'_max_hidden_size_'+str(max(layers))+'_bias_'+str(bias)\n",
    "test_save_key = '_test_per_epoch_layers_'+str(num_layers)+'_max_hidden_size_'+str(max(layers))+'_bias_'+str(bias)\n",
    "\n",
    "train_results_per_epoch = np.loadtxt(basic_save_key+train_save_key+'.csv')\n",
    "test_results_per_epoch = np.loadtxt(basic_save_key+test_save_key+'.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9958333373069763\n"
     ]
    }
   ],
   "source": [
    "print(max(train_results_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "epochs = np.arange(1,101,1)\n",
    "ax.plot(epochs, train_results_per_epoch, label='train')\n",
    "ax.plot(epochs, test_results_per_epoch/test_set_size, label='test')\n",
    "ax.set_ylim(0,1.1)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Average Accuracy')\n",
    "ax.legend(loc='best')\n",
    "plt.savefig(\"Plots/MSA_Comp\"+train_save_key+\".pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 16\n",
      "[0.71458334 0.8645833  0.87291664 0.84166664 0.89166665 0.88125\n",
      " 0.8666667  0.88958335 0.87083334 0.91041666 0.87291664 0.90833336\n",
      " 0.94166666 0.84375    0.93333334 0.97291666 0.9895833  0.9583333\n",
      " 0.9895833  0.99375    0.99583334 0.98541665 0.95208335 0.99375\n",
      " 0.99583334 0.99791664 0.96458334 0.9916667  0.99375    0.99375\n",
      " 0.99791664 1.         0.99583334 0.99375    0.99583334 0.99791664\n",
      " 0.99583334 1.         0.69375    0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[109. 110. 111. 112. 107. 112. 114. 114. 114. 115. 111. 116.  85. 113.\n",
      " 117. 120. 115. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.]\n",
      "[0.68125    0.84791666 0.86875    0.85625    0.8645833  0.8520833\n",
      " 0.8541667  0.86875    0.86875    0.86875    0.85625    0.86041665\n",
      " 0.8645833  0.87083334 0.87708336 0.8854167  0.8979167  0.84791666\n",
      " 0.9145833  0.9270833  0.9479167  0.96875    0.9291667  0.97083336\n",
      " 0.9916667  0.9791667  0.97083336 0.99375    0.99583334 0.99791664\n",
      " 0.97083336 1.         1.         1.         0.99791664 1.\n",
      " 1.         0.99375    0.99791664 0.93333334 0.99583334 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.99791664 0.99791664 1.         1.         0.99791664\n",
      " 0.99791664 1.         0.99791664 0.99791664 0.99791664 0.99791664\n",
      " 0.99583334 1.         0.99375    0.99791664 1.         0.99791664\n",
      " 1.         0.99791664 0.99583334 0.99791664 1.         0.99791664\n",
      " 1.         1.         0.99791664 1.         1.         1.\n",
      " 1.         0.99791664 1.         1.         1.         1.\n",
      " 1.         1.         0.99791664 1.         1.         0.99583334\n",
      " 1.         0.99583334 0.99791664 1.         1.         1.\n",
      " 1.         1.         1.         1.        ]\n",
      "[106. 109. 111. 106. 112. 111. 110. 106. 110. 111. 106. 112. 110. 111.\n",
      " 110. 111. 102. 111. 114. 114. 115. 118. 120. 120. 119. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.]\n",
      "[0.64375    0.8541667  0.86875    0.8520833  0.88125    0.8875\n",
      " 0.9270833  0.87291664 0.88125    0.9291667  0.96875    0.98333335\n",
      " 0.92083335 0.9270833  0.9916667  0.99375    0.99375    0.99375\n",
      " 0.99791664 0.94375    0.60833335 0.83958334 0.85625    0.8645833\n",
      " 0.86875    0.8645833  0.8666667  0.84583336 0.8520833  0.8541667\n",
      " 0.84583336 0.8666667  0.8645833  0.8666667  0.86041665 0.86041665\n",
      " 0.87083334 0.87083334 0.85833335 0.87291664 0.8541667  0.8625\n",
      " 0.87708336 0.8666667  0.8645833  0.86041665 0.87083334 0.87291664\n",
      " 0.85625    0.87083334 0.8666667  0.87291664 0.8666667  0.8625\n",
      " 0.86875    0.87291664 0.86875    0.875      0.87708336 0.86875\n",
      " 0.86875    0.8645833  0.87083334 0.87083334 0.87291664 0.87708336\n",
      " 0.86875    0.8625     0.86875    0.8645833  0.8645833  0.86041665\n",
      " 0.8625     0.86041665 0.87083334 0.875      0.8666667  0.85833335\n",
      " 0.8666667  0.8541667  0.8666667  0.8625     0.86041665 0.8645833\n",
      " 0.8645833  0.8625     0.8645833  0.86875    0.8666667  0.8625\n",
      " 0.86875    0.87291664 0.8666667  0.8666667  0.8645833  0.87083334\n",
      " 0.8666667  0.87083334 0.86041665 0.87708336]\n",
      "[111. 103. 112. 108. 111. 114. 117. 105. 111. 114. 120. 119. 110. 120.\n",
      " 120. 120. 120. 120. 120.  62. 109. 110. 111. 112. 110. 111. 106. 110.\n",
      " 110. 110. 111. 111. 111. 107. 110. 110. 111. 109. 110. 111. 112. 111.\n",
      " 110. 112. 107. 112. 111. 111. 111. 108. 112. 112. 112. 111. 110. 111.\n",
      " 112. 112. 110. 112. 112. 111. 112. 112. 109. 110. 106. 112. 109. 110.\n",
      " 108. 110. 110. 111. 112. 111. 110. 112. 110. 109. 111. 112. 111. 112.\n",
      " 111. 111. 111. 112. 110. 108. 112. 111. 108. 110. 110. 112. 112. 111.\n",
      " 111. 109.]\n",
      "[0.65833336 0.84583336 0.85625    0.86875    0.8833333  0.875\n",
      " 0.86875    0.8666667  0.88125    0.88125    0.8875     0.86875\n",
      " 0.8958333  0.89166665 0.8875     0.89375    0.9125     0.92083335\n",
      " 0.8958333  0.9291667  0.95       0.9604167  0.9604167  0.96875\n",
      " 0.85833335 0.875      0.94375    0.95625    0.95625    0.97291666\n",
      " 0.98541665 0.97291666 0.925      0.9875     0.9916667  0.99375\n",
      " 0.9916667  0.9916667  0.99375    0.8854167  0.9583333  0.9770833\n",
      " 0.9791667  0.98541665 0.98333335 0.99375    0.9875     0.99375\n",
      " 0.99583334 0.99791664 0.99791664 0.99583334 0.99791664 0.99583334\n",
      " 0.79375    0.90208334 0.97291666 0.9916667  0.9916667  0.99375\n",
      " 0.99583334 0.99375    0.99791664 0.99583334 0.99791664 0.99791664\n",
      " 0.99375    0.99791664 0.9916667  0.99791664 0.99791664 0.99791664\n",
      " 0.99791664 0.99791664 0.99791664 0.99791664 0.99583334 0.9916667\n",
      " 0.99583334 0.99791664 0.99791664 0.99583334 0.99583334 0.99375\n",
      " 1.         0.99791664 0.99583334 0.99791664 0.99583334 0.99791664\n",
      " 1.         0.99583334 0.99791664 0.99791664 0.99583334 0.99791664\n",
      " 0.9916667  0.99583334 0.99791664 0.99583334]\n",
      "[108. 111. 106. 110. 110. 111. 111. 111. 111. 111. 111. 111. 113. 113.\n",
      " 111. 115. 113. 106. 118. 117. 118. 118. 119. 118. 109. 116. 118. 118.\n",
      " 120. 120. 118. 120. 120. 120. 120. 120. 120. 120. 120. 115. 118. 118.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 107. 118.\n",
      " 120. 119. 120. 118. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.]\n",
      "[0.6354167  0.80625    0.81458336 0.875      0.8666667  0.86875\n",
      " 0.84791666 0.88125    0.8645833  0.8666667  0.89166665 0.87291664\n",
      " 0.87291664 0.8979167  0.8833333  0.88125    0.8854167  0.87708336\n",
      " 0.875      0.8625     0.8666667  0.8666667  0.8833333  0.8833333\n",
      " 0.8875     0.89375    0.86041665 0.87916666 0.87916666 0.87708336\n",
      " 0.875      0.8854167  0.87291664 0.87083334 0.8833333  0.88958335\n",
      " 0.89375    0.88125    0.87708336 0.8666667  0.8645833  0.88125\n",
      " 0.88958335 0.87916666 0.88125    0.8854167  0.87916666 0.8833333\n",
      " 0.8854167  0.8854167  0.8854167  0.86875    0.8979167  0.87083334\n",
      " 0.88125    0.87916666 0.87291664 0.88958335 0.87708336 0.875\n",
      " 0.88125    0.87708336 0.87916666 0.88125    0.8854167  0.8854167\n",
      " 0.87291664 0.8645833  0.8854167  0.87291664 0.87291664 0.87083334\n",
      " 0.89166665 0.89166665 0.9        0.8958333  0.8833333  0.87083334\n",
      " 0.875      0.88958335 0.8833333  0.88125    0.9        0.90625\n",
      " 0.9125     0.92083335 0.9479167  0.93958336 0.95625    0.975\n",
      " 0.98333335 0.9875     0.96666664 0.97291666 0.99583334 0.99583334\n",
      " 0.99375    1.         0.99791664 0.93541664]\n",
      "[100. 111. 109. 112. 110. 111. 110. 110. 109. 109. 109. 111. 111. 111.\n",
      " 111. 110. 109. 109. 111. 112. 112. 111. 110. 111. 111. 112. 110. 111.\n",
      " 106. 107. 110. 111. 111. 111. 112. 111. 111. 111. 110. 111. 110. 110.\n",
      " 111. 109. 112. 111. 111. 111. 111. 111. 112. 111. 111. 111. 111. 111.\n",
      " 110. 112. 111. 112. 111. 111. 112. 111. 112. 112. 111. 112. 111. 111.\n",
      " 111. 112. 112. 111. 109. 111. 112. 112. 111. 112. 111. 113. 113. 113.\n",
      " 114. 115. 118. 119. 119. 119. 120. 120. 112. 120. 120. 120. 120. 120.\n",
      " 114. 120.]\n",
      "[0.68333334 0.8020833  0.8645833  0.85833335 0.8645833  0.8541667\n",
      " 0.8666667  0.87708336 0.88958335 0.875      0.9        0.83958334\n",
      " 0.9125     0.9270833  0.94375    0.96458334 0.9916667  0.99375\n",
      " 0.99583334 0.97083336 0.9916667  0.99791664 0.99583334 0.99375\n",
      " 0.99583334 0.99583334 0.99791664 0.9916667  0.98125    0.99791664\n",
      " 1.         1.         1.         0.99791664 0.99583334 0.99791664\n",
      " 1.         0.99583334 1.         0.99583334 0.99791664 0.99791664\n",
      " 0.9916667  0.99583334 0.99791664 0.99791664 0.99791664 0.99791664\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.99791664 1.         1.         1.         0.99791664\n",
      " 1.         0.99583334 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1.         1.         1.         1.        ]\n",
      "[106. 107. 113. 112. 109. 113. 111. 111. 111. 114. 114. 113. 117. 116.\n",
      " 118. 120. 120. 120. 120. 120. 118. 120. 120. 120. 120. 120. 120. 117.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 116. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.]\n",
      "[0.51666665 0.82708335 0.84166664 0.86041665 0.93125    0.88125\n",
      " 0.93958336 0.83958334 0.96875    0.98333335 0.9791667  0.99791664\n",
      " 0.98125    0.99791664 0.99583334 0.99791664 0.98541665 0.99375\n",
      " 0.98541665 1.         1.         0.99791664 0.9875     0.99375\n",
      " 0.98333335 0.99791664 0.99791664 1.         0.99791664 1.\n",
      " 0.99791664 0.99583334 0.99791664 1.         1.         0.99375\n",
      " 0.8666667  0.9583333  0.97291666 0.98541665 0.99583334 0.9916667\n",
      " 0.99583334 1.         0.99583334 0.99791664 1.         0.99791664\n",
      " 0.99791664 0.99583334 1.         0.99791664 0.99583334 1.\n",
      " 1.         1.         0.99791664 0.99583334 0.9166667  0.9895833\n",
      " 0.99791664 0.99791664 0.99791664 1.         0.99791664 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.99791664 0.99791664 1.         1.         0.99791664 1.\n",
      " 1.         0.99791664 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         0.99791664 0.99791664 1.\n",
      " 1.         1.         1.         1.        ]\n",
      "[ 97. 110. 114. 112. 113. 114. 109. 116. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 117. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 115. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 119. 120. 120. 120. 120.\n",
      " 120. 107. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.]\n",
      "[0.625      0.84791666 0.85625    0.8666667  0.86875    0.8979167\n",
      " 0.90833336 0.92083335 0.94166666 0.975      0.98333335 0.99583334\n",
      " 0.99583334 1.         0.99791664 0.81875    0.90416664 0.92083335\n",
      " 0.9583333  0.98541665 0.99375    0.98541665 0.95       0.99583334\n",
      " 0.99791664 0.99791664 0.99791664 0.99583334 1.         0.99791664\n",
      " 1.         0.98125    0.90625    0.9875     0.99583334 0.99375\n",
      " 0.99791664 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         0.99791664 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.99583334\n",
      " 1.         1.         1.         1.         0.99791664 1.\n",
      " 1.         1.         1.         1.         1.         0.99791664\n",
      " 1.         1.         1.         1.         1.         0.99791664\n",
      " 0.99583334 0.99791664 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.99791664\n",
      " 1.         0.99375    0.99583334 0.99583334]\n",
      "[ 99. 110. 112. 112. 113. 109. 119. 120. 118. 120. 120. 120. 120. 120.\n",
      " 110. 113. 114. 117. 117. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120.  88. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.]\n",
      "[0.675      0.8333333  0.84583336 0.8645833  0.88958335 0.87291664\n",
      " 0.92291665 0.93541664 0.93958336 0.86041665 0.8333333  0.88125\n",
      " 0.90416664 0.9270833  0.93958336 0.97291666 0.84791666 0.96875\n",
      " 0.975      0.99375    0.93958336 0.9875     0.99791664 0.99791664\n",
      " 0.99375    0.99583334 1.         0.8208333  0.7875     0.8541667\n",
      " 0.86041665 0.8666667  0.87708336 0.88125    0.87916666 0.8854167\n",
      " 0.8854167  0.8854167  0.93333334 0.93333334 0.9479167  0.9625\n",
      " 0.9583333  0.9625     0.9770833  0.9875     0.97291666 0.99375\n",
      " 0.9895833  0.9625     0.75208336 0.8125     0.8666667  0.88125\n",
      " 0.88125    0.87916666 0.8625     0.875      0.87291664 0.87708336\n",
      " 0.87291664 0.8875     0.8979167  0.8833333  0.8854167  0.87916666\n",
      " 0.875      0.87916666 0.8645833  0.87708336 0.87916666 0.87291664\n",
      " 0.89166665 0.8666667  0.88958335 0.8854167  0.8833333  0.8833333\n",
      " 0.88125    0.87291664 0.8854167  0.875      0.87916666 0.87708336\n",
      " 0.8833333  0.875      0.88125    0.88958335 0.8854167  0.8833333\n",
      " 0.87916666 0.8875     0.88958335 0.8854167  0.87083334 0.87708336\n",
      " 0.8875     0.8854167  0.88125    0.8875    ]\n",
      "[104. 112. 108. 111. 112. 112. 115. 117. 118. 109. 111. 112. 116. 117.\n",
      " 116. 116. 117. 119. 120. 105. 120. 120. 120. 120. 120. 120. 120. 102.\n",
      " 109. 109. 110. 112. 112. 111. 111. 111. 113. 112. 117. 116. 116. 118.\n",
      " 118. 119. 120. 113. 120. 120. 120. 100. 106. 108. 109. 111. 111. 112.\n",
      " 111. 111. 111. 110. 112. 111. 110. 111. 111. 111. 112. 112. 111. 111.\n",
      " 112. 111. 111. 112. 111. 111. 111. 112. 111. 111. 110. 111. 110. 112.\n",
      " 111. 111. 111. 111. 111. 112. 111. 111. 112. 112. 111. 111. 110. 112.\n",
      " 111. 111.]\n",
      "[0.68541664 0.8645833  0.8645833  0.90416664 0.9166667  0.98125\n",
      " 0.92291665 0.99583334 0.8333333  0.8520833  0.87708336 0.8854167\n",
      " 0.8645833  0.8625     0.88125    0.85       0.88125    0.8854167\n",
      " 0.86041665 0.87916666 0.875      0.88125    0.8645833  0.87916666\n",
      " 0.87708336 0.88958335 0.8958333  0.87291664 0.87916666 0.87291664\n",
      " 0.8854167  0.87083334 0.88125    0.8854167  0.8833333  0.88958335\n",
      " 0.89166665 0.87708336 0.875      0.86875    0.88125    0.88125\n",
      " 0.8666667  0.875      0.87291664 0.86875    0.87916666 0.88125\n",
      " 0.87291664 0.8833333  0.86041665 0.88958335 0.8666667  0.8833333\n",
      " 0.86875    0.86875    0.87291664 0.8666667  0.875      0.8833333\n",
      " 0.87291664 0.8854167  0.8875     0.87291664 0.87916666 0.8645833\n",
      " 0.89166665 0.87291664 0.8666667  0.87083334 0.875      0.8833333\n",
      " 0.89166665 0.88125    0.87708336 0.87916666 0.8875     0.88958335\n",
      " 0.87708336 0.8833333  0.8875     0.8833333  0.88125    0.8645833\n",
      " 0.89375    0.8854167  0.8875     0.87291664 0.875      0.8854167\n",
      " 0.87916666 0.87916666 0.88125    0.88125    0.88125    0.8666667\n",
      " 0.875      0.8833333  0.8875     0.8875    ]\n",
      "[108. 108. 114. 110. 117. 118. 118. 120. 111. 109. 112. 111. 112. 112.\n",
      " 109. 112. 111. 111. 111. 112. 110. 110. 112. 110. 109. 111. 111. 110.\n",
      " 110. 111. 112. 111. 112. 110. 109. 111. 111. 111. 112. 110. 112. 111.\n",
      " 111. 111. 111. 111. 110. 112. 111. 111. 111. 111. 111. 111. 111. 111.\n",
      " 111. 111. 111. 111. 111. 111. 112. 112. 111. 112. 111. 111. 112. 110.\n",
      " 111. 112. 111. 111. 112. 111. 112. 111. 111. 111. 111. 111. 111. 112.\n",
      " 111. 111. 111. 111. 111. 111. 110. 111. 111. 111. 111. 111. 112. 111.\n",
      " 111. 111.]\n",
      "Number of layers: 17\n",
      "[0.60625    0.85       0.83125    0.88125    0.91041666 0.93958336\n",
      " 0.98541665 0.84166664 0.81041664 0.84375    0.8541667  0.87291664\n",
      " 0.92083335 0.93541664 0.9        0.94375    0.93333334 0.96875\n",
      " 0.9625     0.9791667  0.92083335 0.96666664 0.9770833  0.98333335\n",
      " 0.9895833  0.99583334 0.9875     0.99583334 0.99375    0.9875\n",
      " 0.99583334 0.9875     0.98541665 0.99583334 0.9458333  0.95625\n",
      " 0.9916667  0.98125    1.         0.99583334 0.9916667  0.9895833\n",
      " 0.99791664 0.99791664 0.99791664 0.99791664 0.99791664 1.\n",
      " 0.98333335 0.97083336 0.99583334 0.99375    1.         1.\n",
      " 0.99791664 0.99791664 0.99791664 0.99791664 0.99791664 0.99583334\n",
      " 1.         0.99791664 1.         1.         0.99791664 1.\n",
      " 0.99583334 1.         1.         0.99791664 0.99583334 0.99583334\n",
      " 0.99791664 0.99791664 1.         0.96458334 0.99583334 0.99791664\n",
      " 0.99791664 1.         0.99791664 1.         0.99791664 0.9916667\n",
      " 0.99791664 0.99583334 0.99375    0.99791664 0.99791664 1.\n",
      " 0.99583334 0.99791664 0.99791664 1.         1.         0.99791664\n",
      " 0.99375    0.99583334 0.99791664 1.        ]\n",
      "[ 97. 109. 108. 113. 117. 118. 120. 105. 110. 110.  97. 113. 113. 117.\n",
      " 116. 116. 117. 119. 119. 101. 114. 120. 120. 120. 120. 120. 120. 119.\n",
      " 120. 120. 117. 120. 118. 120.  93. 120. 120. 120. 120. 120. 120. 118.\n",
      " 120. 120. 120. 120. 120. 120. 116. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 120. 120.]\n",
      "[0.6125     0.8625     0.88125    0.8625     0.8666667  0.7875\n",
      " 0.8666667  0.87291664 0.8833333  0.8833333  0.8875     0.875\n",
      " 0.87291664 0.875      0.8833333  0.89166665 0.8833333  0.88125\n",
      " 0.87083334 0.87708336 0.87083334 0.86875    0.8833333  0.8645833\n",
      " 0.875      0.89375    0.875      0.88958335 0.87083334 0.8833333\n",
      " 0.89375    0.88958335 0.88125    0.9        0.90833336 0.9145833\n",
      " 0.93125    0.92083335 0.93958336 0.94375    0.95625    0.93125\n",
      " 0.96458334 0.97291666 0.96875    0.9875     0.96458334 0.97291666\n",
      " 0.98333335 0.87083334 0.95625    0.9770833  0.98541665 0.9895833\n",
      " 0.99583334 0.98333335 0.97291666 0.9916667  0.99791664 0.99583334\n",
      " 0.99583334 0.975      0.99375    0.9895833  0.99375    0.92291665\n",
      " 0.98333335 1.         0.99583334 0.99583334 0.99583334 1.\n",
      " 0.99791664 0.99375    0.99791664 0.99791664 0.99791664 0.99791664\n",
      " 0.95208335 0.9895833  0.99583334 0.99791664 0.99791664 1.\n",
      " 1.         1.         1.         1.         0.99791664 0.99583334\n",
      " 1.         1.         0.9916667  0.99583334 0.99791664 1.\n",
      " 0.99583334 0.99375    0.99583334 0.99791664]\n",
      "[109. 110. 112. 109. 112. 108. 111. 111. 110. 112. 110. 111. 111. 111.\n",
      " 112. 111. 111. 112. 111. 110. 111. 111. 111. 111. 112. 111. 111. 111.\n",
      " 110. 112. 112. 113. 112. 115. 115. 115. 115. 116. 116. 118. 117. 118.\n",
      " 119. 117. 118. 118. 120. 120. 107. 115. 120. 119. 119. 120. 118. 110.\n",
      " 120. 120. 120. 120. 112. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.]\n",
      "[0.62916666 0.85833335 0.84583336 0.84583336 0.8666667  0.85\n",
      " 0.87083334 0.87083334 0.8854167  0.87291664 0.8666667  0.8854167\n",
      " 0.86875    0.87291664 0.88125    0.8833333  0.88958335 0.87916666\n",
      " 0.8875     0.88125    0.8833333  0.86875    0.88125    0.8875\n",
      " 0.87083334 0.87708336 0.8666667  0.87708336 0.8854167  0.8833333\n",
      " 0.88125    0.87708336 0.8854167  0.875      0.875      0.87916666\n",
      " 0.86875    0.8833333  0.8666667  0.8833333  0.88125    0.87708336\n",
      " 0.88125    0.875      0.88125    0.875      0.8666667  0.8875\n",
      " 0.8854167  0.87916666 0.88125    0.8958333  0.8854167  0.87916666\n",
      " 0.8854167  0.8875     0.87708336 0.88125    0.87708336 0.8833333\n",
      " 0.87916666 0.87291664 0.87291664 0.89166665 0.87916666 0.89166665\n",
      " 0.875      0.89375    0.88958335 0.8854167  0.88125    0.88125\n",
      " 0.8854167  0.87708336 0.88958335 0.89166665 0.88125    0.8833333\n",
      " 0.875      0.88125    0.8833333  0.8854167  0.88125    0.87916666\n",
      " 0.87708336 0.8875     0.89166665 0.875      0.88958335 0.8833333\n",
      " 0.8958333  0.89375    0.88125    0.87916666 0.88125    0.8854167\n",
      " 0.875      0.8875     0.8833333  0.88125   ]\n",
      "[106. 112. 110. 104. 109. 112. 112. 112. 110. 110. 112. 112. 110. 110.\n",
      " 112. 110. 110. 111. 110. 111. 111. 111. 111. 110. 111. 110. 111. 111.\n",
      " 111. 110. 111. 111. 111. 111. 111. 111. 112. 111. 111. 109. 111. 111.\n",
      " 112. 111. 110. 111. 111. 110. 112. 111. 111. 110. 111. 111. 111. 111.\n",
      " 112. 111. 111. 111. 111. 112. 111. 111. 111. 111. 111. 111. 111. 111.\n",
      " 111. 112. 111. 112. 112. 111. 111. 112. 111. 109. 111. 112. 111. 111.\n",
      " 110. 111. 111. 111. 109. 111. 111. 110. 111. 112. 112. 111. 111. 111.\n",
      " 111. 111.]\n",
      "[0.46666667 0.48333332 0.4875     0.48333332 0.5208333  0.50416666\n",
      " 0.525      0.5125     0.49583334 0.49583334 0.48333332 0.49583334\n",
      " 0.47083333 0.48333332 0.46666667 0.49583334 0.5083333  0.50416666\n",
      " 0.5125     0.45       0.475      0.5        0.5083333  0.50416666\n",
      " 0.50416666 0.5125     0.50416666 0.49583334 0.4625     0.4625\n",
      " 0.475      0.50416666 0.49166667 0.50416666 0.475      0.4875\n",
      " 0.45416668 0.49166667 0.4875     0.5        0.45416668 0.48333332\n",
      " 0.4625     0.51666665 0.49583334 0.49166667 0.47916666 0.5\n",
      " 0.5125     0.49166667 0.5208333  0.4875     0.475      0.52916664\n",
      " 0.48333332 0.4875     0.5083333  0.45416668 0.5        0.49583334\n",
      " 0.48333332 0.47916666 0.49166667 0.49166667 0.49166667 0.47916666\n",
      " 0.44583333 0.48333332 0.48333332 0.5083333  0.475      0.46666667\n",
      " 0.51666665 0.49583334 0.475      0.475      0.48333332 0.475\n",
      " 0.5        0.5083333  0.5        0.48333332 0.5        0.47916666\n",
      " 0.5125     0.47916666 0.49166667 0.47083333 0.48333332 0.48333332\n",
      " 0.45833334 0.45       0.44166666 0.45416668 0.50416666 0.4625\n",
      " 0.5083333  0.4875     0.47083333 0.47083333]\n",
      "[62. 62. 58. 62. 58. 58. 62. 62. 58. 62. 62. 62. 58. 62. 62. 62. 58. 58.\n",
      " 62. 58. 62. 62. 62. 58. 58. 58. 58. 58. 58. 58. 58. 58. 58. 62. 62. 58.\n",
      " 58. 58. 62. 62. 58. 58. 58. 62. 58. 62. 62. 62. 58. 62. 58. 58. 58. 62.\n",
      " 62. 62. 62. 62. 58. 62. 58. 58. 62. 62. 58. 58. 58. 58. 58. 58. 58. 58.\n",
      " 62. 58. 58. 62. 62. 62. 62. 62. 58. 58. 58. 62. 58. 58. 62. 62. 62. 62.\n",
      " 62. 58. 62. 58. 58. 62. 58. 58. 58. 58.]\n",
      "[0.68333334 0.82708335 0.8625     0.8645833  0.875      0.875\n",
      " 0.8645833  0.8625     0.87708336 0.8666667  0.87708336 0.87708336\n",
      " 0.87291664 0.87916666 0.87291664 0.875      0.89375    0.87708336\n",
      " 0.875      0.8833333  0.88125    0.87708336 0.87083334 0.87083334\n",
      " 0.875      0.88125    0.87291664 0.86875    0.87708336 0.87916666\n",
      " 0.8875     0.86875    0.86875    0.875      0.8833333  0.87916666\n",
      " 0.87708336 0.88125    0.89166665 0.87916666 0.8875     0.87916666\n",
      " 0.87916666 0.8875     0.89375    0.88125    0.88958335 0.8833333\n",
      " 0.88125    0.875      0.89166665 0.87916666 0.8833333  0.88958335\n",
      " 0.8854167  0.87291664 0.8854167  0.87291664 0.8854167  0.88958335\n",
      " 0.88125    0.87708336 0.88958335 0.87083334 0.88125    0.89375\n",
      " 0.89166665 0.8958333  0.89166665 0.90625    0.8979167  0.8958333\n",
      " 0.90625    0.91875    0.92083335 0.93333334 0.9583333  0.9375\n",
      " 0.95       0.9604167  0.97083336 0.97291666 0.975      0.9458333\n",
      " 0.98541665 0.99375    0.98541665 0.98125    0.99375    0.96875\n",
      " 0.9895833  0.98541665 0.99583334 0.99583334 0.9791667  0.99375\n",
      " 0.6479167  0.49583334 0.49583334 0.49583334]\n",
      "[107. 112. 112. 107. 111. 110. 110. 110. 111. 111. 110. 112. 110. 111.\n",
      " 110. 112. 111. 110. 112. 111. 111. 111. 111. 111. 111. 111. 109. 111.\n",
      " 112. 111. 111. 111. 111. 111. 111. 112. 112. 111. 112. 111. 112. 111.\n",
      " 110. 111. 112. 111. 112. 111. 112. 110. 110. 111. 112. 112. 111. 111.\n",
      " 111. 110. 112. 110. 111. 112. 112. 111. 112. 109. 112. 113. 113. 113.\n",
      " 108. 114. 114. 115. 115. 116. 116. 117. 118. 119. 119. 118. 117. 119.\n",
      " 120. 118. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.  62.  62.\n",
      "  62.  62.]\n",
      "[0.50416666 0.82708335 0.8520833  0.86041665 0.86875    0.87291664\n",
      " 0.86875    0.85       0.8645833  0.85625    0.8833333  0.86875\n",
      " 0.86875    0.8520833  0.8625     0.8520833  0.85625    0.86041665\n",
      " 0.87291664 0.87083334 0.875      0.86041665 0.8625     0.86875\n",
      " 0.8375     0.8645833  0.8645833  0.8645833  0.87708336 0.86875\n",
      " 0.87916666 0.875      0.8645833  0.8875     0.875      0.88125\n",
      " 0.8833333  0.8875     0.88125    0.8833333  0.8875     0.86875\n",
      " 0.8833333  0.8854167  0.87291664 0.88958335 0.87708336 0.88958335\n",
      " 0.8854167  0.8833333  0.87916666 0.87708336 0.8854167  0.88958335\n",
      " 0.8854167  0.89166665 0.8833333  0.86875    0.88125    0.8854167\n",
      " 0.8875     0.8833333  0.8833333  0.8854167  0.8833333  0.87708336\n",
      " 0.87708336 0.89166665 0.8645833  0.87916666 0.88958335 0.87916666\n",
      " 0.88958335 0.88958335 0.8666667  0.89375    0.8854167  0.8875\n",
      " 0.87916666 0.8875     0.88958335 0.8833333  0.8979167  0.8854167\n",
      " 0.87916666 0.87708336 0.88958335 0.89166665 0.87291664 0.88958335\n",
      " 0.8875     0.89375    0.8833333  0.87083334 0.89166665 0.88958335\n",
      " 0.87916666 0.89375    0.88958335 0.87916666]\n",
      "[ 85. 109. 105. 111. 111. 111. 111. 109. 112. 110. 111. 111. 111. 110.\n",
      " 111. 109. 112. 111. 111. 112. 110. 112. 110. 110. 110. 112. 110. 110.\n",
      " 110. 110. 111. 111. 111. 110. 111. 111. 111. 110. 111. 110. 111. 111.\n",
      " 112. 110. 111. 109. 111. 111. 112. 110. 111. 111. 112. 112. 111. 109.\n",
      " 111. 112. 111. 111. 111. 111. 111. 111. 112. 111. 111. 110. 111. 111.\n",
      " 110. 111. 111. 111. 111. 111. 112. 112. 110. 111. 112. 112. 111. 111.\n",
      " 112. 110. 112. 111. 112. 111. 111. 111. 111. 112. 111. 112. 111. 111.\n",
      " 111. 111.]\n",
      "[0.68125    0.8520833  0.8541667  0.875      0.85625    0.87083334\n",
      " 0.88125    0.88958335 0.86875    0.86875    0.88125    0.86875\n",
      " 0.88958335 0.8875     0.87916666 0.89166665 0.8833333  0.8979167\n",
      " 0.9125     0.91041666 0.925      0.9583333  0.97083336 0.98541665\n",
      " 0.99375    0.99583334 0.99791664 1.         1.         0.99375\n",
      " 1.         0.99791664 0.99791664 0.99791664 0.99583334 0.99791664\n",
      " 0.99791664 0.5375     0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[112. 107. 111. 110. 110. 109. 111. 109. 110. 111. 110. 111. 109. 111.\n",
      " 111. 111. 111. 112. 114. 115. 115. 118. 120. 120. 120. 119. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.]\n",
      "[0.65208334 0.84166664 0.875      0.85833335 0.85833335 0.87083334\n",
      " 0.86041665 0.8833333  0.87083334 0.87708336 0.87083334 0.87708336\n",
      " 0.8833333  0.88958335 0.8854167  0.88958335 0.89166665 0.87916666\n",
      " 0.88125    0.87916666 0.8666667  0.8833333  0.88958335 0.8833333\n",
      " 0.8854167  0.8875     0.8875     0.87916666 0.8833333  0.89375\n",
      " 0.8958333  0.8854167  0.90416664 0.8979167  0.8854167  0.8958333\n",
      " 0.88958335 0.88125    0.88125    0.87916666 0.8854167  0.8875\n",
      " 0.89375    0.8833333  0.8979167  0.88958335 0.8833333  0.89375\n",
      " 0.89375    0.89166665 0.89375    0.8958333  0.8875     0.8854167\n",
      " 0.8875     0.8854167  0.89166665 0.8854167  0.90208334 0.8958333\n",
      " 0.90208334 0.89166665 0.9        0.89166665 0.87083334 0.8854167\n",
      " 0.90625    0.8958333  0.8979167  0.9        0.89375    0.8979167\n",
      " 0.90416664 0.89166665 0.90416664 0.90416664 0.90208334 0.89166665\n",
      " 0.89166665 0.89166665 0.90833336 0.90208334 0.90625    0.88125\n",
      " 0.88958335 0.8958333  0.8958333  0.9125     0.9166667  0.925\n",
      " 0.93958336 0.95416665 0.97291666 0.98125    0.98125    0.67291665\n",
      " 0.6        0.82916665 0.86041665 0.8625    ]\n",
      "[108. 111. 112. 111. 112. 109. 112. 111. 112. 113. 114. 110. 113. 112.\n",
      " 113. 112. 109. 113. 111. 113. 110. 113. 111. 111. 110. 110. 111. 111.\n",
      " 114. 113. 113. 112. 114. 113. 113. 113. 113. 112. 114. 114. 114. 113.\n",
      " 111. 112. 112. 111. 113. 113. 114. 113. 111. 110. 114. 114. 113. 113.\n",
      " 113. 109. 110. 114. 109. 114. 114. 110. 113. 113. 113. 112. 113. 114.\n",
      " 113. 111. 112. 112. 112. 113. 112. 111. 113. 113. 113. 111. 113. 112.\n",
      " 110. 110. 113. 113. 113. 116. 116. 116. 118. 120.  94.  68.  95. 110.\n",
      " 110. 110.]\n",
      "[0.675      0.8354167  0.87291664 0.9166667  0.95208335 0.9791667\n",
      " 0.93958336 0.9375     0.96458334 0.98125    1.         1.\n",
      " 0.99791664 0.99791664 0.99583334 0.99791664 0.9895833  0.99791664\n",
      " 1.         0.99791664 1.         0.99791664 0.99375    0.99583334\n",
      " 0.98541665 0.99375    0.99791664 0.99791664 1.         0.99791664\n",
      " 0.99791664 0.99791664 0.99791664 0.99791664 1.         0.99791664\n",
      " 1.         0.99791664 0.99375    1.         0.99791664 1.\n",
      " 1.         0.99791664 0.99791664 0.99791664 1.         0.99583334\n",
      " 0.99583334 0.99583334 0.99791664 0.99791664 0.99791664 1.\n",
      " 1.         0.99791664 0.99791664 0.99791664 0.99583334 0.73541665\n",
      " 0.49166667 0.4875     0.47916666 0.58125    0.7104167  0.73333335\n",
      " 0.78125    0.79791665 0.8333333  0.7895833  0.8625     0.86875\n",
      " 0.87083334 0.8625     0.8875     0.8625     0.87916666 0.8854167\n",
      " 0.8645833  0.88958335 0.8854167  0.875      0.8666667  0.88125\n",
      " 0.87916666 0.8875     0.86875    0.86875    0.88125    0.88958335\n",
      " 0.89166665 0.875      0.88125    0.87916666 0.8541667  0.87083334\n",
      " 0.8625     0.8833333  0.8875     0.8833333 ]\n",
      "[111. 109. 113. 114. 118. 120. 114. 113. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 118. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120.  62.  62.  62.  69.  76.  99. 101. 105.  97. 111. 111.\n",
      " 109. 111. 108. 111. 110. 110. 111. 110. 110. 111. 111. 110. 111. 111.\n",
      " 111. 110. 111. 112. 109. 111. 111. 111. 111. 111. 112. 107. 111. 111.\n",
      " 112. 111.]\n",
      "[0.55625    0.775      0.825      0.86875    0.85833335 0.86875\n",
      " 0.88958335 0.80625    0.8666667  0.89166665 0.9        0.8979167\n",
      " 0.94166666 0.89375    0.9145833  0.87916666 0.9458333  0.9125\n",
      " 0.96458334 0.9375     0.9583333  0.975      0.9604167  0.90833336\n",
      " 0.95       0.9875     0.975      0.96458334 0.9916667  0.975\n",
      " 0.98541665 0.9791667  0.9916667  0.99583334 0.9895833  0.99375\n",
      " 0.99791664 0.99583334 0.99583334 0.99583334 0.99375    0.6875\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[ 97. 108. 111. 112. 111. 109. 113. 110. 112. 113. 111. 114.  93. 117.\n",
      " 117. 117. 119. 117. 117. 120. 117. 120. 111. 119. 120. 120. 120. 120.\n",
      " 120. 112. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 117.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.]\n",
      "Number of layers: 18\n",
      "[0.5083333  0.76875    0.81666666 0.85833335 0.8541667  0.87291664\n",
      " 0.8520833  0.875      0.84791666 0.87291664 0.8666667  0.8666667\n",
      " 0.8645833  0.86041665 0.8625     0.8625     0.86875    0.8645833\n",
      " 0.85625    0.85833335 0.87083334 0.8520833  0.87083334 0.86041665\n",
      " 0.8666667  0.8645833  0.85833335 0.86875    0.8645833  0.87708336\n",
      " 0.86875    0.86041665 0.8520833  0.86875    0.8625     0.86875\n",
      " 0.85833335 0.85833335 0.87291664 0.87291664 0.8541667  0.86875\n",
      " 0.8666667  0.86875    0.8625     0.875      0.86041665 0.87291664\n",
      " 0.87083334 0.8666667  0.87083334 0.85625    0.8666667  0.8625\n",
      " 0.86875    0.86041665 0.8645833  0.87083334 0.8645833  0.875\n",
      " 0.86875    0.8625     0.86041665 0.8645833  0.87291664 0.8666667\n",
      " 0.8666667  0.85833335 0.8666667  0.85833335 0.8645833  0.86875\n",
      " 0.85833335 0.85625    0.8645833  0.8625     0.86875    0.87291664\n",
      " 0.8645833  0.85833335 0.8645833  0.87083334 0.8625     0.8625\n",
      " 0.86875    0.87291664 0.86041665 0.8645833  0.86041665 0.8645833\n",
      " 0.8625     0.86041665 0.86875    0.87916666 0.85625    0.86875\n",
      " 0.8625     0.87083334 0.8645833  0.87291664]\n",
      "[102. 105. 109. 112. 107. 107. 105. 109. 109. 111. 110. 108. 112. 106.\n",
      " 112. 112. 111. 111. 110. 110. 110. 110. 112. 112. 110. 106. 111. 110.\n",
      " 111. 111. 112. 112. 112. 111. 112. 111. 112. 111. 112. 111. 112. 111.\n",
      " 111. 107. 110. 109. 111. 111. 112. 112. 109. 111. 112. 111. 111. 110.\n",
      " 111. 110. 112. 106. 111. 113. 110. 110. 112. 112. 111. 113. 112. 111.\n",
      " 111. 112. 111. 112. 110. 112. 112. 112. 111. 112. 110. 108. 112. 112.\n",
      " 111. 111. 112. 112. 111. 112. 110. 112. 111. 110. 109. 112. 111. 110.\n",
      " 111. 111.]\n",
      "[0.62291664 0.84583336 0.8625     0.85833335 0.85833335 0.84375\n",
      " 0.85833335 0.8645833  0.87916666 0.87916666 0.89166665 0.87708336\n",
      " 0.8979167  0.90625    0.90208334 0.9166667  0.9291667  0.94375\n",
      " 0.9625     0.9479167  0.86875    0.9458333  0.96458334 0.98333335\n",
      " 0.9791667  0.9875     0.9895833  0.9583333  0.9895833  0.99375\n",
      " 0.9916667  0.99375    0.99583334 0.9895833  0.99375    0.99375\n",
      " 0.9875     0.99375    0.99583334 0.98541665 0.99583334 0.99791664\n",
      " 1.         0.99791664 0.99791664 1.         0.9895833  0.99583334\n",
      " 1.         0.99583334 0.99583334 0.99791664 1.         0.99791664\n",
      " 1.         1.         0.99791664 1.         0.99791664 0.99583334\n",
      " 0.6479167  0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[109. 109. 107. 111. 110. 110. 112. 111. 112. 110. 112. 111. 112. 114.\n",
      " 114. 115. 118. 119. 118.  76. 116. 117. 120. 120. 120. 120. 108. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 117. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  62.  62.]\n",
      "[0.70208335 0.8020833  0.8833333  0.8979167  0.90833336 0.90416664\n",
      " 0.9270833  0.975      0.9875     0.92291665 0.9875     0.99375\n",
      " 0.99375    0.99375    0.99583334 0.99791664 0.99791664 0.99583334\n",
      " 0.99791664 0.99791664 0.9791667  0.99791664 0.99791664 0.99791664\n",
      " 0.99791664 0.99583334 0.99791664 0.99791664 0.99791664 0.99791664\n",
      " 0.99791664 0.99791664 0.99791664 0.99375    0.9916667  0.99791664\n",
      " 0.99791664 0.99375    0.99375    0.99791664 0.99791664 0.99791664\n",
      " 0.99791664 1.         0.99791664 0.99583334 0.99791664 0.99791664\n",
      " 0.99791664 0.99791664 1.         0.99791664 0.99583334 1.\n",
      " 1.         1.         1.         1.         1.         0.9916667\n",
      " 0.99791664 0.99375    0.99791664 0.99583334 1.         1.\n",
      " 1.         1.         1.         0.99583334 0.99583334 0.99791664\n",
      " 1.         1.         1.         0.99583334 1.         1.\n",
      " 0.99791664 0.99791664 1.         1.         0.99791664 0.99375\n",
      " 1.         1.         1.         0.99791664 0.99583334 1.\n",
      " 1.         1.         1.         1.         1.         0.99583334\n",
      " 0.59375    0.49583334 0.49583334 0.49583334]\n",
      "[110. 109. 113. 114. 117. 117. 120. 120. 118. 120. 119. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.  62.  62.\n",
      "  62.  62.]\n",
      "[0.60625    0.79375    0.8625     0.8625     0.87291664 0.87291664\n",
      " 0.8520833  0.8875     0.86875    0.88958335 0.85       0.8875\n",
      " 0.8833333  0.8833333  0.87708336 0.87708336 0.875      0.87916666\n",
      " 0.8833333  0.87708336 0.8645833  0.88125    0.87291664 0.8833333\n",
      " 0.87291664 0.87708336 0.87916666 0.87083334 0.875      0.87916666\n",
      " 0.88125    0.89166665 0.88125    0.8833333  0.87708336 0.8666667\n",
      " 0.89166665 0.87291664 0.875      0.8875     0.89375    0.88125\n",
      " 0.875      0.87916666 0.88958335 0.8854167  0.9        0.875\n",
      " 0.875      0.86875    0.87916666 0.8875     0.8854167  0.8875\n",
      " 0.87708336 0.875      0.875      0.875      0.86875    0.8875\n",
      " 0.8666667  0.8854167  0.8875     0.87916666 0.87083334 0.88125\n",
      " 0.875      0.8875     0.8854167  0.8875     0.88958335 0.8875\n",
      " 0.8875     0.8875     0.875      0.875      0.87708336 0.875\n",
      " 0.8854167  0.8854167  0.8854167  0.87291664 0.8875     0.87291664\n",
      " 0.8625     0.8833333  0.87291664 0.89166665 0.875      0.8875\n",
      " 0.875      0.87916666 0.89375    0.8625     0.8875     0.87916666\n",
      " 0.87291664 0.8854167  0.87708336 0.8875    ]\n",
      "[ 94. 109. 102. 111. 111.  77. 111. 111. 111. 109. 110. 111. 110. 111.\n",
      " 110. 109. 110. 111. 111. 111. 111. 111. 112. 111. 112. 111. 109. 111.\n",
      " 111. 111. 111. 111. 111. 111. 112. 111. 108. 109. 110. 111. 110. 111.\n",
      " 112. 111. 111. 111. 111. 110. 110. 111. 110. 111. 111. 111. 111. 111.\n",
      " 107. 110. 110. 111. 110. 110. 111. 111. 110. 111. 111. 111. 111. 111.\n",
      " 111. 111. 111. 111. 111. 111. 111. 110. 111. 112. 111. 111. 110. 111.\n",
      " 110. 111. 111. 111. 111. 111. 110. 111. 108. 111. 111. 111. 111. 111.\n",
      " 111. 111.]\n",
      "[0.7        0.85       0.8375     0.85       0.8666667  0.8666667\n",
      " 0.88125    0.88125    0.88125    0.87916666 0.875      0.87083334\n",
      " 0.8875     0.9291667  0.96666664 0.94166666 0.99375    0.9895833\n",
      " 0.99791664 1.         0.99583334 0.99375    0.99791664 0.99583334\n",
      " 0.99791664 0.99791664 1.         1.         1.         1.\n",
      " 0.6145833  0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[107. 107. 109. 109. 111. 112. 112. 111. 110. 112. 110. 113. 113. 116.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.]\n",
      "[0.68333334 0.85       0.85       0.87083334 0.8833333  0.88125\n",
      " 0.8875     0.8854167  0.87916666 0.88125    0.8854167  0.87708336\n",
      " 0.86875    0.87708336 0.87916666 0.89166665 0.87708336 0.8666667\n",
      " 0.8833333  0.8854167  0.8854167  0.88125    0.88958335 0.8958333\n",
      " 0.875      0.87291664 0.87916666 0.87291664 0.88125    0.89375\n",
      " 0.8854167  0.88125    0.89166665 0.875      0.88125    0.87291664\n",
      " 0.86041665 0.875      0.88125    0.875      0.8833333  0.8875\n",
      " 0.875      0.88125    0.87708336 0.87291664 0.87916666 0.8666667\n",
      " 0.8854167  0.8854167  0.8875     0.9        0.87291664 0.8875\n",
      " 0.87083334 0.88125    0.8875     0.87708336 0.89166665 0.875\n",
      " 0.8854167  0.875      0.88125    0.87916666 0.87708336 0.8833333\n",
      " 0.8854167  0.8666667  0.8875     0.88958335 0.8833333  0.88125\n",
      " 0.8833333  0.88958335 0.875      0.88958335 0.88125    0.88125\n",
      " 0.8875     0.875      0.87916666 0.87708336 0.87708336 0.88958335\n",
      " 0.8833333  0.8833333  0.88125    0.8854167  0.8833333  0.8854167\n",
      " 0.8833333  0.87083334 0.875      0.87916666 0.8854167  0.8833333\n",
      " 0.87916666 0.87708336 0.87083334 0.8875    ]\n",
      "[111. 103. 110. 111. 111. 111. 110. 110. 110. 110. 110. 111. 111. 111.\n",
      " 111. 111. 111. 111. 111. 112. 111. 111. 111. 111. 110. 111. 108. 111.\n",
      " 111. 111. 111. 111. 112. 111. 110. 111. 111. 111. 111. 108. 112. 111.\n",
      " 110. 112. 111. 110. 109. 111. 111. 111. 111. 111. 109. 111. 112. 111.\n",
      " 111. 111. 111. 110. 111. 111. 111. 111. 112. 110. 111. 111. 111. 111.\n",
      " 109. 111. 111. 111. 111. 111. 111. 111. 111. 111. 111. 111. 111. 110.\n",
      " 111. 111. 111. 111. 111. 110. 110. 110. 110. 111. 111. 111. 111. 110.\n",
      " 111. 111.]\n",
      "[0.52916664 0.78333336 0.8645833  0.84166664 0.8541667  0.8875\n",
      " 0.88958335 0.90416664 0.92291665 0.94166666 0.76458335 0.85625\n",
      " 0.87083334 0.8625     0.86875    0.8645833  0.87708336 0.86875\n",
      " 0.875      0.8666667  0.88125    0.8875     0.8645833  0.88958335\n",
      " 0.86041665 0.87708336 0.8625     0.88125    0.8833333  0.88125\n",
      " 0.875      0.87083334 0.87916666 0.8833333  0.87291664 0.8833333\n",
      " 0.88125    0.88958335 0.9        0.89166665 0.86875    0.875\n",
      " 0.8645833  0.8833333  0.8875     0.88958335 0.87291664 0.87916666\n",
      " 0.875      0.87916666 0.875      0.8875     0.87708336 0.8958333\n",
      " 0.8645833  0.88958335 0.8833333  0.87083334 0.8854167  0.875\n",
      " 0.87916666 0.87708336 0.8854167  0.86875    0.87708336 0.87291664\n",
      " 0.89166665 0.8875     0.87291664 0.87291664 0.8833333  0.8875\n",
      " 0.8833333  0.8875     0.8875     0.87916666 0.88125    0.87291664\n",
      " 0.8958333  0.8833333  0.8833333  0.8854167  0.8875     0.88125\n",
      " 0.87083334 0.86041665 0.89166665 0.8979167  0.875      0.87916666\n",
      " 0.8875     0.8854167  0.8833333  0.89166665 0.86875    0.8875\n",
      " 0.8833333  0.87916666 0.8833333  0.88125   ]\n",
      "[104. 112. 110. 109. 111. 112. 114. 113. 116. 119. 108. 110. 112. 111.\n",
      " 111. 111. 112. 110. 112. 110. 110. 110. 110. 111. 110. 111. 111. 111.\n",
      " 111. 108. 111. 110. 111. 111. 111. 111. 111. 112. 111. 111. 111. 111.\n",
      " 111. 111. 111. 109. 112. 110. 111. 111. 112. 110. 111. 111. 111. 110.\n",
      " 111. 111. 109. 111. 111. 112. 111. 109. 109. 111. 111. 109. 110. 111.\n",
      " 111. 111. 111. 111. 111. 112. 111. 111. 111. 111. 111. 111. 110. 111.\n",
      " 111. 111. 111. 111. 110. 112. 111. 111. 111. 110. 111. 111. 111. 111.\n",
      " 111. 111.]\n",
      "[0.66875    0.8354167  0.8041667  0.8875     0.9270833  0.9625\n",
      " 0.9145833  0.98333335 0.98541665 0.9625     0.9895833  0.99583334\n",
      " 0.99791664 0.99375    0.99375    0.99375    0.99791664 0.99791664\n",
      " 0.99791664 0.99583334 0.84583336 0.51458335 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[111. 111. 109. 112. 114. 118. 120. 120. 119. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120.  78.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.]\n",
      "[0.69375    0.8354167  0.84375    0.8666667  0.89166665 0.85625\n",
      " 0.9291667  0.96458334 0.96875    0.99583334 0.99375    0.99583334\n",
      " 0.62083334 0.7375     0.75625    0.77708334 0.825      0.87083334\n",
      " 0.8854167  0.88958335 0.8666667  0.89166665 0.875      0.8854167\n",
      " 0.8833333  0.8625     0.87083334 0.8854167  0.8854167  0.8645833\n",
      " 0.88125    0.87291664 0.8833333  0.87708336 0.8666667  0.875\n",
      " 0.8854167  0.8666667  0.8625     0.88125    0.87708336 0.87916666\n",
      " 0.875      0.86041665 0.875      0.89166665 0.8854167  0.8875\n",
      " 0.88958335 0.87083334 0.86875    0.89166665 0.88125    0.8833333\n",
      " 0.87291664 0.875      0.88958335 0.87708336 0.87083334 0.8854167\n",
      " 0.8854167  0.8854167  0.87916666 0.86875    0.8875     0.87916666\n",
      " 0.8854167  0.8854167  0.87083334 0.86875    0.8833333  0.88125\n",
      " 0.875      0.875      0.88125    0.87708336 0.875      0.88958335\n",
      " 0.87708336 0.8625     0.8833333  0.8854167  0.88958335 0.88958335\n",
      " 0.8854167  0.8833333  0.87916666 0.88125    0.87916666 0.89375\n",
      " 0.87083334 0.88958335 0.87916666 0.88958335 0.875      0.88125\n",
      " 0.87916666 0.8875     0.8833333  0.8666667 ]\n",
      "[107. 107. 112. 113. 113. 113. 116. 111. 120. 120. 120. 117.  96.  98.\n",
      " 103. 107. 107. 111. 110. 111. 109. 110. 111. 112. 109. 111. 111. 111.\n",
      " 110. 111. 111. 110. 112. 111. 110. 112. 111. 103. 111. 112. 111. 111.\n",
      " 111. 111. 112. 111. 111. 111. 111. 111. 110. 111. 111. 111. 111. 112.\n",
      " 111. 110. 111. 111. 111. 112. 110. 112. 110. 111. 111. 111. 111. 111.\n",
      " 111. 111. 109. 111. 112. 109. 110. 111. 112. 111. 111. 111. 111. 111.\n",
      " 112. 111. 110. 112. 111. 111. 111. 111. 112. 112. 111. 111. 112. 111.\n",
      " 111. 111.]\n",
      "[0.47916666 0.48541668 0.54375    0.59583336 0.7270833  0.76458335\n",
      " 0.78541666 0.8625     0.8645833  0.85625    0.8354167  0.85833335\n",
      " 0.86041665 0.87083334 0.86875    0.8625     0.85833335 0.84791666\n",
      " 0.8520833  0.8645833  0.8666667  0.87083334 0.8645833  0.8541667\n",
      " 0.8645833  0.8625     0.8645833  0.8666667  0.8666667  0.85\n",
      " 0.8645833  0.8541667  0.84375    0.86875    0.87083334 0.87083334\n",
      " 0.8625     0.8625     0.84791666 0.8666667  0.86041665 0.8625\n",
      " 0.8666667  0.8666667  0.87083334 0.86041665 0.87291664 0.86041665\n",
      " 0.8666667  0.87291664 0.8625     0.8645833  0.85625    0.875\n",
      " 0.8645833  0.8625     0.8520833  0.87083334 0.8520833  0.8645833\n",
      " 0.8625     0.8666667  0.8541667  0.85833335 0.84375    0.86041665\n",
      " 0.86041665 0.86875    0.87291664 0.8625     0.8666667  0.8666667\n",
      " 0.8666667  0.86875    0.875      0.85       0.8645833  0.8666667\n",
      " 0.87291664 0.86875    0.85625    0.85625    0.86041665 0.8541667\n",
      " 0.8645833  0.8645833  0.86875    0.8645833  0.86041665 0.875\n",
      " 0.86041665 0.87291664 0.86041665 0.87083334 0.8625     0.86041665\n",
      " 0.8666667  0.85625    0.87083334 0.87083334]\n",
      "[ 62.  62.  59.  92. 100. 103. 107. 111. 109. 106. 106. 111. 110. 112.\n",
      " 110. 111. 108. 108. 111. 112. 110. 108. 111. 106. 112. 111. 112. 112.\n",
      " 112. 111. 111. 111. 111. 111. 110. 112. 104. 109. 107. 111. 111. 111.\n",
      " 110. 110. 113. 109. 111. 110. 110. 111. 107. 110. 112. 111. 111. 110.\n",
      " 112. 110. 111. 112. 112. 111. 111. 108. 110. 111. 110. 113. 110. 111.\n",
      " 110. 110. 112. 111. 109. 111. 110. 111. 113. 110. 112. 106. 111. 110.\n",
      " 112. 111. 111. 112. 110. 112. 112. 110. 112. 112. 112. 109. 111. 112.\n",
      " 111. 108.]\n",
      "Number of layers: 19\n",
      "[0.7104167  0.8666667  0.8625     0.8833333  0.87708336 0.87291664\n",
      " 0.8625     0.8958333  0.8854167  0.86041665 0.87291664 0.8833333\n",
      " 0.86041665 0.8625     0.8645833  0.875      0.875      0.8645833\n",
      " 0.87708336 0.87916666 0.8645833  0.89166665 0.87708336 0.86875\n",
      " 0.87083334 0.87083334 0.8854167  0.87708336 0.8958333  0.8854167\n",
      " 0.85       0.87291664 0.8958333  0.87916666 0.87291664 0.87708336\n",
      " 0.87083334 0.86875    0.8666667  0.8854167  0.8875     0.87291664\n",
      " 0.8833333  0.87083334 0.8645833  0.87291664 0.88125    0.87916666\n",
      " 0.88125    0.875      0.87916666 0.88125    0.87916666 0.8666667\n",
      " 0.87708336 0.8854167  0.89375    0.86041665 0.87708336 0.88958335\n",
      " 0.87916666 0.88125    0.88125    0.8854167  0.8854167  0.8833333\n",
      " 0.875      0.8833333  0.8833333  0.8833333  0.8645833  0.87291664\n",
      " 0.8854167  0.8645833  0.8833333  0.8875     0.8854167  0.88125\n",
      " 0.875      0.875      0.87916666 0.89375    0.86875    0.875\n",
      " 0.8875     0.8666667  0.87916666 0.87291664 0.8875     0.87916666\n",
      " 0.87916666 0.87708336 0.8875     0.8833333  0.8833333  0.88125\n",
      " 0.8833333  0.85833335 0.87291664 0.87916666]\n",
      "[109. 111. 111. 112. 111. 111. 112. 112. 112. 110. 111. 109. 111. 111.\n",
      " 111. 111. 110. 111. 111. 111. 111. 112. 109. 110. 112. 111. 111. 111.\n",
      " 111. 112. 111. 112. 111. 111. 111. 111. 111. 112. 111. 112. 111. 111.\n",
      " 111. 111. 111. 111. 111. 112. 112. 111. 111. 112. 111. 110. 112. 111.\n",
      " 111. 112. 111. 112. 111. 111. 112. 112. 112. 111. 111. 112. 112. 112.\n",
      " 110. 111. 111. 108. 110. 112. 110. 111. 111. 111. 111. 112. 111. 111.\n",
      " 112. 112. 112. 112. 112. 112. 112. 112. 112. 112. 112. 111. 112. 111.\n",
      " 111. 112.]\n",
      "[0.7395833  0.88125    0.8375     0.87083334 0.8833333  0.89375\n",
      " 0.8875     0.875      0.875      0.8625     0.8875     0.88958335\n",
      " 0.8979167  0.88125    0.88958335 0.9        0.8854167  0.875\n",
      " 0.87291664 0.89166665 0.87916666 0.8979167  0.88958335 0.8854167\n",
      " 0.9        0.89375    0.90625    0.88125    0.88958335 0.89166665\n",
      " 0.89375    0.89375    0.90208334 0.89166665 0.87916666 0.9\n",
      " 0.8854167  0.8958333  0.8854167  0.8854167  0.89375    0.8979167\n",
      " 0.9        0.8979167  0.8979167  0.9        0.8833333  0.9\n",
      " 0.90208334 0.8958333  0.89375    0.8875     0.89375    0.90208334\n",
      " 0.9        0.9        0.88958335 0.8875     0.9        0.8958333\n",
      " 0.8875     0.8854167  0.89166665 0.90625    0.89375    0.89166665\n",
      " 0.88958335 0.89166665 0.8958333  0.8979167  0.8979167  0.8854167\n",
      " 0.8979167  0.90416664 0.90208334 0.90208334 0.9        0.90208334\n",
      " 0.88958335 0.88958335 0.8958333  0.89375    0.8958333  0.89166665\n",
      " 0.8854167  0.8979167  0.88125    0.90208334 0.8979167  0.8979167\n",
      " 0.89166665 0.90416664 0.90208334 0.89166665 0.8958333  0.90208334\n",
      " 0.89166665 0.89375    0.8979167  0.89375   ]\n",
      "[113. 111. 109. 114. 113. 112. 113. 114. 109. 112. 113. 112. 111. 113.\n",
      " 114. 113. 114. 113. 113. 113. 110. 110. 114. 110. 113. 110. 111. 112.\n",
      " 109. 113. 112. 113. 110. 113. 114. 113. 114. 110. 112. 111. 111. 112.\n",
      " 113. 113. 113. 110. 113. 113. 111. 113. 112. 113. 112. 109. 114. 113.\n",
      " 112. 113. 112. 113. 113. 113. 114. 113. 113. 112. 113. 112. 113. 111.\n",
      " 112. 114. 112. 111. 110. 113. 113. 112. 113. 113. 113. 111. 109. 111.\n",
      " 112. 112. 111. 111. 111. 113. 114. 113. 114. 113. 112. 113. 111. 113.\n",
      " 111. 113.]\n",
      "[0.6666667  0.84583336 0.85625    0.87708336 0.8520833  0.87708336\n",
      " 0.89166665 0.9145833  0.90833336 0.93958336 0.94375    0.975\n",
      " 0.95625    0.5083333  0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[111. 105. 109. 111. 112. 112. 113. 108. 111. 118. 116. 120.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  62.  62.]\n",
      "[0.50416666 0.5395833  0.50416666 0.525      0.5083333  0.47916666\n",
      " 0.51458335 0.475      0.4875     0.48333332 0.5125     0.7270833\n",
      " 0.8520833  0.86041665 0.85       0.8520833  0.8645833  0.85625\n",
      " 0.87083334 0.8520833  0.87291664 0.87083334 0.8625     0.8645833\n",
      " 0.85833335 0.85833335 0.86041665 0.8625     0.84791666 0.86875\n",
      " 0.85625    0.8875     0.85833335 0.8666667  0.85833335 0.8520833\n",
      " 0.8645833  0.87291664 0.8666667  0.8645833  0.8520833  0.86875\n",
      " 0.87083334 0.8645833  0.8625     0.8625     0.86875    0.8645833\n",
      " 0.84791666 0.8645833  0.85625    0.8625     0.87708336 0.8625\n",
      " 0.8625     0.87916666 0.8666667  0.85833335 0.87291664 0.87083334\n",
      " 0.8625     0.8666667  0.875      0.85625    0.86041665 0.8666667\n",
      " 0.87708336 0.86041665 0.8666667  0.87291664 0.8645833  0.85625\n",
      " 0.86875    0.86875    0.86875    0.8666667  0.8541667  0.86875\n",
      " 0.86041665 0.86041665 0.86041665 0.8625     0.87083334 0.87708336\n",
      " 0.8666667  0.86875    0.85833335 0.85833335 0.87291664 0.85833335\n",
      " 0.85833335 0.87083334 0.87291664 0.87291664 0.875      0.8645833\n",
      " 0.8625     0.8666667  0.87708336 0.86875   ]\n",
      "[ 62.  58.  58.  62.  58.  58.  62.  58.  58.  58.  62. 107. 109. 111.\n",
      " 111. 108. 112. 110. 110. 112. 108. 112. 111. 111. 111. 111. 106. 113.\n",
      " 111. 110. 111. 110. 110. 110. 108. 112. 111. 111. 112. 112. 110. 112.\n",
      " 109. 113. 111. 112. 112. 111. 109. 110. 112. 110. 113. 111. 110. 110.\n",
      " 112. 110. 112. 112. 111. 110. 110. 110. 111. 111. 113. 112. 111. 111.\n",
      " 110. 112. 109. 110. 110. 110. 111. 112. 111. 111. 111. 111. 110. 111.\n",
      " 112. 110. 113. 111. 110. 111. 111. 113. 111. 110. 112. 109. 110. 111.\n",
      " 112. 112.]\n",
      "[0.5375     0.80625    0.85625    0.86041665 0.8520833  0.8333333\n",
      " 0.8541667  0.8541667  0.85833335 0.8625     0.8541667  0.8520833\n",
      " 0.87083334 0.8645833  0.87083334 0.8645833  0.85833335 0.85833335\n",
      " 0.8666667  0.8520833  0.8625     0.85625    0.86875    0.85833335\n",
      " 0.87083334 0.8625     0.86041665 0.86875    0.87083334 0.86875\n",
      " 0.8666667  0.8645833  0.8625     0.8625     0.85833335 0.87083334\n",
      " 0.8666667  0.8625     0.86875    0.825      0.8666667  0.8666667\n",
      " 0.8625     0.8625     0.8625     0.8625     0.8625     0.875\n",
      " 0.86875    0.875      0.8645833  0.8666667  0.87083334 0.8645833\n",
      " 0.8541667  0.8645833  0.86875    0.875      0.8541667  0.86875\n",
      " 0.8645833  0.8666667  0.8645833  0.8541667  0.8645833  0.87916666\n",
      " 0.8645833  0.86041665 0.87708336 0.86875    0.8666667  0.87083334\n",
      " 0.8645833  0.86875    0.86875    0.8666667  0.86875    0.87291664\n",
      " 0.8625     0.8645833  0.8625     0.8666667  0.8625     0.8645833\n",
      " 0.8645833  0.85833335 0.85833335 0.8645833  0.875      0.87083334\n",
      " 0.8666667  0.87916666 0.85625    0.8645833  0.87083334 0.86875\n",
      " 0.8625     0.8625     0.8666667  0.85833335]\n",
      "[101. 109. 108. 110. 110. 103. 110. 110. 111. 108. 110. 111. 111. 111.\n",
      " 112. 111. 111. 112. 112. 110. 110. 111. 110. 112. 112. 111. 111. 110.\n",
      " 113. 109. 111. 106. 111. 111. 110. 111. 112. 111. 111. 109. 111. 109.\n",
      " 112. 112. 111. 110. 111. 111. 110. 112. 112. 111. 109. 109. 112. 111.\n",
      " 111. 112. 111. 111. 112. 112. 111. 110. 112. 112. 112. 109. 113. 112.\n",
      " 112. 112. 110. 112. 111. 110. 110. 110. 111. 113. 112. 112. 112. 111.\n",
      " 112. 112. 111. 112. 111. 112. 111. 109. 112. 111. 111. 111. 111. 111.\n",
      " 111. 109.]\n",
      "[0.62083334 0.84166664 0.8541667  0.86041665 0.85833335 0.85833335\n",
      " 0.84583336 0.8520833  0.87708336 0.87083334 0.8666667  0.86041665\n",
      " 0.87291664 0.875      0.87916666 0.8833333  0.8979167  0.90208334\n",
      " 0.9166667  0.91875    0.65625    0.83958334 0.875      0.87291664\n",
      " 0.8645833  0.8645833  0.8625     0.8645833  0.86041665 0.87083334\n",
      " 0.87083334 0.86875    0.85625    0.88125    0.8875     0.8645833\n",
      " 0.8854167  0.87916666 0.86875    0.87916666 0.8666667  0.89166665\n",
      " 0.87708336 0.87291664 0.87083334 0.87708336 0.88958335 0.87916666\n",
      " 0.8854167  0.8833333  0.8645833  0.8666667  0.8854167  0.85833335\n",
      " 0.88958335 0.87708336 0.88125    0.87916666 0.87708336 0.8958333\n",
      " 0.87291664 0.87083334 0.86875    0.8875     0.89375    0.87708336\n",
      " 0.87916666 0.875      0.85625    0.8833333  0.8854167  0.8645833\n",
      " 0.88125    0.8666667  0.89166665 0.8875     0.88958335 0.88958335\n",
      " 0.8833333  0.8833333  0.8875     0.87916666 0.85833335 0.8875\n",
      " 0.875      0.87708336 0.88958335 0.8833333  0.89375    0.87083334\n",
      " 0.8854167  0.8833333  0.87291664 0.8854167  0.8875     0.8833333\n",
      " 0.87708336 0.8854167  0.8833333  0.87708336]\n",
      "[111. 112. 103. 110. 112. 110. 112. 111. 109. 111. 111. 110. 113. 110.\n",
      " 111. 112. 108. 116. 117.  93. 104. 111. 111. 110. 110. 110. 106. 111.\n",
      " 112. 110. 111. 111. 112. 110. 111. 108. 110. 109. 112. 112. 110. 110.\n",
      " 111. 111. 111. 110. 111. 111. 111. 109. 111. 111. 111. 111. 111. 112.\n",
      " 111. 110. 112. 111. 111. 111. 112. 111. 111. 111. 112. 110. 111. 111.\n",
      " 111. 110. 111. 111. 110. 111. 112. 110. 111. 110. 110. 111. 112. 111.\n",
      " 111. 111. 110. 111. 112. 111. 111. 111. 111. 111. 112. 111. 111. 109.\n",
      " 111. 111.]\n",
      "[0.6354167  0.85       0.84166664 0.86875    0.86041665 0.8541667\n",
      " 0.85625    0.875      0.8520833  0.8645833  0.87916666 0.8625\n",
      " 0.85       0.85833335 0.8666667  0.85833335 0.8541667  0.8666667\n",
      " 0.8625     0.8645833  0.86041665 0.875      0.8645833  0.875\n",
      " 0.88125    0.875      0.87708336 0.86875    0.89166665 0.89375\n",
      " 0.87708336 0.89166665 0.8833333  0.93541664 0.91875    0.9125\n",
      " 0.93333334 0.9375     0.86875    0.86875    0.9375     0.92083335\n",
      " 0.89166665 0.9604167  0.9375     0.9145833  0.9770833  0.9895833\n",
      " 0.9916667  0.9875     0.9916667  0.9916667  1.         0.99791664\n",
      " 0.99375    0.99375    0.99791664 0.99791664 0.99791664 0.99583334\n",
      " 1.         1.         0.99583334 0.99791664 0.99583334 0.99375\n",
      " 0.99375    0.99791664 0.99583334 0.99791664 0.99583334 0.99791664\n",
      " 0.99583334 0.99791664 0.99583334 0.99583334 0.99583334 0.99791664\n",
      " 1.         0.99583334 0.99583334 0.9916667  0.99583334 0.99791664\n",
      " 0.99791664 0.9895833  0.99375    1.         1.         0.99791664\n",
      " 0.99791664 1.         0.99791664 1.         1.         0.99375\n",
      " 0.99791664 0.99375    0.99791664 1.        ]\n",
      "[108. 108. 103. 110. 108. 113. 112. 107. 111. 111. 108. 107. 111. 111.\n",
      " 112. 112. 109. 106. 112. 112. 111. 109. 112. 112. 111. 109. 111. 112.\n",
      " 110. 112. 112. 113. 112. 112. 118. 114. 112. 116.  95. 112. 113.  76.\n",
      " 116. 120.  97. 119. 118. 120. 120. 120. 120. 120. 120. 120. 119. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 119. 120. 120. 120. 120. 120. 120. 117. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.]\n",
      "[0.6333333  0.76875    0.82916665 0.83125    0.8645833  0.875\n",
      " 0.84583336 0.8625     0.8541667  0.88958335 0.875      0.85\n",
      " 0.89375    0.89375    0.89166665 0.9145833  0.9625     0.96666664\n",
      " 0.9916667  0.8354167  0.84583336 0.86875    0.8625     0.85625\n",
      " 0.85833335 0.84791666 0.8666667  0.85       0.85833335 0.8645833\n",
      " 0.8541667  0.8625     0.8666667  0.86875    0.8645833  0.8625\n",
      " 0.85625    0.86875    0.85       0.87083334 0.85       0.86875\n",
      " 0.8666667  0.86041665 0.8666667  0.8645833  0.8666667  0.8541667\n",
      " 0.8625     0.8645833  0.875      0.8666667  0.8666667  0.8625\n",
      " 0.85625    0.87083334 0.87083334 0.8625     0.8645833  0.8645833\n",
      " 0.8666667  0.85625    0.85625    0.86875    0.8645833  0.85833335\n",
      " 0.8645833  0.85833335 0.8541667  0.8625     0.8666667  0.8520833\n",
      " 0.8666667  0.86041665 0.87083334 0.87291664 0.8625     0.8645833\n",
      " 0.8625     0.8625     0.87083334 0.86875    0.8645833  0.86875\n",
      " 0.8625     0.8625     0.8541667  0.8645833  0.87291664 0.87083334\n",
      " 0.8625     0.87083334 0.8666667  0.85833335 0.85833335 0.8625\n",
      " 0.86875    0.8645833  0.8666667  0.86041665]\n",
      "[ 58. 106. 109. 109. 110. 112. 106. 110. 110. 107. 106. 110. 114. 109.\n",
      " 111. 118. 114. 120. 120.  92. 111. 109. 112. 110. 112. 110. 112. 111.\n",
      " 110. 111. 111. 110. 106. 110. 112. 113. 111. 108. 111. 110. 106. 112.\n",
      " 112. 112. 111. 112. 111. 111. 112. 111. 112. 112. 111. 110. 109. 111.\n",
      " 111. 106. 111. 112. 110. 112. 111. 112. 112. 112. 111. 110. 111. 111.\n",
      " 110. 111. 112. 110. 111. 110. 111. 110. 112. 111. 111. 110. 110. 113.\n",
      " 112. 106. 110. 109. 112. 110. 111. 110. 109. 111. 112. 109. 112. 112.\n",
      " 111. 112.]\n",
      "[0.6166667  0.84166664 0.8520833  0.85       0.8375     0.87083334\n",
      " 0.8520833  0.8625     0.87083334 0.86875    0.8625     0.85\n",
      " 0.85833335 0.8666667  0.87083334 0.86041665 0.8666667  0.8645833\n",
      " 0.84375    0.8625     0.8645833  0.86875    0.8541667  0.8645833\n",
      " 0.87916666 0.87708336 0.86875    0.84375    0.86875    0.87083334\n",
      " 0.86041665 0.87708336 0.87291664 0.87291664 0.8625     0.8645833\n",
      " 0.8541667  0.85625    0.8645833  0.88125    0.87083334 0.88125\n",
      " 0.9291667  0.93958336 0.95208335 0.99583334 0.99375    0.99375\n",
      " 0.90416664 0.6666667  0.8229167  0.8541667  0.8666667  0.8645833\n",
      " 0.87083334 0.87708336 0.8833333  0.87916666 0.88958335 0.87916666\n",
      " 0.8666667  0.88958335 0.87083334 0.8875     0.8875     0.88125\n",
      " 0.875      0.875      0.875      0.87083334 0.87916666 0.88125\n",
      " 0.88958335 0.875      0.87083334 0.875      0.875      0.8625\n",
      " 0.8833333  0.87916666 0.8854167  0.875      0.8833333  0.8833333\n",
      " 0.86875    0.8625     0.875      0.875      0.88125    0.86875\n",
      " 0.87708336 0.8854167  0.8854167  0.8666667  0.87708336 0.87083334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.8645833  0.85833335 0.875      0.875     ]\n",
      "[110. 108. 112. 107. 110. 111. 108. 112. 112. 109. 110. 111. 111. 110.\n",
      " 110. 112. 112. 111. 111. 112. 107. 111. 111. 110. 111. 110. 111. 111.\n",
      " 109. 110. 109. 110. 112. 111. 106. 111. 110. 110. 111. 112. 104. 112.\n",
      " 113. 116. 120. 120. 120. 120.  75.  86. 111. 112. 111. 110. 110. 111.\n",
      " 110. 110. 112. 110. 111. 110. 112. 111. 110. 106. 111. 111. 111. 111.\n",
      " 111. 111. 107. 111. 111. 111. 110. 111. 111. 110. 111. 111. 112. 112.\n",
      " 111. 112. 109. 111. 111. 111. 112. 112. 108. 109. 111. 112. 111. 111.\n",
      " 110. 112.]\n",
      "[0.675      0.84791666 0.83125    0.85625    0.8625     0.86041665\n",
      " 0.8666667  0.86875    0.84583336 0.8666667  0.84583336 0.87291664\n",
      " 0.86041665 0.86875    0.8666667  0.85833335 0.8520833  0.86041665\n",
      " 0.875      0.8645833  0.87291664 0.8625     0.84375    0.86875\n",
      " 0.87291664 0.86875    0.86875    0.8666667  0.86041665 0.87916666\n",
      " 0.8666667  0.875      0.875      0.87916666 0.87916666 0.8875\n",
      " 0.85       0.88958335 0.8958333  0.88125    0.8875     0.89166665\n",
      " 0.89166665 0.8875     0.8875     0.88958335 0.89166665 0.89375\n",
      " 0.8979167  0.88125    0.8958333  0.88958335 0.88958335 0.86875\n",
      " 0.8979167  0.8875     0.87291664 0.8875     0.89375    0.89375\n",
      " 0.88958335 0.87916666 0.8958333  0.88958335 0.8833333  0.8833333\n",
      " 0.87916666 0.86875    0.8854167  0.8833333  0.8854167  0.88958335\n",
      " 0.8875     0.8875     0.8833333  0.89166665 0.88958335 0.8875\n",
      " 0.8854167  0.89375    0.89375    0.89166665 0.89375    0.875\n",
      " 0.89166665 0.87916666 0.89166665 0.8958333  0.89166665 0.8958333\n",
      " 0.8979167  0.88958335 0.89375    0.8854167  0.89375    0.87916666\n",
      " 0.90208334 0.89375    0.89375    0.8833333 ]\n",
      "[ 89. 111. 111. 110. 111. 111. 110. 111. 108. 111. 111. 108. 112. 112.\n",
      " 109. 111. 111. 108. 112. 112. 110. 111. 110. 111. 112. 109. 108. 110.\n",
      " 113. 112. 111. 112. 112. 107. 112. 113. 112. 112. 112. 112. 112. 113.\n",
      " 109. 112. 113. 112. 113. 113. 113. 112. 112. 111. 111. 114. 113. 110.\n",
      " 113. 113. 109. 111. 112. 112. 112. 111. 112. 114. 109. 111. 111. 111.\n",
      " 113. 112. 113. 113. 111. 113. 113. 113. 113. 113. 114. 112. 111. 113.\n",
      " 112. 113. 112. 111. 113. 113. 112. 113. 110. 114. 112. 113. 111. 112.\n",
      " 114. 113.]\n",
      "Number of layers: 20\n",
      "[0.49166667 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.]\n",
      "[0.74375    0.85       0.8645833  0.8645833  0.84375    0.83958334\n",
      " 0.8520833  0.86041665 0.8625     0.84791666 0.86875    0.8666667\n",
      " 0.8541667  0.87708336 0.8666667  0.8666667  0.86041665 0.8666667\n",
      " 0.8625     0.8666667  0.8666667  0.87708336 0.8645833  0.8666667\n",
      " 0.87291664 0.87291664 0.8666667  0.8541667  0.8625     0.8645833\n",
      " 0.85625    0.87291664 0.8666667  0.8645833  0.87291664 0.8625\n",
      " 0.85833335 0.8645833  0.8625     0.8666667  0.86875    0.86875\n",
      " 0.87708336 0.8645833  0.8645833  0.8625     0.8625     0.8625\n",
      " 0.8666667  0.87708336 0.8645833  0.85625    0.8645833  0.8625\n",
      " 0.875      0.8645833  0.85833335 0.85625    0.8645833  0.86041665\n",
      " 0.8645833  0.8645833  0.87083334 0.85833335 0.85625    0.8625\n",
      " 0.8666667  0.87291664 0.875      0.8625     0.8666667  0.86875\n",
      " 0.87083334 0.8666667  0.87916666 0.8625     0.85833335 0.85625\n",
      " 0.87291664 0.87916666 0.875      0.85625    0.86875    0.86875\n",
      " 0.85625    0.8625     0.8625     0.86875    0.8666667  0.8625\n",
      " 0.8645833  0.8645833  0.8625     0.85833335 0.8666667  0.875\n",
      " 0.8645833  0.86041665 0.8666667  0.86875   ]\n",
      "[108. 111. 110. 110. 111. 111. 111. 111. 108. 111. 111. 111. 110. 111.\n",
      " 109. 112. 112. 112. 109. 112. 110. 111. 111. 111. 112. 112. 112. 111.\n",
      " 112. 111. 111. 112. 111. 107. 112. 111. 110. 112. 112. 111. 110. 112.\n",
      " 113. 112. 111. 111. 112. 112. 110. 112. 111. 111. 110. 111. 112. 112.\n",
      " 110. 111. 110. 109. 111. 113. 110. 109. 112. 111. 111. 112. 112. 110.\n",
      " 111. 111. 110. 111. 112. 110. 112. 111. 113. 111. 110. 112. 112. 111.\n",
      " 110. 112. 112. 110. 111. 112. 112. 112. 112. 111. 112. 111. 112. 111.\n",
      " 109. 111.]\n",
      "[0.65833336 0.8208333  0.86041665 0.875      0.86041665 0.8833333\n",
      " 0.84791666 0.89166665 0.90625    0.9166667  0.9291667  0.9\n",
      " 0.90416664 0.95       0.93125    0.92291665 0.97291666 0.98333335\n",
      " 0.92291665 0.9791667  0.94375    0.9875     0.9895833  0.97291666\n",
      " 1.         0.99583334 0.99583334 0.99791664 0.99583334 1.\n",
      " 1.         1.         0.975      0.99791664 1.         1.\n",
      " 0.99791664 0.99791664 0.99791664 0.99791664 0.99583334 0.99375\n",
      " 0.98541665 0.99791664 1.         1.         0.97083336 0.90833336\n",
      " 0.9916667  1.         1.         0.99791664 0.99791664 0.99583334\n",
      " 0.99583334 0.99791664 0.99791664 1.         0.99791664 1.\n",
      " 0.99583334 1.         1.         1.         1.         0.99791664\n",
      " 0.99791664 1.         1.         1.         1.         1.\n",
      " 1.         0.99791664 1.         0.99791664 1.         1.\n",
      " 0.775      0.7875     0.87291664 0.89166665 0.93958336 0.9625\n",
      " 0.95625    0.9625     0.9916667  0.95       1.         1.\n",
      " 0.99583334 0.97291666 0.71666664 0.82708335 0.88958335 0.96666664\n",
      " 0.98541665 0.9875     0.9916667  0.98333335]\n",
      "[107. 108. 110. 109. 109. 112. 112. 113. 115. 100. 118. 114. 118. 111.\n",
      " 110. 118. 118. 118. 120. 116. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 107. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 100. 107. 112. 116. 118. 113.\n",
      " 120. 118. 115. 116. 120. 120. 120.  87. 104. 110. 115. 119. 120. 120.\n",
      " 120. 120.]\n",
      "[0.5416667  0.8125     0.82916665 0.85625    0.8854167  0.8875\n",
      " 0.8833333  0.89166665 0.87083334 0.8854167  0.88958335 0.89375\n",
      " 0.89166665 0.87291664 0.8979167  0.8958333  0.8833333  0.8854167\n",
      " 0.8875     0.87916666 0.8854167  0.8958333  0.87708336 0.8854167\n",
      " 0.8854167  0.8833333  0.89375    0.88958335 0.87083334 0.87916666\n",
      " 0.89166665 0.86875    0.89375    0.8854167  0.87916666 0.88125\n",
      " 0.87916666 0.8854167  0.8875     0.8875     0.8875     0.9\n",
      " 0.89166665 0.88958335 0.88958335 0.8875     0.8854167  0.89375\n",
      " 0.88958335 0.8833333  0.8875     0.8854167  0.89166665 0.875\n",
      " 0.8833333  0.89375    0.8833333  0.8854167  0.8833333  0.87708336\n",
      " 0.87916666 0.89166665 0.8854167  0.87708336 0.87916666 0.88125\n",
      " 0.8833333  0.8854167  0.8833333  0.88125    0.87916666 0.88125\n",
      " 0.8854167  0.90208334 0.89166665 0.9        0.88958335 0.89375\n",
      " 0.88958335 0.89166665 0.88958335 0.88958335 0.87708336 0.89166665\n",
      " 0.8854167  0.8854167  0.8854167  0.88958335 0.8958333  0.8833333\n",
      " 0.89166665 0.88125    0.8875     0.89166665 0.8854167  0.9\n",
      " 0.8854167  0.8958333  0.88958335 0.89166665]\n",
      "[105.  85. 110. 113. 113. 112. 111. 112. 113. 113. 114. 111. 112. 113.\n",
      " 111. 112. 111. 113. 112. 113. 113. 109. 109. 112. 111. 111. 112. 113.\n",
      " 112. 113. 111. 112. 113. 111. 109. 110. 110. 111. 112. 112. 110. 112.\n",
      " 112. 113. 112. 112. 112. 111. 111. 112. 111. 112. 113. 114. 111. 112.\n",
      " 113. 110. 108. 112. 113. 112. 111. 111. 112. 112. 114. 113. 112. 113.\n",
      " 112. 112. 112. 112. 112. 112. 112. 109. 113. 112. 112. 111. 113. 112.\n",
      " 114. 110. 113. 110. 112. 113. 113. 111. 112. 111. 112. 109. 114. 110.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 114. 112.]\n",
      "[0.65833336 0.84375    0.90416664 0.87916666 0.94375    0.95208335\n",
      " 0.99375    0.9916667  0.99583334 0.99583334 0.99583334 1.\n",
      " 1.         0.99583334 1.         0.99791664 0.99791664 0.57916665\n",
      " 0.53125    0.5541667  0.56875    0.575      0.5833333  0.62083334\n",
      " 0.6333333  0.6354167  0.6354167  0.6458333  0.6479167  0.6479167\n",
      " 0.65       0.65       0.65       0.65       0.65       0.65\n",
      " 0.65       0.65       0.65       0.65       0.65       0.65\n",
      " 0.65       0.65       0.65       0.65       0.65       0.65\n",
      " 0.65       0.65       0.65       0.65       0.65       0.65\n",
      " 0.65       0.65       0.65       0.65       0.65       0.65\n",
      " 0.65       0.65       0.65       0.65       0.65       0.65\n",
      " 0.65       0.65       0.65       0.65       0.65       0.65\n",
      " 0.65       0.6625     0.6770833  0.68125    0.68125    0.68125\n",
      " 0.68125    0.68125    0.68333334 0.6979167  0.68958336 0.70416665\n",
      " 0.71458334 0.71875    0.71875    0.72083336 0.72083336 0.72083336\n",
      " 0.72083336 0.72083336 0.72083336 0.73541665 0.7395833  0.7416667\n",
      " 0.7375     0.7375     0.74375    0.74791664]\n",
      "[106. 110. 115. 113. 119. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 119.  59.  61.  62.  64.  65.  71.  75.  76.  76.  77.  79.\n",
      "  79.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.\n",
      "  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.\n",
      "  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.  82.\n",
      "  82.  82.  82.  86.  86.  86.  86.  86.  86.  86.  87.  88.  87.  88.\n",
      "  90.  90.  90.  91.  91.  91.  92.  92.  93.  93.  93.  93.  94.  94.\n",
      "  94.  94.]\n",
      "[0.5125     0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.]\n",
      "[0.71666664 0.8375     0.86875    0.86875    0.87291664 0.83958334\n",
      " 0.8645833  0.8666667  0.87083334 0.8833333  0.83125    0.85833335\n",
      " 0.87291664 0.8666667  0.85833335 0.8666667  0.88125    0.89375\n",
      " 0.89375    0.95       0.9        0.93541664 0.94375    0.97291666\n",
      " 0.9479167  0.89166665 0.92083335 0.95       0.9604167  0.9583333\n",
      " 0.96666664 0.9291667  0.95416665 0.9791667  0.9604167  0.95\n",
      " 0.95208335 0.98541665 0.92291665 0.9791667  0.975      0.86875\n",
      " 0.95416665 0.9916667  0.9791667  1.         0.925      0.9791667\n",
      " 0.9895833  0.99791664 0.9916667  0.99375    0.9895833  0.74375\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[111. 109. 111. 112. 105. 112. 111. 112. 110. 104. 112. 107. 113. 112.\n",
      " 111. 113. 115. 113. 116. 117. 113. 111. 111. 119. 117. 116. 118. 119.\n",
      " 120. 115. 120. 118. 118. 114. 112. 120. 120.  90. 120. 120. 116. 117.\n",
      " 118. 112. 120. 120. 118. 120. 120. 120. 120. 120. 120.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.  62.\n",
      "  62.  62.]\n",
      "[0.4875     0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.]\n",
      "[0.48333332 0.67083335 0.87083334 0.84583336 0.87916666 0.8854167\n",
      " 0.87083334 0.87916666 0.88958335 0.86875    0.87916666 0.88125\n",
      " 0.87083334 0.8979167  0.88125    0.88125    0.8854167  0.88125\n",
      " 0.88125    0.88958335 0.86875    0.87708336 0.8875     0.88125\n",
      " 0.8875     0.89166665 0.8979167  0.88958335 0.88125    0.8854167\n",
      " 0.89375    0.89375    0.89375    0.9        0.88958335 0.8958333\n",
      " 0.89166665 0.8875     0.88958335 0.88958335 0.90625    0.8979167\n",
      " 0.8979167  0.8958333  0.8958333  0.8958333  0.89166665 0.89375\n",
      " 0.8979167  0.8958333  0.8958333  0.8958333  0.88958335 0.8958333\n",
      " 0.8875     0.90625    0.89166665 0.89166665 0.89375    0.8958333\n",
      " 0.89375    0.9        0.89375    0.8958333  0.9        0.8875\n",
      " 0.9        0.88958335 0.9        0.88958335 0.89166665 0.90208334\n",
      " 0.8979167  0.89375    0.9        0.88958335 0.90208334 0.8875\n",
      " 0.89375    0.9        0.89166665 0.9        0.8979167  0.90208334\n",
      " 0.89375    0.8979167  0.8833333  0.8958333  0.90208334 0.9\n",
      " 0.8979167  0.90208334 0.9        0.8979167  0.90208334 0.9\n",
      " 0.8958333  0.8958333  0.90625    0.9       ]\n",
      "[ 58. 109. 107. 109. 111. 111. 113. 111. 111. 114. 108. 111. 114. 113.\n",
      " 114. 113. 111. 114. 114. 111. 111. 111. 112. 112. 112. 114. 111. 111.\n",
      " 113. 113. 113. 114. 114. 111. 114. 113. 112. 113. 111. 111. 111. 110.\n",
      " 113. 112. 112. 113. 112. 113. 113. 112. 112. 113. 112. 111. 114. 110.\n",
      " 112. 114. 113. 112. 112. 113. 114. 111. 113. 113. 110. 113. 112. 111.\n",
      " 114. 113. 114. 111. 112. 112. 112. 113. 111. 113. 111. 111. 113. 111.\n",
      " 113. 113. 111. 113. 113. 113. 112. 113. 113. 112. 112. 112. 113. 112.\n",
      " 113. 112.]\n",
      "[0.53125    0.7625     0.83958334 0.88125    0.89166665 0.87083334\n",
      " 0.85833335 0.90625    0.94375    0.97083336 0.89375    0.97291666\n",
      " 0.95       0.9875     0.9916667  0.99791664 0.9916667  0.8354167\n",
      " 0.96458334 0.97291666 0.9895833  0.99375    0.99375    0.99791664\n",
      " 0.99583334 0.99583334 0.99791664 0.99375    0.99791664 0.99791664\n",
      " 0.99791664 0.99375    1.         0.99583334 0.99583334 1.\n",
      " 0.99791664 1.         1.         0.99791664 0.9895833  0.99791664\n",
      " 0.99583334 0.99583334 0.9916667  1.         1.         0.99791664\n",
      " 0.99583334 0.99791664 1.         1.         1.         0.99791664\n",
      " 1.         0.99791664 1.         0.99583334 0.8541667  0.9458333\n",
      " 0.98125    0.99583334 0.99583334 1.         0.9916667  0.99791664\n",
      " 0.99583334 1.         1.         1.         1.         0.99791664\n",
      " 0.99791664 1.         0.99791664 1.         1.         0.99791664\n",
      " 0.99791664 0.99791664 1.         1.         1.         0.99791664\n",
      " 1.         1.         1.         0.99791664 0.99791664 1.\n",
      " 1.         1.         1.         0.99791664 1.         1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1.         1.         0.99791664 0.99791664]\n",
      "[106. 111. 111. 112. 108. 114. 113. 115. 118. 102. 120. 120. 120. 120.\n",
      " 120. 120. 120. 117. 118. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 119. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 114. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.]\n",
      "Number of layers: 21\n",
      "[0.5208333  0.51875    0.68125    0.73333335 0.76458335 0.81041664\n",
      " 0.82708335 0.84791666 0.86041665 0.8520833  0.80625    0.8666667\n",
      " 0.85625    0.85625    0.8625     0.85       0.8645833  0.8625\n",
      " 0.86875    0.84791666 0.8645833  0.85833335 0.86875    0.85833335\n",
      " 0.875      0.8666667  0.875      0.87916666 0.8833333  0.88125\n",
      " 0.88958335 0.88125    0.88958335 0.8854167  0.87708336 0.87916666\n",
      " 0.8833333  0.88125    0.8854167  0.875      0.87291664 0.88958335\n",
      " 0.8833333  0.87708336 0.8854167  0.88125    0.87083334 0.87708336\n",
      " 0.87291664 0.875      0.89375    0.85833335 0.87708336 0.87291664\n",
      " 0.87916666 0.87708336 0.8875     0.88125    0.8854167  0.8666667\n",
      " 0.88958335 0.88125    0.89375    0.8854167  0.87708336 0.8875\n",
      " 0.89166665 0.87083334 0.87916666 0.86875    0.875      0.87708336\n",
      " 0.88125    0.87916666 0.88958335 0.8854167  0.87708336 0.88125\n",
      " 0.8854167  0.86875    0.875      0.8875     0.8833333  0.87291664\n",
      " 0.8666667  0.86875    0.87291664 0.8833333  0.8875     0.88958335\n",
      " 0.88125    0.87916666 0.86875    0.88958335 0.8875     0.87291664\n",
      " 0.86875    0.87291664 0.88125    0.875     ]\n",
      "[ 62.  80.  92. 100. 102. 108. 108. 106. 106. 111. 111. 111. 107. 112.\n",
      " 112. 111. 105. 111. 110. 111. 112. 112. 112. 109. 111. 109. 111. 110.\n",
      " 110. 112. 110. 111. 111. 109. 111. 110. 111. 111. 112. 111. 111. 111.\n",
      " 112. 112. 112. 110. 110. 111. 111. 110. 111. 111. 110. 112. 110. 110.\n",
      " 111. 111. 112. 111. 112. 111. 112. 111. 111. 111. 110. 111. 111. 111.\n",
      " 111. 112. 112. 112. 112. 111. 112. 111. 109. 111. 111. 111. 111. 111.\n",
      " 111. 110. 111. 111. 111. 110. 108. 111. 112. 111. 112. 112. 111. 111.\n",
      " 112. 111.]\n",
      "[0.66875    0.84166664 0.8645833  0.87291664 0.8854167  0.8854167\n",
      " 0.86875    0.86875    0.86875    0.8645833  0.87291664 0.8833333\n",
      " 0.875      0.87083334 0.8854167  0.88125    0.8833333  0.8833333\n",
      " 0.87708336 0.86875    0.87291664 0.87708336 0.88125    0.89375\n",
      " 0.87083334 0.87083334 0.8854167  0.87291664 0.875      0.87708336\n",
      " 0.87916666 0.8833333  0.86875    0.87083334 0.87291664 0.88125\n",
      " 0.87083334 0.86875    0.8833333  0.89166665 0.8833333  0.89375\n",
      " 0.875      0.87083334 0.87708336 0.8645833  0.875      0.88125\n",
      " 0.87708336 0.88125    0.8854167  0.88125    0.88125    0.8854167\n",
      " 0.8833333  0.8645833  0.88958335 0.8833333  0.87708336 0.8645833\n",
      " 0.89375    0.8666667  0.88958335 0.8854167  0.8875     0.8854167\n",
      " 0.88125    0.8833333  0.8833333  0.875      0.86875    0.8875\n",
      " 0.875      0.8833333  0.87916666 0.88125    0.87916666 0.89375\n",
      " 0.87708336 0.88125    0.88125    0.8666667  0.88125    0.8833333\n",
      " 0.88125    0.8833333  0.8875     0.87916666 0.87708336 0.87291664\n",
      " 0.88125    0.87291664 0.86875    0.87708336 0.87916666 0.87916666\n",
      " 0.8854167  0.88125    0.88125    0.8875    ]\n",
      "[104. 110. 111. 112. 109. 110. 111. 111. 109. 110. 111. 111. 111. 111.\n",
      " 111. 112. 110. 110. 109. 111. 110. 111. 111. 111. 112. 111. 111. 111.\n",
      " 110. 112. 110. 110. 112. 111. 111. 111. 110. 111. 111. 111. 111. 111.\n",
      " 111. 111. 110. 111. 110. 111. 112. 112. 111. 112. 112. 111. 112. 112.\n",
      " 111. 111. 109. 111. 111. 111. 112. 111. 111. 111. 112. 111. 112. 111.\n",
      " 110. 112. 111. 112. 112. 110. 111. 112. 112. 111. 112. 111. 111. 111.\n",
      " 112. 112. 111. 111. 109. 111. 109. 111. 111. 111. 111. 111. 112. 111.\n",
      " 111. 111.]\n",
      "[0.5        0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.]\n",
      "[0.66041666 0.84166664 0.84791666 0.8625     0.88125    0.8645833\n",
      " 0.875      0.875      0.88125    0.8854167  0.87708336 0.87916666\n",
      " 0.89375    0.8875     0.87708336 0.87708336 0.875      0.88958335\n",
      " 0.87083334 0.88958335 0.89166665 0.89375    0.925      0.88958335\n",
      " 0.9270833  0.925      0.95208335 0.93125    0.9375     0.95416665\n",
      " 0.95       0.9625     0.91875    0.95416665 0.97291666 0.95625\n",
      " 0.98333335 0.95416665 0.98125    0.98541665 0.99583334 0.9770833\n",
      " 0.975      0.9916667  0.99791664 0.99375    0.9791667  0.99375\n",
      " 0.99583334 0.99375    0.9875     0.99583334 0.99583334 0.99791664\n",
      " 1.         0.99791664 0.99791664 0.99375    0.9916667  0.9895833\n",
      " 0.99583334 0.99375    0.99791664 0.99375    1.         0.99583334\n",
      " 1.         0.99583334 0.99375    0.99791664 0.99583334 0.99583334\n",
      " 0.98125    0.9375     1.         1.         0.9895833  0.99583334\n",
      " 0.99791664 0.99791664 0.99791664 0.99583334 0.99583334 0.99375\n",
      " 0.99791664 0.99791664 0.99583334 0.99791664 0.99583334 0.99375\n",
      " 0.99375    0.99791664 0.99375    0.99791664 0.99583334 0.99791664\n",
      " 0.99791664 0.99583334 0.99583334 0.99791664]\n",
      "[103.  58. 108. 111. 111. 109. 111. 112. 110. 110. 111. 112. 113. 109.\n",
      " 112. 112. 111. 112. 113. 113. 113. 115. 115. 115. 117. 118. 116. 114.\n",
      " 118. 119. 120. 118. 115. 116. 120. 120. 119. 120. 120. 120. 120. 120.\n",
      " 120. 120. 120. 120. 120. 120. 120. 118. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120. 117. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.  96. 120. 120. 120. 120. 120. 120. 120. 120. 120. 118. 120.\n",
      " 120. 120. 120. 120. 117. 120. 120. 120. 120. 120. 120. 120. 120. 120.\n",
      " 120. 120.]\n",
      "[0.6125     0.81041664 0.85625    0.87291664 0.9        0.94375\n",
      " 0.8979167  0.98125    0.98125    0.9604167  0.81041664 0.86875\n",
      " 0.87083334 0.8645833  0.8833333  0.8875     0.8875     0.8854167\n",
      " 0.90208334 0.8833333  0.8958333  0.89166665 0.87916666 0.8833333\n",
      " 0.8854167  0.88125    0.8875     0.89375    0.89375    0.87291664\n",
      " 0.89166665 0.8958333  0.8875     0.8854167  0.8833333  0.89375\n",
      " 0.89166665 0.8875     0.89166665 0.9        0.8833333  0.8625\n",
      " 0.87916666 0.8875     0.88125    0.87083334 0.88958335 0.89375\n",
      " 0.87916666 0.8833333  0.89166665 0.87708336 0.89166665 0.8958333\n",
      " 0.8979167  0.89166665 0.88958335 0.8854167  0.88958335 0.89166665\n",
      " 0.90416664 0.8958333  0.8979167  0.90208334 0.8875     0.89375\n",
      " 0.8979167  0.89166665 0.8958333  0.8979167  0.90416664 0.8979167\n",
      " 0.8979167  0.8875     0.89375    0.89166665 0.9        0.90208334\n",
      " 0.8875     0.8958333  0.9        0.8958333  0.8979167  0.8958333\n",
      " 0.89375    0.8979167  0.9        0.90208334 0.8979167  0.89375\n",
      " 0.8833333  0.90625    0.8979167  0.8979167  0.9        0.90208334\n",
      " 0.8854167  0.90208334 0.90416664 0.90416664]\n",
      "[ 62. 106. 106. 110. 117.  81. 118. 120. 120.  86. 111. 113. 112. 107.\n",
      " 111. 113. 114. 114. 109. 113. 109. 110. 113. 113. 109. 109. 114. 112.\n",
      " 112. 110. 109. 112. 111. 110. 106. 113. 112. 113. 111. 113. 114. 110.\n",
      " 112. 112. 109. 110. 111. 111. 113. 110. 114. 112. 113. 111. 112. 109.\n",
      " 113. 113. 111. 113. 114. 111. 113. 111. 111. 114. 112. 111. 113. 112.\n",
      " 111. 112. 111. 113. 113. 112. 111. 114. 111. 113. 114. 112. 113. 113.\n",
      " 111. 111. 112. 111. 111. 111. 114. 110. 111. 112. 111. 112. 110. 113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 110. 109.]\n",
      "[0.53333336 0.8125     0.86041665 0.8645833  0.87708336 0.88125\n",
      " 0.8645833  0.87083334 0.85833335 0.8833333  0.88958335 0.86875\n",
      " 0.87291664 0.86875    0.8645833  0.88125    0.875      0.87916666\n",
      " 0.875      0.875      0.8833333  0.87291664 0.875      0.87708336\n",
      " 0.88958335 0.87291664 0.8875     0.8833333  0.87916666 0.89166665\n",
      " 0.88958335 0.87291664 0.87708336 0.86875    0.875      0.87916666\n",
      " 0.8833333  0.87083334 0.88125    0.86875    0.87291664 0.8625\n",
      " 0.87083334 0.89166665 0.88125    0.8875     0.8875     0.87916666\n",
      " 0.87083334 0.8833333  0.87916666 0.89166665 0.8833333  0.87708336\n",
      " 0.8958333  0.87291664 0.875      0.8833333  0.875      0.87916666\n",
      " 0.89166665 0.86875    0.87708336 0.88125    0.87083334 0.87916666\n",
      " 0.8875     0.8854167  0.88958335 0.875      0.8875     0.87916666\n",
      " 0.88125    0.88958335 0.87916666 0.87291664 0.8833333  0.8645833\n",
      " 0.8854167  0.85625    0.87916666 0.8854167  0.87083334 0.8854167\n",
      " 0.86875    0.8854167  0.88125    0.88958335 0.88958335 0.8625\n",
      " 0.875      0.8645833  0.8833333  0.875      0.8854167  0.88958335\n",
      " 0.89375    0.8854167  0.87708336 0.8833333 ]\n",
      "[106. 111. 109. 111. 111. 112. 109. 111. 111. 111. 112. 112. 109. 111.\n",
      " 111. 111. 110. 111. 111. 111. 110. 110. 111. 110. 111. 111. 110. 111.\n",
      " 111. 108. 110. 109. 110. 110. 111. 111. 111. 111. 111. 111. 109. 111.\n",
      " 110. 110. 110. 111. 110. 111. 111. 111. 112. 111. 112. 111. 112. 111.\n",
      " 111. 111. 111. 111. 110. 111. 112. 112. 111. 111. 111. 111. 109. 111.\n",
      " 111. 112. 111. 111. 111. 112. 111. 111. 111. 111. 112. 112. 111. 112.\n",
      " 111. 111. 111. 112. 111. 112. 110. 112. 111. 111. 111. 111. 111. 111.\n",
      " 111. 112.]\n",
      "[0.60625    0.8354167  0.8541667  0.8333333  0.8520833  0.87291664\n",
      " 0.86875    0.8625     0.87291664 0.8666667  0.86041665 0.86875\n",
      " 0.8645833  0.8645833  0.8645833  0.85833335 0.85625    0.8625\n",
      " 0.84791666 0.86875    0.8666667  0.8666667  0.87916666 0.8666667\n",
      " 0.8666667  0.8520833  0.87708336 0.86041665 0.8541667  0.86875\n",
      " 0.86875    0.8625     0.85833335 0.8520833  0.85833335 0.8625\n",
      " 0.86875    0.86875    0.8625     0.875      0.86041665 0.8541667\n",
      " 0.86041665 0.86041665 0.8625     0.8645833  0.8625     0.8625\n",
      " 0.86041665 0.86041665 0.85625    0.85625    0.85833335 0.8666667\n",
      " 0.8520833  0.85833335 0.8666667  0.86875    0.8666667  0.8666667\n",
      " 0.8666667  0.86875    0.8666667  0.85625    0.8625     0.8625\n",
      " 0.87083334 0.85833335 0.8666667  0.87291664 0.8875     0.8875\n",
      " 0.9458333  0.9        0.925      0.9458333  0.94375    0.95208335\n",
      " 0.9291667  0.74791664 0.8645833  0.8666667  0.8541667  0.8645833\n",
      " 0.84791666 0.8520833  0.86041665 0.86875    0.8541667  0.86875\n",
      " 0.87916666 0.85625    0.86875    0.87708336 0.86041665 0.86875\n",
      " 0.87291664 0.8625     0.86041665 0.8666667 ]\n",
      "[112. 108. 108. 112. 112. 112. 110. 112. 111. 111. 111. 111. 112. 109.\n",
      " 110. 111. 112. 106. 111. 110. 112. 110. 110. 112. 111. 111. 111. 112.\n",
      " 111. 110. 107. 109. 108. 112. 111. 112. 112. 111. 111. 111. 111. 108.\n",
      " 112. 109. 106. 112. 111. 111. 112. 109. 106. 110. 111. 108. 109. 112.\n",
      " 112. 108. 111. 110. 110. 111. 108. 110. 110. 111. 112. 112. 112. 112.\n",
      " 111. 113. 110. 116. 115. 113. 117. 117.  58. 112. 111. 111. 110. 112.\n",
      " 111. 109. 111. 112. 112. 111. 107. 111. 109. 112. 110. 108. 110. 111.\n",
      " 110. 112.]\n",
      "[0.48333332 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334 0.49583334\n",
      " 0.49583334 0.49583334 0.49583334 0.49583334]\n",
      "[62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.\n",
      " 62. 62. 62. 62. 62. 62. 62. 62. 62. 62.]\n",
      "[0.5208333  0.5729167  0.8020833  0.82916665 0.84583336 0.87708336\n",
      " 0.87916666 0.87291664 0.87708336 0.87916666 0.87916666 0.87083334\n",
      " 0.8854167  0.8875     0.88125    0.8875     0.88125    0.8854167\n",
      " 0.8875     0.8958333  0.875      0.87916666 0.875      0.8854167\n",
      " 0.87916666 0.86041665 0.875      0.8854167  0.88125    0.88958335\n",
      " 0.87291664 0.88958335 0.8666667  0.875      0.8875     0.88958335\n",
      " 0.88125    0.88125    0.8875     0.89375    0.8854167  0.8833333\n",
      " 0.87291664 0.8979167  0.90833336 0.8979167  0.8958333  0.8833333\n",
      " 0.88958335 0.89375    0.8854167  0.89166665 0.88958335 0.91041666\n",
      " 0.88958335 0.8854167  0.87708336 0.87708336 0.89375    0.88958335\n",
      " 0.8833333  0.89375    0.89166665 0.87916666 0.8979167  0.87083334\n",
      " 0.8833333  0.89166665 0.87916666 0.89166665 0.88125    0.87708336\n",
      " 0.89375    0.8958333  0.8854167  0.8875     0.88958335 0.90625\n",
      " 0.9        0.89166665 0.89166665 0.88958335 0.88958335 0.8854167\n",
      " 0.88125    0.88958335 0.8875     0.8875     0.8958333  0.8979167\n",
      " 0.89166665 0.8979167  0.8854167  0.8979167  0.88958335 0.87916666\n",
      " 0.8958333  0.87916666 0.88125    0.8979167 ]\n",
      "[ 92.  97. 109. 110. 111. 110. 110. 106. 113. 113. 113. 112. 107. 110.\n",
      " 113. 111. 113. 113. 109. 111. 109. 112. 111. 113. 112. 112. 112. 111.\n",
      " 111. 112. 111. 112. 111. 112. 110. 111. 110. 112. 110. 113. 113. 110.\n",
      " 111. 112. 112. 112. 112. 114. 113. 112. 113. 111. 112. 111. 113. 112.\n",
      " 114. 111. 111. 111. 112. 114. 113. 113. 114. 112. 113. 113. 113. 110.\n",
      " 113. 110. 113. 113. 112. 111. 113. 110. 109. 113. 112. 111. 111. 112.\n",
      " 113. 111. 114. 112. 113. 114. 113. 114. 114. 109. 112. 113. 114. 113.\n",
      " 111. 113.]\n",
      "[0.54583335 0.6875     0.75625    0.80625    0.85625    0.8666667\n",
      " 0.86875    0.875      0.8854167  0.875      0.8833333  0.87291664\n",
      " 0.89375    0.87916666 0.87916666 0.89166665 0.88125    0.87708336\n",
      " 0.8854167  0.8833333  0.8875     0.8666667  0.87083334 0.88958335\n",
      " 0.88125    0.8875     0.87708336 0.89166665 0.88958335 0.8833333\n",
      " 0.8958333  0.9125     0.9145833  0.8354167  0.84583336 0.8833333\n",
      " 0.8875     0.8875     0.88958335 0.89166665 0.87916666 0.8875\n",
      " 0.87708336 0.8833333  0.89375    0.88958335 0.87291664 0.8833333\n",
      " 0.875      0.87708336 0.87083334 0.8833333  0.87708336 0.88125\n",
      " 0.8875     0.87291664 0.875      0.8625     0.8854167  0.8875\n",
      " 0.8645833  0.87708336 0.88958335 0.8833333  0.87083334 0.8875\n",
      " 0.89375    0.8854167  0.87708336 0.86875    0.87708336 0.8833333\n",
      " 0.8625     0.87291664 0.875      0.875      0.88125    0.875\n",
      " 0.87916666 0.88125    0.8854167  0.87291664 0.87708336 0.8979167\n",
      " 0.87708336 0.87916666 0.89375    0.86875    0.87708336 0.88125\n",
      " 0.87916666 0.88125    0.8875     0.8666667  0.88958335 0.8875\n",
      " 0.875      0.88958335 0.88125    0.87916666]\n",
      "[ 79.  99. 105. 107. 109. 110. 112. 112. 112. 110. 112. 110. 112. 112.\n",
      " 112. 111. 111. 112. 111. 111. 111. 111. 111. 111. 111. 112. 112. 111.\n",
      " 112. 112. 113. 113. 116. 107. 109. 111. 111. 110. 111. 111. 111. 112.\n",
      " 112. 111. 112. 112. 109. 111. 112. 111. 111. 111. 111. 110. 112. 111.\n",
      " 111. 111. 111. 111. 112. 112. 111. 112. 111. 112. 111. 111. 111. 111.\n",
      " 112. 111. 110. 110. 111. 111. 111. 111. 110. 112. 111. 111. 112. 109.\n",
      " 111. 111. 111. 111. 111. 111. 111. 111. 109. 111. 112. 111. 111. 111.\n",
      " 111. 112.]\n"
     ]
    }
   ],
   "source": [
    "from Networks.ResNet import ResAntiSymNet\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "num_epochs = 100\n",
    "num_features = 2\n",
    "num_classes = 2\n",
    "size_hidden = 8\n",
    "gamma = 0.3\n",
    "h = 1\n",
    "lr = 0.5\n",
    "\n",
    "max_layers = 20\n",
    "train_results_per_layer = np.zeros(max_layers)\n",
    "test_results_per_layer = np.zeros(max_layers)\n",
    "basic_save_key = 'AntiSym_Comparison_results//AntiSymNet'\n",
    "\n",
    "for num_hidden_layers in range(15,max_layers+1):\n",
    "    tic = timeit.default_timer()\n",
    "    num_layers = num_hidden_layers + 1\n",
    "    print('Number of layers: '+str(num_layers))\n",
    "    train_results_per_epoch = np.zeros(num_epochs)\n",
    "    test_results_per_epoch = np.zeros(num_epochs)\n",
    "    for i in range(10):\n",
    "        net = ResAntiSymNet(features=num_features, classes=num_classes, num_layers=num_layers, gamma=gamma, h=h, bias=True, hidden_size=size_hidden)\n",
    "        net.set_test_tracking(True)\n",
    "        net.train(num_epochs, train_loader, test_loader, print_output=False)\n",
    "        print(net.avg_correct_pred.numpy())\n",
    "        print(net.test_results.numpy())\n",
    "        del net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
