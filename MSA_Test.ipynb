{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rho 0.0 ###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:340: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:343: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  compared_weight_signs = torch.tensor(torch.ne(new_weight_signs, old_weight_signs).detach(), dtype=torch.float32)\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:148: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_weights = new_weight_signs*torch.tensor(torch.ge(torch.abs(self.m_accumulated),self.rho*max_weight_elem*torch.ones_like(self.m_accumulated)).detach(),dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 2 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 3 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 4 ##############\n",
      "Correct predictions: 0.50625\n",
      "Epoch 5 ##############\n",
      "Correct predictions: 0.50625\n",
      "Epoch 6 ##############\n",
      "Correct predictions: 0.50625\n",
      "Epoch 7 ##############\n",
      "Correct predictions: 0.6875\n",
      "Epoch 8 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 9 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 10 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 11 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 12 ##############\n",
      "Correct predictions: 0.6875\n",
      "Epoch 13 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 14 ##############\n",
      "Correct predictions: 0.50625\n",
      "Epoch 15 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 16 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 17 ##############\n",
      "Correct predictions: 0.50625\n",
      "Epoch 18 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 19 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 20 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 21 ##############\n",
      "Correct predictions: 0.6875\n",
      "Epoch 22 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 23 ##############\n",
      "Correct predictions: 0.50625\n",
      "Epoch 24 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 25 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 26 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 27 ##############\n",
      "Correct predictions: 0.50625\n",
      "Epoch 28 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 29 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 30 ##############\n",
      "Correct predictions: 0.79375\n",
      "Time elapsed:  9.984411265000006\n",
      "Rho 0.1 ###############################\n",
      "Epoch 1 ##############\n",
      "Correct predictions: 0.50625\n",
      "Epoch 2 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 3 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 4 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 5 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 6 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 7 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 8 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 9 ##############\n",
      "Correct predictions: 0.3375\n",
      "Epoch 10 ##############\n",
      "Correct predictions: 0.85625\n",
      "Epoch 11 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 12 ##############\n",
      "Correct predictions: 0.85625\n",
      "Epoch 13 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 14 ##############\n",
      "Correct predictions: 0.3125\n",
      "Epoch 15 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 16 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 17 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 18 ##############\n",
      "Correct predictions: 0.6875\n",
      "Epoch 19 ##############\n",
      "Correct predictions: 0.76875\n",
      "Epoch 20 ##############\n",
      "Correct predictions: 0.6875\n",
      "Epoch 21 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 22 ##############\n",
      "Correct predictions: 0.85625\n",
      "Epoch 23 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 24 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 25 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 26 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 27 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 28 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 29 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 30 ##############\n",
      "Correct predictions: 0.40625\n",
      "Time elapsed:  9.910109674000012\n",
      "Rho 0.2 ###############################\n",
      "Epoch 1 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 2 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 3 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 4 ##############\n",
      "Correct predictions: 0.50625\n",
      "Epoch 5 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 6 ##############\n",
      "Correct predictions: 0.59375\n",
      "Epoch 7 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 8 ##############\n",
      "Correct predictions: 0.50625\n",
      "Epoch 9 ##############\n",
      "Correct predictions: 0.76875\n",
      "Epoch 10 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 11 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 12 ##############\n",
      "Correct predictions: 0.83125\n",
      "Epoch 13 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 14 ##############\n",
      "Correct predictions: 0.83125\n",
      "Epoch 15 ##############\n",
      "Correct predictions: 0.2\n",
      "Epoch 16 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 17 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 18 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 19 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 20 ##############\n",
      "Correct predictions: 0.83125\n",
      "Epoch 21 ##############\n",
      "Correct predictions: 0.83125\n",
      "Epoch 22 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 23 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 24 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 25 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 26 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 27 ##############\n",
      "Correct predictions: 0.575\n",
      "Epoch 28 ##############\n",
      "Correct predictions: 0.83125\n",
      "Epoch 29 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 30 ##############\n",
      "Correct predictions: 0.85625\n",
      "Time elapsed:  10.426659949999987\n",
      "Rho 0.3 ###############################\n",
      "Epoch 1 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 2 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 3 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 4 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 5 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 6 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 7 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 8 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 9 ##############\n",
      "Correct predictions: 0.85625\n",
      "Epoch 10 ##############\n",
      "Correct predictions: 0.50625\n",
      "Epoch 11 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 12 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 13 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 14 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 15 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 16 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 17 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 18 ##############\n",
      "Correct predictions: 0.59375\n",
      "Epoch 19 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 20 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 21 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 22 ##############\n",
      "Correct predictions: 0.50625\n",
      "Epoch 23 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 24 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 25 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 26 ##############\n",
      "Correct predictions: 0.85625\n",
      "Epoch 27 ##############\n",
      "Correct predictions: 0.85625\n",
      "Epoch 28 ##############\n",
      "Correct predictions: 0.74375\n",
      "Epoch 29 ##############\n",
      "Correct predictions: 0.20625\n",
      "Epoch 30 ##############\n",
      "Correct predictions: 0.40625\n",
      "Time elapsed:  10.701523404\n",
      "Rho 0.4 ###############################\n",
      "Epoch 1 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 2 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 3 ##############\n",
      "Correct predictions: 0.59375\n",
      "Epoch 4 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 5 ##############\n",
      "Correct predictions: 0.76875\n",
      "Epoch 6 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 7 ##############\n",
      "Correct predictions: 0.83125\n",
      "Epoch 8 ##############\n",
      "Correct predictions: 0.6875\n",
      "Epoch 9 ##############\n",
      "Correct predictions: 0.575\n",
      "Epoch 10 ##############\n",
      "Correct predictions: 0.575\n",
      "Epoch 11 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 12 ##############\n",
      "Correct predictions: 0.6875\n",
      "Epoch 13 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 14 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 15 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 16 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 17 ##############\n",
      "Correct predictions: 0.85625\n",
      "Epoch 18 ##############\n",
      "Correct predictions: 0.3125\n",
      "Epoch 19 ##############\n",
      "Correct predictions: 0.83125\n",
      "Epoch 20 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 21 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 22 ##############\n",
      "Correct predictions: 0.6875\n",
      "Epoch 23 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 24 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 25 ##############\n",
      "Correct predictions: 0.83125\n",
      "Epoch 26 ##############\n",
      "Correct predictions: 0.76875\n",
      "Epoch 27 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 28 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 29 ##############\n",
      "Correct predictions: 0.74375\n",
      "Epoch 30 ##############\n",
      "Correct predictions: 0.79375\n",
      "Time elapsed:  10.200948946999972\n",
      "Rho 0.5 ###############################\n",
      "Epoch 1 ##############\n",
      "Correct predictions: 0.6875\n",
      "Epoch 2 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 3 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 4 ##############\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 0.3125\n",
      "Epoch 5 ##############\n",
      "Correct predictions: 0.83125\n",
      "Epoch 6 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 7 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 8 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 9 ##############\n",
      "Correct predictions: 0.83125\n",
      "Epoch 10 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 11 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 12 ##############\n",
      "Correct predictions: 0.50625\n",
      "Epoch 13 ##############\n",
      "Correct predictions: 0.50625\n",
      "Epoch 14 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 15 ##############\n",
      "Correct predictions: 0.83125\n",
      "Epoch 16 ##############\n",
      "Correct predictions: 0.50625\n",
      "Epoch 17 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 18 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 19 ##############\n",
      "Correct predictions: 0.50625\n",
      "Epoch 20 ##############\n",
      "Correct predictions: 0.50625\n",
      "Epoch 21 ##############\n",
      "Correct predictions: 0.50625\n",
      "Epoch 22 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 23 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 24 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 25 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 26 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 27 ##############\n",
      "Correct predictions: 0.50625\n",
      "Epoch 28 ##############\n",
      "Correct predictions: 0.50625\n",
      "Epoch 29 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 30 ##############\n",
      "Correct predictions: 0.76875\n",
      "Time elapsed:  10.584652871000003\n",
      "Rho 0.6 ###############################\n",
      "Epoch 1 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 2 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 3 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 4 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 5 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 6 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 7 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 8 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 9 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 10 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 11 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 12 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 13 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 14 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 15 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 16 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 17 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 18 ##############\n",
      "Correct predictions: 0.3125\n",
      "Epoch 19 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 20 ##############\n",
      "Correct predictions: 0.575\n",
      "Epoch 21 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 22 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 23 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 24 ##############\n",
      "Correct predictions: 0.575\n",
      "Epoch 25 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 26 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 27 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 28 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 29 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 30 ##############\n",
      "Correct predictions: 0.40625\n",
      "Time elapsed:  10.57511148499998\n",
      "Rho 0.7 ###############################\n",
      "Epoch 1 ##############\n",
      "Correct predictions: 0.6875\n",
      "Epoch 2 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 3 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 4 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 5 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 6 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 7 ##############\n",
      "Correct predictions: 0.50625\n",
      "Epoch 8 ##############\n",
      "Correct predictions: 0.6875\n",
      "Epoch 9 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 10 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 11 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 12 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 13 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 14 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 15 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 16 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 17 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 18 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 19 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 20 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 21 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 22 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 23 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 24 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 25 ##############\n",
      "Correct predictions: 0.2\n",
      "Epoch 26 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 27 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 28 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 29 ##############\n",
      "Correct predictions: 0.50625\n",
      "Epoch 30 ##############\n",
      "Correct predictions: 0.6875\n",
      "Time elapsed:  10.415114260999985\n",
      "Rho 0.8 ###############################\n",
      "Epoch 1 ##############\n",
      "Correct predictions: 0.6875\n",
      "Epoch 2 ##############\n",
      "Correct predictions: 0.20625\n",
      "Epoch 3 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 4 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 5 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 6 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 7 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 8 ##############\n",
      "Correct predictions: 0.83125\n",
      "Epoch 9 ##############\n",
      "Correct predictions: 0.83125\n",
      "Epoch 10 ##############\n",
      "Correct predictions: 0.83125\n",
      "Epoch 11 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 12 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 13 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 14 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 15 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 16 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 17 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 18 ##############\n",
      "Correct predictions: 0.50625\n",
      "Epoch 19 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 20 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 21 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 22 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 23 ##############\n",
      "Correct predictions: 0.83125\n",
      "Epoch 24 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 25 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 26 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 27 ##############\n",
      "Correct predictions: 0.83125\n",
      "Epoch 28 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 29 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 30 ##############\n",
      "Correct predictions: 0.50625\n",
      "Time elapsed:  10.209844677999968\n",
      "Rho 0.9 ###############################\n",
      "Epoch 1 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 2 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 3 ##############\n",
      "Correct predictions: 0.85625\n",
      "Epoch 4 ##############\n",
      "Correct predictions: 0.2\n",
      "Epoch 5 ##############\n",
      "Correct predictions: 0.76875\n",
      "Epoch 6 ##############\n",
      "Correct predictions: 0.76875\n",
      "Epoch 7 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 8 ##############\n",
      "Correct predictions: 0.76875\n",
      "Epoch 9 ##############\n",
      "Correct predictions: 0.20625\n",
      "Epoch 10 ##############\n",
      "Correct predictions: 0.50625\n",
      "Epoch 11 ##############\n",
      "Correct predictions: 0.3125\n",
      "Epoch 12 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 13 ##############\n",
      "Correct predictions: 0.50625\n",
      "Epoch 14 ##############\n",
      "Correct predictions: 0.6875\n",
      "Epoch 15 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 16 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 17 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 18 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 19 ##############\n",
      "Correct predictions: 0.6875\n",
      "Epoch 20 ##############\n",
      "Correct predictions: 0.83125\n",
      "Epoch 21 ##############\n",
      "Correct predictions: 0.40625\n",
      "Epoch 22 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 23 ##############\n",
      "Correct predictions: 0.6875\n",
      "Epoch 24 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 25 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 26 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 27 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 28 ##############\n",
      "Correct predictions: 0.79375\n",
      "Epoch 29 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 30 ##############\n",
      "Correct predictions: 0.50625\n",
      "Time elapsed:  10.372183940000014\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import ConvNet, FCNet\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "train_loader, test_loader = makeMoonsDataset()\n",
    "torch.manual_seed(3)\n",
    "\n",
    "for i in range(10):\n",
    "    print('Rho '+str(i/10)+' ###############################')\n",
    "    net = FCNet(num_fc=2,sizes_fc=[2,4,2], bias=False, test=False)\n",
    "    net.set_rho(i/10)\n",
    "    net.train_msa(30,train_loader)\n",
    "    \n",
    "    \n",
    "#net = ConvNet(3, [1], [3,6], num_fc=2, sizes_fc=[100,10])\n",
    "\n",
    "#train_loader, test_loader = loadMNIST('Dataset/input/mnist_train.csv', 'Dataset/input/mnist_test.csv')\n",
    "\n",
    "#net.train_backprop(1,train_loader)\n",
    "#net.test(test_loader,10000)\n",
    "#print(train_loader.batch_size)\n",
    "\n",
    "#\n",
    "#for index, (data, label) in enumerate(train_loader):\n",
    "#    print(data)\n",
    "#    print(net.forward(data))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for index, (data, label) in enumerate(train_loader):\n",
    "#    print(data)\n",
    "#    print(net.forward(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:340: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:343: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  compared_weight_signs = torch.tensor(torch.ne(new_weight_signs, old_weight_signs).detach(), dtype=torch.float32)\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:148: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_weights = new_weight_signs*torch.tensor(torch.ge(torch.abs(self.m_accumulated),self.rho*max_weight_elem*torch.ones_like(self.m_accumulated)).detach(),dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed:  10.146643088\n",
      "Seed 0   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.60604992\n",
      "Seed 1   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.699145381999998\n",
      "Seed 2   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.273395311000002\n",
      "Seed 3   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.354167834000002\n",
      "Seed 4   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.854717102000002\n",
      "Seed 5   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.360114429000006\n",
      "Seed 6   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.212407759000001\n",
      "Seed 7   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.896629595000007\n",
      "Seed 8   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.603672412999998\n",
      "Seed 9   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.097800613000004\n",
      "Seed 10   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.248547408000007\n",
      "Seed 11   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.193569678999978\n",
      "Seed 12   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.238679597000015\n",
      "Seed 13   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.202587755999986\n",
      "Seed 14   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.563680431999984\n",
      "Seed 15   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.800362409000002\n",
      "Seed 16   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.172943199000002\n",
      "Seed 17   ###   Best-Avg 0.8\n",
      "Time elapsed:  11.664164929999998\n",
      "Seed 18   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.64493668099999\n",
      "Seed 19   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.808216150000021\n",
      "Seed 20   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.073397882000023\n",
      "Seed 21   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.22652783800001\n",
      "Seed 22   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.820626478999998\n",
      "Seed 23   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.476716109999984\n",
      "Seed 24   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.439701024999977\n",
      "Seed 25   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.500485010999967\n",
      "Seed 26   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.450659146999953\n",
      "Seed 27   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.663561942000001\n",
      "Seed 28   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.897770284000046\n",
      "Seed 29   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.072501369000008\n",
      "Seed 30   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.462130296999987\n",
      "Seed 31   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.181674433000012\n",
      "Seed 32   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.720457097999997\n",
      "Seed 33   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.911982378000005\n",
      "Seed 34   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.12446210600001\n",
      "Seed 35   ###   Best-Avg 0.8\n",
      "Time elapsed:  11.751844816999949\n",
      "Seed 36   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.914494053999988\n",
      "Seed 37   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.721946830999968\n",
      "Seed 38   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.854742291999969\n",
      "Seed 39   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.348190395000017\n",
      "Seed 40   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.857938689000036\n",
      "Seed 41   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.154126195999993\n",
      "Seed 42   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.182050207999964\n",
      "Seed 43   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.263982440000007\n",
      "Seed 44   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.456949645000066\n",
      "Seed 45   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.78960939500007\n",
      "Seed 46   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.985499520000076\n",
      "Seed 47   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.699192676000052\n",
      "Seed 48   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.425626183999952\n",
      "Seed 49   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.54046722499993\n",
      "Seed 50   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.030507655000065\n",
      "Seed 51   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.125098506000086\n",
      "Seed 52   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.919794993999972\n",
      "Seed 53   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.273062202999995\n",
      "Seed 54   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.648992066000005\n",
      "Seed 55   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.420492824999997\n",
      "Seed 56   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.824312771999985\n",
      "Seed 57   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.768213887999991\n",
      "Seed 58   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.478139016\n",
      "Seed 59   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.264060061999999\n",
      "Seed 60   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.428517747\n",
      "Seed 61   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.804717488000051\n",
      "Seed 62   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.830327735999958\n",
      "Seed 63   ###   Best-Avg 0.84375\n",
      "Time elapsed:  13.149567551000018\n",
      "Seed 64   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.077774036999926\n",
      "Seed 65   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.304072090999966\n",
      "Seed 66   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.241903238999953\n",
      "Seed 67   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.745793354000057\n",
      "Seed 68   ###   Best-Avg 0.84375\n",
      "Time elapsed:  13.113821154999982\n",
      "Seed 69   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.56083924699999\n",
      "Seed 70   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.023275922000039\n",
      "Seed 71   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.34733552099999\n",
      "Seed 72   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.205158547999986\n",
      "Seed 73   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.781024666000008\n",
      "Seed 74   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.818582595000066\n",
      "Seed 75   ###   Best-Avg 0.8\n",
      "Time elapsed:  12.775833219999981\n",
      "Seed 76   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.596671489999949\n",
      "Seed 77   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.738887018000014\n",
      "Seed 78   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.079107496999995\n",
      "Seed 79   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.085618524999973\n",
      "Seed 80   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.447313674000043\n",
      "Seed 81   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.547355056000015\n",
      "Seed 82   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.684601208999993\n",
      "Seed 83   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.680478483000002\n",
      "Seed 84   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.97397130999991\n",
      "Seed 85   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.147157143999948\n",
      "Seed 86   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.294491636999965\n",
      "Seed 87   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.387910954999938\n",
      "Seed 88   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.786750217999952\n",
      "Seed 89   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.385938010000018\n",
      "Seed 90   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.857001564999791\n",
      "Seed 91   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.818907478000028\n",
      "Seed 92   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.13523568200003\n",
      "Seed 93   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.777255096999852\n",
      "Seed 94   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.35629602400013\n",
      "Seed 95   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.564049009999962\n",
      "Seed 96   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.523687422999956\n",
      "Seed 97   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.850773782000033\n",
      "Seed 98   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.6953454840002\n",
      "Seed 99   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.015339932000188\n",
      "Seed 100   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.39123638000001\n",
      "Seed 101   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.594417870000143\n",
      "Seed 102   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.783619105999833\n",
      "Seed 103   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.876350617000071\n",
      "Seed 104   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.439500544000111\n",
      "Seed 105   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.181604522000043\n",
      "Seed 106   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.076896543999965\n",
      "Seed 107   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.832593179000014\n",
      "Seed 108   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.307717772999922\n",
      "Seed 109   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.43920239099998\n",
      "Seed 110   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.498897093999858\n",
      "Seed 111   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.657795781000004\n",
      "Seed 112   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.814789377999887\n",
      "Seed 113   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.931495412999993\n",
      "Seed 114   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.288754776999895\n",
      "Seed 115   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.244204666000087\n",
      "Seed 116   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.550813107000067\n",
      "Seed 117   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.388158215999965\n",
      "Seed 118   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.750045622000016\n",
      "Seed 119   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.235819392000167\n",
      "Seed 120   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.996203699000034\n",
      "Seed 121   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.989480880999963\n",
      "Seed 122   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.056708553999897\n",
      "Seed 123   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.285523938999859\n",
      "Seed 124   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.525031678000005\n",
      "Seed 125   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.853770211999972\n",
      "Seed 126   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.874691244999894\n",
      "Seed 127   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.638378874000182\n",
      "Seed 128   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.042981213999838\n",
      "Seed 129   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.147106766999968\n",
      "Seed 130   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.252185378999911\n",
      "Seed 131   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.136590218000038\n",
      "Seed 132   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.631353791000038\n",
      "Seed 133   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.084455215999924\n",
      "Seed 134   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.278537409000137\n",
      "Seed 135   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.179119578000154\n",
      "Seed 136   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.569564312000011\n",
      "Seed 137   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.627897280999832\n",
      "Seed 138   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.784591183999964\n",
      "Seed 139   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.15707995899993\n",
      "Seed 140   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.181525357000055\n",
      "Seed 141   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.311887792000107\n",
      "Seed 142   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.489734566999914\n",
      "Seed 143   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.879751607999879\n",
      "Seed 144   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.325272771000073\n",
      "Seed 145   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.095041162999905\n",
      "Seed 146   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.463122424999938\n",
      "Seed 147   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.492982883999957\n",
      "Seed 148   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.48772615300004\n",
      "Seed 149   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.673118235000175\n",
      "Seed 150   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.028434469000103\n",
      "Seed 151   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.975339212000108\n",
      "Seed 152   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.189310215000205\n",
      "Seed 153   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.902211723999926\n",
      "Seed 154   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.252953891999823\n",
      "Seed 155   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.101709491000065\n",
      "Seed 156   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.322991391999949\n",
      "Seed 157   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.231441695000058\n",
      "Seed 158   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.062093285999936\n",
      "Seed 159   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.669411380999918\n",
      "Seed 160   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.712783790999993\n",
      "Seed 161   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.289970005000214\n",
      "Seed 162   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.054718644999866\n",
      "Seed 163   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.402621168999985\n",
      "Seed 164   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.74273369599996\n",
      "Seed 165   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.994736071000034\n",
      "Seed 166   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.28945646399984\n",
      "Seed 167   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.997000486000161\n",
      "Seed 168   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.741073809999989\n",
      "Seed 169   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.930781389999993\n",
      "Seed 170   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.058178237999982\n",
      "Seed 171   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.299073929000087\n",
      "Seed 172   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.528790453000056\n",
      "Seed 173   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.644680681000182\n",
      "Seed 174   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.82024248000016\n",
      "Seed 175   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.990435996000087\n",
      "Seed 176   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.8661229679999\n",
      "Seed 177   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.123140467999974\n",
      "Seed 178   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.590671946999919\n",
      "Seed 179   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.689152142000012\n",
      "Seed 180   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.67604732399991\n",
      "Seed 181   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.032750995000015\n",
      "Seed 182   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.450169252000023\n",
      "Seed 183   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.201859338999839\n",
      "Seed 184   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.017197729000145\n",
      "Seed 185   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.891306548999637\n",
      "Seed 186   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.39308081199988\n",
      "Seed 187   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.244145035999736\n",
      "Seed 188   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.604870163000214\n",
      "Seed 189   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.960879856999782\n",
      "Seed 190   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.77327322300016\n",
      "Seed 191   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.03098315699981\n",
      "Seed 192   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.10578955100027\n",
      "Seed 193   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.368450354000288\n",
      "Seed 194   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.467802901000141\n",
      "Seed 195   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.05645203999984\n",
      "Seed 196   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.349908370000321\n",
      "Seed 197   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.354014645000007\n",
      "Seed 198   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.494870496999738\n",
      "Seed 199   ###   Best-Avg 0.825\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import ConvNet, FCNet\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset()\n",
    "\n",
    "for seed in range(200):\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    net = FCNet(num_fc=2,sizes_fc=[2,4,2], bias=False, test=False)\n",
    "    \n",
    "    net.train_msa(60,train_loader)\n",
    "    print('Seed '+str(seed)+'   ###   Best-Avg '+str(net.best_avg))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  ###   Avg-Loss 0.03437475562095642   ###   Correct predictions 0.525\n",
      "Epoch 2  ###   Avg-Loss 0.03403787612915039   ###   Correct predictions 0.61875\n",
      "Epoch 3  ###   Avg-Loss 0.033597889542579654   ###   Correct predictions 0.60625\n",
      "Epoch 4  ###   Avg-Loss 0.03306081593036651   ###   Correct predictions 0.6625\n",
      "Epoch 5  ###   Avg-Loss 0.032476571202278134   ###   Correct predictions 0.6375\n",
      "Epoch 6  ###   Avg-Loss 0.03148809373378754   ###   Correct predictions 0.7\n",
      "Epoch 7  ###   Avg-Loss 0.030320319533348083   ###   Correct predictions 0.7\n",
      "Epoch 8  ###   Avg-Loss 0.029096662998199463   ###   Correct predictions 0.6875\n",
      "Epoch 9  ###   Avg-Loss 0.027504265308380127   ###   Correct predictions 0.71875\n",
      "Epoch 10  ###   Avg-Loss 0.02602536976337433   ###   Correct predictions 0.71875\n",
      "Epoch 11  ###   Avg-Loss 0.024707286059856413   ###   Correct predictions 0.75\n",
      "Epoch 12  ###   Avg-Loss 0.023344963788986206   ###   Correct predictions 0.7375\n",
      "Epoch 13  ###   Avg-Loss 0.022469085454940797   ###   Correct predictions 0.7875\n",
      "Epoch 14  ###   Avg-Loss 0.02126854658126831   ###   Correct predictions 0.7875\n",
      "Epoch 15  ###   Avg-Loss 0.020223084092140197   ###   Correct predictions 0.7875\n",
      "Epoch 16  ###   Avg-Loss 0.01908561438322067   ###   Correct predictions 0.825\n",
      "Epoch 17  ###   Avg-Loss 0.01827494353055954   ###   Correct predictions 0.825\n",
      "Epoch 18  ###   Avg-Loss 0.017472554743289948   ###   Correct predictions 0.8375\n",
      "Epoch 19  ###   Avg-Loss 0.01655877083539963   ###   Correct predictions 0.8375\n",
      "Epoch 20  ###   Avg-Loss 0.015839333832263946   ###   Correct predictions 0.85625\n",
      "Epoch 21  ###   Avg-Loss 0.01550280600786209   ###   Correct predictions 0.85\n",
      "Epoch 22  ###   Avg-Loss 0.01506531536579132   ###   Correct predictions 0.85\n",
      "Epoch 23  ###   Avg-Loss 0.014990772306919097   ###   Correct predictions 0.8625\n",
      "Epoch 24  ###   Avg-Loss 0.014799822866916657   ###   Correct predictions 0.85625\n",
      "Epoch 25  ###   Avg-Loss 0.014737404882907867   ###   Correct predictions 0.85\n",
      "Epoch 26  ###   Avg-Loss 0.015444470942020417   ###   Correct predictions 0.84375\n",
      "Epoch 27  ###   Avg-Loss 0.014612197875976562   ###   Correct predictions 0.86875\n",
      "Epoch 28  ###   Avg-Loss 0.014810052514076234   ###   Correct predictions 0.8625\n",
      "Epoch 29  ###   Avg-Loss 0.014188997447490692   ###   Correct predictions 0.8625\n",
      "Epoch 30  ###   Avg-Loss 0.014685487747192383   ###   Correct predictions 0.85625\n",
      "Epoch 31  ###   Avg-Loss 0.014190948009490967   ###   Correct predictions 0.85\n",
      "Epoch 32  ###   Avg-Loss 0.014508916437625885   ###   Correct predictions 0.85\n",
      "Epoch 33  ###   Avg-Loss 0.0138005793094635   ###   Correct predictions 0.85625\n",
      "Epoch 34  ###   Avg-Loss 0.013933828473091126   ###   Correct predictions 0.85\n",
      "Epoch 35  ###   Avg-Loss 0.014268833398818969   ###   Correct predictions 0.86875\n",
      "Epoch 36  ###   Avg-Loss 0.01403689980506897   ###   Correct predictions 0.8625\n",
      "Epoch 37  ###   Avg-Loss 0.014650769531726837   ###   Correct predictions 0.8625\n",
      "Epoch 38  ###   Avg-Loss 0.013478754460811615   ###   Correct predictions 0.85625\n",
      "Epoch 39  ###   Avg-Loss 0.013431890308856964   ###   Correct predictions 0.88125\n",
      "Epoch 40  ###   Avg-Loss 0.014199352264404297   ###   Correct predictions 0.85\n",
      "Epoch 41  ###   Avg-Loss 0.013317318260669708   ###   Correct predictions 0.85625\n",
      "Epoch 42  ###   Avg-Loss 0.013320791721343993   ###   Correct predictions 0.85625\n",
      "Epoch 43  ###   Avg-Loss 0.013442695140838623   ###   Correct predictions 0.8625\n",
      "Epoch 44  ###   Avg-Loss 0.013873571157455444   ###   Correct predictions 0.8625\n",
      "Epoch 45  ###   Avg-Loss 0.013620680570602417   ###   Correct predictions 0.85\n",
      "Epoch 46  ###   Avg-Loss 0.013160236179828644   ###   Correct predictions 0.85625\n",
      "Epoch 47  ###   Avg-Loss 0.013158756494522094   ###   Correct predictions 0.86875\n",
      "Epoch 48  ###   Avg-Loss 0.013062146306037904   ###   Correct predictions 0.84375\n",
      "Epoch 49  ###   Avg-Loss 0.013201622664928437   ###   Correct predictions 0.85625\n",
      "Epoch 50  ###   Avg-Loss 0.013338328897953033   ###   Correct predictions 0.8625\n",
      "Epoch 51  ###   Avg-Loss 0.013170592486858368   ###   Correct predictions 0.88125\n",
      "Epoch 52  ###   Avg-Loss 0.013176913559436797   ###   Correct predictions 0.85\n",
      "Epoch 53  ###   Avg-Loss 0.013779737055301666   ###   Correct predictions 0.875\n",
      "Epoch 54  ###   Avg-Loss 0.014511215686798095   ###   Correct predictions 0.88125\n",
      "Epoch 55  ###   Avg-Loss 0.014076924324035645   ###   Correct predictions 0.84375\n",
      "Epoch 56  ###   Avg-Loss 0.013449697196483612   ###   Correct predictions 0.8625\n",
      "Epoch 57  ###   Avg-Loss 0.014046886563301086   ###   Correct predictions 0.85\n",
      "Epoch 58  ###   Avg-Loss 0.013201718032360078   ###   Correct predictions 0.8625\n",
      "Epoch 59  ###   Avg-Loss 0.015341933071613311   ###   Correct predictions 0.83125\n",
      "Epoch 60  ###   Avg-Loss 0.013065189123153687   ###   Correct predictions 0.8625\n",
      "Epoch 61  ###   Avg-Loss 0.013219170272350311   ###   Correct predictions 0.875\n",
      "Epoch 62  ###   Avg-Loss 0.013396385312080383   ###   Correct predictions 0.84375\n",
      "Epoch 63  ###   Avg-Loss 0.013470773398876191   ###   Correct predictions 0.875\n",
      "Epoch 64  ###   Avg-Loss 0.013682921230793   ###   Correct predictions 0.84375\n",
      "Epoch 65  ###   Avg-Loss 0.013259419798851013   ###   Correct predictions 0.85625\n",
      "Epoch 66  ###   Avg-Loss 0.01370035856962204   ###   Correct predictions 0.8625\n",
      "Epoch 67  ###   Avg-Loss 0.013078434765338898   ###   Correct predictions 0.86875\n",
      "Epoch 68  ###   Avg-Loss 0.012513828277587891   ###   Correct predictions 0.8625\n",
      "Epoch 69  ###   Avg-Loss 0.012576594948768616   ###   Correct predictions 0.85\n",
      "Epoch 70  ###   Avg-Loss 0.012652423977851868   ###   Correct predictions 0.875\n",
      "Epoch 71  ###   Avg-Loss 0.012533575296401978   ###   Correct predictions 0.875\n",
      "Epoch 72  ###   Avg-Loss 0.011995188146829604   ###   Correct predictions 0.86875\n",
      "Epoch 73  ###   Avg-Loss 0.012221866101026536   ###   Correct predictions 0.86875\n",
      "Epoch 74  ###   Avg-Loss 0.012150802463293076   ###   Correct predictions 0.86875\n",
      "Epoch 75  ###   Avg-Loss 0.012550941109657288   ###   Correct predictions 0.8625\n",
      "Epoch 76  ###   Avg-Loss 0.01299712508916855   ###   Correct predictions 0.875\n",
      "Epoch 77  ###   Avg-Loss 0.012512941658496857   ###   Correct predictions 0.875\n",
      "Epoch 78  ###   Avg-Loss 0.011615315079689026   ###   Correct predictions 0.85\n",
      "Epoch 79  ###   Avg-Loss 0.012027522921562195   ###   Correct predictions 0.9\n",
      "Epoch 80  ###   Avg-Loss 0.011313224583864212   ###   Correct predictions 0.875\n",
      "Epoch 81  ###   Avg-Loss 0.010980334877967835   ###   Correct predictions 0.8875\n",
      "Epoch 82  ###   Avg-Loss 0.011381042748689651   ###   Correct predictions 0.86875\n",
      "Epoch 83  ###   Avg-Loss 0.010612208396196365   ###   Correct predictions 0.90625\n",
      "Epoch 84  ###   Avg-Loss 0.010340522229671478   ###   Correct predictions 0.9\n",
      "Epoch 85  ###   Avg-Loss 0.010397825390100479   ###   Correct predictions 0.88125\n",
      "Epoch 86  ###   Avg-Loss 0.010047973692417144   ###   Correct predictions 0.9\n",
      "Epoch 87  ###   Avg-Loss 0.00960089936852455   ###   Correct predictions 0.90625\n",
      "Epoch 88  ###   Avg-Loss 0.008348696678876878   ###   Correct predictions 0.9375\n",
      "Epoch 89  ###   Avg-Loss 0.008384684473276139   ###   Correct predictions 0.925\n",
      "Epoch 90  ###   Avg-Loss 0.00793372243642807   ###   Correct predictions 0.94375\n",
      "Epoch 91  ###   Avg-Loss 0.007119672000408172   ###   Correct predictions 0.9375\n",
      "Epoch 92  ###   Avg-Loss 0.006680969148874283   ###   Correct predictions 0.95\n",
      "Epoch 93  ###   Avg-Loss 0.00654827356338501   ###   Correct predictions 0.95\n",
      "Epoch 94  ###   Avg-Loss 0.006420309841632843   ###   Correct predictions 0.95\n",
      "Epoch 95  ###   Avg-Loss 0.005244392529129982   ###   Correct predictions 0.9625\n",
      "Epoch 96  ###   Avg-Loss 0.00477382205426693   ###   Correct predictions 0.96875\n",
      "Epoch 97  ###   Avg-Loss 0.005167572945356369   ###   Correct predictions 0.95625\n",
      "Epoch 98  ###   Avg-Loss 0.004002885520458221   ###   Correct predictions 0.96875\n",
      "Epoch 99  ###   Avg-Loss 0.004299940541386604   ###   Correct predictions 0.98125\n",
      "Epoch 100  ###   Avg-Loss 0.00342341847717762   ###   Correct predictions 0.98125\n",
      "Epoch 101  ###   Avg-Loss 0.0033155716955661774   ###   Correct predictions 0.975\n",
      "Epoch 102  ###   Avg-Loss 0.003072446584701538   ###   Correct predictions 0.98125\n",
      "Epoch 103  ###   Avg-Loss 0.0029605085030198095   ###   Correct predictions 0.9875\n",
      "Epoch 104  ###   Avg-Loss 0.0025154590606689454   ###   Correct predictions 0.99375\n",
      "Epoch 105  ###   Avg-Loss 0.002787162363529205   ###   Correct predictions 0.99375\n",
      "Epoch 106  ###   Avg-Loss 0.002531791105866432   ###   Correct predictions 0.9875\n",
      "Epoch 107  ###   Avg-Loss 0.0021460149437189102   ###   Correct predictions 0.99375\n",
      "Epoch 108  ###   Avg-Loss 0.002054961398243904   ###   Correct predictions 0.99375\n",
      "Epoch 109  ###   Avg-Loss 0.0024909861385822296   ###   Correct predictions 0.9875\n",
      "Epoch 110  ###   Avg-Loss 0.0019441859796643258   ###   Correct predictions 0.99375\n",
      "Epoch 111  ###   Avg-Loss 0.0017635172232985496   ###   Correct predictions 0.99375\n",
      "Epoch 112  ###   Avg-Loss 0.0019753661006689073   ###   Correct predictions 0.9875\n",
      "Epoch 113  ###   Avg-Loss 0.0016155904158949852   ###   Correct predictions 0.99375\n",
      "Epoch 114  ###   Avg-Loss 0.0018413538113236428   ###   Correct predictions 0.99375\n",
      "Epoch 115  ###   Avg-Loss 0.0016367867588996886   ###   Correct predictions 0.99375\n",
      "Epoch 116  ###   Avg-Loss 0.0014754066243767739   ###   Correct predictions 0.99375\n",
      "Epoch 117  ###   Avg-Loss 0.00150737501680851   ###   Correct predictions 0.99375\n",
      "Epoch 118  ###   Avg-Loss 0.0013377727940678597   ###   Correct predictions 0.99375\n",
      "Epoch 119  ###   Avg-Loss 0.0014603572897613048   ###   Correct predictions 0.99375\n",
      "Epoch 120  ###   Avg-Loss 0.0012958093546330928   ###   Correct predictions 0.99375\n",
      "Epoch 121  ###   Avg-Loss 0.0012244334444403648   ###   Correct predictions 0.99375\n",
      "Epoch 122  ###   Avg-Loss 0.0011688649654388427   ###   Correct predictions 1.0\n",
      "Epoch 123  ###   Avg-Loss 0.0010741700418293477   ###   Correct predictions 1.0\n",
      "Epoch 124  ###   Avg-Loss 0.0011224309913814069   ###   Correct predictions 0.99375\n",
      "Epoch 125  ###   Avg-Loss 0.001119146402925253   ###   Correct predictions 1.0\n",
      "Epoch 126  ###   Avg-Loss 0.0011391081847250462   ###   Correct predictions 0.99375\n",
      "Epoch 127  ###   Avg-Loss 0.0010315742343664168   ###   Correct predictions 1.0\n",
      "Epoch 128  ###   Avg-Loss 0.0008185401558876038   ###   Correct predictions 1.0\n",
      "Epoch 129  ###   Avg-Loss 0.001008896715939045   ###   Correct predictions 0.99375\n",
      "Epoch 130  ###   Avg-Loss 0.0007932381704449654   ###   Correct predictions 1.0\n",
      "Epoch 131  ###   Avg-Loss 0.0008593238890171051   ###   Correct predictions 0.99375\n",
      "Epoch 132  ###   Avg-Loss 0.0008853483945131302   ###   Correct predictions 1.0\n",
      "Epoch 133  ###   Avg-Loss 0.000796498078852892   ###   Correct predictions 0.99375\n",
      "Epoch 134  ###   Avg-Loss 0.0006980234757065773   ###   Correct predictions 1.0\n",
      "Epoch 135  ###   Avg-Loss 0.0008030121214687824   ###   Correct predictions 0.99375\n",
      "Epoch 136  ###   Avg-Loss 0.0006811345927417278   ###   Correct predictions 1.0\n",
      "Epoch 137  ###   Avg-Loss 0.0007383035030215978   ###   Correct predictions 0.99375\n",
      "Epoch 138  ###   Avg-Loss 0.0007504169829189777   ###   Correct predictions 1.0\n",
      "Epoch 139  ###   Avg-Loss 0.0006550038233399391   ###   Correct predictions 1.0\n",
      "Epoch 140  ###   Avg-Loss 0.0006720185279846192   ###   Correct predictions 1.0\n",
      "Epoch 141  ###   Avg-Loss 0.0007017632480710745   ###   Correct predictions 1.0\n",
      "Epoch 142  ###   Avg-Loss 0.0006072117481380701   ###   Correct predictions 1.0\n",
      "Epoch 143  ###   Avg-Loss 0.0005939907394349575   ###   Correct predictions 1.0\n",
      "Epoch 144  ###   Avg-Loss 0.0005999019835144281   ###   Correct predictions 1.0\n",
      "Epoch 145  ###   Avg-Loss 0.0006430903449654579   ###   Correct predictions 1.0\n",
      "Epoch 146  ###   Avg-Loss 0.0006323282141238451   ###   Correct predictions 1.0\n",
      "Epoch 147  ###   Avg-Loss 0.0005223453044891357   ###   Correct predictions 1.0\n",
      "Epoch 148  ###   Avg-Loss 0.0005990971811115741   ###   Correct predictions 1.0\n",
      "Epoch 149  ###   Avg-Loss 0.0005174027755856514   ###   Correct predictions 1.0\n",
      "Epoch 150  ###   Avg-Loss 0.0005918710492551326   ###   Correct predictions 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('lin1',nn.Linear(2, 4, bias=True)),\n",
    "    ('relu1',nn.ReLU()),\n",
    "    ('lin2',nn.Linear(4, 8, bias=True)),\n",
    "    ('relu2',nn.ReLU()),\n",
    "    ('lin3',nn.Linear(8, 8, bias=True)),\n",
    "    ('relu3',nn.ReLU()),\n",
    "    ('lin4',nn.Linear(8, 2, bias=True))\n",
    "])\n",
    ")\n",
    "\n",
    "output_frequency = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(150):\n",
    "    loss_sum = 0\n",
    "    correct_pred = 0\n",
    "    for index, (data, target) in enumerate(train_loader):\n",
    "        #print(index)\n",
    "        output = model.forward(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss_sum = loss_sum + loss.data\n",
    "        for i in range(len(output)):\n",
    "            _, ind = torch.max(output[i],0)\n",
    "            label = target[i]\n",
    "            \n",
    "            if ind.data == label.data:\n",
    "                correct_pred +=1\n",
    "        #if index % (10*(output_frequency)) == 0:\n",
    "        #    print(\"#  Epoch  #  Batch  #  Avg-Loss ###############\")\n",
    "        #if index % (output_frequency) == 0 and index > 0:\n",
    "        #    print(\"#  %d  #  %d  #  %f  #\" % (epoch+1, index, loss_sum/output_frequency))\n",
    "        #    loss_sum = 0\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch '+str(epoch+1)+'  ###   Avg-Loss '+str(loss_sum.item()/160)+'   ###   Correct predictions '+str(correct_pred/160))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7115,  1.4547]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.6434, -2.7388]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3886,  2.0311]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.9741, -3.1041]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-0.8089,  0.1275]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3526, -2.2834]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.8463, -2.8233]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.9055, -2.8244]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-1.7085,  1.4306]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.7206,  2.3536]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 0.7751, -1.6380]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.4437,  2.1093]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.1609,  1.7324]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-0.5344,  0.0122]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3842,  2.1159]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-1.5523,  1.2432]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.5676, -2.6339]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 2.0958, -3.2221]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 0.9023, -1.6219]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.7551,  2.3714]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3445, -2.3139]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.5533,  2.1857]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3841, -2.4257]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3780,  2.0977]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 0.0020, -0.6190]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 0.1119, -0.9363]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3832, -2.3543]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.5189, -2.5425]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-1.9774,  1.4456]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.2206, -2.2302]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-0.1666, -0.6191]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.3856,  2.0124]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.4732, -2.4705]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.4295,  2.1136]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-0.6206,  0.0895]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.7165, -2.7979]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.8907, -2.8468]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.8076, -2.8020]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3121,  1.9327]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.0975,  1.7790]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "for index, (data, target) in enumerate(test_loader):\n",
    "    print(model.forward(data))\n",
    "    print(target)\n",
    "    print('####################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:340: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:343: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 0.825\n"
     ]
    }
   ],
   "source": [
    "net.test(test_loader,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:340: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:343: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "from pandas import DataFrame\n",
    "import torch\n",
    "\n",
    "x1 = []\n",
    "x2 = []\n",
    "y = []\n",
    "for (data,label) in train_loader:\n",
    "    for point in data:\n",
    "        x1.append(point[0].item())\n",
    "        x2.append(point[1].item())\n",
    "        y.append(torch.argmax(net.forward(point)).item())\n",
    "#print(x1)\n",
    "df = DataFrame(dict(x=x1, y=x2, label=y))\n",
    "print(y)\n",
    "colors = {0:'red', 1:'blue'}\n",
    "fig, ax = pyplot.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 0.875\n"
     ]
    }
   ],
   "source": [
    "correct_pred = 0\n",
    "for _, (data, label) in enumerate(test_loader):\n",
    "    prediction = model.forward(data)\n",
    "    _, ind = torch.max(prediction,1)\n",
    "    if ind.data == label.data:\n",
    "        correct_pred +=1\n",
    "print('Correct predictions: '+str(correct_pred/40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################\n",
      "tensor([-0.7453,  0.6668])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:340: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:343: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.5517,  3.5517], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([ 1.4441, -0.3960])\n",
      "tensor([ 3.8032, -3.8032], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([-0.7237,  0.6901])\n",
      "tensor([-3.5532,  3.5532], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([0.4723, 0.8815])\n",
      "tensor([-1.5511,  1.5511], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([0.9284, 0.3717])\n",
      "tensor([ 2.0221, -2.0221], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([ 0.1587, -0.0406])\n",
      "tensor([ 0.7871, -0.7871], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([1.9754, 0.2797])\n",
      "tensor([ 3.7395, -3.7395], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([0.0246, 0.2797])\n",
      "tensor([-0.9989,  0.9989], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([0.0476, 0.9989])\n",
      "tensor([-2.9615,  2.9615], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([ 1.8053, -0.0929])\n",
      "tensor([ 3.8243, -3.8243], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([ 1.6056, -0.2958])\n",
      "tensor([ 3.8254, -3.8254], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([0.1736, 0.9848])\n",
      "tensor([-2.6809,  2.6809], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([ 0.5000, -0.3660])\n",
      "tensor([ 2.7974, -2.7974], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([0.9819, 0.1893])\n",
      "tensor([ 2.6397, -2.6397], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([ 0.8264, -0.4848])\n",
      "tensor([ 3.4583, -3.4583], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([-0.9029,  0.4298])\n",
      "tensor([-3.4797,  3.4797], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([ 1.8580, -0.0137])\n",
      "tensor([ 3.8150, -3.8150], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([0.7453, 0.6668])\n",
      "tensor([ 0.3133, -0.3133], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([0.2048, 0.9788])\n",
      "tensor([-2.5970,  2.5970], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([0.1262, 0.0138])\n",
      "tensor([ 0.4475, -0.4475], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "for index, (data, label) in enumerate(train_loader):\n",
    "    if index == 2:\n",
    "        for i in range(len(data)):\n",
    "            print('######################')\n",
    "            print(data[i])\n",
    "            print(net.forward(data[i]))\n",
    "            print(label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FC0', 'FC1', 'FC2', 'FC3']\n",
      "MSALinearLayer(\n",
      "  (linear): Linear(in_features=2, out_features=4, bias=False)\n",
      ")\n",
      "ReluLayer()\n",
      "MSALinearLayer(\n",
      "  (linear): Linear(in_features=4, out_features=2, bias=False)\n",
      ")\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'FC3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-1a0203474ed2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'FC1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'FC2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'FC3'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'FC3'"
     ]
    }
   ],
   "source": [
    "print(net.fc_keys)\n",
    "print(net.layers_dict['FC0'])\n",
    "print(net.layers_dict['FC1'])\n",
    "print(net.layers_dict['FC2'])\n",
    "print(net.layers_dict['FC3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:340: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:343: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  compared_weight_signs = torch.tensor(torch.ne(new_weight_signs, old_weight_signs).detach(), dtype=torch.float32)\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:148: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_weights = new_weight_signs*torch.tensor(torch.ge(torch.abs(self.m_accumulated),self.rho*max_weight_elem*torch.ones_like(self.m_accumulated)).detach(),dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import loadMNIST\n",
    "from Networks.ResNet import ConvNet, FCNet\n",
    "\n",
    "#net = ConvNet(3, [1], [3,6], num_fc=2, sizes_fc=[100,10])\n",
    "net = FCNet(num_fc=4,sizes_fc=[784,2048,2048,2048,10], bias=False)\n",
    "train_loader, test_loader = loadMNIST('Dataset/input/mnist_train.csv', 'Dataset/input/mnist_test.csv')\n",
    "#net.train_backprop(1,train_loader)\n",
    "#net.test(test_loader,10000)\n",
    "print(train_loader.batch_size)\n",
    "\n",
    "net.train_msa(1,train_loader)\n",
    "\n",
    "#for index, (data, target) in enumerate(train_loader):\n",
    "#    if index == 1:\n",
    "#        print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 4, 1])\n",
      "tensor([[-0.5047, -0.7449,  0.6850,  0.4106, -0.2220],\n",
      "        [-1.2125, -2.6010, -1.0037,  0.0987,  1.0532],\n",
      "        [-0.4957, -0.2819, -2.5197,  0.0198, -0.2908]])\n",
      "tensor(1.4569)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "batch_size = 1\n",
    "a = torch.LongTensor(batch_size).random_(5)\n",
    "b = torch.randn(batch_size, 5)\n",
    "print(a)\n",
    "print(b)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(b, a)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "index 10 is out of bounds for dim with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-f8db13b9caa0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'FC4'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: index 10 is out of bounds for dim with size 10"
     ]
    }
   ],
   "source": [
    "test = net.layers_dict['FC4'].linear.weight\n",
    "print(test[(test == 0).nonzero()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.test(test_loader,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1574, -0.1913,  0.3191],\n",
      "        [ 0.4584,  0.3066, -0.3247],\n",
      "        [ 0.2495,  0.4553,  0.0500],\n",
      "        [ 0.3111, -0.3222,  0.2152]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.4820, -0.5667, -0.2615,  0.5324], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "test_layer = torch.nn.Linear(3, 4)\n",
    "print(test_layer.weight)\n",
    "print(test_layer.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lin1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-ce97ca40c88a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlin1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'lin1' is not defined"
     ]
    }
   ],
   "source": [
    "print(model[lin1].parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.1500, 3.1500, 3.1500, 3.1500, 3.1500, 3.1500, 3.1500, 3.1500, 3.1500,\n",
      "        3.1500, 3.1500, 3.1500, 3.1500, 3.1500, 3.1500, 3.1500, 3.1500, 3.1500,\n",
      "        3.1500, 3.1500, 3.1500, 3.1500, 3.1500, 3.1500, 3.1500, 3.1500, 3.1500,\n",
      "        3.1500, 3.1500, 3.1500, 3.1500, 4.3300, 3.6900, 4.8500, 4.8100, 4.6700,\n",
      "        4.6100], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "print(net.avg_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
