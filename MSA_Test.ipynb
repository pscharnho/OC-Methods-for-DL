{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:164: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  compared_weight_signs = torch.tensor(torch.ne(new_weight_signs, old_weight_signs).detach(), dtype=torch.float32)\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:169: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_weights = new_weight_signs*torch.tensor(torch.ge(torch.abs(self.m_accumulated),self.rho*max_weight_elem*torch.ones_like(self.m_accumulated)).detach(),dtype=torch.float32)\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:187: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  compared_bias_signs = torch.tensor(torch.ne(new_bias_signs, old_bias_signs).detach(), dtype=torch.float32)\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:190: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_biases = new_bias_signs*torch.tensor(torch.ge(torch.abs(self.m2_accumulated),self.rho*max_bias_elem*torch.ones_like(self.m2_accumulated)).detach(),dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ##############\n",
      "Correct predictions: 0.8020833333333334\n",
      "Epoch 2 ##############\n",
      "Correct predictions: 0.9125\n",
      "Epoch 3 ##############\n",
      "Correct predictions: 0.8541666666666666\n",
      "Epoch 4 ##############\n",
      "Correct predictions: 0.80625\n",
      "Epoch 5 ##############\n",
      "Correct predictions: 0.8041666666666667\n",
      "Epoch 6 ##############\n",
      "Correct predictions: 0.74375\n",
      "Epoch 7 ##############\n",
      "Correct predictions: 0.8895833333333333\n",
      "Epoch 8 ##############\n",
      "Correct predictions: 0.8354166666666667\n",
      "Epoch 9 ##############\n",
      "Correct predictions: 0.825\n",
      "Epoch 10 ##############\n",
      "Correct predictions: 0.9520833333333333\n",
      "Epoch 11 ##############\n",
      "Correct predictions: 0.8270833333333333\n",
      "Epoch 12 ##############\n",
      "Correct predictions: 0.8645833333333334\n",
      "Epoch 13 ##############\n",
      "Correct predictions: 0.9625\n",
      "Epoch 14 ##############\n",
      "Correct predictions: 0.9020833333333333\n",
      "Epoch 15 ##############\n",
      "Correct predictions: 0.825\n",
      "Epoch 16 ##############\n",
      "Correct predictions: 0.7479166666666667\n",
      "Epoch 17 ##############\n",
      "Correct predictions: 0.7416666666666667\n",
      "Epoch 18 ##############\n",
      "Correct predictions: 0.8541666666666666\n",
      "Epoch 19 ##############\n",
      "Correct predictions: 0.8416666666666667\n",
      "Epoch 20 ##############\n",
      "Correct predictions: 0.8229166666666666\n",
      "Epoch 21 ##############\n",
      "Correct predictions: 0.8020833333333334\n",
      "Epoch 22 ##############\n",
      "Correct predictions: 0.8166666666666667\n",
      "Epoch 23 ##############\n",
      "Correct predictions: 0.8291666666666667\n",
      "Epoch 24 ##############\n",
      "Correct predictions: 0.68125\n",
      "Epoch 25 ##############\n",
      "Correct predictions: 0.8666666666666667\n",
      "Epoch 26 ##############\n",
      "Correct predictions: 0.86875\n",
      "Epoch 27 ##############\n",
      "Correct predictions: 0.9666666666666667\n",
      "Epoch 28 ##############\n",
      "Correct predictions: 0.9645833333333333\n",
      "Epoch 29 ##############\n",
      "Correct predictions: 0.9604166666666667\n",
      "Epoch 30 ##############\n",
      "Correct predictions: 0.9125\n",
      "Epoch 31 ##############\n",
      "Correct predictions: 0.9291666666666667\n",
      "Epoch 32 ##############\n",
      "Correct predictions: 0.8708333333333333\n",
      "Epoch 33 ##############\n",
      "Correct predictions: 0.9270833333333334\n",
      "Epoch 34 ##############\n",
      "Correct predictions: 0.95625\n",
      "Epoch 35 ##############\n",
      "Correct predictions: 0.9145833333333333\n",
      "Epoch 36 ##############\n",
      "Correct predictions: 0.975\n",
      "Epoch 37 ##############\n",
      "Correct predictions: 0.95\n",
      "Epoch 38 ##############\n",
      "Correct predictions: 0.95\n",
      "Epoch 39 ##############\n",
      "Correct predictions: 0.71875\n",
      "Epoch 40 ##############\n",
      "Correct predictions: 0.9541666666666667\n",
      "Epoch 41 ##############\n",
      "Correct predictions: 0.9479166666666666\n",
      "Epoch 42 ##############\n",
      "Correct predictions: 0.9291666666666667\n",
      "Epoch 43 ##############\n",
      "Correct predictions: 0.85625\n",
      "Epoch 44 ##############\n",
      "Correct predictions: 0.925\n",
      "Epoch 45 ##############\n",
      "Correct predictions: 0.9375\n",
      "Epoch 46 ##############\n",
      "Correct predictions: 0.9083333333333333\n",
      "Epoch 47 ##############\n",
      "Correct predictions: 0.9854166666666667\n",
      "Epoch 48 ##############\n",
      "Correct predictions: 0.9895833333333334\n",
      "Epoch 49 ##############\n",
      "Correct predictions: 0.85625\n",
      "Epoch 50 ##############\n",
      "Correct predictions: 0.8395833333333333\n",
      "Epoch 51 ##############\n",
      "Correct predictions: 0.9583333333333334\n",
      "Epoch 52 ##############\n",
      "Correct predictions: 0.9479166666666666\n",
      "Epoch 53 ##############\n",
      "Correct predictions: 0.6895833333333333\n",
      "Epoch 54 ##############\n",
      "Correct predictions: 0.98125\n",
      "Epoch 55 ##############\n",
      "Correct predictions: 0.9354166666666667\n",
      "Epoch 56 ##############\n",
      "Correct predictions: 0.9833333333333333\n",
      "Epoch 57 ##############\n",
      "Correct predictions: 0.9708333333333333\n",
      "Epoch 58 ##############\n",
      "Correct predictions: 0.9791666666666666\n",
      "Epoch 59 ##############\n",
      "Correct predictions: 0.9916666666666667\n",
      "Time elapsed:  85.49956459399982\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import ConvNet, FCNet\n",
    "import torch\n",
    "#torch.manual_seed(0)\n",
    "train_loader, test_loader = makeMoonsDataset(600,40)\n",
    "torch.manual_seed(3)\n",
    "\n",
    "#for i in range(5,7):\n",
    "#    for j in range(97,100):\n",
    "#        print('Rho '+str(i/10)+' #############################################')\n",
    "#        print('EMA Alpha '+str(j/100)+' ###############################')\n",
    "#        net = FCNet(num_fc=4,sizes_fc=[2,4,8,16,2], bias=True, batchnorm=True, test=False)\n",
    "#        net.set_rho(i/10)\n",
    "#        net.set_ema_alpha(j/100)\n",
    "#        net.train_msa(150,train_loader)\n",
    "    \n",
    "net = FCNet(num_fc=4,sizes_fc=[2,4,8,16,2], bias=True, batchnorm=True, test=False)  \n",
    "net.set_rho(0.6)\n",
    "net.set_ema_alpha(0.99)\n",
    "net.train_msa(59,train_loader)\n",
    "\n",
    "#net = ConvNet(3, [1], [3,6], num_fc=2, sizes_fc=[100,10])\n",
    "\n",
    "#train_loader, test_loader = loadMNIST('Dataset/input/mnist_train.csv', 'Dataset/input/mnist_test.csv')\n",
    "\n",
    "#net.train_backprop(1,train_loader)\n",
    "#net.test(test_loader,10000)\n",
    "#print(train_loader.batch_size)\n",
    "\n",
    "#\n",
    "#for index, (data, label) in enumerate(train_loader):\n",
    "#    print(data)\n",
    "#    print(net.forward(data))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for index, (data, label) in enumerate(train_loader):\n",
    "#    print(data)\n",
    "#    print(net.forward(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:340: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:343: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  compared_weight_signs = torch.tensor(torch.ne(new_weight_signs, old_weight_signs).detach(), dtype=torch.float32)\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:148: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_weights = new_weight_signs*torch.tensor(torch.ge(torch.abs(self.m_accumulated),self.rho*max_weight_elem*torch.ones_like(self.m_accumulated)).detach(),dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed:  10.146643088\n",
      "Seed 0   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.60604992\n",
      "Seed 1   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.699145381999998\n",
      "Seed 2   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.273395311000002\n",
      "Seed 3   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.354167834000002\n",
      "Seed 4   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.854717102000002\n",
      "Seed 5   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.360114429000006\n",
      "Seed 6   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.212407759000001\n",
      "Seed 7   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.896629595000007\n",
      "Seed 8   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.603672412999998\n",
      "Seed 9   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.097800613000004\n",
      "Seed 10   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.248547408000007\n",
      "Seed 11   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.193569678999978\n",
      "Seed 12   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.238679597000015\n",
      "Seed 13   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.202587755999986\n",
      "Seed 14   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.563680431999984\n",
      "Seed 15   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.800362409000002\n",
      "Seed 16   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.172943199000002\n",
      "Seed 17   ###   Best-Avg 0.8\n",
      "Time elapsed:  11.664164929999998\n",
      "Seed 18   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.64493668099999\n",
      "Seed 19   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.808216150000021\n",
      "Seed 20   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.073397882000023\n",
      "Seed 21   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.22652783800001\n",
      "Seed 22   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.820626478999998\n",
      "Seed 23   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.476716109999984\n",
      "Seed 24   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.439701024999977\n",
      "Seed 25   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.500485010999967\n",
      "Seed 26   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.450659146999953\n",
      "Seed 27   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.663561942000001\n",
      "Seed 28   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.897770284000046\n",
      "Seed 29   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.072501369000008\n",
      "Seed 30   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.462130296999987\n",
      "Seed 31   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.181674433000012\n",
      "Seed 32   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.720457097999997\n",
      "Seed 33   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.911982378000005\n",
      "Seed 34   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.12446210600001\n",
      "Seed 35   ###   Best-Avg 0.8\n",
      "Time elapsed:  11.751844816999949\n",
      "Seed 36   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.914494053999988\n",
      "Seed 37   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.721946830999968\n",
      "Seed 38   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.854742291999969\n",
      "Seed 39   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.348190395000017\n",
      "Seed 40   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.857938689000036\n",
      "Seed 41   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.154126195999993\n",
      "Seed 42   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.182050207999964\n",
      "Seed 43   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.263982440000007\n",
      "Seed 44   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.456949645000066\n",
      "Seed 45   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.78960939500007\n",
      "Seed 46   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.985499520000076\n",
      "Seed 47   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.699192676000052\n",
      "Seed 48   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.425626183999952\n",
      "Seed 49   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.54046722499993\n",
      "Seed 50   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.030507655000065\n",
      "Seed 51   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.125098506000086\n",
      "Seed 52   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.919794993999972\n",
      "Seed 53   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.273062202999995\n",
      "Seed 54   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.648992066000005\n",
      "Seed 55   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.420492824999997\n",
      "Seed 56   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.824312771999985\n",
      "Seed 57   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.768213887999991\n",
      "Seed 58   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.478139016\n",
      "Seed 59   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.264060061999999\n",
      "Seed 60   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.428517747\n",
      "Seed 61   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.804717488000051\n",
      "Seed 62   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.830327735999958\n",
      "Seed 63   ###   Best-Avg 0.84375\n",
      "Time elapsed:  13.149567551000018\n",
      "Seed 64   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.077774036999926\n",
      "Seed 65   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.304072090999966\n",
      "Seed 66   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.241903238999953\n",
      "Seed 67   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.745793354000057\n",
      "Seed 68   ###   Best-Avg 0.84375\n",
      "Time elapsed:  13.113821154999982\n",
      "Seed 69   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.56083924699999\n",
      "Seed 70   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.023275922000039\n",
      "Seed 71   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.34733552099999\n",
      "Seed 72   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.205158547999986\n",
      "Seed 73   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.781024666000008\n",
      "Seed 74   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.818582595000066\n",
      "Seed 75   ###   Best-Avg 0.8\n",
      "Time elapsed:  12.775833219999981\n",
      "Seed 76   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.596671489999949\n",
      "Seed 77   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.738887018000014\n",
      "Seed 78   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.079107496999995\n",
      "Seed 79   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.085618524999973\n",
      "Seed 80   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.447313674000043\n",
      "Seed 81   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.547355056000015\n",
      "Seed 82   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.684601208999993\n",
      "Seed 83   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.680478483000002\n",
      "Seed 84   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.97397130999991\n",
      "Seed 85   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.147157143999948\n",
      "Seed 86   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.294491636999965\n",
      "Seed 87   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.387910954999938\n",
      "Seed 88   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.786750217999952\n",
      "Seed 89   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.385938010000018\n",
      "Seed 90   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.857001564999791\n",
      "Seed 91   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.818907478000028\n",
      "Seed 92   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.13523568200003\n",
      "Seed 93   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.777255096999852\n",
      "Seed 94   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.35629602400013\n",
      "Seed 95   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.564049009999962\n",
      "Seed 96   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.523687422999956\n",
      "Seed 97   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.850773782000033\n",
      "Seed 98   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.6953454840002\n",
      "Seed 99   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.015339932000188\n",
      "Seed 100   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.39123638000001\n",
      "Seed 101   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.594417870000143\n",
      "Seed 102   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.783619105999833\n",
      "Seed 103   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.876350617000071\n",
      "Seed 104   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.439500544000111\n",
      "Seed 105   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.181604522000043\n",
      "Seed 106   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.076896543999965\n",
      "Seed 107   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.832593179000014\n",
      "Seed 108   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.307717772999922\n",
      "Seed 109   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.43920239099998\n",
      "Seed 110   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.498897093999858\n",
      "Seed 111   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.657795781000004\n",
      "Seed 112   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.814789377999887\n",
      "Seed 113   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.931495412999993\n",
      "Seed 114   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.288754776999895\n",
      "Seed 115   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.244204666000087\n",
      "Seed 116   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.550813107000067\n",
      "Seed 117   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.388158215999965\n",
      "Seed 118   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.750045622000016\n",
      "Seed 119   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.235819392000167\n",
      "Seed 120   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.996203699000034\n",
      "Seed 121   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.989480880999963\n",
      "Seed 122   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.056708553999897\n",
      "Seed 123   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.285523938999859\n",
      "Seed 124   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.525031678000005\n",
      "Seed 125   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.853770211999972\n",
      "Seed 126   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.874691244999894\n",
      "Seed 127   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.638378874000182\n",
      "Seed 128   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.042981213999838\n",
      "Seed 129   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.147106766999968\n",
      "Seed 130   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.252185378999911\n",
      "Seed 131   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.136590218000038\n",
      "Seed 132   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.631353791000038\n",
      "Seed 133   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.084455215999924\n",
      "Seed 134   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.278537409000137\n",
      "Seed 135   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.179119578000154\n",
      "Seed 136   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.569564312000011\n",
      "Seed 137   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.627897280999832\n",
      "Seed 138   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.784591183999964\n",
      "Seed 139   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.15707995899993\n",
      "Seed 140   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.181525357000055\n",
      "Seed 141   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.311887792000107\n",
      "Seed 142   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.489734566999914\n",
      "Seed 143   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.879751607999879\n",
      "Seed 144   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.325272771000073\n",
      "Seed 145   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.095041162999905\n",
      "Seed 146   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.463122424999938\n",
      "Seed 147   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.492982883999957\n",
      "Seed 148   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.48772615300004\n",
      "Seed 149   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.673118235000175\n",
      "Seed 150   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.028434469000103\n",
      "Seed 151   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.975339212000108\n",
      "Seed 152   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.189310215000205\n",
      "Seed 153   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.902211723999926\n",
      "Seed 154   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.252953891999823\n",
      "Seed 155   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.101709491000065\n",
      "Seed 156   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.322991391999949\n",
      "Seed 157   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.231441695000058\n",
      "Seed 158   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.062093285999936\n",
      "Seed 159   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.669411380999918\n",
      "Seed 160   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.712783790999993\n",
      "Seed 161   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.289970005000214\n",
      "Seed 162   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.054718644999866\n",
      "Seed 163   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.402621168999985\n",
      "Seed 164   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.74273369599996\n",
      "Seed 165   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.994736071000034\n",
      "Seed 166   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.28945646399984\n",
      "Seed 167   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.997000486000161\n",
      "Seed 168   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.741073809999989\n",
      "Seed 169   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.930781389999993\n",
      "Seed 170   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.058178237999982\n",
      "Seed 171   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.299073929000087\n",
      "Seed 172   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.528790453000056\n",
      "Seed 173   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.644680681000182\n",
      "Seed 174   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.82024248000016\n",
      "Seed 175   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.990435996000087\n",
      "Seed 176   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.8661229679999\n",
      "Seed 177   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.123140467999974\n",
      "Seed 178   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.590671946999919\n",
      "Seed 179   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.689152142000012\n",
      "Seed 180   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.67604732399991\n",
      "Seed 181   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.032750995000015\n",
      "Seed 182   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.450169252000023\n",
      "Seed 183   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.201859338999839\n",
      "Seed 184   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.017197729000145\n",
      "Seed 185   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.891306548999637\n",
      "Seed 186   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.39308081199988\n",
      "Seed 187   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.244145035999736\n",
      "Seed 188   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.604870163000214\n",
      "Seed 189   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.960879856999782\n",
      "Seed 190   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.77327322300016\n",
      "Seed 191   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.03098315699981\n",
      "Seed 192   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.10578955100027\n",
      "Seed 193   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.368450354000288\n",
      "Seed 194   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.467802901000141\n",
      "Seed 195   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.05645203999984\n",
      "Seed 196   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.349908370000321\n",
      "Seed 197   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.354014645000007\n",
      "Seed 198   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.494870496999738\n",
      "Seed 199   ###   Best-Avg 0.825\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import ConvNet, FCNet\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset()\n",
    "\n",
    "for seed in range(200):\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    net = FCNet(num_fc=2,sizes_fc=[2,4,2], bias=False, test=False)\n",
    "    \n",
    "    net.train_msa(60,train_loader)\n",
    "    print('Seed '+str(seed)+'   ###   Best-Avg '+str(net.best_avg))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  ###   Avg-Loss 0.017509247859319052   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 2  ###   Avg-Loss 0.01714958349863688   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 3  ###   Avg-Loss 0.016889464855194092   ###   Correct predictions 0.6541666666666667\n",
      "Epoch 4  ###   Avg-Loss 0.016587518652280173   ###   Correct predictions 0.7479166666666667\n",
      "Epoch 5  ###   Avg-Loss 0.016111701726913452   ###   Correct predictions 0.7979166666666667\n",
      "Epoch 6  ###   Avg-Loss 0.015334117412567138   ###   Correct predictions 0.7916666666666666\n",
      "Epoch 7  ###   Avg-Loss 0.014049425721168518   ###   Correct predictions 0.80625\n",
      "Epoch 8  ###   Avg-Loss 0.012173200647036234   ###   Correct predictions 0.8291666666666667\n",
      "Epoch 9  ###   Avg-Loss 0.010075881083806356   ###   Correct predictions 0.84375\n",
      "Epoch 10  ###   Avg-Loss 0.008558314045270283   ###   Correct predictions 0.8458333333333333\n",
      "Epoch 11  ###   Avg-Loss 0.007800989846388499   ###   Correct predictions 0.8604166666666667\n",
      "Epoch 12  ###   Avg-Loss 0.007225486139456431   ###   Correct predictions 0.8708333333333333\n",
      "Epoch 13  ###   Avg-Loss 0.007126175363858541   ###   Correct predictions 0.8791666666666667\n",
      "Epoch 14  ###   Avg-Loss 0.006790060798327128   ###   Correct predictions 0.8770833333333333\n",
      "Epoch 15  ###   Avg-Loss 0.006649807095527649   ###   Correct predictions 0.8770833333333333\n",
      "Epoch 16  ###   Avg-Loss 0.006590724488099416   ###   Correct predictions 0.8791666666666667\n",
      "Epoch 17  ###   Avg-Loss 0.006580385565757752   ###   Correct predictions 0.8770833333333333\n",
      "Epoch 18  ###   Avg-Loss 0.006403144200642904   ###   Correct predictions 0.8875\n",
      "Epoch 19  ###   Avg-Loss 0.006433373192946116   ###   Correct predictions 0.89375\n",
      "Epoch 20  ###   Avg-Loss 0.00632957269748052   ###   Correct predictions 0.88125\n",
      "Epoch 21  ###   Avg-Loss 0.006264880796273549   ###   Correct predictions 0.8854166666666666\n",
      "Epoch 22  ###   Avg-Loss 0.00621228963136673   ###   Correct predictions 0.8895833333333333\n",
      "Epoch 23  ###   Avg-Loss 0.0062507768472035725   ###   Correct predictions 0.89375\n",
      "Epoch 24  ###   Avg-Loss 0.006164952615896861   ###   Correct predictions 0.8770833333333333\n",
      "Epoch 25  ###   Avg-Loss 0.006144644320011139   ###   Correct predictions 0.8895833333333333\n",
      "Epoch 26  ###   Avg-Loss 0.00583211878935496   ###   Correct predictions 0.8958333333333334\n",
      "Epoch 27  ###   Avg-Loss 0.005940474569797516   ###   Correct predictions 0.8916666666666667\n",
      "Epoch 28  ###   Avg-Loss 0.005522573490937551   ###   Correct predictions 0.9\n",
      "Epoch 29  ###   Avg-Loss 0.005590124428272248   ###   Correct predictions 0.89375\n",
      "Epoch 30  ###   Avg-Loss 0.005476273596286774   ###   Correct predictions 0.8979166666666667\n",
      "Epoch 31  ###   Avg-Loss 0.005192050834496816   ###   Correct predictions 0.90625\n",
      "Epoch 32  ###   Avg-Loss 0.0049936880668004354   ###   Correct predictions 0.9125\n",
      "Epoch 33  ###   Avg-Loss 0.004743824402491252   ###   Correct predictions 0.9166666666666666\n",
      "Epoch 34  ###   Avg-Loss 0.004398226241270701   ###   Correct predictions 0.9229166666666667\n",
      "Epoch 35  ###   Avg-Loss 0.004132906099160513   ###   Correct predictions 0.9229166666666667\n",
      "Epoch 36  ###   Avg-Loss 0.004227842887242635   ###   Correct predictions 0.91875\n",
      "Epoch 37  ###   Avg-Loss 0.003825996071100235   ###   Correct predictions 0.9395833333333333\n",
      "Epoch 38  ###   Avg-Loss 0.0037828062971433005   ###   Correct predictions 0.94375\n",
      "Epoch 39  ###   Avg-Loss 0.003230867038170497   ###   Correct predictions 0.9458333333333333\n",
      "Epoch 40  ###   Avg-Loss 0.002921409159898758   ###   Correct predictions 0.9583333333333334\n",
      "Epoch 41  ###   Avg-Loss 0.0027190243204434712   ###   Correct predictions 0.9583333333333334\n",
      "Epoch 42  ###   Avg-Loss 0.002570880949497223   ###   Correct predictions 0.96875\n",
      "Epoch 43  ###   Avg-Loss 0.002161802848180135   ###   Correct predictions 0.9666666666666667\n",
      "Epoch 44  ###   Avg-Loss 0.0019961851338545483   ###   Correct predictions 0.9770833333333333\n",
      "Epoch 45  ###   Avg-Loss 0.0018340418736139932   ###   Correct predictions 0.9833333333333333\n",
      "Epoch 46  ###   Avg-Loss 0.0016212052355209987   ###   Correct predictions 0.98125\n",
      "Epoch 47  ###   Avg-Loss 0.0014380127191543578   ###   Correct predictions 0.9854166666666667\n",
      "Epoch 48  ###   Avg-Loss 0.001300876960158348   ###   Correct predictions 0.9875\n",
      "Epoch 49  ###   Avg-Loss 0.0012439378847678502   ###   Correct predictions 0.9854166666666667\n",
      "Epoch 50  ###   Avg-Loss 0.0011409111320972443   ###   Correct predictions 0.9854166666666667\n",
      "Epoch 51  ###   Avg-Loss 0.0009104576582709948   ###   Correct predictions 0.99375\n",
      "Epoch 52  ###   Avg-Loss 0.0009422303487857183   ###   Correct predictions 0.99375\n",
      "Epoch 53  ###   Avg-Loss 0.0007639643425742785   ###   Correct predictions 0.99375\n",
      "Epoch 54  ###   Avg-Loss 0.0007527730738123258   ###   Correct predictions 0.99375\n",
      "Epoch 55  ###   Avg-Loss 0.000635402463376522   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 56  ###   Avg-Loss 0.0006214913601676624   ###   Correct predictions 0.99375\n",
      "Epoch 57  ###   Avg-Loss 0.000515043983856837   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 58  ###   Avg-Loss 0.0005534132321675619   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 59  ###   Avg-Loss 0.000499645434319973   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 60  ###   Avg-Loss 0.0004596441984176636   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 61  ###   Avg-Loss 0.00048100377122561134   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 62  ###   Avg-Loss 0.0004042477657397588   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 63  ###   Avg-Loss 0.00040099738786617917   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 64  ###   Avg-Loss 0.00038076375300685565   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 65  ###   Avg-Loss 0.0003561533987522125   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 66  ###   Avg-Loss 0.00040928320959210396   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 67  ###   Avg-Loss 0.0003414447419345379   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 68  ###   Avg-Loss 0.00033780957261721293   ###   Correct predictions 1.0\n",
      "Epoch 69  ###   Avg-Loss 0.0002959280585249265   ###   Correct predictions 1.0\n",
      "Epoch 70  ###   Avg-Loss 0.0003317606635391712   ###   Correct predictions 1.0\n",
      "Epoch 71  ###   Avg-Loss 0.0003220666199922562   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 72  ###   Avg-Loss 0.0002945887545744578   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 73  ###   Avg-Loss 0.00035180958608786267   ###   Correct predictions 1.0\n",
      "Epoch 74  ###   Avg-Loss 0.00028336504474282267   ###   Correct predictions 1.0\n",
      "Epoch 75  ###   Avg-Loss 0.0002697288058698177   ###   Correct predictions 1.0\n",
      "Epoch 76  ###   Avg-Loss 0.00024309959262609483   ###   Correct predictions 1.0\n",
      "Epoch 77  ###   Avg-Loss 0.00023261720004181068   ###   Correct predictions 1.0\n",
      "Epoch 78  ###   Avg-Loss 0.00025881221517920494   ###   Correct predictions 1.0\n",
      "Epoch 79  ###   Avg-Loss 0.00023492619705696902   ###   Correct predictions 1.0\n",
      "Epoch 80  ###   Avg-Loss 0.0002299058095862468   ###   Correct predictions 1.0\n",
      "Epoch 81  ###   Avg-Loss 0.00021986557791630427   ###   Correct predictions 1.0\n",
      "Epoch 82  ###   Avg-Loss 0.0002467127051204443   ###   Correct predictions 1.0\n",
      "Epoch 83  ###   Avg-Loss 0.00020120142338176568   ###   Correct predictions 1.0\n",
      "Epoch 84  ###   Avg-Loss 0.00019068244534234207   ###   Correct predictions 1.0\n",
      "Epoch 85  ###   Avg-Loss 0.0002098688700546821   ###   Correct predictions 1.0\n",
      "Epoch 86  ###   Avg-Loss 0.00022621676325798034   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 87  ###   Avg-Loss 0.00024229128224154313   ###   Correct predictions 1.0\n",
      "Epoch 88  ###   Avg-Loss 0.0001787327074756225   ###   Correct predictions 1.0\n",
      "Epoch 89  ###   Avg-Loss 0.00018850779160857202   ###   Correct predictions 1.0\n",
      "Epoch 90  ###   Avg-Loss 0.00018518210078279177   ###   Correct predictions 1.0\n",
      "Epoch 91  ###   Avg-Loss 0.00017173771436015766   ###   Correct predictions 1.0\n",
      "Epoch 92  ###   Avg-Loss 0.00017279090049366158   ###   Correct predictions 1.0\n",
      "Epoch 93  ###   Avg-Loss 0.0002224902156740427   ###   Correct predictions 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94  ###   Avg-Loss 0.0001763194954643647   ###   Correct predictions 1.0\n",
      "Epoch 95  ###   Avg-Loss 0.00018750418288012347   ###   Correct predictions 1.0\n",
      "Epoch 96  ###   Avg-Loss 0.00016904893952111402   ###   Correct predictions 1.0\n",
      "Epoch 97  ###   Avg-Loss 0.00020724992888669172   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 98  ###   Avg-Loss 0.00015976778231561184   ###   Correct predictions 1.0\n",
      "Epoch 99  ###   Avg-Loss 0.00018422151915729046   ###   Correct predictions 1.0\n",
      "Epoch 100  ###   Avg-Loss 0.00015569510869681836   ###   Correct predictions 1.0\n",
      "Epoch 101  ###   Avg-Loss 0.00015788615370790164   ###   Correct predictions 1.0\n",
      "Epoch 102  ###   Avg-Loss 0.0001526924781501293   ###   Correct predictions 1.0\n",
      "Epoch 103  ###   Avg-Loss 0.0001502152221898238   ###   Correct predictions 1.0\n",
      "Epoch 104  ###   Avg-Loss 0.00014202630457778773   ###   Correct predictions 1.0\n",
      "Epoch 105  ###   Avg-Loss 0.00014278668289383252   ###   Correct predictions 1.0\n",
      "Epoch 106  ###   Avg-Loss 0.00014592623338103295   ###   Correct predictions 1.0\n",
      "Epoch 107  ###   Avg-Loss 0.00014255718948940436   ###   Correct predictions 1.0\n",
      "Epoch 108  ###   Avg-Loss 0.0001302220703413089   ###   Correct predictions 1.0\n",
      "Epoch 109  ###   Avg-Loss 0.0001465068974842628   ###   Correct predictions 1.0\n",
      "Epoch 110  ###   Avg-Loss 0.0001356246415525675   ###   Correct predictions 1.0\n",
      "Epoch 111  ###   Avg-Loss 0.00011801136036713918   ###   Correct predictions 1.0\n",
      "Epoch 112  ###   Avg-Loss 0.00014029365653793016   ###   Correct predictions 1.0\n",
      "Epoch 113  ###   Avg-Loss 0.00010946866435309251   ###   Correct predictions 1.0\n",
      "Epoch 114  ###   Avg-Loss 0.0001292335024724404   ###   Correct predictions 1.0\n",
      "Epoch 115  ###   Avg-Loss 0.00012051329637567203   ###   Correct predictions 1.0\n",
      "Epoch 116  ###   Avg-Loss 0.00012256608655055365   ###   Correct predictions 1.0\n",
      "Epoch 117  ###   Avg-Loss 0.0001178044748182098   ###   Correct predictions 1.0\n",
      "Epoch 118  ###   Avg-Loss 0.00010934892731408278   ###   Correct predictions 1.0\n",
      "Epoch 119  ###   Avg-Loss 0.0001507451757788658   ###   Correct predictions 1.0\n",
      "Epoch 120  ###   Avg-Loss 0.00010797327850013972   ###   Correct predictions 1.0\n",
      "Epoch 121  ###   Avg-Loss 0.00012762423139065505   ###   Correct predictions 1.0\n",
      "Epoch 122  ###   Avg-Loss 0.00011855963772783677   ###   Correct predictions 1.0\n",
      "Epoch 123  ###   Avg-Loss 0.0001113381003960967   ###   Correct predictions 1.0\n",
      "Epoch 124  ###   Avg-Loss 0.00011754363464812437   ###   Correct predictions 1.0\n",
      "Epoch 125  ###   Avg-Loss 0.00011777323670685292   ###   Correct predictions 1.0\n",
      "Epoch 126  ###   Avg-Loss 0.00010826393651465575   ###   Correct predictions 1.0\n",
      "Epoch 127  ###   Avg-Loss 0.00010721622966229916   ###   Correct predictions 1.0\n",
      "Epoch 128  ###   Avg-Loss 0.0001076213006551067   ###   Correct predictions 1.0\n",
      "Epoch 129  ###   Avg-Loss 0.00010192733413229386   ###   Correct predictions 1.0\n",
      "Epoch 130  ###   Avg-Loss 0.00010265320694694917   ###   Correct predictions 1.0\n",
      "Epoch 131  ###   Avg-Loss 9.842016734182835e-05   ###   Correct predictions 1.0\n",
      "Epoch 132  ###   Avg-Loss 0.00010161063789079586   ###   Correct predictions 1.0\n",
      "Epoch 133  ###   Avg-Loss 9.769682462016742e-05   ###   Correct predictions 1.0\n",
      "Epoch 134  ###   Avg-Loss 0.00010577191133052111   ###   Correct predictions 1.0\n",
      "Epoch 135  ###   Avg-Loss 0.00010423844990630945   ###   Correct predictions 1.0\n",
      "Epoch 136  ###   Avg-Loss 9.604642788569132e-05   ###   Correct predictions 1.0\n",
      "Epoch 137  ###   Avg-Loss 0.00010160741706689199   ###   Correct predictions 1.0\n",
      "Epoch 138  ###   Avg-Loss 9.37063479796052e-05   ###   Correct predictions 1.0\n",
      "Epoch 139  ###   Avg-Loss 8.933168525497119e-05   ###   Correct predictions 1.0\n",
      "Epoch 140  ###   Avg-Loss 8.194010394314925e-05   ###   Correct predictions 1.0\n",
      "Epoch 141  ###   Avg-Loss 9.461008788396915e-05   ###   Correct predictions 1.0\n",
      "Epoch 142  ###   Avg-Loss 9.656945864359538e-05   ###   Correct predictions 1.0\n",
      "Epoch 143  ###   Avg-Loss 0.00010483420919626951   ###   Correct predictions 1.0\n",
      "Epoch 144  ###   Avg-Loss 9.418870322406293e-05   ###   Correct predictions 1.0\n",
      "Epoch 145  ###   Avg-Loss 8.449126034975052e-05   ###   Correct predictions 1.0\n",
      "Epoch 146  ###   Avg-Loss 0.00011608054240544638   ###   Correct predictions 1.0\n",
      "Epoch 147  ###   Avg-Loss 8.987473944822947e-05   ###   Correct predictions 1.0\n",
      "Epoch 148  ###   Avg-Loss 8.720354332278172e-05   ###   Correct predictions 1.0\n",
      "Epoch 149  ###   Avg-Loss 9.063794277608395e-05   ###   Correct predictions 1.0\n",
      "Epoch 150  ###   Avg-Loss 7.794344952950875e-05   ###   Correct predictions 1.0\n",
      "Epoch 151  ###   Avg-Loss 8.646685164421797e-05   ###   Correct predictions 1.0\n",
      "Epoch 152  ###   Avg-Loss 6.874662358313799e-05   ###   Correct predictions 1.0\n",
      "Epoch 153  ###   Avg-Loss 8.730355184525252e-05   ###   Correct predictions 1.0\n",
      "Epoch 154  ###   Avg-Loss 7.255924089501301e-05   ###   Correct predictions 1.0\n",
      "Epoch 155  ###   Avg-Loss 7.464995142072439e-05   ###   Correct predictions 1.0\n",
      "Epoch 156  ###   Avg-Loss 8.950150416543086e-05   ###   Correct predictions 1.0\n",
      "Epoch 157  ###   Avg-Loss 8.271606639027595e-05   ###   Correct predictions 1.0\n",
      "Epoch 158  ###   Avg-Loss 7.715256263812383e-05   ###   Correct predictions 1.0\n",
      "Epoch 159  ###   Avg-Loss 7.472374321271976e-05   ###   Correct predictions 1.0\n",
      "Epoch 160  ###   Avg-Loss 7.987101562321187e-05   ###   Correct predictions 1.0\n",
      "Epoch 161  ###   Avg-Loss 7.649590261280537e-05   ###   Correct predictions 1.0\n",
      "Epoch 162  ###   Avg-Loss 7.4496166780591e-05   ###   Correct predictions 1.0\n",
      "Epoch 163  ###   Avg-Loss 7.752625582118829e-05   ###   Correct predictions 1.0\n",
      "Epoch 164  ###   Avg-Loss 7.306807674467564e-05   ###   Correct predictions 1.0\n",
      "Epoch 165  ###   Avg-Loss 7.939237790803115e-05   ###   Correct predictions 1.0\n",
      "Epoch 166  ###   Avg-Loss 6.458775606006384e-05   ###   Correct predictions 1.0\n",
      "Epoch 167  ###   Avg-Loss 7.370665359000365e-05   ###   Correct predictions 1.0\n",
      "Epoch 168  ###   Avg-Loss 7.371510534236829e-05   ###   Correct predictions 1.0\n",
      "Epoch 169  ###   Avg-Loss 7.742695355166992e-05   ###   Correct predictions 1.0\n",
      "Epoch 170  ###   Avg-Loss 6.326203389714161e-05   ###   Correct predictions 1.0\n",
      "Epoch 171  ###   Avg-Loss 7.245252685000499e-05   ###   Correct predictions 1.0\n",
      "Epoch 172  ###   Avg-Loss 6.742580638577541e-05   ###   Correct predictions 1.0\n",
      "Epoch 173  ###   Avg-Loss 6.853889984389146e-05   ###   Correct predictions 1.0\n",
      "Epoch 174  ###   Avg-Loss 8.35937603066365e-05   ###   Correct predictions 1.0\n",
      "Epoch 175  ###   Avg-Loss 7.381471029172341e-05   ###   Correct predictions 1.0\n",
      "Epoch 176  ###   Avg-Loss 6.511483031014601e-05   ###   Correct predictions 1.0\n",
      "Epoch 177  ###   Avg-Loss 6.161606482540568e-05   ###   Correct predictions 1.0\n",
      "Epoch 178  ###   Avg-Loss 6.814618439724049e-05   ###   Correct predictions 1.0\n",
      "Epoch 179  ###   Avg-Loss 7.270519466449817e-05   ###   Correct predictions 1.0\n",
      "Epoch 180  ###   Avg-Loss 6.522024050354957e-05   ###   Correct predictions 1.0\n",
      "Epoch 181  ###   Avg-Loss 6.595867841194073e-05   ###   Correct predictions 1.0\n",
      "Epoch 182  ###   Avg-Loss 6.324700467909375e-05   ###   Correct predictions 1.0\n",
      "Epoch 183  ###   Avg-Loss 5.8441368552545706e-05   ###   Correct predictions 1.0\n",
      "Epoch 184  ###   Avg-Loss 7.033594883978366e-05   ###   Correct predictions 1.0\n",
      "Epoch 185  ###   Avg-Loss 5.950239719823003e-05   ###   Correct predictions 1.0\n",
      "Epoch 186  ###   Avg-Loss 6.835521974911293e-05   ###   Correct predictions 1.0\n",
      "Epoch 187  ###   Avg-Loss 6.536475072304408e-05   ###   Correct predictions 1.0\n",
      "Epoch 188  ###   Avg-Loss 6.248203571885824e-05   ###   Correct predictions 1.0\n",
      "Epoch 189  ###   Avg-Loss 6.362803590794404e-05   ###   Correct predictions 1.0\n",
      "Epoch 190  ###   Avg-Loss 5.900887384389837e-05   ###   Correct predictions 1.0\n",
      "Epoch 191  ###   Avg-Loss 6.3311952787141e-05   ###   Correct predictions 1.0\n",
      "Epoch 192  ###   Avg-Loss 5.586339005579551e-05   ###   Correct predictions 1.0\n",
      "Epoch 193  ###   Avg-Loss 6.112878909334541e-05   ###   Correct predictions 1.0\n",
      "Epoch 194  ###   Avg-Loss 5.821308974797527e-05   ###   Correct predictions 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 195  ###   Avg-Loss 5.824346638595064e-05   ###   Correct predictions 1.0\n",
      "Epoch 196  ###   Avg-Loss 5.634997893745701e-05   ###   Correct predictions 1.0\n",
      "Epoch 197  ###   Avg-Loss 6.0553891429056725e-05   ###   Correct predictions 1.0\n",
      "Epoch 198  ###   Avg-Loss 5.993066200365623e-05   ###   Correct predictions 1.0\n",
      "Epoch 199  ###   Avg-Loss 5.520546498397986e-05   ###   Correct predictions 1.0\n",
      "Epoch 200  ###   Avg-Loss 5.6035711895674464e-05   ###   Correct predictions 1.0\n",
      "Epoch 201  ###   Avg-Loss 5.242166031772892e-05   ###   Correct predictions 1.0\n",
      "Epoch 202  ###   Avg-Loss 5.184403853490949e-05   ###   Correct predictions 1.0\n",
      "Epoch 203  ###   Avg-Loss 4.683453201626738e-05   ###   Correct predictions 1.0\n",
      "Epoch 204  ###   Avg-Loss 5.6097280078878006e-05   ###   Correct predictions 1.0\n",
      "Epoch 205  ###   Avg-Loss 5.741083684066931e-05   ###   Correct predictions 1.0\n",
      "Epoch 206  ###   Avg-Loss 5.549422154823939e-05   ###   Correct predictions 1.0\n",
      "Epoch 207  ###   Avg-Loss 5.628007929772139e-05   ###   Correct predictions 1.0\n",
      "Epoch 208  ###   Avg-Loss 5.027116276323795e-05   ###   Correct predictions 1.0\n",
      "Epoch 209  ###   Avg-Loss 5.313633009791374e-05   ###   Correct predictions 1.0\n",
      "Epoch 210  ###   Avg-Loss 5.22430093648533e-05   ###   Correct predictions 1.0\n",
      "Epoch 211  ###   Avg-Loss 5.331027787178755e-05   ###   Correct predictions 1.0\n",
      "Epoch 212  ###   Avg-Loss 5.5232721691330275e-05   ###   Correct predictions 1.0\n",
      "Epoch 213  ###   Avg-Loss 5.351238263150056e-05   ###   Correct predictions 1.0\n",
      "Epoch 214  ###   Avg-Loss 5.273375427350402e-05   ###   Correct predictions 1.0\n",
      "Epoch 215  ###   Avg-Loss 5.5716318699220815e-05   ###   Correct predictions 1.0\n",
      "Epoch 216  ###   Avg-Loss 5.068123961488406e-05   ###   Correct predictions 1.0\n",
      "Epoch 217  ###   Avg-Loss 5.9576379135251047e-05   ###   Correct predictions 1.0\n",
      "Epoch 218  ###   Avg-Loss 5.225882632657886e-05   ###   Correct predictions 1.0\n",
      "Epoch 219  ###   Avg-Loss 4.9849650046477716e-05   ###   Correct predictions 1.0\n",
      "Epoch 220  ###   Avg-Loss 4.67413531926771e-05   ###   Correct predictions 1.0\n",
      "Epoch 221  ###   Avg-Loss 4.4393322120110194e-05   ###   Correct predictions 1.0\n",
      "Epoch 222  ###   Avg-Loss 4.6887691132724286e-05   ###   Correct predictions 1.0\n",
      "Epoch 223  ###   Avg-Loss 4.6570711613943176e-05   ###   Correct predictions 1.0\n",
      "Epoch 224  ###   Avg-Loss 5.2933856689681606e-05   ###   Correct predictions 1.0\n",
      "Epoch 225  ###   Avg-Loss 4.6409585047513245e-05   ###   Correct predictions 1.0\n",
      "Epoch 226  ###   Avg-Loss 5.03044769478341e-05   ###   Correct predictions 1.0\n",
      "Epoch 227  ###   Avg-Loss 4.9149089803298315e-05   ###   Correct predictions 1.0\n",
      "Epoch 228  ###   Avg-Loss 4.889323997000853e-05   ###   Correct predictions 1.0\n",
      "Epoch 229  ###   Avg-Loss 4.5208438920478025e-05   ###   Correct predictions 1.0\n",
      "Epoch 230  ###   Avg-Loss 4.7004812707503636e-05   ###   Correct predictions 1.0\n",
      "Epoch 231  ###   Avg-Loss 4.514964918295542e-05   ###   Correct predictions 1.0\n",
      "Epoch 232  ###   Avg-Loss 4.594233275080721e-05   ###   Correct predictions 1.0\n",
      "Epoch 233  ###   Avg-Loss 4.430419843023022e-05   ###   Correct predictions 1.0\n",
      "Epoch 234  ###   Avg-Loss 4.0409927411625786e-05   ###   Correct predictions 1.0\n",
      "Epoch 235  ###   Avg-Loss 4.70082702425619e-05   ###   Correct predictions 1.0\n",
      "Epoch 236  ###   Avg-Loss 4.169570747762919e-05   ###   Correct predictions 1.0\n",
      "Epoch 237  ###   Avg-Loss 4.080469176794092e-05   ###   Correct predictions 1.0\n",
      "Epoch 238  ###   Avg-Loss 4.469903263573845e-05   ###   Correct predictions 1.0\n",
      "Epoch 239  ###   Avg-Loss 4.052617975200216e-05   ###   Correct predictions 1.0\n",
      "Epoch 240  ###   Avg-Loss 4.284778842702508e-05   ###   Correct predictions 1.0\n",
      "Epoch 241  ###   Avg-Loss 4.260853165760636e-05   ###   Correct predictions 1.0\n",
      "Epoch 242  ###   Avg-Loss 4.37496230006218e-05   ###   Correct predictions 1.0\n",
      "Epoch 243  ###   Avg-Loss 4.4220942072570324e-05   ###   Correct predictions 1.0\n",
      "Epoch 244  ###   Avg-Loss 3.932707477360964e-05   ###   Correct predictions 1.0\n",
      "Epoch 245  ###   Avg-Loss 4.666893122096856e-05   ###   Correct predictions 1.0\n",
      "Epoch 246  ###   Avg-Loss 4.673985143502553e-05   ###   Correct predictions 1.0\n",
      "Epoch 247  ###   Avg-Loss 4.464584247519572e-05   ###   Correct predictions 1.0\n",
      "Epoch 248  ###   Avg-Loss 4.556519367421667e-05   ###   Correct predictions 1.0\n",
      "Epoch 249  ###   Avg-Loss 3.885288412372271e-05   ###   Correct predictions 1.0\n",
      "Epoch 250  ###   Avg-Loss 3.8344327670832475e-05   ###   Correct predictions 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('lin1',nn.Linear(2, 4, bias=True)),\n",
    "    ('relu1',nn.ReLU()),\n",
    "    ('lin2',nn.Linear(4, 8, bias=True)),\n",
    "    ('relu2',nn.ReLU()),\n",
    "    ('lin3',nn.Linear(8, 16, bias=True)),\n",
    "    ('relu3',nn.ReLU()),\n",
    "    ('lin4',nn.Linear(16, 2, bias=True))\n",
    "])\n",
    ")\n",
    "\n",
    "output_frequency = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(250):\n",
    "    loss_sum = 0\n",
    "    correct_pred = 0\n",
    "    for index, (data, target) in enumerate(train_loader):\n",
    "        #print(index)\n",
    "        output = model.forward(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss_sum = loss_sum + loss.data\n",
    "        for i in range(len(output)):\n",
    "            _, ind = torch.max(output[i],0)\n",
    "            label = target[i]\n",
    "            \n",
    "            if ind.data == label.data:\n",
    "                correct_pred +=1\n",
    "        #if index % (10*(output_frequency)) == 0:\n",
    "        #    print(\"#  Epoch  #  Batch  #  Avg-Loss ###############\")\n",
    "        #if index % (output_frequency) == 0 and index > 0:\n",
    "        #    print(\"#  %d  #  %d  #  %f  #\" % (epoch+1, index, loss_sum/output_frequency))\n",
    "        #    loss_sum = 0\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch '+str(epoch+1)+'  ###   Avg-Loss '+str(loss_sum.item()/480)+'   ###   Correct predictions '+str(correct_pred/480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7115,  1.4547]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.6434, -2.7388]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3886,  2.0311]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.9741, -3.1041]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-0.8089,  0.1275]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3526, -2.2834]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.8463, -2.8233]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.9055, -2.8244]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-1.7085,  1.4306]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.7206,  2.3536]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 0.7751, -1.6380]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.4437,  2.1093]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.1609,  1.7324]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-0.5344,  0.0122]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3842,  2.1159]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-1.5523,  1.2432]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.5676, -2.6339]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 2.0958, -3.2221]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 0.9023, -1.6219]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.7551,  2.3714]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3445, -2.3139]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.5533,  2.1857]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3841, -2.4257]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3780,  2.0977]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 0.0020, -0.6190]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 0.1119, -0.9363]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3832, -2.3543]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.5189, -2.5425]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-1.9774,  1.4456]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.2206, -2.2302]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-0.1666, -0.6191]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.3856,  2.0124]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.4732, -2.4705]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.4295,  2.1136]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-0.6206,  0.0895]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.7165, -2.7979]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.8907, -2.8468]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.8076, -2.8020]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3121,  1.9327]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.0975,  1.7790]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "for index, (data, target) in enumerate(test_loader):\n",
    "    print(model.forward(data))\n",
    "    print(target)\n",
    "    print('####################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 1.0\n"
     ]
    }
   ],
   "source": [
    "net.test(test_loader,120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,\n",
      "        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
      "        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1])\n",
      "tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,\n",
      "        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0])\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1])\n",
      "tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
      "        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0])\n",
      "tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1])\n",
      "tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0])\n",
      "tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1])\n",
      "tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
      "        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1])\n",
      "tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0])\n",
      "tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,\n",
      "        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0])\n",
      "tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0])\n",
      "[0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "from pandas import DataFrame\n",
    "import torch\n",
    "\n",
    "x1 = []\n",
    "x2 = []\n",
    "y = []\n",
    "for (data,label) in train_loader:\n",
    "    res = torch.argmax(net.forward(data), dim=1)\n",
    "    print(res)\n",
    "    for point in range(len(data)):\n",
    "        x1.append(data[point][0].item())\n",
    "        x2.append(data[point][1].item())\n",
    "        y.append(res[point].item())\n",
    "#print(x1)\n",
    "df = DataFrame(dict(x=x1, y=x2, label=y))\n",
    "print(y)\n",
    "colors = {0:'red', 1:'blue'}\n",
    "fig, ax = pyplot.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 0.875\n"
     ]
    }
   ],
   "source": [
    "correct_pred = 0\n",
    "for _, (data, label) in enumerate(test_loader):\n",
    "    prediction = model.forward(data)\n",
    "    _, ind = torch.max(prediction,1)\n",
    "    if ind.data == label.data:\n",
    "        correct_pred +=1\n",
    "print('Correct predictions: '+str(correct_pred/40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################\n",
      "tensor([-0.7453,  0.6668])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:340: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:343: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.5517,  3.5517], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([ 1.4441, -0.3960])\n",
      "tensor([ 3.8032, -3.8032], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([-0.7237,  0.6901])\n",
      "tensor([-3.5532,  3.5532], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([0.4723, 0.8815])\n",
      "tensor([-1.5511,  1.5511], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([0.9284, 0.3717])\n",
      "tensor([ 2.0221, -2.0221], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([ 0.1587, -0.0406])\n",
      "tensor([ 0.7871, -0.7871], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([1.9754, 0.2797])\n",
      "tensor([ 3.7395, -3.7395], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([0.0246, 0.2797])\n",
      "tensor([-0.9989,  0.9989], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([0.0476, 0.9989])\n",
      "tensor([-2.9615,  2.9615], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([ 1.8053, -0.0929])\n",
      "tensor([ 3.8243, -3.8243], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([ 1.6056, -0.2958])\n",
      "tensor([ 3.8254, -3.8254], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([0.1736, 0.9848])\n",
      "tensor([-2.6809,  2.6809], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([ 0.5000, -0.3660])\n",
      "tensor([ 2.7974, -2.7974], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([0.9819, 0.1893])\n",
      "tensor([ 2.6397, -2.6397], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([ 0.8264, -0.4848])\n",
      "tensor([ 3.4583, -3.4583], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([-0.9029,  0.4298])\n",
      "tensor([-3.4797,  3.4797], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([ 1.8580, -0.0137])\n",
      "tensor([ 3.8150, -3.8150], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([0.7453, 0.6668])\n",
      "tensor([ 0.3133, -0.3133], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([0.2048, 0.9788])\n",
      "tensor([-2.5970,  2.5970], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([0.1262, 0.0138])\n",
      "tensor([ 0.4475, -0.4475], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "for index, (data, label) in enumerate(train_loader):\n",
    "    if index == 2:\n",
    "        for i in range(len(data)):\n",
    "            print('######################')\n",
    "            print(data[i])\n",
    "            print(net.forward(data[i]))\n",
    "            print(label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FC0', 'FC1', 'FC2', 'FC3']\n",
      "MSALinearLayer(\n",
      "  (linear): Linear(in_features=2, out_features=4, bias=False)\n",
      ")\n",
      "ReluLayer()\n",
      "MSALinearLayer(\n",
      "  (linear): Linear(in_features=4, out_features=2, bias=False)\n",
      ")\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'FC3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-1a0203474ed2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'FC1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'FC2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'FC3'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'FC3'"
     ]
    }
   ],
   "source": [
    "print(net.fc_keys)\n",
    "print(net.layers_dict['FC0'])\n",
    "print(net.layers_dict['FC1'])\n",
    "print(net.layers_dict['FC2'])\n",
    "print(net.layers_dict['FC3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:164: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  compared_weight_signs = torch.tensor(torch.ne(new_weight_signs, old_weight_signs).detach(), dtype=torch.float32)\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:169: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_weights = new_weight_signs*torch.tensor(torch.ge(torch.abs(self.m_accumulated),self.rho*max_weight_elem*torch.ones_like(self.m_accumulated)).detach(),dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import loadMNIST\n",
    "from Networks.ResNet import ConvNet, FCNet\n",
    "\n",
    "#net = ConvNet(3, [1], [3,6], num_fc=2, sizes_fc=[100,10])\n",
    "net = FCNet(num_fc=4,sizes_fc=[784,2048,2048,2048,10], bias=False)\n",
    "train_loader, test_loader = loadMNIST('Dataset/input/mnist_train.csv', 'Dataset/input/mnist_test.csv')\n",
    "#net.train_backprop(1,train_loader)\n",
    "#net.test(test_loader,10000)\n",
    "print(train_loader.batch_size)\n",
    "\n",
    "net.train_msa(1,train_loader)\n",
    "\n",
    "#for index, (data, target) in enumerate(train_loader):\n",
    "#    if index == 1:\n",
    "#        print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 4, 1])\n",
      "tensor([[-0.5047, -0.7449,  0.6850,  0.4106, -0.2220],\n",
      "        [-1.2125, -2.6010, -1.0037,  0.0987,  1.0532],\n",
      "        [-0.4957, -0.2819, -2.5197,  0.0198, -0.2908]])\n",
      "tensor(1.4569)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "batch_size = 1\n",
    "a = torch.LongTensor(batch_size).random_(5)\n",
    "b = torch.randn(batch_size, 5)\n",
    "print(a)\n",
    "print(b)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(b, a)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "index 10 is out of bounds for dim with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-f8db13b9caa0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'FC4'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: index 10 is out of bounds for dim with size 10"
     ]
    }
   ],
   "source": [
    "test = net.layers_dict['FC4'].linear.weight\n",
    "print(test[(test == 0).nonzero()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.test(test_loader,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1574, -0.1913,  0.3191],\n",
      "        [ 0.4584,  0.3066, -0.3247],\n",
      "        [ 0.2495,  0.4553,  0.0500],\n",
      "        [ 0.3111, -0.3222,  0.2152]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.4820, -0.5667, -0.2615,  0.5324], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "test_layer = torch.nn.Linear(3, 4)\n",
    "print(test_layer.weight)\n",
    "print(test_layer.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lin1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-ce97ca40c88a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlin1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'lin1' is not defined"
     ]
    }
   ],
   "source": [
    "print(model[lin1].parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-673cdcf4d885>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavg_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "print(net.avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6., 18., 36.], grad_fn=<SumBackward2>)\n",
      "tensor([[1., 1., 1.],\n",
      "        [2., 2., 2.],\n",
      "        [3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([[1,2,3],[2,3,4],[3,4,5]],dtype=torch.float32, requires_grad=True)\n",
    "y = torch.tensor([[1,1,1],[2,2,2],[3,3,3]],dtype=torch.float32)\n",
    "\n",
    "z = torch.sum(x*y, dim=1)\n",
    "print(z)\n",
    "#z.backward(torch.FloatTensor([0,0,1]), retain_graph=True)\n",
    "#z.backward(torch.FloatTensor([0,1,0]), retain_graph=True)\n",
    "z[0].backward(retain_graph=True)\n",
    "z[1].backward(retain_graph=True)\n",
    "z[2].backward(retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3490, 0.1698, 0.0949])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "x = torch.tensor([[-1,1,2],[1,-2,-1],[-1,2,-1]], dtype=torch.float32)\n",
    "y = torch.tensor([2,0,1])\n",
    "\n",
    "print(criterion(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(3%3 == 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
