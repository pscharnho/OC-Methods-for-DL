{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.751936  #  0.485417  #\n",
      "Correct predictions: 0.45\n",
      "#  2  #  0.589305  #  0.697917  #\n",
      "Correct predictions: 0.825\n",
      "#  3  #  0.422648  #  0.900000  #\n",
      "Correct predictions: 0.9083333333333333\n",
      "#  4  #  0.371970  #  0.937500  #\n",
      "Correct predictions: 1.0\n",
      "#  5  #  0.352882  #  0.964583  #\n",
      "Correct predictions: 0.9833333333333333\n",
      "#  6  #  0.326944  #  0.981250  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  7  #  0.333226  #  0.968750  #\n",
      "Correct predictions: 0.9416666666666667\n",
      "#  8  #  0.320557  #  0.979167  #\n",
      "Correct predictions: 0.975\n",
      "#  9  #  0.311846  #  0.987500  #\n",
      "Correct predictions: 1.0\n",
      "#  10  #  0.331784  #  0.968750  #\n",
      "Correct predictions: 0.9166666666666666\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  11  #  0.335173  #  0.954167  #\n",
      "Correct predictions: 0.9166666666666666\n",
      "#  12  #  0.311650  #  0.981250  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  13  #  0.315962  #  0.972917  #\n",
      "Correct predictions: 0.9833333333333333\n",
      "#  14  #  0.309494  #  0.983333  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  15  #  0.306481  #  0.987500  #\n",
      "Correct predictions: 1.0\n",
      "#  16  #  0.302570  #  0.991667  #\n",
      "Correct predictions: 1.0\n",
      "#  17  #  0.315907  #  0.981250  #\n",
      "Correct predictions: 1.0\n",
      "#  18  #  0.308167  #  0.987500  #\n",
      "Correct predictions: 1.0\n",
      "#  19  #  0.308543  #  0.983333  #\n",
      "Correct predictions: 1.0\n",
      "#  20  #  0.306518  #  0.981250  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  21  #  0.297188  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  22  #  0.299646  #  0.989583  #\n",
      "Correct predictions: 1.0\n",
      "#  23  #  0.309424  #  0.983333  #\n",
      "Correct predictions: 1.0\n",
      "#  24  #  0.308645  #  0.981250  #\n",
      "Correct predictions: 1.0\n",
      "#  25  #  0.296398  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  26  #  0.296957  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  27  #  0.307647  #  0.981250  #\n",
      "Correct predictions: 1.0\n",
      "#  28  #  0.301601  #  0.987500  #\n",
      "Correct predictions: 1.0\n",
      "#  29  #  0.302017  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  30  #  0.308935  #  0.981250  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  31  #  0.309145  #  0.985417  #\n",
      "Correct predictions: 1.0\n",
      "#  32  #  0.296174  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  33  #  0.302925  #  0.987500  #\n",
      "Correct predictions: 1.0\n",
      "#  34  #  0.308004  #  0.985417  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  35  #  0.298649  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  36  #  0.293926  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  37  #  0.296989  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  38  #  0.296677  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  39  #  0.299825  #  0.987500  #\n",
      "Correct predictions: 1.0\n",
      "#  40  #  0.299526  #  0.991667  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  41  #  0.299133  #  0.987500  #\n",
      "Correct predictions: 1.0\n",
      "#  42  #  0.292878  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  43  #  0.305693  #  0.981250  #\n",
      "Correct predictions: 1.0\n",
      "#  44  #  0.293462  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  45  #  0.299919  #  0.987500  #\n",
      "Correct predictions: 1.0\n",
      "#  46  #  0.292893  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  47  #  0.298284  #  0.989583  #\n",
      "Correct predictions: 1.0\n",
      "#  48  #  0.300698  #  0.985417  #\n",
      "Correct predictions: 1.0\n",
      "#  49  #  0.304333  #  0.989583  #\n",
      "Correct predictions: 1.0\n",
      "#  50  #  0.293419  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  51  #  0.297701  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  52  #  0.295071  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  53  #  0.295106  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  54  #  0.294107  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  55  #  0.289995  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  56  #  0.308990  #  0.987500  #\n",
      "Correct predictions: 1.0\n",
      "#  57  #  0.298395  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  58  #  0.300317  #  0.991667  #\n",
      "Correct predictions: 1.0\n",
      "#  59  #  0.298133  #  0.993750  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  60  #  0.302124  #  0.985417  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  61  #  0.297630  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  62  #  0.292992  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  63  #  0.297523  #  0.991667  #\n",
      "Correct predictions: 1.0\n",
      "#  64  #  0.298264  #  0.991667  #\n",
      "Correct predictions: 1.0\n",
      "#  65  #  0.295191  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  66  #  0.294634  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  67  #  0.299905  #  0.987500  #\n",
      "Correct predictions: 1.0\n",
      "#  68  #  0.303261  #  0.987500  #\n",
      "Correct predictions: 1.0\n",
      "#  69  #  0.305390  #  0.983333  #\n",
      "Correct predictions: 1.0\n",
      "#  70  #  0.295489  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  71  #  0.307613  #  0.981250  #\n",
      "Correct predictions: 1.0\n",
      "#  72  #  0.295194  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  73  #  0.295488  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  74  #  0.302234  #  0.983333  #\n",
      "Correct predictions: 1.0\n",
      "#  75  #  0.295881  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  76  #  0.295184  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  77  #  0.299095  #  0.995833  #\n",
      "Correct predictions: 0.9916666666666667\n",
      "#  78  #  0.299033  #  0.991667  #\n",
      "Correct predictions: 1.0\n",
      "#  79  #  0.297258  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  80  #  0.304185  #  0.989583  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  81  #  0.303683  #  0.985417  #\n",
      "Correct predictions: 1.0\n",
      "#  82  #  0.294482  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  83  #  0.306612  #  0.981250  #\n",
      "Correct predictions: 1.0\n",
      "#  84  #  0.291191  #  1.000000  #\n",
      "Correct predictions: 1.0\n",
      "#  85  #  0.293945  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  86  #  0.295899  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  87  #  0.291582  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  88  #  0.297444  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  89  #  0.300121  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  90  #  0.298336  #  0.989583  #\n",
      "Correct predictions: 1.0\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  91  #  0.292760  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  92  #  0.294207  #  0.997917  #\n",
      "Correct predictions: 1.0\n",
      "#  93  #  0.292729  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  94  #  0.298209  #  0.989583  #\n",
      "Correct predictions: 1.0\n",
      "#  95  #  0.295672  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  96  #  0.299487  #  0.991667  #\n",
      "Correct predictions: 1.0\n",
      "#  97  #  0.298609  #  0.993750  #\n",
      "Correct predictions: 1.0\n",
      "#  98  #  0.305127  #  0.991667  #\n",
      "Correct predictions: 1.0\n",
      "#  99  #  0.295646  #  0.995833  #\n",
      "Correct predictions: 1.0\n",
      "#  100  #  0.300097  #  0.991667  #\n",
      "Correct predictions: 1.0\n",
      "Time elapsed:  24.03110603299865\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import FCMSANet\n",
    "import torch\n",
    "\n",
    "\n",
    "def make_uniform_layer_list(layers, num_features):\n",
    "    return [num_features] * (layers+1)\n",
    "    \n",
    "def make_uniform_hidden_layer_list(layers, num_features, num_classes, size_hidden):\n",
    "    res = [num_features]\n",
    "    res.extend([size_hidden]*(layers-1))\n",
    "    res.extend([num_classes])\n",
    "    return res\n",
    "\n",
    "\n",
    "dataset_size = 600\n",
    "batch_size = 50\n",
    "test_set_size = dataset_size * 0.2\n",
    "\n",
    "\n",
    "#torch.manual_seed(0)\n",
    "train_loader, test_loader = makeMoonsDataset(dataset_size, batch_size)\n",
    "#torch.manual_seed(3)\n",
    "\n",
    "num_layers = 10\n",
    "#layers = make_uniform_layer_list(num_layers, 2)\n",
    "layers = make_uniform_hidden_layer_list(num_layers, 2, 2, 64)\n",
    "#print(layers)\n",
    "net = FCMSANet(num_fc=num_layers, sizes_fc=layers, bias=False, batchnorm=True, test=False)   \n",
    "#net = FCMSANet(num_fc=25,sizes_fc=[2,8,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,2], bias=False, batchnorm=True, test=False)  \n",
    "net.set_rho(0.5)\n",
    "net.set_ema_alpha(0.99)\n",
    "net.set_test_tracking(True)\n",
    "net.train_msa(100,train_loader, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  nan  #  0.493750  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  2  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  3  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  4  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  5  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  6  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  7  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  8  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  9  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  10  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  11  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  12  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  13  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  14  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  15  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  16  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  17  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  18  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  19  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  20  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  21  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  22  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  23  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  24  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  25  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  26  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  27  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  28  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  29  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  30  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  31  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  32  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  33  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  34  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  35  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  36  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  37  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  38  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  39  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  40  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  41  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  42  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  43  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  44  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  45  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  46  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  47  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  48  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  49  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  50  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  51  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  52  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  53  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  54  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  55  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  56  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  57  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  58  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  59  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  60  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  61  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  62  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  63  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  64  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  65  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  66  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  67  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  68  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  69  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  70  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  71  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  72  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  73  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  74  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  75  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  76  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  77  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  78  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  79  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  80  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  81  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  82  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  83  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  84  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  85  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  86  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  87  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  88  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  89  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  90  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  91  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  92  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  93  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  94  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  95  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  96  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  97  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  98  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  99  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  100  #  nan  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "Time elapsed:  16.692435770994052\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import ResAntiSymNet\n",
    "import torch\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset(600,40)\n",
    "gamma = 0.3\n",
    "h = 1\n",
    "net = ResAntiSymNet(features=2, classes=2, num_layers=25, gamma=gamma, h=h, bias=True, hidden_size=64)\n",
    "net.set_test_tracking(True)\n",
    "net.train(num_epochs=100, dataloader=train_loader, testloader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.017352  #  0.468750  #\n",
      "Correct predictions: 0.5416666666666666\n",
      "#  2  #  0.017347  #  0.468750  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  3  #  0.017335  #  0.485417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  4  #  0.017336  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  5  #  0.017338  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  6  #  0.017351  #  0.477083  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  7  #  0.017335  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  8  #  0.017356  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  9  #  0.017342  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  10  #  0.017340  #  0.489583  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  11  #  0.017341  #  0.481250  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  12  #  0.017334  #  0.489583  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  13  #  0.017338  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  14  #  0.017343  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  15  #  0.017342  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  16  #  0.017337  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  17  #  0.017339  #  0.510417  #\n",
      "Correct predictions: 0.5416666666666666\n",
      "#  18  #  0.017339  #  0.502083  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  19  #  0.017356  #  0.481250  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  20  #  0.017344  #  0.477083  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  21  #  0.017343  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  22  #  0.017340  #  0.489583  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  23  #  0.017334  #  0.493750  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  24  #  0.017337  #  0.489583  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  25  #  0.017352  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  26  #  0.017347  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  27  #  0.017329  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  28  #  0.017332  #  0.497917  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  29  #  0.017343  #  0.477083  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  30  #  0.017337  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  31  #  0.017326  #  0.497917  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  32  #  0.017340  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  33  #  0.017326  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  34  #  0.017329  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  35  #  0.017350  #  0.489583  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  36  #  0.017343  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  37  #  0.017363  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  38  #  0.017342  #  0.489583  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  39  #  0.017356  #  0.502083  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  40  #  0.017360  #  0.485417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  41  #  0.017342  #  0.489583  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  42  #  0.017341  #  0.485417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  43  #  0.017334  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  44  #  0.017334  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  45  #  0.017350  #  0.477083  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  46  #  0.017336  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  47  #  0.017329  #  0.502083  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  48  #  0.017330  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  49  #  0.017339  #  0.497917  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  50  #  0.017340  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  51  #  0.017338  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  52  #  0.017336  #  0.510417  #\n",
      "Correct predictions: 0.7083333333333334\n",
      "#  53  #  0.017342  #  0.508333  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  54  #  0.017341  #  0.504167  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  55  #  0.017337  #  0.502083  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  56  #  0.017334  #  0.489583  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  57  #  0.017345  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  58  #  0.017360  #  0.477083  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  59  #  0.017335  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  60  #  0.017333  #  0.464583  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  61  #  0.017333  #  0.504167  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  62  #  0.017330  #  0.510417  #\n",
      "Correct predictions: 0.7583333333333333\n",
      "#  63  #  0.017337  #  0.518750  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  64  #  0.017330  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  65  #  0.017339  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  66  #  0.017355  #  0.520833  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  67  #  0.017323  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  68  #  0.017338  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  69  #  0.017338  #  0.481250  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  70  #  0.017334  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  71  #  0.017321  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  72  #  0.017334  #  0.489583  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  73  #  0.017327  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  74  #  0.017337  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  75  #  0.017320  #  0.497917  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  76  #  0.017332  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  77  #  0.017327  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  78  #  0.017332  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  79  #  0.017336  #  0.497917  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  80  #  0.017337  #  0.512500  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  81  #  0.017320  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  82  #  0.017325  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  83  #  0.017321  #  0.489583  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  84  #  0.017336  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  85  #  0.017318  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  86  #  0.017309  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  87  #  0.017318  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  88  #  0.017311  #  0.539583  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  89  #  0.017311  #  0.497917  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  90  #  0.017312  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  91  #  0.017303  #  0.537500  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  92  #  0.017297  #  0.514583  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  93  #  0.017317  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  94  #  0.017294  #  0.495833  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  95  #  0.017281  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  96  #  0.017268  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  97  #  0.017266  #  0.533333  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  98  #  0.017256  #  0.579167  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  99  #  0.017247  #  0.510417  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "#  100  #  0.017217  #  0.589583  #\n",
      "Correct predictions: 0.4583333333333333\n",
      "Time elapsed:  5.87158577000082\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import FCNet\n",
    "import torch\n",
    "\n",
    "dataset_size = 600\n",
    "batch_size = 40\n",
    "test_set_size = dataset_size * 0.2\n",
    "num_layers = 11\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset(dataset_size,batch_size)\n",
    "layers = make_uniform_hidden_layer_list(num_layers, 2, 2, 64)\n",
    "net = FCNet(num_layers=num_layers, layers=layers, bias=True)\n",
    "net.set_test_tracking(True)\n",
    "net.train(100, train_loader, test_loader)\n",
    "\n",
    "#net.test(test_loader, test_set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.472702  #  0.641667  #\n",
      "#  2  #  0.005676  #  0.914583  #\n",
      "#  3  #  0.003665  #  0.952083  #\n",
      "#  4  #  0.002344  #  0.975000  #\n",
      "#  5  #  0.001609  #  0.987500  #\n",
      "#  6  #  0.001037  #  0.991667  #\n",
      "#  7  #  0.000751  #  0.995833  #\n",
      "#  8  #  0.000607  #  0.995833  #\n",
      "#  9  #  0.000501  #  0.995833  #\n",
      "#  10  #  0.000413  #  0.997917  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  11  #  0.000426  #  0.993750  #\n",
      "#  12  #  0.000313  #  0.997917  #\n",
      "#  13  #  0.000319  #  0.995833  #\n",
      "#  14  #  0.000263  #  0.995833  #\n",
      "#  15  #  0.000228  #  1.000000  #\n",
      "#  16  #  0.000187  #  0.997917  #\n",
      "#  17  #  0.000175  #  0.997917  #\n",
      "#  18  #  0.000161  #  1.000000  #\n",
      "#  19  #  0.000157  #  1.000000  #\n",
      "#  20  #  0.000161  #  0.997917  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  21  #  0.000129  #  1.000000  #\n",
      "#  22  #  0.000172  #  1.000000  #\n",
      "#  23  #  0.000179  #  0.997917  #\n",
      "#  24  #  0.000146  #  0.997917  #\n",
      "#  25  #  0.000090  #  1.000000  #\n",
      "#  26  #  0.000087  #  1.000000  #\n",
      "#  27  #  0.000106  #  0.997917  #\n",
      "#  28  #  0.000091  #  1.000000  #\n",
      "#  29  #  0.000135  #  0.997917  #\n",
      "#  30  #  0.000126  #  0.997917  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  31  #  0.000100  #  1.000000  #\n",
      "#  32  #  0.000090  #  1.000000  #\n",
      "#  33  #  0.000109  #  0.997917  #\n",
      "#  34  #  0.000074  #  1.000000  #\n",
      "#  35  #  0.000074  #  1.000000  #\n",
      "#  36  #  0.000158  #  0.997917  #\n",
      "#  37  #  0.000055  #  1.000000  #\n",
      "#  38  #  0.000051  #  1.000000  #\n",
      "#  39  #  0.000048  #  1.000000  #\n",
      "#  40  #  0.000041  #  1.000000  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  41  #  0.000037  #  1.000000  #\n",
      "#  42  #  0.000050  #  1.000000  #\n",
      "#  43  #  0.000072  #  0.997917  #\n",
      "#  44  #  0.000137  #  0.995833  #\n",
      "#  45  #  0.000062  #  1.000000  #\n",
      "#  46  #  0.000165  #  0.995833  #\n",
      "#  47  #  0.000074  #  1.000000  #\n",
      "#  48  #  0.000147  #  0.995833  #\n",
      "#  49  #  0.000153  #  0.995833  #\n",
      "#  50  #  0.000083  #  0.997917  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  51  #  0.000075  #  1.000000  #\n",
      "#  52  #  0.000029  #  1.000000  #\n",
      "#  53  #  0.000053  #  1.000000  #\n",
      "#  54  #  0.000063  #  0.997917  #\n",
      "#  55  #  0.000051  #  1.000000  #\n",
      "#  56  #  0.000056  #  1.000000  #\n",
      "#  57  #  0.000041  #  1.000000  #\n",
      "#  58  #  0.000039  #  1.000000  #\n",
      "#  59  #  0.000030  #  1.000000  #\n",
      "#  60  #  0.000038  #  1.000000  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  61  #  0.000067  #  1.000000  #\n",
      "#  62  #  0.000058  #  0.997917  #\n",
      "#  63  #  0.000214  #  0.993750  #\n",
      "#  64  #  0.000050  #  1.000000  #\n",
      "#  65  #  0.000022  #  1.000000  #\n",
      "#  66  #  0.000025  #  1.000000  #\n",
      "#  67  #  0.000024  #  1.000000  #\n",
      "#  68  #  0.000030  #  1.000000  #\n",
      "#  69  #  0.000020  #  1.000000  #\n",
      "#  70  #  0.000021  #  1.000000  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  71  #  0.000028  #  1.000000  #\n",
      "#  72  #  0.000018  #  1.000000  #\n",
      "#  73  #  0.000019  #  1.000000  #\n",
      "#  74  #  0.000018  #  1.000000  #\n",
      "#  75  #  0.000019  #  1.000000  #\n",
      "#  76  #  0.000014  #  1.000000  #\n",
      "#  77  #  0.000015  #  1.000000  #\n",
      "#  78  #  0.000015  #  1.000000  #\n",
      "#  79  #  0.000020  #  1.000000  #\n",
      "#  80  #  0.000014  #  1.000000  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  81  #  0.000014  #  1.000000  #\n",
      "#  82  #  0.000013  #  1.000000  #\n",
      "#  83  #  0.000018  #  1.000000  #\n",
      "#  84  #  0.000015  #  1.000000  #\n",
      "#  85  #  0.000014  #  1.000000  #\n",
      "#  86  #  0.000012  #  1.000000  #\n",
      "#  87  #  0.000012  #  1.000000  #\n",
      "#  88  #  0.000013  #  1.000000  #\n",
      "#  89  #  0.000011  #  1.000000  #\n",
      "#  90  #  0.000012  #  1.000000  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  91  #  0.000011  #  1.000000  #\n",
      "#  92  #  0.000012  #  1.000000  #\n",
      "#  93  #  0.000012  #  1.000000  #\n",
      "#  94  #  0.000011  #  1.000000  #\n",
      "#  95  #  0.000010  #  1.000000  #\n",
      "#  96  #  0.000012  #  1.000000  #\n",
      "#  97  #  0.000011  #  1.000000  #\n",
      "#  98  #  0.000010  #  1.000000  #\n",
      "#  99  #  0.000010  #  1.000000  #\n",
      "#  100  #  0.000009  #  1.000000  #\n",
      "Time elapsed:  6.256591480996576\n",
      "Correct predictions: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import ResFCNet\n",
    "import torch\n",
    "\n",
    "def make_uniform_layer_list(layers, num_features):\n",
    "        return [num_features] * (layers+1)\n",
    "\n",
    "def make_uniform_hidden_layer_list(layers, num_features, num_classes, size_hidden):\n",
    "    res = [num_features]\n",
    "    res.extend([size_hidden]*(layers-1))\n",
    "    res.extend([num_classes])\n",
    "    return res\n",
    "\n",
    "\n",
    "dataset_size = 600\n",
    "batch_size = 40\n",
    "test_set_size = dataset_size * 0.2\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset(dataset_size,batch_size)\n",
    "\n",
    "num_layers = 14\n",
    "#layers = make_uniform_layer_list(num_layers, 2)\n",
    "layers = make_uniform_hidden_layer_list(num_layers, 2, 2, 64)\n",
    "print(len(layers))\n",
    "net = ResFCNet(num_layers=num_layers, layers=layers, bias=True)\n",
    "\n",
    "#net = ResFCNet(num_layers=30, layers=[2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2], bias=True)\n",
    "net.train(100, train_loader)\n",
    "\n",
    "net.test(test_loader, test_set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:340: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:343: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  compared_weight_signs = torch.tensor(torch.ne(new_weight_signs, old_weight_signs).detach(), dtype=torch.float32)\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:148: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_weights = new_weight_signs*torch.tensor(torch.ge(torch.abs(self.m_accumulated),self.rho*max_weight_elem*torch.ones_like(self.m_accumulated)).detach(),dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed:  10.146643088\n",
      "Seed 0   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.60604992\n",
      "Seed 1   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.699145381999998\n",
      "Seed 2   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.273395311000002\n",
      "Seed 3   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.354167834000002\n",
      "Seed 4   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.854717102000002\n",
      "Seed 5   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.360114429000006\n",
      "Seed 6   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.212407759000001\n",
      "Seed 7   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.896629595000007\n",
      "Seed 8   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.603672412999998\n",
      "Seed 9   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.097800613000004\n",
      "Seed 10   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.248547408000007\n",
      "Seed 11   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.193569678999978\n",
      "Seed 12   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.238679597000015\n",
      "Seed 13   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.202587755999986\n",
      "Seed 14   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.563680431999984\n",
      "Seed 15   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.800362409000002\n",
      "Seed 16   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.172943199000002\n",
      "Seed 17   ###   Best-Avg 0.8\n",
      "Time elapsed:  11.664164929999998\n",
      "Seed 18   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.64493668099999\n",
      "Seed 19   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.808216150000021\n",
      "Seed 20   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.073397882000023\n",
      "Seed 21   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.22652783800001\n",
      "Seed 22   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.820626478999998\n",
      "Seed 23   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.476716109999984\n",
      "Seed 24   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.439701024999977\n",
      "Seed 25   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.500485010999967\n",
      "Seed 26   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.450659146999953\n",
      "Seed 27   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.663561942000001\n",
      "Seed 28   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.897770284000046\n",
      "Seed 29   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.072501369000008\n",
      "Seed 30   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.462130296999987\n",
      "Seed 31   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.181674433000012\n",
      "Seed 32   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.720457097999997\n",
      "Seed 33   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.911982378000005\n",
      "Seed 34   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.12446210600001\n",
      "Seed 35   ###   Best-Avg 0.8\n",
      "Time elapsed:  11.751844816999949\n",
      "Seed 36   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.914494053999988\n",
      "Seed 37   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.721946830999968\n",
      "Seed 38   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.854742291999969\n",
      "Seed 39   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.348190395000017\n",
      "Seed 40   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.857938689000036\n",
      "Seed 41   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.154126195999993\n",
      "Seed 42   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.182050207999964\n",
      "Seed 43   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.263982440000007\n",
      "Seed 44   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.456949645000066\n",
      "Seed 45   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.78960939500007\n",
      "Seed 46   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.985499520000076\n",
      "Seed 47   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.699192676000052\n",
      "Seed 48   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.425626183999952\n",
      "Seed 49   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.54046722499993\n",
      "Seed 50   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.030507655000065\n",
      "Seed 51   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.125098506000086\n",
      "Seed 52   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.919794993999972\n",
      "Seed 53   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.273062202999995\n",
      "Seed 54   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.648992066000005\n",
      "Seed 55   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.420492824999997\n",
      "Seed 56   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.824312771999985\n",
      "Seed 57   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.768213887999991\n",
      "Seed 58   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.478139016\n",
      "Seed 59   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.264060061999999\n",
      "Seed 60   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.428517747\n",
      "Seed 61   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.804717488000051\n",
      "Seed 62   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.830327735999958\n",
      "Seed 63   ###   Best-Avg 0.84375\n",
      "Time elapsed:  13.149567551000018\n",
      "Seed 64   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.077774036999926\n",
      "Seed 65   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.304072090999966\n",
      "Seed 66   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.241903238999953\n",
      "Seed 67   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.745793354000057\n",
      "Seed 68   ###   Best-Avg 0.84375\n",
      "Time elapsed:  13.113821154999982\n",
      "Seed 69   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.56083924699999\n",
      "Seed 70   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.023275922000039\n",
      "Seed 71   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.34733552099999\n",
      "Seed 72   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.205158547999986\n",
      "Seed 73   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.781024666000008\n",
      "Seed 74   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.818582595000066\n",
      "Seed 75   ###   Best-Avg 0.8\n",
      "Time elapsed:  12.775833219999981\n",
      "Seed 76   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.596671489999949\n",
      "Seed 77   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.738887018000014\n",
      "Seed 78   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.079107496999995\n",
      "Seed 79   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.085618524999973\n",
      "Seed 80   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.447313674000043\n",
      "Seed 81   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.547355056000015\n",
      "Seed 82   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.684601208999993\n",
      "Seed 83   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.680478483000002\n",
      "Seed 84   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.97397130999991\n",
      "Seed 85   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.147157143999948\n",
      "Seed 86   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.294491636999965\n",
      "Seed 87   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.387910954999938\n",
      "Seed 88   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.786750217999952\n",
      "Seed 89   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.385938010000018\n",
      "Seed 90   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.857001564999791\n",
      "Seed 91   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.818907478000028\n",
      "Seed 92   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.13523568200003\n",
      "Seed 93   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.777255096999852\n",
      "Seed 94   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.35629602400013\n",
      "Seed 95   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.564049009999962\n",
      "Seed 96   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.523687422999956\n",
      "Seed 97   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.850773782000033\n",
      "Seed 98   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.6953454840002\n",
      "Seed 99   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.015339932000188\n",
      "Seed 100   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.39123638000001\n",
      "Seed 101   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.594417870000143\n",
      "Seed 102   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.783619105999833\n",
      "Seed 103   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.876350617000071\n",
      "Seed 104   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.439500544000111\n",
      "Seed 105   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.181604522000043\n",
      "Seed 106   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.076896543999965\n",
      "Seed 107   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.832593179000014\n",
      "Seed 108   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.307717772999922\n",
      "Seed 109   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.43920239099998\n",
      "Seed 110   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.498897093999858\n",
      "Seed 111   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.657795781000004\n",
      "Seed 112   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.814789377999887\n",
      "Seed 113   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.931495412999993\n",
      "Seed 114   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.288754776999895\n",
      "Seed 115   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.244204666000087\n",
      "Seed 116   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.550813107000067\n",
      "Seed 117   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.388158215999965\n",
      "Seed 118   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.750045622000016\n",
      "Seed 119   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.235819392000167\n",
      "Seed 120   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.996203699000034\n",
      "Seed 121   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.989480880999963\n",
      "Seed 122   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.056708553999897\n",
      "Seed 123   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.285523938999859\n",
      "Seed 124   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.525031678000005\n",
      "Seed 125   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.853770211999972\n",
      "Seed 126   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.874691244999894\n",
      "Seed 127   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.638378874000182\n",
      "Seed 128   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.042981213999838\n",
      "Seed 129   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.147106766999968\n",
      "Seed 130   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.252185378999911\n",
      "Seed 131   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.136590218000038\n",
      "Seed 132   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.631353791000038\n",
      "Seed 133   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.084455215999924\n",
      "Seed 134   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.278537409000137\n",
      "Seed 135   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.179119578000154\n",
      "Seed 136   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.569564312000011\n",
      "Seed 137   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.627897280999832\n",
      "Seed 138   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.784591183999964\n",
      "Seed 139   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.15707995899993\n",
      "Seed 140   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.181525357000055\n",
      "Seed 141   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.311887792000107\n",
      "Seed 142   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.489734566999914\n",
      "Seed 143   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.879751607999879\n",
      "Seed 144   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.325272771000073\n",
      "Seed 145   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.095041162999905\n",
      "Seed 146   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.463122424999938\n",
      "Seed 147   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.492982883999957\n",
      "Seed 148   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.48772615300004\n",
      "Seed 149   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.673118235000175\n",
      "Seed 150   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.028434469000103\n",
      "Seed 151   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.975339212000108\n",
      "Seed 152   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.189310215000205\n",
      "Seed 153   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.902211723999926\n",
      "Seed 154   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.252953891999823\n",
      "Seed 155   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.101709491000065\n",
      "Seed 156   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.322991391999949\n",
      "Seed 157   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.231441695000058\n",
      "Seed 158   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.062093285999936\n",
      "Seed 159   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.669411380999918\n",
      "Seed 160   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.712783790999993\n",
      "Seed 161   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.289970005000214\n",
      "Seed 162   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.054718644999866\n",
      "Seed 163   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.402621168999985\n",
      "Seed 164   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.74273369599996\n",
      "Seed 165   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.994736071000034\n",
      "Seed 166   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.28945646399984\n",
      "Seed 167   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.997000486000161\n",
      "Seed 168   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.741073809999989\n",
      "Seed 169   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.930781389999993\n",
      "Seed 170   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.058178237999982\n",
      "Seed 171   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.299073929000087\n",
      "Seed 172   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.528790453000056\n",
      "Seed 173   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.644680681000182\n",
      "Seed 174   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.82024248000016\n",
      "Seed 175   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.990435996000087\n",
      "Seed 176   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.8661229679999\n",
      "Seed 177   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.123140467999974\n",
      "Seed 178   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.590671946999919\n",
      "Seed 179   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.689152142000012\n",
      "Seed 180   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.67604732399991\n",
      "Seed 181   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.032750995000015\n",
      "Seed 182   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.450169252000023\n",
      "Seed 183   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.201859338999839\n",
      "Seed 184   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.017197729000145\n",
      "Seed 185   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.891306548999637\n",
      "Seed 186   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.39308081199988\n",
      "Seed 187   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.244145035999736\n",
      "Seed 188   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.604870163000214\n",
      "Seed 189   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.960879856999782\n",
      "Seed 190   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.77327322300016\n",
      "Seed 191   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.03098315699981\n",
      "Seed 192   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.10578955100027\n",
      "Seed 193   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.368450354000288\n",
      "Seed 194   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.467802901000141\n",
      "Seed 195   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.05645203999984\n",
      "Seed 196   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.349908370000321\n",
      "Seed 197   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.354014645000007\n",
      "Seed 198   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.494870496999738\n",
      "Seed 199   ###   Best-Avg 0.825\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import ConvNet, FCMSANet\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset()\n",
    "\n",
    "for seed in range(200):\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    net = FCMSANet(num_fc=2,sizes_fc=[2,4,2], bias=False, test=False)\n",
    "    \n",
    "    net.train_msa(60,train_loader)\n",
    "    print('Seed '+str(seed)+'   ###   Best-Avg '+str(net.best_avg))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  ###   Avg-Loss 0.016693341732025146   ###   Correct predictions 0.7354166666666667\n",
      "Epoch 2  ###   Avg-Loss 0.01539247731367747   ###   Correct predictions 0.7895833333333333\n",
      "Epoch 3  ###   Avg-Loss 0.013269580403963725   ###   Correct predictions 0.7958333333333333\n",
      "Epoch 4  ###   Avg-Loss 0.010812340180079143   ###   Correct predictions 0.8125\n",
      "Epoch 5  ###   Avg-Loss 0.009092676639556884   ###   Correct predictions 0.8375\n",
      "Epoch 6  ###   Avg-Loss 0.007869457205136618   ###   Correct predictions 0.8583333333333333\n",
      "Epoch 7  ###   Avg-Loss 0.007090519865353902   ###   Correct predictions 0.8625\n",
      "Epoch 8  ###   Avg-Loss 0.00648987740278244   ###   Correct predictions 0.8916666666666667\n",
      "Epoch 9  ###   Avg-Loss 0.006088708837827046   ###   Correct predictions 0.8979166666666667\n",
      "Epoch 10  ###   Avg-Loss 0.005733463664849599   ###   Correct predictions 0.9\n",
      "Epoch 11  ###   Avg-Loss 0.005439147353172302   ###   Correct predictions 0.9041666666666667\n",
      "Epoch 12  ###   Avg-Loss 0.005182546377182007   ###   Correct predictions 0.9125\n",
      "Epoch 13  ###   Avg-Loss 0.0049934898813565574   ###   Correct predictions 0.9104166666666667\n",
      "Epoch 14  ###   Avg-Loss 0.004753189285596212   ###   Correct predictions 0.91875\n",
      "Epoch 15  ###   Avg-Loss 0.004416432480017344   ###   Correct predictions 0.9229166666666667\n",
      "Epoch 16  ###   Avg-Loss 0.004125663638114929   ###   Correct predictions 0.93125\n",
      "Epoch 17  ###   Avg-Loss 0.003898448000351588   ###   Correct predictions 0.93125\n",
      "Epoch 18  ###   Avg-Loss 0.003615226596593857   ###   Correct predictions 0.9375\n",
      "Epoch 19  ###   Avg-Loss 0.0030299144486586253   ###   Correct predictions 0.95\n",
      "Epoch 20  ###   Avg-Loss 0.0029660664498806   ###   Correct predictions 0.95625\n",
      "Epoch 21  ###   Avg-Loss 0.00286577045917511   ###   Correct predictions 0.9479166666666666\n",
      "Epoch 22  ###   Avg-Loss 0.002026676634947459   ###   Correct predictions 0.9770833333333333\n",
      "Epoch 23  ###   Avg-Loss 0.00176669346789519   ###   Correct predictions 0.9791666666666666\n",
      "Epoch 24  ###   Avg-Loss 0.0016901899129152299   ###   Correct predictions 0.98125\n",
      "Epoch 25  ###   Avg-Loss 0.001406506821513176   ###   Correct predictions 0.9854166666666667\n",
      "Epoch 26  ###   Avg-Loss 0.0012301721920569737   ###   Correct predictions 0.9895833333333334\n",
      "Epoch 27  ###   Avg-Loss 0.0012656405568122863   ###   Correct predictions 0.9895833333333334\n",
      "Epoch 28  ###   Avg-Loss 0.0009948708117008208   ###   Correct predictions 0.9916666666666667\n",
      "Epoch 29  ###   Avg-Loss 0.0008995652198791504   ###   Correct predictions 0.99375\n",
      "Epoch 30  ###   Avg-Loss 0.0008839219808578491   ###   Correct predictions 0.99375\n",
      "Epoch 31  ###   Avg-Loss 0.0007204620788494746   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 32  ###   Avg-Loss 0.0006353583186864853   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 33  ###   Avg-Loss 0.0006963463500142097   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 34  ###   Avg-Loss 0.0006097466374437014   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 35  ###   Avg-Loss 0.0006425640856226285   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 36  ###   Avg-Loss 0.0004952810704708099   ###   Correct predictions 1.0\n",
      "Epoch 37  ###   Avg-Loss 0.00046397863576809565   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 38  ###   Avg-Loss 0.00047715206940968834   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 39  ###   Avg-Loss 0.0004436716747780641   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 40  ###   Avg-Loss 0.00044246241450309756   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 41  ###   Avg-Loss 0.0003799566999077797   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 42  ###   Avg-Loss 0.00037729634592930473   ###   Correct predictions 1.0\n",
      "Epoch 43  ###   Avg-Loss 0.00042406826590498287   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 44  ###   Avg-Loss 0.0003210838573674361   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 45  ###   Avg-Loss 0.00035742980738480884   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 46  ###   Avg-Loss 0.0003395354375243187   ###   Correct predictions 1.0\n",
      "Epoch 47  ###   Avg-Loss 0.0003530730493366718   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 48  ###   Avg-Loss 0.0003453268048663934   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 49  ###   Avg-Loss 0.0003007656273742517   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 50  ###   Avg-Loss 0.0002979370454947154   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 51  ###   Avg-Loss 0.0002575600054115057   ###   Correct predictions 1.0\n",
      "Epoch 52  ###   Avg-Loss 0.0003250787034630775   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 53  ###   Avg-Loss 0.00023143263533711433   ###   Correct predictions 1.0\n",
      "Epoch 54  ###   Avg-Loss 0.0002476739386717478   ###   Correct predictions 1.0\n",
      "Epoch 55  ###   Avg-Loss 0.0002808337099850178   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 56  ###   Avg-Loss 0.00022636189435919126   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 57  ###   Avg-Loss 0.00020725494250655173   ###   Correct predictions 1.0\n",
      "Epoch 58  ###   Avg-Loss 0.0002392620158692201   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 59  ###   Avg-Loss 0.0002071976816902558   ###   Correct predictions 1.0\n",
      "Epoch 60  ###   Avg-Loss 0.00020230164130528768   ###   Correct predictions 1.0\n",
      "Epoch 61  ###   Avg-Loss 0.00018292929356296858   ###   Correct predictions 1.0\n",
      "Epoch 62  ###   Avg-Loss 0.00022179240671296914   ###   Correct predictions 1.0\n",
      "Epoch 63  ###   Avg-Loss 0.00019362818760176498   ###   Correct predictions 1.0\n",
      "Epoch 64  ###   Avg-Loss 0.00022219737681249778   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 65  ###   Avg-Loss 0.00018464084714651108   ###   Correct predictions 1.0\n",
      "Epoch 66  ###   Avg-Loss 0.00021055651207764944   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 67  ###   Avg-Loss 0.00020628821415205796   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 68  ###   Avg-Loss 0.00019506152408818405   ###   Correct predictions 1.0\n",
      "Epoch 69  ###   Avg-Loss 0.0001632713247090578   ###   Correct predictions 1.0\n",
      "Epoch 70  ###   Avg-Loss 0.00017592293831209343   ###   Correct predictions 1.0\n",
      "Epoch 71  ###   Avg-Loss 0.00016277733569343886   ###   Correct predictions 1.0\n",
      "Epoch 72  ###   Avg-Loss 0.0001429748721420765   ###   Correct predictions 1.0\n",
      "Epoch 73  ###   Avg-Loss 0.0001682426780462265   ###   Correct predictions 1.0\n",
      "Epoch 74  ###   Avg-Loss 0.00015255645848810673   ###   Correct predictions 1.0\n",
      "Epoch 75  ###   Avg-Loss 0.00013346169143915176   ###   Correct predictions 1.0\n",
      "Epoch 76  ###   Avg-Loss 0.00015274204003314177   ###   Correct predictions 1.0\n",
      "Epoch 77  ###   Avg-Loss 0.0001632209246357282   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 78  ###   Avg-Loss 0.00016444246284663677   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 79  ###   Avg-Loss 0.0001559902292986711   ###   Correct predictions 1.0\n",
      "Epoch 80  ###   Avg-Loss 0.00012979219512393076   ###   Correct predictions 1.0\n",
      "Epoch 81  ###   Avg-Loss 0.00014563739920655887   ###   Correct predictions 1.0\n",
      "Epoch 82  ###   Avg-Loss 0.00012268397646645704   ###   Correct predictions 1.0\n",
      "Epoch 83  ###   Avg-Loss 0.00014513544738292694   ###   Correct predictions 1.0\n",
      "Epoch 84  ###   Avg-Loss 0.00012508279954393705   ###   Correct predictions 1.0\n",
      "Epoch 85  ###   Avg-Loss 0.00013901602166394394   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 86  ###   Avg-Loss 0.0001333181746304035   ###   Correct predictions 1.0\n",
      "Epoch 87  ###   Avg-Loss 0.00012044358688096205   ###   Correct predictions 1.0\n",
      "Epoch 88  ###   Avg-Loss 0.00014618256439765295   ###   Correct predictions 1.0\n",
      "Epoch 89  ###   Avg-Loss 0.00012452843754241864   ###   Correct predictions 1.0\n",
      "Epoch 90  ###   Avg-Loss 0.00011009617398182551   ###   Correct predictions 1.0\n",
      "Epoch 91  ###   Avg-Loss 0.00012323612657686074   ###   Correct predictions 1.0\n",
      "Epoch 92  ###   Avg-Loss 0.00011581191793084145   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 93  ###   Avg-Loss 0.0001325206986318032   ###   Correct predictions 1.0\n",
      "Epoch 94  ###   Avg-Loss 0.00010437506716698408   ###   Correct predictions 1.0\n",
      "Epoch 95  ###   Avg-Loss 0.00010432829149067401   ###   Correct predictions 1.0\n",
      "Epoch 96  ###   Avg-Loss 0.00011434933015455803   ###   Correct predictions 1.0\n",
      "Epoch 97  ###   Avg-Loss 0.00010554267403980096   ###   Correct predictions 1.0\n",
      "Epoch 98  ###   Avg-Loss 0.00013531527171532312   ###   Correct predictions 1.0\n",
      "Epoch 99  ###   Avg-Loss 9.843817291160425e-05   ###   Correct predictions 1.0\n",
      "Epoch 100  ###   Avg-Loss 9.126464525858562e-05   ###   Correct predictions 1.0\n",
      "Epoch 101  ###   Avg-Loss 8.715117195000251e-05   ###   Correct predictions 1.0\n",
      "Epoch 102  ###   Avg-Loss 9.70254031320413e-05   ###   Correct predictions 1.0\n",
      "Epoch 103  ###   Avg-Loss 0.00010043517686426639   ###   Correct predictions 1.0\n",
      "Epoch 104  ###   Avg-Loss 8.699353784322739e-05   ###   Correct predictions 1.0\n",
      "Epoch 105  ###   Avg-Loss 9.705725436409315e-05   ###   Correct predictions 1.0\n",
      "Epoch 106  ###   Avg-Loss 8.577681922664246e-05   ###   Correct predictions 1.0\n",
      "Epoch 107  ###   Avg-Loss 0.00010882464703172445   ###   Correct predictions 1.0\n",
      "Epoch 108  ###   Avg-Loss 0.00010686044115573167   ###   Correct predictions 1.0\n",
      "Epoch 109  ###   Avg-Loss 0.00010626811999827623   ###   Correct predictions 1.0\n",
      "Epoch 110  ###   Avg-Loss 0.00010530042927712203   ###   Correct predictions 1.0\n",
      "Epoch 111  ###   Avg-Loss 0.00010232297548403342   ###   Correct predictions 1.0\n",
      "Epoch 112  ###   Avg-Loss 8.531966401884954e-05   ###   Correct predictions 1.0\n",
      "Epoch 113  ###   Avg-Loss 8.699455453703801e-05   ###   Correct predictions 1.0\n",
      "Epoch 114  ###   Avg-Loss 8.856581213573615e-05   ###   Correct predictions 1.0\n",
      "Epoch 115  ###   Avg-Loss 7.67768050233523e-05   ###   Correct predictions 1.0\n",
      "Epoch 116  ###   Avg-Loss 8.174949325621129e-05   ###   Correct predictions 1.0\n",
      "Epoch 117  ###   Avg-Loss 7.995864531646172e-05   ###   Correct predictions 1.0\n",
      "Epoch 118  ###   Avg-Loss 7.204515859484672e-05   ###   Correct predictions 1.0\n",
      "Epoch 119  ###   Avg-Loss 8.846645553906758e-05   ###   Correct predictions 1.0\n",
      "Epoch 120  ###   Avg-Loss 8.003233621517817e-05   ###   Correct predictions 1.0\n",
      "Epoch 121  ###   Avg-Loss 7.330861408263444e-05   ###   Correct predictions 1.0\n",
      "Epoch 122  ###   Avg-Loss 6.652111187577248e-05   ###   Correct predictions 1.0\n",
      "Epoch 123  ###   Avg-Loss 7.577384822070599e-05   ###   Correct predictions 1.0\n",
      "Epoch 124  ###   Avg-Loss 6.504503932471076e-05   ###   Correct predictions 1.0\n",
      "Epoch 125  ###   Avg-Loss 9.361816725383202e-05   ###   Correct predictions 1.0\n",
      "Epoch 126  ###   Avg-Loss 6.556252483278513e-05   ###   Correct predictions 1.0\n",
      "Epoch 127  ###   Avg-Loss 6.442156542713443e-05   ###   Correct predictions 1.0\n",
      "Epoch 128  ###   Avg-Loss 7.758416080226501e-05   ###   Correct predictions 1.0\n",
      "Epoch 129  ###   Avg-Loss 7.533366636683544e-05   ###   Correct predictions 1.0\n",
      "Epoch 130  ###   Avg-Loss 6.837865803390741e-05   ###   Correct predictions 1.0\n",
      "Epoch 131  ###   Avg-Loss 7.118641709287961e-05   ###   Correct predictions 1.0\n",
      "Epoch 132  ###   Avg-Loss 8.623798688252766e-05   ###   Correct predictions 1.0\n",
      "Epoch 133  ###   Avg-Loss 6.864879590769608e-05   ###   Correct predictions 1.0\n",
      "Epoch 134  ###   Avg-Loss 7.340631758173307e-05   ###   Correct predictions 1.0\n",
      "Epoch 135  ###   Avg-Loss 5.6213835099091135e-05   ###   Correct predictions 1.0\n",
      "Epoch 136  ###   Avg-Loss 5.9551175218075515e-05   ###   Correct predictions 1.0\n",
      "Epoch 137  ###   Avg-Loss 7.160236903776725e-05   ###   Correct predictions 1.0\n",
      "Epoch 138  ###   Avg-Loss 6.109048845246435e-05   ###   Correct predictions 1.0\n",
      "Epoch 139  ###   Avg-Loss 7.094782777130603e-05   ###   Correct predictions 1.0\n",
      "Epoch 140  ###   Avg-Loss 7.791101622084776e-05   ###   Correct predictions 1.0\n",
      "Epoch 141  ###   Avg-Loss 6.113395793363451e-05   ###   Correct predictions 1.0\n",
      "Epoch 142  ###   Avg-Loss 6.176692356045048e-05   ###   Correct predictions 1.0\n",
      "Epoch 143  ###   Avg-Loss 7.38037284463644e-05   ###   Correct predictions 1.0\n",
      "Epoch 144  ###   Avg-Loss 6.984003509084384e-05   ###   Correct predictions 1.0\n",
      "Epoch 145  ###   Avg-Loss 6.579244509339333e-05   ###   Correct predictions 1.0\n",
      "Epoch 146  ###   Avg-Loss 5.51832839846611e-05   ###   Correct predictions 1.0\n",
      "Epoch 147  ###   Avg-Loss 5.711079575121403e-05   ###   Correct predictions 1.0\n",
      "Epoch 148  ###   Avg-Loss 5.0937927638490996e-05   ###   Correct predictions 1.0\n",
      "Epoch 149  ###   Avg-Loss 5.608038045465946e-05   ###   Correct predictions 1.0\n",
      "Epoch 150  ###   Avg-Loss 6.162119098007679e-05   ###   Correct predictions 1.0\n",
      "Epoch 151  ###   Avg-Loss 5.776884887988369e-05   ###   Correct predictions 1.0\n",
      "Epoch 152  ###   Avg-Loss 6.204020076741776e-05   ###   Correct predictions 1.0\n",
      "Epoch 153  ###   Avg-Loss 5.5288333290567e-05   ###   Correct predictions 1.0\n",
      "Epoch 154  ###   Avg-Loss 4.973360725368063e-05   ###   Correct predictions 1.0\n",
      "Epoch 155  ###   Avg-Loss 5.3552817553281785e-05   ###   Correct predictions 1.0\n",
      "Epoch 156  ###   Avg-Loss 4.901509576787551e-05   ###   Correct predictions 1.0\n",
      "Epoch 157  ###   Avg-Loss 5.7395625238617264e-05   ###   Correct predictions 1.0\n",
      "Epoch 158  ###   Avg-Loss 4.948605395232638e-05   ###   Correct predictions 1.0\n",
      "Epoch 159  ###   Avg-Loss 4.963681955511371e-05   ###   Correct predictions 1.0\n",
      "Epoch 160  ###   Avg-Loss 5.433883440370361e-05   ###   Correct predictions 1.0\n",
      "Epoch 161  ###   Avg-Loss 5.3806684445589784e-05   ###   Correct predictions 1.0\n",
      "Epoch 162  ###   Avg-Loss 4.746093569944302e-05   ###   Correct predictions 1.0\n",
      "Epoch 163  ###   Avg-Loss 7.585322794814905e-05   ###   Correct predictions 1.0\n",
      "Epoch 164  ###   Avg-Loss 4.612901248037815e-05   ###   Correct predictions 1.0\n",
      "Epoch 165  ###   Avg-Loss 4.46078289921085e-05   ###   Correct predictions 1.0\n",
      "Epoch 166  ###   Avg-Loss 4.7858059406280515e-05   ###   Correct predictions 1.0\n",
      "Epoch 167  ###   Avg-Loss 5.252285239597161e-05   ###   Correct predictions 1.0\n",
      "Epoch 168  ###   Avg-Loss 6.123439331228535e-05   ###   Correct predictions 1.0\n",
      "Epoch 169  ###   Avg-Loss 5.4478478462745744e-05   ###   Correct predictions 1.0\n",
      "Epoch 170  ###   Avg-Loss 5.318818924327691e-05   ###   Correct predictions 1.0\n",
      "Epoch 171  ###   Avg-Loss 4.756053676828742e-05   ###   Correct predictions 1.0\n",
      "Epoch 172  ###   Avg-Loss 4.761051774645845e-05   ###   Correct predictions 1.0\n",
      "Epoch 173  ###   Avg-Loss 5.6479397850732006e-05   ###   Correct predictions 1.0\n",
      "Epoch 174  ###   Avg-Loss 4.788977870096763e-05   ###   Correct predictions 1.0\n",
      "Epoch 175  ###   Avg-Loss 6.446146095792452e-05   ###   Correct predictions 1.0\n",
      "Epoch 176  ###   Avg-Loss 4.3726297250638405e-05   ###   Correct predictions 1.0\n",
      "Epoch 177  ###   Avg-Loss 5.430232267826796e-05   ###   Correct predictions 1.0\n",
      "Epoch 178  ###   Avg-Loss 5.736845002199213e-05   ###   Correct predictions 1.0\n",
      "Epoch 179  ###   Avg-Loss 5.051289529850086e-05   ###   Correct predictions 1.0\n",
      "Epoch 180  ###   Avg-Loss 5.14243069725732e-05   ###   Correct predictions 1.0\n",
      "Epoch 181  ###   Avg-Loss 3.913740317026774e-05   ###   Correct predictions 1.0\n",
      "Epoch 182  ###   Avg-Loss 5.9283819670478506e-05   ###   Correct predictions 1.0\n",
      "Epoch 183  ###   Avg-Loss 4.4025426420072714e-05   ###   Correct predictions 1.0\n",
      "Epoch 184  ###   Avg-Loss 4.923796902100245e-05   ###   Correct predictions 1.0\n",
      "Epoch 185  ###   Avg-Loss 4.7920217427114645e-05   ###   Correct predictions 1.0\n",
      "Epoch 186  ###   Avg-Loss 4.310249350965023e-05   ###   Correct predictions 1.0\n",
      "Epoch 187  ###   Avg-Loss 4.640198312699795e-05   ###   Correct predictions 1.0\n",
      "Epoch 188  ###   Avg-Loss 4.455156158655882e-05   ###   Correct predictions 1.0\n",
      "Epoch 189  ###   Avg-Loss 4.795944939057032e-05   ###   Correct predictions 1.0\n",
      "Epoch 190  ###   Avg-Loss 3.5239538798729576e-05   ###   Correct predictions 1.0\n",
      "Epoch 191  ###   Avg-Loss 4.360287372643749e-05   ###   Correct predictions 1.0\n",
      "Epoch 192  ###   Avg-Loss 4.576954136913021e-05   ###   Correct predictions 1.0\n",
      "Epoch 193  ###   Avg-Loss 3.716207187001904e-05   ###   Correct predictions 1.0\n",
      "Epoch 194  ###   Avg-Loss 4.478615010157227e-05   ###   Correct predictions 1.0\n",
      "Epoch 195  ###   Avg-Loss 4.2566253493229546e-05   ###   Correct predictions 1.0\n",
      "Epoch 196  ###   Avg-Loss 4.4678539658586185e-05   ###   Correct predictions 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197  ###   Avg-Loss 3.961453524728616e-05   ###   Correct predictions 1.0\n",
      "Epoch 198  ###   Avg-Loss 4.014573448027174e-05   ###   Correct predictions 1.0\n",
      "Epoch 199  ###   Avg-Loss 4.041891467447082e-05   ###   Correct predictions 1.0\n",
      "Epoch 200  ###   Avg-Loss 3.8891894898066916e-05   ###   Correct predictions 1.0\n",
      "Epoch 201  ###   Avg-Loss 3.913562589635452e-05   ###   Correct predictions 1.0\n",
      "Epoch 202  ###   Avg-Loss 4.7289044596254824e-05   ###   Correct predictions 1.0\n",
      "Epoch 203  ###   Avg-Loss 3.683992739145954e-05   ###   Correct predictions 1.0\n",
      "Epoch 204  ###   Avg-Loss 3.6930983575681846e-05   ###   Correct predictions 1.0\n",
      "Epoch 205  ###   Avg-Loss 3.837041634445389e-05   ###   Correct predictions 1.0\n",
      "Epoch 206  ###   Avg-Loss 3.6403711419552565e-05   ###   Correct predictions 1.0\n",
      "Epoch 207  ###   Avg-Loss 3.761501672367255e-05   ###   Correct predictions 1.0\n",
      "Epoch 208  ###   Avg-Loss 3.68371062601606e-05   ###   Correct predictions 1.0\n",
      "Epoch 209  ###   Avg-Loss 4.079292993992567e-05   ###   Correct predictions 1.0\n",
      "Epoch 210  ###   Avg-Loss 3.946068463847041e-05   ###   Correct predictions 1.0\n",
      "Epoch 211  ###   Avg-Loss 4.385570452238123e-05   ###   Correct predictions 1.0\n",
      "Epoch 212  ###   Avg-Loss 4.163978931804498e-05   ###   Correct predictions 1.0\n",
      "Epoch 213  ###   Avg-Loss 3.332675745089849e-05   ###   Correct predictions 1.0\n",
      "Epoch 214  ###   Avg-Loss 4.257323065151771e-05   ###   Correct predictions 1.0\n",
      "Epoch 215  ###   Avg-Loss 4.2936753015965226e-05   ###   Correct predictions 1.0\n",
      "Epoch 216  ###   Avg-Loss 3.636674955487251e-05   ###   Correct predictions 1.0\n",
      "Epoch 217  ###   Avg-Loss 3.94138313519458e-05   ###   Correct predictions 1.0\n",
      "Epoch 218  ###   Avg-Loss 3.3846831259628135e-05   ###   Correct predictions 1.0\n",
      "Epoch 219  ###   Avg-Loss 3.627579426392913e-05   ###   Correct predictions 1.0\n",
      "Epoch 220  ###   Avg-Loss 4.044672629485528e-05   ###   Correct predictions 1.0\n",
      "Epoch 221  ###   Avg-Loss 3.899487977226575e-05   ###   Correct predictions 1.0\n",
      "Epoch 222  ###   Avg-Loss 3.2388655624041956e-05   ###   Correct predictions 1.0\n",
      "Epoch 223  ###   Avg-Loss 3.148379425207774e-05   ###   Correct predictions 1.0\n",
      "Epoch 224  ###   Avg-Loss 4.479126849522193e-05   ###   Correct predictions 1.0\n",
      "Epoch 225  ###   Avg-Loss 3.618796666463216e-05   ###   Correct predictions 1.0\n",
      "Epoch 226  ###   Avg-Loss 3.777043893933296e-05   ###   Correct predictions 1.0\n",
      "Epoch 227  ###   Avg-Loss 3.232196516667803e-05   ###   Correct predictions 1.0\n",
      "Epoch 228  ###   Avg-Loss 3.010243138608833e-05   ###   Correct predictions 1.0\n",
      "Epoch 229  ###   Avg-Loss 3.525512292981148e-05   ###   Correct predictions 1.0\n",
      "Epoch 230  ###   Avg-Loss 3.3884646836668256e-05   ###   Correct predictions 1.0\n",
      "Epoch 231  ###   Avg-Loss 3.234480197230975e-05   ###   Correct predictions 1.0\n",
      "Epoch 232  ###   Avg-Loss 3.662161761894822e-05   ###   Correct predictions 1.0\n",
      "Epoch 233  ###   Avg-Loss 3.697317248831193e-05   ###   Correct predictions 1.0\n",
      "Epoch 234  ###   Avg-Loss 2.878650751275321e-05   ###   Correct predictions 1.0\n",
      "Epoch 235  ###   Avg-Loss 3.556391845146815e-05   ###   Correct predictions 1.0\n",
      "Epoch 236  ###   Avg-Loss 2.7765481111903984e-05   ###   Correct predictions 1.0\n",
      "Epoch 237  ###   Avg-Loss 3.6760074241707726e-05   ###   Correct predictions 1.0\n",
      "Epoch 238  ###   Avg-Loss 3.500080201774835e-05   ###   Correct predictions 1.0\n",
      "Epoch 239  ###   Avg-Loss 3.9084527331093946e-05   ###   Correct predictions 1.0\n",
      "Epoch 240  ###   Avg-Loss 3.469903022050858e-05   ###   Correct predictions 1.0\n",
      "Epoch 241  ###   Avg-Loss 3.173545701429248e-05   ###   Correct predictions 1.0\n",
      "Epoch 242  ###   Avg-Loss 3.360264624158541e-05   ###   Correct predictions 1.0\n",
      "Epoch 243  ###   Avg-Loss 3.289517092828949e-05   ###   Correct predictions 1.0\n",
      "Epoch 244  ###   Avg-Loss 2.8689632502694926e-05   ###   Correct predictions 1.0\n",
      "Epoch 245  ###   Avg-Loss 3.048427946244677e-05   ###   Correct predictions 1.0\n",
      "Epoch 246  ###   Avg-Loss 3.113332592571775e-05   ###   Correct predictions 1.0\n",
      "Epoch 247  ###   Avg-Loss 2.8724937389294308e-05   ###   Correct predictions 1.0\n",
      "Epoch 248  ###   Avg-Loss 3.149114587965111e-05   ###   Correct predictions 1.0\n",
      "Epoch 249  ###   Avg-Loss 3.239394476016362e-05   ###   Correct predictions 1.0\n",
      "Epoch 250  ###   Avg-Loss 2.6755715953186155e-05   ###   Correct predictions 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('lin1',nn.Linear(2, 16, bias=True)),\n",
    "    ('relu1',nn.ReLU()),\n",
    "    ('lin2',nn.Linear(16, 32, bias=True)),\n",
    "    ('relu2',nn.ReLU()),\n",
    "    ('lin3',nn.Linear(32, 64, bias=True)),\n",
    "    ('relu3',nn.ReLU()),\n",
    "    ('lin4',nn.Linear(64, 2, bias=True))\n",
    "])\n",
    ")\n",
    "\n",
    "output_frequency = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(250):\n",
    "    loss_sum = 0\n",
    "    correct_pred = 0\n",
    "    for index, (data, target) in enumerate(train_loader):\n",
    "        #print(index)\n",
    "        output = model.forward(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss_sum = loss_sum + loss.data\n",
    "        for i in range(len(output)):\n",
    "            _, ind = torch.max(output[i],0)\n",
    "            label = target[i]\n",
    "            \n",
    "            if ind.data == label.data:\n",
    "                correct_pred +=1\n",
    "        #if index % (10*(output_frequency)) == 0:\n",
    "        #    print(\"#  Epoch  #  Batch  #  Avg-Loss ###############\")\n",
    "        #if index % (output_frequency) == 0 and index > 0:\n",
    "        #    print(\"#  %d  #  %d  #  %f  #\" % (epoch+1, index, loss_sum/output_frequency))\n",
    "        #    loss_sum = 0\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch '+str(epoch+1)+'  ###   Avg-Loss '+str(loss_sum.item()/480)+'   ###   Correct predictions '+str(correct_pred/480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  ###   Avg-Loss 0.019352535406748455   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 2  ###   Avg-Loss 0.017910422881444295   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 3  ###   Avg-Loss 0.01750417153040568   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 4  ###   Avg-Loss 0.017391308148701986   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 5  ###   Avg-Loss 0.017361233631769817   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 6  ###   Avg-Loss 0.017345829804738363   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 7  ###   Avg-Loss 0.017331127325693765   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 8  ###   Avg-Loss 0.01733660101890564   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 9  ###   Avg-Loss 0.01734957695007324   ###   Correct predictions 0.4895833333333333\n",
      "Epoch 10  ###   Avg-Loss 0.01734344959259033   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 11  ###   Avg-Loss 0.017335693041483562   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 12  ###   Avg-Loss 0.01733403404553731   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 13  ###   Avg-Loss 0.017345335086186728   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 14  ###   Avg-Loss 0.017336952686309814   ###   Correct predictions 0.50625\n",
      "Epoch 15  ###   Avg-Loss 0.017330414056777953   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 16  ###   Avg-Loss 0.01733735203742981   ###   Correct predictions 0.49375\n",
      "Epoch 17  ###   Avg-Loss 0.017340999841690064   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 18  ###   Avg-Loss 0.01733502944310506   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 19  ###   Avg-Loss 0.017336159944534302   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 20  ###   Avg-Loss 0.01733076572418213   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 21  ###   Avg-Loss 0.017335595687230428   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 22  ###   Avg-Loss 0.017331228653589884   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 23  ###   Avg-Loss 0.017345523834228514   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 24  ###   Avg-Loss 0.017328786849975585   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 25  ###   Avg-Loss 0.017338613669077556   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 26  ###   Avg-Loss 0.017339388529459637   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 27  ###   Avg-Loss 0.017334789037704468   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 28  ###   Avg-Loss 0.017332659165064494   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 29  ###   Avg-Loss 0.017332019408543904   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 30  ###   Avg-Loss 0.017343803246816   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 31  ###   Avg-Loss 0.017337658007939658   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 32  ###   Avg-Loss 0.017327052354812623   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 33  ###   Avg-Loss 0.017350995540618898   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 34  ###   Avg-Loss 0.017341856161753336   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 35  ###   Avg-Loss 0.01733092466990153   ###   Correct predictions 0.49375\n",
      "Epoch 36  ###   Avg-Loss 0.017343801259994508   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 37  ###   Avg-Loss 0.017331286271413168   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 38  ###   Avg-Loss 0.0173449436823527   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 39  ###   Avg-Loss 0.017336622873942057   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 40  ###   Avg-Loss 0.01733430624008179   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 41  ###   Avg-Loss 0.01733665664990743   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 42  ###   Avg-Loss 0.01732741594314575   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 43  ###   Avg-Loss 0.017335259914398195   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 44  ###   Avg-Loss 0.017344200611114503   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 45  ###   Avg-Loss 0.017335255940755207   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 46  ###   Avg-Loss 0.01732981006304423   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 47  ###   Avg-Loss 0.017340336243311563   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 48  ###   Avg-Loss 0.017337270577748618   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 49  ###   Avg-Loss 0.01734388470649719   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 50  ###   Avg-Loss 0.017336700359980264   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 51  ###   Avg-Loss 0.017336754004160564   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 52  ###   Avg-Loss 0.017329061031341554   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 53  ###   Avg-Loss 0.017343024412790935   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 54  ###   Avg-Loss 0.017335404952367146   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 55  ###   Avg-Loss 0.017333406209945678   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 56  ###   Avg-Loss 0.01734092434247335   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 57  ###   Avg-Loss 0.017334075768788655   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 58  ###   Avg-Loss 0.017333996295928956   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 59  ###   Avg-Loss 0.01734001040458679   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 60  ###   Avg-Loss 0.017343183358510334   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 61  ###   Avg-Loss 0.017334266503651937   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 62  ###   Avg-Loss 0.017326943079630532   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 63  ###   Avg-Loss 0.017331435283025106   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 64  ###   Avg-Loss 0.01734351714452108   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 65  ###   Avg-Loss 0.01733464002609253   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 66  ###   Avg-Loss 0.017335947354634604   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 67  ###   Avg-Loss 0.017339940865834555   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 68  ###   Avg-Loss 0.017335885763168336   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 69  ###   Avg-Loss 0.017330139875411987   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 70  ###   Avg-Loss 0.017336839437484743   ###   Correct predictions 0.48125\n",
      "Epoch 71  ###   Avg-Loss 0.01733196576436361   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 72  ###   Avg-Loss 0.01733160416285197   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 73  ###   Avg-Loss 0.017341158787409463   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 74  ###   Avg-Loss 0.01734090844790141   ###   Correct predictions 0.4895833333333333\n",
      "Epoch 75  ###   Avg-Loss 0.01733473539352417   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 76  ###   Avg-Loss 0.01733032464981079   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 77  ###   Avg-Loss 0.01734740932782491   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 78  ###   Avg-Loss 0.017335404952367146   ###   Correct predictions 0.49375\n",
      "Epoch 79  ###   Avg-Loss 0.017347671588261924   ###   Correct predictions 0.4895833333333333\n",
      "Epoch 80  ###   Avg-Loss 0.017328637838363647   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 81  ###   Avg-Loss 0.017335180441538492   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 82  ###   Avg-Loss 0.017335520188013712   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 83  ###   Avg-Loss 0.01733883221944173   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 84  ###   Avg-Loss 0.017346135775248208   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 85  ###   Avg-Loss 0.0173315425713857   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 86  ###   Avg-Loss 0.017339418331782024   ###   Correct predictions 0.48125\n",
      "Epoch 87  ###   Avg-Loss 0.01734487811724345   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 88  ###   Avg-Loss 0.01733728249867757   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 89  ###   Avg-Loss 0.01733392079671224   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 90  ###   Avg-Loss 0.017338260014851888   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 91  ###   Avg-Loss 0.017348573605219523   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 92  ###   Avg-Loss 0.017336924870808918   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 93  ###   Avg-Loss 0.017339259386062622   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 94  ###   Avg-Loss 0.017346447706222533   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 95  ###   Avg-Loss 0.01733930706977844   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 96  ###   Avg-Loss 0.01734011769294739   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 97  ###   Avg-Loss 0.01733176310857137   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 98  ###   Avg-Loss 0.017335659265518187   ###   Correct predictions 0.48125\n",
      "Epoch 99  ###   Avg-Loss 0.017331586281458537   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 100  ###   Avg-Loss 0.017337832848230997   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 101  ###   Avg-Loss 0.017333618799845376   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 102  ###   Avg-Loss 0.017344723145167034   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 103  ###   Avg-Loss 0.0173422376314799   ###   Correct predictions 0.48541666666666666\n",
      "Epoch 104  ###   Avg-Loss 0.017340991894404092   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 105  ###   Avg-Loss 0.01734160582224528   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 106  ###   Avg-Loss 0.017327898740768434   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 107  ###   Avg-Loss 0.01732935905456543   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 108  ###   Avg-Loss 0.017331963777542113   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 109  ###   Avg-Loss 0.01732992132504781   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 110  ###   Avg-Loss 0.017332820097605388   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 111  ###   Avg-Loss 0.017328357696533202   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 112  ###   Avg-Loss 0.017340288559595744   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 113  ###   Avg-Loss 0.017335466543833413   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 114  ###   Avg-Loss 0.017333012819290162   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 115  ###   Avg-Loss 0.017334548632303874   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 116  ###   Avg-Loss 0.017332889636357627   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 117  ###   Avg-Loss 0.017335166533788044   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 118  ###   Avg-Loss 0.01733022133509318   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 119  ###   Avg-Loss 0.017332543929417927   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 120  ###   Avg-Loss 0.017333388328552246   ###   Correct predictions 0.48541666666666666\n",
      "Epoch 121  ###   Avg-Loss 0.017331175009409585   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 122  ###   Avg-Loss 0.017329037189483643   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 123  ###   Avg-Loss 0.017340620358784992   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 124  ###   Avg-Loss 0.01733303666114807   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 125  ###   Avg-Loss 0.0173516849676768   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 126  ###   Avg-Loss 0.0173428475856781   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 127  ###   Avg-Loss 0.017335009574890137   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 128  ###   Avg-Loss 0.017333370447158814   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 129  ###   Avg-Loss 0.017340290546417236   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 130  ###   Avg-Loss 0.01732499400774638   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 131  ###   Avg-Loss 0.01733763813972473   ###   Correct predictions 0.48541666666666666\n",
      "Epoch 132  ###   Avg-Loss 0.017332053184509276   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 133  ###   Avg-Loss 0.017353246609369915   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 134  ###   Avg-Loss 0.017330745855967205   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 135  ###   Avg-Loss 0.01732763648033142   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 136  ###   Avg-Loss 0.017330714066823325   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 137  ###   Avg-Loss 0.017336773872375488   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 138  ###   Avg-Loss 0.017343024412790935   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 139  ###   Avg-Loss 0.01733009417851766   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 140  ###   Avg-Loss 0.017354045311609903   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 141  ###   Avg-Loss 0.017332154512405395   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 142  ###   Avg-Loss 0.017334461212158203   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 143  ###   Avg-Loss 0.017343628406524658   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 144  ###   Avg-Loss 0.017340620358784992   ###   Correct predictions 0.46875\n",
      "Epoch 145  ###   Avg-Loss 0.01733053723971049   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 146  ###   Avg-Loss 0.017334272464116413   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 147  ###   Avg-Loss 0.017340207099914552   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 148  ###   Avg-Loss 0.01733244260152181   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 149  ###   Avg-Loss 0.017334314187367757   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 150  ###   Avg-Loss 0.0173399825890859   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 151  ###   Avg-Loss 0.017332218090693154   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 152  ###   Avg-Loss 0.017343036333719888   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 153  ###   Avg-Loss 0.01734464367230733   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 154  ###   Avg-Loss 0.017339040835698444   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 155  ###   Avg-Loss 0.01733132799466451   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 156  ###   Avg-Loss 0.0173313041528066   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 157  ###   Avg-Loss 0.017345698674519856   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 158  ###   Avg-Loss 0.01734340190887451   ###   Correct predictions 0.47708333333333336\n",
      "Epoch 159  ###   Avg-Loss 0.017333271106084187   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 160  ###   Avg-Loss 0.01733576456705729   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 161  ###   Avg-Loss 0.01733560562133789   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 162  ###   Avg-Loss 0.017329166332880657   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 163  ###   Avg-Loss 0.017341987291971842   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 164  ###   Avg-Loss 0.017338371276855467   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 165  ###   Avg-Loss 0.017340058088302614   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 166  ###   Avg-Loss 0.017344150940577188   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 167  ###   Avg-Loss 0.017344385385513306   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 168  ###   Avg-Loss 0.017346356312433878   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 169  ###   Avg-Loss 0.017333459854125977   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 170  ###   Avg-Loss 0.01733231743176778   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 171  ###   Avg-Loss 0.01732916235923767   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 172  ###   Avg-Loss 0.017331536610921225   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 173  ###   Avg-Loss 0.01733509699503581   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 174  ###   Avg-Loss 0.01732465426127116   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 175  ###   Avg-Loss 0.017342160145441692   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 176  ###   Avg-Loss 0.017330139875411987   ###   Correct predictions 0.5104166666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177  ###   Avg-Loss 0.017332039276758828   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 178  ###   Avg-Loss 0.01733199159304301   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 179  ###   Avg-Loss 0.017338985204696657   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 180  ###   Avg-Loss 0.017342575391133628   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 181  ###   Avg-Loss 0.017335110902786256   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 182  ###   Avg-Loss 0.017331095536549886   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 183  ###   Avg-Loss 0.017341347535451253   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 184  ###   Avg-Loss 0.017332384983698528   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 185  ###   Avg-Loss 0.017330108086268108   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 186  ###   Avg-Loss 0.017338589827219645   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 187  ###   Avg-Loss 0.017340189218521117   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 188  ###   Avg-Loss 0.017331349849700927   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 189  ###   Avg-Loss 0.017359145482381187   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 190  ###   Avg-Loss 0.017331101497014365   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 191  ###   Avg-Loss 0.017330982287724814   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 192  ###   Avg-Loss 0.017326368888219198   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 193  ###   Avg-Loss 0.017343854904174803   ###   Correct predictions 0.47708333333333336\n",
      "Epoch 194  ###   Avg-Loss 0.017337441444396973   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 195  ###   Avg-Loss 0.017333298921585083   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 196  ###   Avg-Loss 0.017332341273625693   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 197  ###   Avg-Loss 0.017343161503473918   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 198  ###   Avg-Loss 0.017330855131149292   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 199  ###   Avg-Loss 0.01732876698176066   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 200  ###   Avg-Loss 0.017356745402018228   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 201  ###   Avg-Loss 0.017346465587615968   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 202  ###   Avg-Loss 0.017329835891723634   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 203  ###   Avg-Loss 0.01734454035758972   ###   Correct predictions 0.4895833333333333\n",
      "Epoch 204  ###   Avg-Loss 0.01733029286066691   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 205  ###   Avg-Loss 0.01733484069506327   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 206  ###   Avg-Loss 0.017335607608159383   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 207  ###   Avg-Loss 0.017337212959925335   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 208  ###   Avg-Loss 0.017347023884455363   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 209  ###   Avg-Loss 0.017332309484481813   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 210  ###   Avg-Loss 0.01733231743176778   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 211  ###   Avg-Loss 0.01734496553738912   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 212  ###   Avg-Loss 0.0173362930615743   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 213  ###   Avg-Loss 0.017337344090143838   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 214  ###   Avg-Loss 0.017353427410125733   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 215  ###   Avg-Loss 0.017331113417943318   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 216  ###   Avg-Loss 0.01733746329943339   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 217  ###   Avg-Loss 0.017332746585210165   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 218  ###   Avg-Loss 0.01733715335528056   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 219  ###   Avg-Loss 0.017337143421173096   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 220  ###   Avg-Loss 0.0173479954401652   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 221  ###   Avg-Loss 0.01733072797457377   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 222  ###   Avg-Loss 0.01734137535095215   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 223  ###   Avg-Loss 0.017333173751831056   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 224  ###   Avg-Loss 0.017326873540878297   ###   Correct predictions 0.50625\n",
      "Epoch 225  ###   Avg-Loss 0.01732667684555054   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 226  ###   Avg-Loss 0.017343149582544962   ###   Correct predictions 0.47708333333333336\n",
      "Epoch 227  ###   Avg-Loss 0.017348565657933555   ###   Correct predictions 0.49375\n",
      "Epoch 228  ###   Avg-Loss 0.017334616184234618   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 229  ###   Avg-Loss 0.017330666383107502   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 230  ###   Avg-Loss 0.017340850830078126   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 231  ###   Avg-Loss 0.01733234922091166   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 232  ###   Avg-Loss 0.01734009782473246   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 233  ###   Avg-Loss 0.017341488599777223   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 234  ###   Avg-Loss 0.017334944009780882   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 235  ###   Avg-Loss 0.017334401607513428   ###   Correct predictions 0.50625\n",
      "Epoch 236  ###   Avg-Loss 0.01733740170796712   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 237  ###   Avg-Loss 0.017334469159444175   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 238  ###   Avg-Loss 0.01733429233233134   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 239  ###   Avg-Loss 0.017332722743352253   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 240  ###   Avg-Loss 0.01733097235361735   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 241  ###   Avg-Loss 0.017334298292795817   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 242  ###   Avg-Loss 0.017329289515813192   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 243  ###   Avg-Loss 0.01733185847600301   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 244  ###   Avg-Loss 0.017335520188013712   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 245  ###   Avg-Loss 0.017344415187835693   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 246  ###   Avg-Loss 0.017336014906565347   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 247  ###   Avg-Loss 0.017329887549082438   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 248  ###   Avg-Loss 0.017341379324595133   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 249  ###   Avg-Loss 0.01733956535657247   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 250  ###   Avg-Loss 0.01733067234357198   ###   Correct predictions 0.5104166666666666\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "from Dataset.Dataset import makeMoonsDataset\n",
    "\n",
    "dataset_size = 600\n",
    "batch_size = 40\n",
    "train_size = dataset_size*0.8\n",
    "test_size = dataset_size*0.2\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset(dataset_size,batch_size)\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('lin1',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu1',nn.ReLU()),\n",
    "    ('lin2',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu2',nn.ReLU()),\n",
    "    ('lin3',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu3',nn.ReLU()),\n",
    "    ('lin4',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu4',nn.ReLU()),\n",
    "    ('lin5',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu5',nn.ReLU()),\n",
    "    ('lin6',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu6',nn.ReLU()),\n",
    "    ('lin7',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu7',nn.ReLU()),\n",
    "    ('lin8',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu8',nn.ReLU()),\n",
    "    ('lin9',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu9',nn.ReLU()),\n",
    "    ('lin10',nn.Linear(2, 2, bias=True))\n",
    "])\n",
    ")\n",
    "\n",
    "output_frequency = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(250):\n",
    "    loss_sum = 0\n",
    "    correct_pred = 0\n",
    "    for index, (data, target) in enumerate(train_loader):\n",
    "        #print(index)\n",
    "        output = model.forward(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss_sum = loss_sum + loss.data\n",
    "        for i in range(len(output)):\n",
    "            _, ind = torch.max(output[i],0)\n",
    "            label = target[i]\n",
    "            \n",
    "            if ind.data == label.data:\n",
    "                correct_pred +=1\n",
    "        #if index % (10*(output_frequency)) == 0:\n",
    "        #    print(\"#  Epoch  #  Batch  #  Avg-Loss ###############\")\n",
    "        #if index % (output_frequency) == 0 and index > 0:\n",
    "        #    print(\"#  %d  #  %d  #  %f  #\" % (epoch+1, index, loss_sum/output_frequency))\n",
    "        #    loss_sum = 0\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch '+str(epoch+1)+'  ###   Avg-Loss '+str(loss_sum.item()/train_size)+'   ###   Correct predictions '+str(correct_pred/train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7115,  1.4547]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.6434, -2.7388]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3886,  2.0311]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.9741, -3.1041]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-0.8089,  0.1275]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3526, -2.2834]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.8463, -2.8233]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.9055, -2.8244]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-1.7085,  1.4306]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.7206,  2.3536]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 0.7751, -1.6380]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.4437,  2.1093]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.1609,  1.7324]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-0.5344,  0.0122]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3842,  2.1159]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-1.5523,  1.2432]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.5676, -2.6339]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 2.0958, -3.2221]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 0.9023, -1.6219]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.7551,  2.3714]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3445, -2.3139]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.5533,  2.1857]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3841, -2.4257]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3780,  2.0977]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 0.0020, -0.6190]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 0.1119, -0.9363]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3832, -2.3543]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.5189, -2.5425]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-1.9774,  1.4456]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.2206, -2.2302]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-0.1666, -0.6191]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.3856,  2.0124]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.4732, -2.4705]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.4295,  2.1136]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-0.6206,  0.0895]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.7165, -2.7979]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.8907, -2.8468]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.8076, -2.8020]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3121,  1.9327]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.0975,  1.7790]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "for index, (data, target) in enumerate(test_loader):\n",
    "    print(model.forward(data))\n",
    "    print(target)\n",
    "    print('####################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 1.0\n"
     ]
    }
   ],
   "source": [
    "net.test(test_loader,120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
      "        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1])\n",
      "tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,\n",
      "        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1])\n",
      "tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n",
      "        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1])\n",
      "tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1])\n",
      "tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1])\n",
      "tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n",
      "        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0])\n",
      "tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,\n",
      "        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1])\n",
      "tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
      "        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,\n",
      "        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0])\n",
      "tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
      "        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0])\n",
      "tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0])\n",
      "tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1])\n",
      "[0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEKCAYAAADuEgmxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJztnX+QHMV59799d/tLtycM6PwacxInXlyOsCvvKySRkLj8C9k41GvAsqF8VBLLuhRQ8dkKqaRyBiep4oCQyBVZQF6fnAhOqXp1EByC7ffFPr8HpFIkZR/6YRv79BKDc6ATlO8gtoyw7ofu+v2jt9nZnu6ent2ZnZnd51M1tbuzs7O9vTP99POjn4dxzkEQBEEQLnQk3QCCIAgiO5DQIAiCIJwhoUEQBEE4Q0KDIAiCcIaEBkEQBOEMCQ2CIAjCGRIaBEEQhDMkNAiCIAhnSGgQBEEQznQl3YCoWbduHe/v70+6GQRBEJniyJEjr3LOe4OOazmh0d/fj8OHDyfdDIIgiEzBGHvR5TgyTxEEQRDOkNAgCIIgnCGhQRAEQTjTcj4NgiCIpFheXsbs7CwWFhaSboqRYrGIvr4+5HK5uj5PQoMgCCIiZmdn0dPTg/7+fjDGkm6OD845XnvtNczOzmLjxo11nYPMUwRBEBGxsLCA888/P5UCAwAYYzj//PMb0oRIaBDZZH4eeOYZ8UgQKSKtAkPSaPtIaBDZY3wcuOgi4EMfEo/j40m3iCDaBhIaRLaYnwcGB4EzZ4BTp8Tj4CBpHARR4Vvf+hbe+c534pJLLsE999wT+flJaBDZYmYGyOdr9+VyYj9BtDkrKyv4zGc+g29+85uYnp7G+Pg4pqenI/0OEhpEtujvB5aWavctL4v9BJFFIvTPTU1N4ZJLLsHFF1+MfD6PT37yk/ja174WQSOrkNAgskVvL3DgAFAqAWvXiscDB8R+Gy43ZpiblxzxRBRE7J87efIk1q9f/+brvr4+nDx5stFW1kBCg2guUQy2AwPAiy8Ck5PicWDAfrzLjRnm5iVHPBEFMfjnOOe+fVFHc5HQIJpHlINtby+wbZubhhF0Y4a5eckRT0RFDP65vr4+nDhx4s3Xs7OzePvb3173+XSQ0CCaQ9BgG5e5x+XGDHPzkiOeiIoY/HPbtm3Dj3/8Y/zHf/wHlpaW8NBDD+Gaa65pqJkqJDSI5mAbbOM097jcmGFuXnLEE1FRr3/OQldXF+6//35cddVV2LRpE2644Qa8613virDREDawVtq2bNnCiRQyN8d5qcQ5UN1KJc6np/X75+ai++5Dh8Q5164Vj4cO1XdMPccSbcX09HT4D83NcT41Fe01H4CunQAOc4cxlhIWEs1BzqoGB4WGsbwsXp8+LTSQM2eqx0oNpIEZVw0DA8D27eKc/f3687ocU8+xBBFEb2+mriESGkTz0A228/PNMfe43Jhhbt6M3egEERXk0yCaixr1FNauG4XDnNZYEETdkNAg4idokHZddxGFwzzMOUi4EIQPEhpEvLgO0kHrLqJYHxHmHLSAjyC0kNAg4iPKhXBRrI9wPQct4CMIIyQ0iPiIciFcFOsjXM8xMwN0KTEitICPyAi7du3CW9/6Vrz73e+O5fwkNIj4iHIhXBQLoVzPcfQo8Prrwe0mnweRQnbu3IlvfetbsZ2fhAYRH1GveHV1mNsG86BzzM8Dt97q/9wdd9S2m3weREREPfd473vfi/POOy+ak2kgoUHEy8AAcOQIcO+94jEoI20QQQ7z/fuB9euBK680D+a2c+hMagDwhS9Uz0U+DyIisjj3IKFBxMv4OLBlC7B7t3iM867Yvx+45RZgcVGYl+oZzHUmNUCcU56LkhYSEZDVuQcJDSI+XDLbfvvbYmv0TpmfF4JJpbMz3GAuTWqFgv89KRgoaSERAVmde5DQIOIjKLPthRcCV10ltr6+xrQQk1mpnsF8YAA4dky01cvCgjjX5CRw9mx1fz7fcHZSov3I6twjUaHBGHuAMTbHGPuh4X3GGLuXMfY8Y+wHjLHLmt1GogF0d8Xioti3a5e4QyRLS43p5uWy3qy0b59/MHfxPK5b59/HOfDqq6Kd3rZ3dIicWgQRghgyowMABgYGcMUVV+C5555DX18fDhw4EE2DKyStaYwB+Ijl/d8C8I7KdhOALzehTUSU3HabuBtKJfG6o0M4qTVlKdHRUZ9uLv0mHZXLuVgU5qXhYWDHDv+xLp7HmRlgzZrafaUSMDXl12jy+dp2Uygu4UjYysUujI+P45VXXsHy8jJmZ2cxODjY+Ek9JCo0OOf/AuA/LYdcC+DvK+nevwPgLYyxC5rTOsIZ3SApB+cvfhFYXa3OzM+cEdrG4qL/PKurVd3cdeD1+k1kevXlZYAx4MtfrhUMYTyPJtvBJZcIM5W6X7Y7i+EwRKK4Vi5OC0lrGkFcCOCE5/VsZR+RFsbHgQ0bgA98QDyOj/sH58XFWh+ADq9fYP9+4eN4//ur5zSh82WsrIiB3SsYjh8HHn/cfaW3znYwOCiEgVej8doUdEJp167oNQ5VoJJmQzQTl0pNcW4A+gH80PDe/wHwHs/rJwBs0Rx3E4DDAA5v2LCh7mpWREjm5jjP5Wqr7uVynN97L+c9PbX7TVs+z/nDD1erlo2O+o/J5cxVzXQVAdWtVOK8UNC3KahKoKyqpqswWCiI/ZKpKc7POcf/HSMj0fW5rBp4zjnicWio9jVVEUyU6elpvrq6mnQzrKyurjZUuS/tmsYsgPWe130AXlYP4px/hXO+lXO+tTcrOl4rcOxYrUMYEK//5E/8aTi6uvRhrMUisHFjdab+uc/5j1leFt+lQ9UIikW/5iFNYmqbAKEZ2K4ZaTuQFQa9FArAiRPVWX5/v97sdvfd0WgBx48Dn/50rSZz//3ZC/RvYYrFIl577TU5mU0dnHO89tprKBaLdZ8j7ZX7vg5giDH2EIBfA3CKc/5Kwm3KHnJBWrNKk3pLt0o6OoA77xQrq70Dq9cfIBMF6qKgvKi/R60IODkpBk/O/f4HlQMHgD/7s+B+0fk4FhaAa68VwmNpSZzr9tuBP/3T2uOiKF87Pi4Ehk4oRf1dRN309fVhdnYW8ykW3MViEX19ffWfwEUdiWsDMA7gFQDLEFrFIIBbANxSeZ8B+BsALwB4FsDWoHNu2bKlEc2t9VDNGY2aL6S5Zm6O8z173ExQXnPOnj2iHWvX+tszN8d5sWg3T7n+nulp8X1BbVq7VvyeMH0p266a5kolvRnLawLz9l+YPg8ywbma2wjCABzNU4kKjTg2EhoedINNI4OKOmB3doYTGlJwjI6aB85Dh2oH41yuKhjC/J6pKTe/Stj+kIP+xITffyEFkCpcZPvl/u5uIRxHRty+2+QrKRSqPg2dECaIEJDQIPSDTZiZtZcws91yWQyK+bz+/WIx2Pk8MSE273Fhfo/OoW7SdHSz/yCNQKfJ2DSKuTl9fxSL/oFe91mbE97UXwQRAlehkXZHONEIUeYpMKXpUNmzB3jySeCll4CxMb3ze2FBhNWa6O0FNm8Gzj23dr/r7zGlN9+3D/ijP6pdTQX411UErbVQFxPKxYu25bzHjul9NQsLtc7r8XGRpfe97xWP4+P68N8HHwQ2bRKfmZwErrsOuOEGWhtCxI+LZMnSRpqGwqFDtTNcr7knDLrZbj4vZso9PVWzk8r0tH6GbTML2fwWJtOPF51G0tPj10hM5i7Vr6JqEEGht7r2T0yYNTOpLc3Ncd7RUfteR4fdHxK1CZJoW0CaBgFARBV1eP7m5eX6wjJ1s92xMaFRPPGECD29+ebaz8zPi1DVP/xD//lMi+qCVm2reRe2b/cvbNNpJGfPiv3ehXA67amjQ2TGNbVV1+aODvE7be1fv96fAFEitaWnnhKr4r2sror9gH7pcFZTpRKZhYRGqzMz4zcR1TuoyAH7kUeAxx4TA7YpB4LXxPOlL/kHTJOZzGUQlN85Oak3I5kywanHHz3qFy6rq2JFuamt5bI/pPjMGbHf1v7Tp4GDB6t5sQD/ivKf/tTfH4B5P2A32dFKcSIOXNSRLG1knlLQOWDz+egiqHTmIZ3JJJdzi/JxNbe4HOc155iOHx31t8tmApua8p+nWKyavoLaJdv09NOcj43VmrVMIcya1bs1v3FkRHxHuVw1E0Ydak20PKDoKYJzrk/10dlZOxC5rh1wHdBNUU4TE/bvke3QDeQqYSPDbMeHiZ7S9YEaDRbkd9EN6KbotF279L9HPU9Xl/ife3r0kWvk5yACIKFBCGwx/t5ZtcuMVLf2QTdQh3HOqoJCtsO2liPsd9RzvA11LUk+7w/dDSN0SiUhUHWBBiYhGCYEOkigEgQnoUFIbINLUKSQim7tg+n40VEhmMplszCSAqueRILez7subLMdH2alts38phO+3nObhPjnPqf/j55+Wt8G03lMG2kaRAAkNIgqhw7pU2p0d4vNZUZqEj66MFuvMDCF4gbNlF1nxmHTcuiOD2v/dxmw5SCtnltqVOrxuZw+hYrUCMP2n9oW8mkQAZDQaHXCDpY33qgfTGyrmr3oBspyuX7TVNDAa3JqT0+Hz91kox6zlWs6dp3JqVTifHhY/xnV9+T9jO53ezWnXE74NXQTg4mJaPqKaGlchQaF3GaRsNXhjh8HDh3y7//Yx8TQIsnlzKuadWm/V1b8YbOu6wZ0oaIA0NPjX10tf+/73gdceql4jGrlcz3rHHTp2FXOnAF+/nP9uT/wAf1KeUC/n3OxQl79v71rVk6eBH7wA//nV1fFZwkiKlwkS5a2ltc06pkZj43pZ7BhImxcV5aHaZ/qY9A5v4N8Mo1qHI04yL3JC3URVRMT+iJVMkxWZ5J7+OHg7Ly2HFdh/TwEUQGkabQoLjNjdVHX5Zfrz+U6w5arnL2aQVeXWNynYlpYp2ov8/Oi3vaRI9XV3Tff7LbiOai9YXBtr+mz27bpZ/KMiVXgjPn3A+K3lkq17y0vCy3kwQer7SkU/MfJ363TONUV8wMDzl1BEE64SJYsbW2vaZicukNDtZ8ZHHSfYdeTLdfmcwnjeI5b03Bprwu6GX5Qv7lEc5nqcwTV7SCIkIAc4Rki7IBlGmyCBMr0dO0qZFdTRr0mHN3vCkopbvu9MrqoVEqn6cUlpblt1bqJegQSQYSEhEZWqDfdg26wmZhwD6GV53Ctw+Difwj6XabQX5fBLq7oqbiJyseg/t864atm2yWIEJDQyAJRr1LWxfkHLb7r6XEfzEyrt3VFhHSO4WaYmdJIo6YvFSmIZH/KUNu0amBEJiChkQV0ye9KpfC+ApPd3zSg69YJyHUFUeSf0plOdAsJ5eyYBjl3XNeItLIQJmLBVWhQ9FSSBKXZVjGtz9BFGHV3i/Tl3uiZ8XFgwwbgnnv85z5zBtixI3j9g0v0lm4NxsqKv1ZEoSAq2iUd4RN3CvGw57cd71JBsaND9GsUUHp1QsVFsmRpy7ym4U2z7cU2ww+a/Uvfhc58FXam6mpS09ny07iGQJr1urv19bqjOH8Yn1XQ8a7pQ6LoX0qv3laAzFMZIIxPw1TCdGysNseROiDL/TrTkGkLckyHibpyTTkestsicRHMzZkX30VBXJl41f4fGgrnz4qj7cSbRO3CahYkNLKCHADKZbHienhYf7WZZpheR7ZLyKe65fP+nEX1htM2gUgnv6a63VHlaoqy5oeKt//n5ji/9159LfaREf/xcbSd4JxnWzkjoZEWXG7W0dHaGa8pRUfYVOKmpIDSFDMyYtdSUkbkk984hYY0CcahaXgJ0iSLxeBot6ja0uZkvctIaKQBl7BWkzagVoPzHj821lgxJF2UVAZ06sgnv3Nz0ZbClXinm7mcOGccNT9cNMk1a8IvpnRpC+Ej68oZCY2kcS1YNDWlnyV2d4er2mYaCDJ445vkVywzOe9MPYr+Ma1RcQln9p5DXcios3u41PWQk5Z6R7IMTCbSAmkaGd1SITTm5vQrn13rT9g0DUkYYaCboaZ0IAiyCcciA6Psj6imm2pHqA77UklU9bNlxM3n9UWfsjSSZYwMztHexFVoMHFs67B161Z++PDhZBvxzDPAlVcCr79eu79QAE6c8GdQHR8Hdu6srm3I5YCDB4PXL8zPi3j8n/8ceMtbRLbVoOys4+MiY20+L77vwIHk10lUmJ8Xy0S8S1dKJZGs1fuz5ufFcoX+frdktE3F9UeEPYdKsShEQEeHOK5YFBl09+4FNm6svSYmJ8V/3tUl/vN9+0SWXSIWUn19WmCMHeGcbw080EWyZGlLjaahq8KmK3vq/YwuD5TNVjM8zHlHR+3MMmzG2BTNOuudpKdOcWp0uhm2/rc0Q6mJKL3qWj1pY4iGSd21aQFknkoQnYPVFP9vu6pMtppDh+ylQU1XaMo9dY0ED6UuxLGR0ULXEfm8MFmaFmjK/9HkU1E/l6LJQquS2mvTAAmNJHEdnG1XlWkE1dVRUAeXKBzoCRHWVZPyn1M/uo7QZbZVf7hJS1E/10g9FCKQLF6brkKDck9FiczTUy77cy8tL9fW05bV8M6cAU6dEo+Dg9UcP6YcT1NTwo5tYmnJnLuqkSp1TSJM4bl6yntnBtkRjzwicoht3w6cPq2vR14oVP9HU+11tb770hLws5/pc0p5c5xt2ADceSflngpJS1+bLpIlS1timoaqNQwN2afMJm1kYiK4Ypsth1SplEEnQH3YlLEW+Hn+a0oXCaWroaGrP14simPXrq2uHXHVcGXHpt2+kiJaWdNIfJCPektEaNQzeuk+k8u5CR7doJCVKzNidGmYsmRHNmK6pqTgsNnv5ub0Pozp6eBV6jYnfJtdW42StfBbEhrNpF4Hs/eqKhb9znOT4NENCm08G/Suhcva7M6I7Zpy0RRNI1bQtWpbZZ6ioIms0AylPqrvIKHRTBrRReU/bpoBmm5Sbx4qW6LDNiLlwWHhiMK+odaEdz1v2CqQRGJEGaHlKjQSdYQzxj7CGHuOMfY8Y2xY8/5Oxtg8Y+x7le33kmhnIFE4mLu7wxVkGhgQC7mWloQjdN8+4T1uY3Q+YDX+IDM0ek2NjwNbtgC7d4tHWVjL5bwDA8BLLwEjI6kOmmh3gmJpYsNFssSxAegE8AKAiwHkAXwfwKXKMTsB3B/mvImG3IbVE73ThELBv/aiUKjWy9B9V6M5jlJOPWp31uzIgdTTCS7ahGtqmRYJmkgrjXRv1Jo10m6eAnAFgAnP688D+LxyTLaERhhcMpQC5hW8trTnLTBaNqJ2t/04F3Y00XV223di/DRqWoo6QisLQuMTAP7O8/p3VAFRERqvAPgBgK8CWB903swIDd2NLcMiy2W/MNDNFG1CJ8P25yyGK6YKXUYCU8p3lyi+jE9A0khU13iUmrWr0EjSp8E0+7jy+hsA+jnnvwpgEsBB7YkYu4kxdpgxdng+K4uQdAZ4xkQCwvvvB3p6at9TVwZ5bdPd3f7zp2glkVzz6PrXtPTCqGbBuf21RNfZy8sJGMrbi6iu8TCLYaMiSaExC2C953UfgJe9B3DOX+Ocy6Wsfwtgi+5EnPOvcM63cs639mbFUWdySG7aBFx+uZtHV14xjz4qPh90fAJ4FxdfdFHVH2sjbod2WCGWOWZmgDVraveVSvoRybSC3AtJ7Mip5xo3Xbe9vcC2bU2MUXBRR+LYAHQB+AmAjag6wt+lHHOB5/nHAHwn6LyZMU9JVNux1Del7iqfN1LxLSEaUcHD/hxXE3zWksjV4Pojw3a8y3ohsg1GippzVJeg2vt3N+O6Rdp9GqKNuBrAv0NEUd1e2XcHgGsqz/8CwI8qAuUpAL8SdM7MCQ0vuptdlybC9vkUOS8bje6IWhBk2lcSdtSoV+pOT4uMAymbgGQZXaCaLvBRXSrjXYalq8EV9XWbCaERx5ZpodFSq9OaM0iH+Y7Mdm+9HdlICHixKIRHJiRqOtB1t2uVXtdF+XFet65Cg7Lcxk0YA3pLrU5rTlJdnUOxo0PEE6hktnvr9ZqGMXarK8UWFoC77663xW2HzndnWnwXlAR7ZkYUWbSR5HVLQiNKVAER1gucgdTlYYk7ukMnCN54A7juOn93Z7Z7myHtKGStbkzC4dgxfZeePm2/Dm2xCd3dKbhuXdSRLG2pSY2uS2MdNh8VmQacOHSI864uvwpv6u5Mdm/Y6lRxrCIP+nzmOtUd28+zVTmwdantnKOj+us5zoQPIJ9GEzE5sHt6/FeS13AZ103W4jewytycvqBdT08G/BVhCJPdtp4wm3oj8DIdkhZM0M+zyVtblwb9nc0u605Co5nopho9Pf6RTL2S5DGjo9G1pcVvYB1TUyJ7iio0CoW2kZuCKCIPvCOZi5DKdEhaMK4/L6xwCBPx16z5HwmNZjE3Z9ZDdQVzTKERUQiOFr+BTehU+ai6NFPoJi/d3eL6tNHIqJbZkDQ3wvy8uJbRNAsSGlETdGPJEprqVEP93NSU32zlOi0Ouipb/AbWYZLBe/Yk3bIEMHWGLXzWlKzQdVRL6wgYEVH9PO+tm9bblIRGlLjeWDI1eVCZV50Bvly2XzUuM78Wv4F16G7AoK5saUwFlHTCw3S9TEzUlyW3RRcDNvrz5OdlAuqwMTLNMlGR0IiKsDeWXE1rGtxHRznv7PTf0EFXjetV1uI3sEobyslgJib0Th6v8LCtMgsK+9HR4sEXQT/P9P7cnD7hsMlyXa+VMApIaERF2BtLneV5bzaT8T3oagirz2bwBm6kyW0mJ4OZntZrs+o1ZyuqTp3qjG1gn5jQd78MnbXllmr2hIiERlSEiacbGTEP7nNzZg3DxVHZwtPpemdTukAfm2VQ97mWQ014adrkdWmL62zpjoqGoFvTJjSCzhHWStgoJDSixDWeznYFma4eU3GcMG3IMPXKQ52gcRE+LR2RrOvMzk691uE1rscR+t0muOSRUpMNArVdHaWVsBFIaESN66xLN7jPzXF+7716oTE8HH0bMkQ9kSS6sTGf91sG1ZLpLa6wmUsADw8LLbhYrF6XjWQsIN7E5ZrSWaW9yavrXRwYNSQ06iGqQVlnrNSF2eZybvaUFqaegdw0Nuo2b8n0tIY6RoYp5Lary79YL+wChImJeHNYZJggQ8TYmL6Cc6FQPVZaCcvl5KyEJDTCEuUSTa+BXXcTr1kj9g8NtbCtxJ16Sj/oVH7bFuT3bRmGh/UdoPrNXKX1oUO14T+5XNtepzZskU+6+aJXcOzZkw4rIQmNMIS5gXSDvCkMolDwn7enR0w92mIEcyfMbEoXxhi0yUl0i7qGqrh4XiW26S3n9sWCpHVYMXWd6wQnia51FRoBWdvbBJkW+syZ6j6ZFlrmH/bmP5bHDQ4Cv/gFcOut4vOLi8Dqqshr7D2Xl7NngauvdvvONqK31/1nz8yI9NBBpa29yEzi27YB27eLc/T3t2BXb94sritv5+TzYr+X8fHqdbu0BOzb589bPzMjipOoLCwAO3aIa/3Agejz3bcAupoY6t9iIu3DANXTANzqFejqDXR2Art31xauUc9TKgGFAtDTIx737hVXQ2YrAkVDmNpUKrZ6A16KRX29Atn9MzP1fX+q6e0FxsbEj5bFF8bGakcg7wTo9dfFZOfWW/2d0d8vBIOON96oFo5ouU5snKNHRdd6cZ3kpH4YcFFHsrQ17NMw2S10+qYu/blO19yzRx8L3/K2Ej1RhL2qXfdrv1bb7bt2CQugtARG/f2px2bvC+MEV30aJrtfC0b21YvJNBW03tJkJWwWIJ9GHQRd+OpIpQtbzOWCj/EaLdvsZosyAZwa0LNnj+j+7m4xzuVyevdT27uSdJEEuZw9R8bEBOcPP6zPeCCv8ZaWwm7IaCl1Llkum4M3pPM76QA1EhpxoQ7ypnUZaU9pmRBRdIdrygWdYKC/g5sTItWzyJTWe7yJLVqqWBSTGp3AmJ5Oh/YbmdAAMATgXJeTpWFLrHKfLast3VRv0mh3mD5vijTVWVHa/u9oVHLSpMhH0KQlnxeCQBewlpZr0lVouDjC3wbgGcbYPzDGPsIYY7E5WLJKb68Iy/FGWkkvb2+vvYp8m9Fod+jiEbq6gL/+a/vnpHOR/g40HoThvd7bPKBDorsuvSwtiZiBHTuAEyeAJ58EXnxRBJ7pIq1kBFUqcZEsABiAqwA8BOB5AHcD+K8un2321lRNw7aix7aWg6i7O0zxCLZM4Ka4hrb+O1zyqblmf5TlANosoMOLy7oMkwKmSzOSZk3DeTAG8N8AfAnA/wPwZQDHAPyV6+ebtTVNaKQhl3GbojOrmyJT5Do0QoNt0iOvY/k8KPujrTpgm+C9LotFv9tINxSYhE0Sq8IjExoAPgfgCIAJANcDyFX2dwB4weVLmrk1RWjEmctYzZrb1tNhM2rXmEqVFIv2oCDqXg+26bI64tEESYsuOYQUIjqZqnMJ9fQkUyrHVWi4+DTWAdjBOb+Kc/4I53y5YtZaBfA/IrCQZQ+dATOXE4+N2HfHx4GLLgI+9CGgrw+48ELx/KKLxHvEm6hupB07gOHhWttwZycwMqJfxOftaureCjbDvGpkN90DqTXENwfvdTkwIPwWf/zHAGPAF7/ov9Z0LqGzZ/VDRmquWRfJkqWtaZqGLs59YkJfx9H1nC4xo4QP1Ury0Y/WpkpXLSw0STZAmkbk6Iooervp0KHaoURGWak0o7tBuadiRg0iW14GPvEJMU3Yuxe47DK35Ebz82J29rOf+XNReUl7QpqE0KUE+8Y3ao/xpgqTead0ab+OHQPOPbdFc1K5IEPLBgfF6zNnRC4WxvwhZt5jczlx/e/dW9U0WrQD5e3qvUbkvnIZOH26+t74OPDpT4ssLV68CtngoOg6SUeHuEZVUpWqzkWyZGlriqYRVNDBdQqgOtNt+b5pFqclTG2NUsm8VkMu5G/LRc2qoTxs9JQsG9sCHWjzGdiqRcrrqVisTXduu5XDljRJi6aR+CAf9Rar0PDeTDZTkovzW3cVSJvK2rXVPBhtHMboQtg06TIPVT2RLi2Jy1Jkk1Dxvm4BU5WtK3Q/sVi0DwO6iD5v4aWw3RZ3qjoSGlGjXlEf+pD5arGF7EhshYEpesoZnXvJto2M1H52aiqaoLdM4jJqqde9rnBYC6xbxbhMAAAbpUlEQVQKD+oK3U/s7ravD9IJDFPyzDAFyJKOnkp8kI96i0VouKzcUTUGl3+/BWZnSRCUxSLIRKXK47b9K4IGe5frvlRqiYJi9XRFkKYhTVVBAiEtc0NXoUH1NFwIyhGgInMG2OoMePNZlMui1sbu3Q03tdVRww6PHg1XjCmXA/bvrz3H5GSbphYJSgHict3ncsL7m/EODOoKXfqZBx6o7isW/edkTARXTE5WU4boUMPHU4+LZMnSlgpNI4x6Pjpaa1+hGsxGTBqB9MGWy8F/i64Cr077aDlMPy4onYiLpmHydWQMF1OR7ifKfTfeWNs1Q0Phvj/p7gOZpyLGtOTY5WYyYbopXXwibcbcnL5OgVoDSAoQ01jX2el/L2Pm9/AEObvV0cpbvUodSaVPo0UDNOoZuOfmhF9MV2pElzbEJrvlX5RERpZMCA0AHwHwHEQSxGHN+wUAD1fe/y6A/qBzxiY0pqaCq/QB4cpvTU3pPWnd3S0+ioXDVqdA57ctFjlfsyZa+Z5ZTOqZKZx2aKj22KGh4OipNkZem7rbWJ2M2HKZmuaOzZTJqRcaADoBvADgYgB5AN8HcKlyzO8DGK08/ySAh4POG5vQsAX3ezPnhbmZSNMIxNRFauVc27HqpkZbhTUjZAqdh7dUEnY6OXrJ6/bpp/Udpob8EJzz4OvNGy1lC7awBXM0c0LjKjSSdIRfDuB5zvlPOOdLEGnXr1WOuRbAwcrzrwK4MrF6HjpP2MGDwsMlPV033xzOoyXP6XU25nLCw5YZr1i86Hyx5TJw331+56Lu2O5u/z7vClxA/AUyZsFbCqUl0Hl4z5wRy5RPnRLPb7kFuPJK4IMf1J9jaqr6POMdFGXzTXECcl9HB7BliwjesKXq0v1F6jGpwkWyxLEB+ASAv/O8/h0A9yvH/BBAn+f1CwDW2c7b9HKvUZ0z6QLBKSVMOGyQo3ztWr0jXJoR0lByMxa8fgldBwRt6orIjHZQ1M03heHqFooGRSXLtmVB00hSaFyvERr3Kcf8SCM0ztec6yYAhwEc3rBhQ9R9SSRMmAVQpmOlb/fpp80m/owvNbDjms1AHfGk7U43QupWq6WUuNbiqNfbyIh5vUfQdTw3Jz4vk0I0Wy5nQWhcAWDC8/rzAD6vHDMB4IrK8y4ArwJgtvMmUiOciJ0wCp56rGlRs9cVNTbmd2a2bFSV7BBdjHKpJCSrjJ6SmAzv3rwYKSbORethFoq6XMdJxRlkQWh0AfgJgI2oOsLfpRzzGdQ6wv8h6LwkNAgvQcFDo6NiZqdLRdJSmoaKGqMcNLXV5fjOUEfVq2nUM4C7FF9KI6kXGqKNuBrAv1fMTrdX9t0B4JrK8yKARyBCbqcAXBx0ThIahBfbDDMob9XoaJtElwb9SDkKmjJDZkQlczEP2TRU22I/NYJZmpqSXnsRhkwIjTg2EhqEF9sMc2LCLDB6eqr25e7u5sfMp4awq8JTjuviOqmA2X6mmha9UKheJ2lZexEGV6FBuaeIlqa3V9QGKhSAnh73tEjLy8AddwALC8Abb4jHT30qs5Gm9ROUf6pQyFSeKTXP0/w88O1vA7t2iehjGYW8e3dt6WCgNvxVV/xrcbF6nTz1lAi5VVlYCE5Ll3ZIaBAtzf791QFgaUkIELm2Y/Nm/XhYKgF/8Af+9RzLy2IwyPAyhfDYFhEUCiIjnykTX8qRyS937BCDuZdcrv5cjsvLwG//tphs6Ejl2osQkNAgWpb9+8W6tcVFcQMvLgoBcvy4eL+3FxgbE0Kiu1tkKh0ZEYsGP/AB/TlvvLGaHXd8vGk/JTm8i1pLJbGvWBTPH3wQ2LQp2fbViVdT0A3uKyvAvn3mxL02WQr4JxxelpaqwieLMGHKah22bt3KDx8+nHQziISZnwfWr/fXZwbEBPnBB6sTZF3d5+PHgV/9VVHy3USpJARMRiwzjWEqhJ1wc+ptxjPPCOF/6lTt/u5uYHVVCIiBAfv3jI8Ls5aqpQSRz4vJStoUNMbYEc751sDjSGgQrcgzz4jMGK+/rn9fWlZ0E+XxcTELXVmxzybXrgUeeQQ499zEx9C2Qv4/+bz4f+QAH4b5eaEtSn8EICYBjz0mzJau/+X8vNBo77oL6OwU18zKil3TkN+VtgmHq9Ag8xTRkvT327WExUUxOKgmJq/ZIqi405kzwHXXtZm5KmG8/490WtfjWNalkjtwAPjwh8MN5L29wBe+ALz0kvB3vfSSSElXKNg/l2W/BgkNoiVRCyPqWFz0DzgmB2d3t7jR83kxyMhKbY0OXkQ4bIn/wjIwUJtv1EVbkQkPjx+vDYjwRmUNDAgt1iY4vE71rEFCg2hZ5KDw5JPA6Kj+JlYHnHLZb6MuFoFHHwVOngRmZ8Ugo4uuyvLsMSsElWUNS5hSqzLa6n3vAy69VDyaNMxNm4TfTGoy3glHBqvh1kA+DaJtOH5cmKS8znGvbVnaygGhOchgob17gcsuqw5Mx44B117rFy5ptFO3IvJ/yuWEwKjHp+GC1wkO+H0gEtv/rp6jEed93Lj6NLqCDiCIVkHO/tQBp7e31lYuWV0F7rwTuPVWMUv85S8BxsRndREzt93mNhg0GvnT7gwMANu31/Zh1H2qOttvu0081wkNqWHqvre3t3Z/K/zfpGkQbYdugNGFYPb0iAFDF7arUiwKJ2jQoBBF5A9RSz19ahMypsgqzvWTBa+mkeUJAUVPEYQBnR1bZytfWrJn0ACEg7xUEsUWAftq8agif4gq9fSp9E2Yot5MzvbbbxeP6n6prQadt1UgoUEQ0Idg7ttnD9uVDvIXXxSvbQPG/Dzw+OP2fEZEeMJGU7kIGZOz/eMf9+eT6uoSprJ2mhCQ0CCICmoI5s031woSNQLmgQdEXD9gHzDkDPSzn/UvNsxy6GUzCKrpHTaaykXImNZw/Pmf+02V8rNRhgKnHXKEE4QH1XGpOl0Bv81aDhheG7h3wFAd7IDwl5w9m+3Qy7hx8VXIAV4X3KDDVcio//urr4q8YypLSyJM+8QJv0Bp1QkBCQ2CqKBzYur2qQOSbSDSCZRyGbjvPuDqq0lgmNClHh8cFAO52me6aCoTYYSMdwLx+OP68/3mbwJbtoj/eHVVnLNUChZeWYaEBkFAP6sF3KJy5EC0c2dVeJw9K8xc27f7BcrKCgmMIGzam0toqw0XIaNOFi6/XH+uf/1XEVEl21kqiXxkYfJXZQ3yaRBtj8mJqRbm2bVLFOzR2de3b691ki4vVxcK6uzjMjyzrWpzhCCKld+2/rWtBNdFQa1bB1x/fe1x11/vzzKQy4kElq0qMABQuVeC0NURX7NGlOZUy3V2d+vrRdtqkXNeX/3pdieopjfn+vKtuvrcYT6rlmnN58W1cM454vGWW0RNcFsp4SwCqhFOEOaa0OoxQWWwg8pihxlAWm2wiRPb/6cTvIcO6YW9qb63KlR0wt92Lnke72TC5ZpLI65Cg8xTRMviutjKJSOuii5MU9YiL5ftSelcwzOzar6Kst3SjATUntNmUtSt2u7oEDnDbJ+dnw+uyAf4/yvOq4//9m9tsMDPRbJkaSNNg+A8/Mx/aorzPXs4z+X8M8s1azgvFNxmrj094tjR0cballXzla3drlqfeozunDqNoLtbbCYNoVgUn52Y8B/nNSWqZjH1mpD/lYuGmiUNEmSeItqZIB+DxDvY22780VGzfb0ec5M6MI2MVI+v13zlahaJy3xia7eLENQdYzrn9LR+v8405d1yOTfzlbePTL4VF1OW7ppLKyQ0iLbGZeB1mSkWCtVBQg4kTz/N+diYGLg4N896JyaC2zgyUnWyemfRqhALGnxcNZM4NRiToJ6YMP8Xsk9NQmBiwiz8dYO53BckPHQaiA1XpzlpGhncSGgQkqDom6CZYqFQFQySoaHaY4aGzINH0KBsEmx79oQbfFw1k6DjGtVATOc3DfzeCKdCgfOuLv9gbhM4pjbr+s+0uQh3G+o1NjQUHPGVVkhoEAS3D4Smwb6nR3/DT0/rB57pafeoHS86oVUu+/0n0qxiGoBcTXG246LSQFyFarHoFrEm+9Z1IJ6b0/cfIEJn83n3/8cV9Rpr9eipxAf5qDcSGkQY1AFpdNR8w4+N6QejsTHxfpCDVWVuzu9k7ew0+1caDeEN6x8IO+i5+DS8PpwgfwAgjpPndhmIdaY9KXTDCiDT92ZVKARBQoMgHHEdBGyahjyPOpPN5exmJd3xJnu8TQC5Doa641w1lSDCLHB0XRsTVniZzuuNZnPVDEzrQLIY1eYCCQ2CiAGd+UWi0xzkgKUbmEyaiXSOhx1A642esmkgYWbUYTSeqSnRL0EO6zVrOL/33nCCI0z4s0kImH6L2t4sObqDIKFBEDExPV0bPSUxmUY6O/UzVptgkJFVcTpVbWGl0qEbdkYdpPGog/TwsH1thdzy+XC/v95MALL/XdeBZCmkNghXoUE1wgkiIubngfXrg2uKm+pNy1Xk3ky6cdWc1mX1lZlfy2WR7lutkS3rYAdharOu9naxCDDmrzeiI6gNYftKVxd+7VqRnbi/361OeJh+STtUI5wgmkxvrygRG0RHB9DZWbuvuxt47DF9kSFvNtYoUnSY0mgA4rtOn26sCp0pg6wufUo+D9x2mxAeQTBmbkM99bltmXRN1fseeECfsbitcFFHsrSReYpImtFRYUsvl4UJShfmabONBzlmTZl2XXFxWEcRTaViO6/Ov6NuhUJj0WM6gsxpFD1FPg2CaApBaShMg5XNMasKn3y+voHL5VxhQ1NdMZ3XJZrK5NAOG+qs0qpCICyuQoN8GgTRBFxKyeps/oWCyM564gRw1VX+805MAB/+cPi2XHihMMVIOjuBZ58FNm2ytzkKTOeVfpZcTvgNOBfPV1aE2e/mm/2fn5zUZ7ZtJV9Ds3D1aVC5V4JoArpypOo+XYnTxUVROlQOmFEwMwN0ddUKjZUV8T0PPlj1q4QpoRoG03nVMqyyrV7hojrwz56t/R2AEBh791b9HyQ4oiURTYMxdh6AhwH0A5gBcAPn/Gea41YAPFt5+RLn/Jqgc5OmQaQZnXbhHSRVTcNGLgecPBl+UDx+HLj0Uv17thl6XJpHEPJ7dVFdKt3dwGc/KzSToNruRC1pj54aBvAE5/wdAJ6ovNZxhnP+3ytboMAgiDSjRvh89rO1rycnxQCn1p3WUSwC990nBtOwkVSnTwvhoGN5WR+hpItOMkVyefc3Gu3l/d7Nm4OPl6Ys19ruRB24OD6i3gA8B+CCyvMLADxnOO502HOTI5xII2HSaE9Pm5Puye2aa+pPZxHUlm98I/j4XE7//V5Hfi4nHOz1ptxw6TNZvzsop1WjEWftANIcPQXg58rrnxmOOwvgMIDvALjO5dwkNIg0ErZgjxx8y2X7Z1xCTHXRQYcO6VOeSIHgjWoaG7MXqZLfr0t82EjYrq7PikUhUL3RV2FyWrVS2o+ocRUasTnCGWOTAN6meev2EKfZwDl/mTF2MYAnGWPPcs5f0HzXTQBuAoANGzbU1V6CiBOX2tNyYRlQ6xR+9FHgnnvsn5WL71Rfg27l98CA2DZsAN7zHn07BgeBX/wCuPVWsRjxjTeCv39qyu/I99LZCdx/v4j2yueDfSO6PmMMOHpUmNi8n/ee58AB0X5du039RITARbJEvcHRPKV8ZgzAJ4KOI02DSCv1FuwxJUIMmkHrzFzqcYOD+vOZ6nqYzGYumoZqVnIxF9W7XkQuGIxjkWKrgpSbp/YAGK48HwbwV5pjzgVQqDxfB+DHAC4NOjcJDSLN1FOwR7cYTyZBtCUG1A3wOhPYmjV64aBbod3Vpd8v6154B3np07CZ2FwG8UYW38W1SLEVcRUaSYXcng/gHwBsAPASgOs55//JGNsK4BbO+e8xxn4DwH4AqxBRXl/inB8IOjeF3BKthi6xXk8PcNddwDvfKaKKghIDSmRILaA/plAQZp29e4Hdu/3JF8tlYb7y7i8WgZdeEs9laKw0HwHCJHXHHfrfJhMEbtsW1Av1k1SocNZwDblNRNOIcyNNg2g1TM7dNWv0s2eT071QqB4bVOGOc5G2Q6cZjI6a06LoIqVMxavIXJQukLQjnCCI+lBnxjLj6uCgWMn9+uviuF/+Ujzu3Cmc5nIWrXMgy3QkMk2IyTFfKAgtAaiuQt+9Wziuz56tplDfuFG8J9dOSK1Fai6Dg9U2bdoEDA0JjUOSzwvHeFtmic04lBqdIFKEKcX3wIAwK911l/8zS0tCIEh0ab0ffLA2r9TkJLC66j/XykrVrAQIwXHiBPDEE7VmrRtuAK67TpxHl/JcTaV+333A9DQwNgY8/bTYXnyRVmpnEUpYSBApQeeLUNN6fPvb7okLwxRDAoRv4oEHzAO5qYjS0aP2ok3kU8gGaU8jQhCEgsuMffNmsU89xiXFhu17uruBr33NPvPXfW5hAfjHf9QXLOrtra84EpFuSGgQREqwVZKT9PYCBw+KGX53t3g8eFC/qM80WOu+Z3XVLHhk/qhyWV/K9u67hf/ixReFuUqanUwVAikHVLYhoUEQKcFUYlQVCAMDIsT1qafEo9QO5OB+/Lh9sJbf49Uazp4VA76KV/hs2QJ89KP+Y7yrrL1lXl00JyJ7kE+DIFJGPT4Ab7qQhQWx1sLrY1DXQ8zPizQi3uJFqv/E5MMA7J/z/o4gH02jkL8kOsinQRAZRZ2xB6GagRYX/U5u1cw1M+NPwa5qATpNIZ8Hbr/dTUtx1ZzqhfwlyUBCgyAyjm5wL5WEUDAN1i7+k/7+6loQyZkzwMc/LjQZ7+dMvgoZKuz1dURBWH9Jo3U9iCokNAgi45gW6h07Zh6sXbUAxvyvT5wI1lLU7wqjObkQxl9CGkm00Ipwgsg43hXjuZyY+R84ULuYT4dak1sd1GdmhDDxCiTp0wjSUuLGRVMCajUS3Wp1IjykaRBEC1CvGcimBZgG5s2bg7WUuM1BrpoSRXBFD0VPEQRhREZleTUYb4ivTksxFX6Kg6DoqWZEcLUKrtFTJDQIgrASJqw1jYO0TfARVVyFBvk0CIKwIjPtuiDNQV6hkXSJ1SDfDREOEhoEQUSGq4O62YQRfIQdcoQTBBEZcS/oI5KHNA2CICKFzEGtDQkNgiAih8xBrQuZpwiCIAhnSGgQBEEQzpDQIAiCIJwhoUEQBEE4Q0KDIAiCcIaEBkEQBOEMCQ2CIAjCGRIaBEEQhDMkNAiCIAhnSGgQRIah2tdEsyGhQRAZhWpfE0lAQoMgMoi39vWpU+JxcJA0DiJ+SGgQRAah2tdEUpDQIIgMktZiR0TrQ0KDIDIIFTsikoLqaRBERqFiR0QSkNAgiAxDxY6IZpOIeYoxdj1j7EeMsVXG2FbLcR9hjD3HGHueMTbczDYSBEEQfpLyafwQwA4A/2I6gDHWCeBvAPwWgEsBDDDGLm1O8wiCIAgdiZinOOfHAYAxZjvscgDPc85/Ujn2IQDXApiOvYEEQRCEljRHT10I4ITn9WxlH0EQBJEQsWkajLFJAG/TvHU75/xrLqfQ7OOG77oJwE0AsGHDBuc2EgRBEOGITWhwzrc3eIpZAOs9r/sAvGz4rq8A+AoAbN26VStYCIIgiMZJc8jtMwDewRjbCOAkgE8CuDHoQ0eOHHmVMfZi3I0LyToArybdiBBQe+OF2hsvWWpvmtp6kctBjPPmT8wZYx8DcB+AXgA/B/A9zvlVjLG3A/g7zvnVleOuBvAlAJ0AHuCc39X0xkYAY+ww59wYWpw2qL3xQu2Nlyy1N0ttlSQVPfVPAP5Js/9lAFd7Xj8O4PEmNo0gCIKwkOboKYIgCCJlkNBoDl9JugEhofbGC7U3XrLU3iy1FUBCPg2CIAgim5CmQRAEQThDQiMGspaQkTF2HmPs/zLGflx5PNdw3Apj7HuV7esJtNPaX4yxAmPs4cr732WM9Te7jUp7gtq7kzE27+nT30uinZW2PMAYm2OM/dDwPmOM3Vv5LT9gjF3W7DYq7Qlq7/sZY6c8fftnzW6jpy3rGWNPMcaOV8aF3ZpjUtW/VjjntEW8AdgE4J0A/hnAVsMxnQBeAHAxgDyA7wO4NKH2/hWA4crzYQB/aTjudIJ9GthfAH4fwGjl+ScBPJzy9u4EcH9SbVTa8l4AlwH4oeH9qwF8EyJTw68D+G7K2/t+AP876X6ttOUCAJdVnvcA+HfNtZCq/rVtpGnEAOf8OOf8uYDD3kzIyDlfAiATMibBtQAOVp4fBHBdQu2w4dJf3t/xVQBXsoCsmDGSpv83EM75vwD4T8sh1wL4ey74DoC3MMYuaE7r/Di0NzVwzl/hnB+tPH8dwHH48+ilqn9tkNBIjjQlZPwvnPNXAHGBA3ir4bgiY+wwY+w7jLFmCxaX/nrzGM75WQCnAJzflNb5cf1/P14xR3yVMbZe835aSNP16soVjLHvM8a+yRh7V9KNAYCKyXQzgO8qb2Wmf9OcRiTVNDMhYxTY2hviNBs45y8zxi4G8CRj7FnO+QvRtDAQl/5qap8G4NKWbwAY55wvMsZugdCSPhh7y+ojTX3rwlEAF3HOT1cySzwG4B1JNogxVgbwjwD+gHP+C/VtzUdS2b8kNOqENzEhYxTY2ssY+ylj7ALO+SsVlXjOcI6XK48/YYz9M8SMqVlCw6W/5DGzjLEuAOcgORNGYHs55695Xv4tgL9sQrvqpanXa6N4B2XO+eOMsf/JGFvHOU8kzxNjLAchMP4X5/xRzSGZ6V8yTyXHmwkZGWN5CMdt0yOSKnwdwKcqzz8FwKcpMcbOZYwVKs/XAfhNNLcglkt/eX/HJwA8yStexgQIbK9is74GwtadVr4O4HcrUT6/DuCUNGmmEcbY26Q/izF2OcRY95r9U7G1hQE4AOA45/yvDYdlp3+T9sS34gbgYxAzh0UAPwUwUdn/dgCPe467GiKS4gUIs1ZS7T0fwBMAflx5PK+yfytEAkkA+A0Az0JEAT0LYDCBdvr6C8AdAK6pPC8CeATA8wCmAFyc8HUQ1N6/APCjSp8+BeBXEmzrOIBXACxXrt1BALcAuKXyPoMov/xC5f/XRgWmqL1Dnr79DoDfSLCt74EwNf0AwPcq29Vp7l/bRivCCYIgCGfIPEUQBEE4Q0KDIAiCcIaEBkEQBOEMCQ2CIAjCGRIaBEEQhDMkNAiCIAhnSGgQBEEQzpDQIIiYYYxtqyQlLDLGuis1Fd6ddLsIoh5ocR9BNAHG2J0QK9ZLAGY553+RcJMIoi5IaBBEE6jkn3oGwAJESouVhJtEEHVB5imCaA7nAShDVG4rJtwWgqgb0jQIoglUaqo/BGAjgAs450MJN4kg6oLqaRBEzDDGfhfAWc75IcZYJ4B/Y4x9kHP+ZNJtI4iwkKZBEARBOEM+DYIgCMIZEhoEQRCEMyQ0CIIgCGdIaBAEQRDOkNAgCIIgnCGhQRAEQThDQoMgCIJwhoQGQRAE4cz/B8wu736oHpSlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "from pandas import DataFrame\n",
    "import torch\n",
    "\n",
    "x1 = []\n",
    "x2 = []\n",
    "y = []\n",
    "for (data,label) in train_loader:\n",
    "    res = torch.argmax(net.forward(data), dim=1)\n",
    "    print(res)\n",
    "    for point in range(len(data)):\n",
    "        x1.append(data[point][0].item())\n",
    "        x2.append(data[point][1].item())\n",
    "        y.append(res[point].item())\n",
    "#print(x1)\n",
    "df = DataFrame(dict(x=x1, y=x2, label=y))\n",
    "print(y)\n",
    "colors = {0:'red', 1:'blue'}\n",
    "fig, ax = pyplot.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 1.0\n"
     ]
    }
   ],
   "source": [
    "correct_pred = 0\n",
    "for _, (data, label) in enumerate(test_loader):\n",
    "    prediction = net.forward(data)\n",
    "    _, ind = torch.max(prediction,1)\n",
    "    if ind.data == label.data:\n",
    "        correct_pred +=1\n",
    "print('Correct predictions: '+str(correct_pred/120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:168: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  compared_weight_signs = torch.tensor(torch.ne(new_weight_signs, old_weight_signs).detach(), dtype=torch.float32)\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:173: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_weights = new_weight_signs*torch.tensor(torch.ge(torch.abs(self.m_accumulated),self.rho*max_weight_elem*torch.ones_like(self.m_accumulated)).detach(),dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import loadMNIST\n",
    "from Networks.ResNet import ConvNet, FCMSANet\n",
    "\n",
    "#net = ConvNet(3, [1], [3,6], num_fc=2, sizes_fc=[100,10])\n",
    "net = FCMSANet(num_fc=4,sizes_fc=[784,1024,1024,1024,10], bias=False, batchnorm=True, test=False)\n",
    "train_loader, test_loader = loadMNIST('Dataset/input/mnist_train.csv', 'Dataset/input/mnist_test.csv')\n",
    "#net.train_backprop(1,train_loader)\n",
    "#net.test(test_loader,10000)\n",
    "print(train_loader.batch_size)\n",
    "\n",
    "net.train_msa(1,train_loader)\n",
    "\n",
    "#for index, (data, target) in enumerate(train_loader):\n",
    "#    if index == 1:\n",
    "#        print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "60000\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.021136  #  0.286883  #\n",
      "#  2  #  0.013666  #  0.829800  #\n",
      "#  3  #  0.013053  #  0.897417  #\n",
      "#  4  #  0.012986  #  0.906250  #\n",
      "#  5  #  0.012988  #  0.906350  #\n",
      "#  6  #  0.012940  #  0.911467  #\n",
      "#  7  #  0.012933  #  0.911933  #\n",
      "#  8  #  0.012929  #  0.911917  #\n",
      "#  9  #  0.012904  #  0.915100  #\n",
      "#  10  #  0.012890  #  0.917233  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  11  #  0.012865  #  0.920600  #\n",
      "#  12  #  0.012873  #  0.919233  #\n",
      "#  13  #  0.012850  #  0.921300  #\n",
      "#  14  #  0.012837  #  0.922417  #\n",
      "#  15  #  0.012826  #  0.923083  #\n",
      "#  16  #  0.012819  #  0.925200  #\n",
      "#  17  #  0.012815  #  0.925233  #\n",
      "#  18  #  0.012803  #  0.924800  #\n",
      "#  19  #  0.012793  #  0.925500  #\n",
      "#  20  #  0.012786  #  0.927700  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  21  #  0.012767  #  0.928617  #\n",
      "#  22  #  0.012764  #  0.929817  #\n",
      "#  23  #  0.012761  #  0.929783  #\n",
      "#  24  #  0.012750  #  0.931117  #\n",
      "#  25  #  0.012755  #  0.930150  #\n",
      "#  26  #  0.012739  #  0.930950  #\n",
      "#  27  #  0.012737  #  0.932067  #\n",
      "#  28  #  0.012714  #  0.934367  #\n",
      "#  29  #  0.012710  #  0.933683  #\n",
      "#  30  #  0.012709  #  0.934067  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  31  #  0.012702  #  0.935467  #\n",
      "#  32  #  0.012690  #  0.937067  #\n",
      "#  33  #  0.012683  #  0.936800  #\n",
      "#  34  #  0.012679  #  0.937833  #\n",
      "#  35  #  0.012673  #  0.937717  #\n",
      "#  36  #  0.012665  #  0.939350  #\n",
      "#  37  #  0.012664  #  0.938350  #\n",
      "#  38  #  0.012651  #  0.939567  #\n",
      "#  39  #  0.012656  #  0.939817  #\n",
      "#  40  #  0.012646  #  0.941567  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  41  #  0.012644  #  0.940967  #\n",
      "#  42  #  0.012632  #  0.942317  #\n",
      "#  43  #  0.012623  #  0.944083  #\n",
      "#  44  #  0.012625  #  0.943300  #\n",
      "#  45  #  0.012613  #  0.944450  #\n",
      "#  46  #  0.012618  #  0.943500  #\n",
      "#  47  #  0.012606  #  0.944933  #\n",
      "#  48  #  0.012599  #  0.945317  #\n",
      "#  49  #  0.012598  #  0.946067  #\n",
      "#  50  #  0.012588  #  0.946917  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  51  #  0.012578  #  0.947050  #\n",
      "#  52  #  0.012578  #  0.947650  #\n",
      "#  53  #  0.012573  #  0.948067  #\n",
      "#  54  #  0.012572  #  0.948083  #\n",
      "#  55  #  0.012566  #  0.948533  #\n",
      "#  56  #  0.012561  #  0.948933  #\n",
      "#  57  #  0.012555  #  0.949333  #\n",
      "#  58  #  0.012557  #  0.949850  #\n",
      "#  59  #  0.012548  #  0.950500  #\n",
      "#  60  #  0.012550  #  0.950417  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  61  #  0.012548  #  0.950367  #\n",
      "#  62  #  0.012538  #  0.951200  #\n",
      "#  63  #  0.012539  #  0.951067  #\n",
      "#  64  #  0.012532  #  0.952583  #\n",
      "#  65  #  0.012539  #  0.951133  #\n",
      "#  66  #  0.012519  #  0.952433  #\n",
      "#  67  #  0.012516  #  0.953233  #\n",
      "#  68  #  0.012518  #  0.953417  #\n",
      "#  69  #  0.012520  #  0.952833  #\n",
      "#  70  #  0.012513  #  0.954000  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  71  #  0.012516  #  0.955067  #\n",
      "#  72  #  0.012513  #  0.955033  #\n",
      "#  73  #  0.012505  #  0.955567  #\n",
      "#  74  #  0.012507  #  0.954783  #\n",
      "#  75  #  0.012495  #  0.955800  #\n",
      "#  76  #  0.012499  #  0.955583  #\n",
      "#  77  #  0.012494  #  0.956133  #\n",
      "#  78  #  0.012495  #  0.955817  #\n",
      "#  79  #  0.012485  #  0.957150  #\n",
      "#  80  #  0.012484  #  0.956950  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  81  #  0.012489  #  0.956650  #\n",
      "#  82  #  0.012486  #  0.956533  #\n",
      "#  83  #  0.012485  #  0.956917  #\n",
      "#  84  #  0.012485  #  0.957567  #\n",
      "#  85  #  0.012472  #  0.958333  #\n",
      "#  86  #  0.012476  #  0.957617  #\n",
      "#  87  #  0.012479  #  0.957600  #\n",
      "#  88  #  0.012475  #  0.957917  #\n",
      "#  89  #  0.012474  #  0.957683  #\n",
      "#  90  #  0.012469  #  0.958500  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  91  #  0.012469  #  0.957800  #\n",
      "#  92  #  0.012466  #  0.958800  #\n",
      "#  93  #  0.012467  #  0.958267  #\n",
      "#  94  #  0.012462  #  0.959483  #\n",
      "#  95  #  0.012464  #  0.958783  #\n",
      "#  96  #  0.012465  #  0.959083  #\n",
      "#  97  #  0.012455  #  0.960100  #\n",
      "#  98  #  0.012458  #  0.959733  #\n",
      "#  99  #  0.012456  #  0.960383  #\n",
      "#  100  #  0.012450  #  0.960350  #\n",
      "Time elapsed:  42948.913511782994\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import loadMNIST\n",
    "from Networks.ResNet import ConvMSANet\n",
    "\n",
    "#net = ConvNet(3, [1], [3,6], num_fc=2, sizes_fc=[100,10])\n",
    "net = ConvMSANet(num_conv=5, num_channels=[1,3,6,6,12,12], subsample_points=[2,4], num_fc=2,sizes_fc=[588,900,10], batchnorm=True, test=False)\n",
    "train_loader, test_loader = loadMNIST('Dataset/input/mnist_train.csv', 'Dataset/input/mnist_test.csv')\n",
    "#net.train_backprop(1,train_loader)\n",
    "#net.test(test_loader,10000)\n",
    "print(train_loader.batch_size)\n",
    "print(len(train_loader.dataset))\n",
    "\n",
    "net.train_msa(100,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "60000\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.021318  #  0.282983  #\n",
      "#  2  #  0.013563  #  0.806183  #\n",
      "#  3  #  0.012885  #  0.862233  #\n",
      "#  4  #  0.012822  #  0.867867  #\n",
      "#  5  #  0.012795  #  0.870683  #\n",
      "#  6  #  0.012774  #  0.871517  #\n",
      "#  7  #  0.012752  #  0.873517  #\n",
      "#  8  #  0.012751  #  0.873633  #\n",
      "#  9  #  0.012717  #  0.875800  #\n",
      "#  10  #  0.012708  #  0.876800  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  11  #  0.012702  #  0.877467  #\n",
      "#  12  #  0.012699  #  0.876667  #\n",
      "#  13  #  0.012677  #  0.878633  #\n",
      "#  14  #  0.012669  #  0.880550  #\n",
      "#  15  #  0.012644  #  0.881283  #\n",
      "#  16  #  0.012645  #  0.882050  #\n",
      "#  17  #  0.012624  #  0.883467  #\n",
      "#  18  #  0.012623  #  0.882917  #\n",
      "#  19  #  0.012607  #  0.884183  #\n",
      "#  20  #  0.012616  #  0.884483  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  21  #  0.012576  #  0.887333  #\n",
      "#  22  #  0.012588  #  0.887333  #\n",
      "#  23  #  0.012569  #  0.886800  #\n",
      "#  24  #  0.012565  #  0.888267  #\n",
      "#  25  #  0.012573  #  0.889050  #\n",
      "#  26  #  0.012534  #  0.890700  #\n",
      "#  27  #  0.012530  #  0.890050  #\n",
      "#  28  #  0.012518  #  0.892150  #\n",
      "#  29  #  0.012511  #  0.893217  #\n",
      "#  30  #  0.012497  #  0.894183  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  31  #  0.012494  #  0.893133  #\n",
      "#  32  #  0.012490  #  0.892850  #\n",
      "#  33  #  0.012474  #  0.895450  #\n",
      "#  34  #  0.012465  #  0.896083  #\n",
      "#  35  #  0.012455  #  0.897367  #\n",
      "#  36  #  0.012441  #  0.898100  #\n",
      "#  37  #  0.012436  #  0.897667  #\n",
      "#  38  #  0.012432  #  0.899317  #\n",
      "#  39  #  0.012422  #  0.899717  #\n",
      "#  40  #  0.012419  #  0.900050  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  41  #  0.012415  #  0.899800  #\n",
      "#  42  #  0.012398  #  0.901533  #\n",
      "#  43  #  0.012395  #  0.902367  #\n",
      "#  44  #  0.012379  #  0.903100  #\n",
      "#  45  #  0.012380  #  0.903933  #\n",
      "#  46  #  0.012363  #  0.904383  #\n",
      "#  47  #  0.012352  #  0.904850  #\n",
      "#  48  #  0.012355  #  0.905150  #\n",
      "#  49  #  0.012348  #  0.905417  #\n",
      "#  50  #  0.012349  #  0.905767  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  51  #  0.012332  #  0.906833  #\n",
      "#  52  #  0.012335  #  0.906883  #\n",
      "#  53  #  0.012310  #  0.908233  #\n",
      "#  54  #  0.012308  #  0.908583  #\n",
      "#  55  #  0.012312  #  0.908317  #\n",
      "#  56  #  0.012301  #  0.909717  #\n",
      "#  57  #  0.012289  #  0.909850  #\n",
      "#  58  #  0.012293  #  0.910850  #\n",
      "#  59  #  0.012282  #  0.912300  #\n",
      "#  60  #  0.012275  #  0.910617  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  61  #  0.012274  #  0.912250  #\n",
      "#  62  #  0.012258  #  0.914300  #\n",
      "#  63  #  0.012264  #  0.912167  #\n",
      "#  64  #  0.012248  #  0.913900  #\n",
      "#  65  #  0.012246  #  0.914233  #\n",
      "#  66  #  0.012240  #  0.914600  #\n",
      "#  67  #  0.012226  #  0.915383  #\n",
      "#  68  #  0.012230  #  0.914733  #\n",
      "#  69  #  0.012225  #  0.915783  #\n",
      "#  70  #  0.012228  #  0.915350  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  71  #  0.012222  #  0.915467  #\n",
      "#  72  #  0.012210  #  0.916050  #\n",
      "#  73  #  0.012193  #  0.917767  #\n",
      "#  74  #  0.012201  #  0.917683  #\n",
      "#  75  #  0.012192  #  0.919400  #\n",
      "#  76  #  0.012180  #  0.919383  #\n",
      "#  77  #  0.012197  #  0.919333  #\n",
      "#  78  #  0.012183  #  0.920050  #\n",
      "#  79  #  0.012173  #  0.920567  #\n",
      "#  80  #  0.012173  #  0.920167  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  81  #  0.012166  #  0.920733  #\n",
      "#  82  #  0.012170  #  0.920367  #\n",
      "#  83  #  0.012152  #  0.922383  #\n",
      "#  84  #  0.012162  #  0.920633  #\n",
      "#  85  #  0.012157  #  0.920083  #\n",
      "#  86  #  0.012157  #  0.921667  #\n",
      "#  87  #  0.012143  #  0.921100  #\n",
      "#  88  #  0.012150  #  0.921067  #\n",
      "#  89  #  0.012139  #  0.922300  #\n",
      "#  90  #  0.012126  #  0.922500  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  91  #  0.012136  #  0.922817  #\n",
      "#  92  #  0.012129  #  0.922833  #\n",
      "#  93  #  0.012123  #  0.924333  #\n",
      "#  94  #  0.012123  #  0.924483  #\n",
      "#  95  #  0.012112  #  0.926050  #\n",
      "#  96  #  0.012118  #  0.924417  #\n",
      "#  97  #  0.012111  #  0.924783  #\n",
      "#  98  #  0.012117  #  0.925517  #\n",
      "#  99  #  0.012104  #  0.926117  #\n",
      "#  100  #  0.012094  #  0.926283  #\n",
      "Time elapsed:  26136.839193304\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import loadMNIST\n",
    "from Networks.ResNet import ConvMSANet\n",
    "\n",
    "#net = ConvNet(3, [1], [3,6], num_fc=2, sizes_fc=[100,10])\n",
    "net = ConvMSANet(num_conv=3, num_channels=[1,3,6,12], subsample_points=[0,1], num_fc=2,sizes_fc=[588,900,10], batchnorm=True, test=False)\n",
    "train_loader, test_loader = loadMNIST('Dataset/input/mnist_train.csv', 'Dataset/input/mnist_test.csv')\n",
    "#net.train_backprop(1,train_loader)\n",
    "#net.test(test_loader,10000)\n",
    "print(train_loader.batch_size)\n",
    "print(len(train_loader.dataset))\n",
    "\n",
    "net.train_msa(100,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n"
     ]
    }
   ],
   "source": [
    "net.train_msa(1,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy:  0.9253\n"
     ]
    }
   ],
   "source": [
    "net.test(test_loader,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "60000\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.010105  #  0.672850  #\n",
      "#  2  #  0.001262  #  0.961583  #\n",
      "#  3  #  0.000785  #  0.976367  #\n",
      "#  4  #  0.000575  #  0.982483  #\n",
      "#  5  #  0.000435  #  0.986500  #\n",
      "#  6  #  0.000330  #  0.989950  #\n",
      "#  7  #  0.000251  #  0.992450  #\n",
      "#  8  #  0.000219  #  0.993333  #\n",
      "#  9  #  0.000174  #  0.994683  #\n",
      "#  10  #  0.000160  #  0.994983  #\n",
      "Time elapsed:  110.92861831799999\n"
     ]
    }
   ],
   "source": [
    "from Networks.ResNet import ConvNet\n",
    "from Dataset.Dataset import loadMNIST\n",
    "\n",
    "net = ConvNet(num_conv=3, num_channels=[1,3,6,12], subsample_points=[0,1], num_fc=2,sizes_fc=[588,900,10])\n",
    "train_loader, test_loader = loadMNIST('Dataset/input/mnist_train.csv', 'Dataset/input/mnist_test.csv')\n",
    "#net.train_backprop(1,train_loader)\n",
    "#net.test(test_loader,10000)\n",
    "print(train_loader.batch_size)\n",
    "print(len(train_loader.dataset))\n",
    "\n",
    "net.train(10,train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 0.9812\n"
     ]
    }
   ],
   "source": [
    "net.test(test_loader, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1574, -0.1913,  0.3191],\n",
      "        [ 0.4584,  0.3066, -0.3247],\n",
      "        [ 0.2495,  0.4553,  0.0500],\n",
      "        [ 0.3111, -0.3222,  0.2152]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.4820, -0.5667, -0.2615,  0.5324], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "test_layer = torch.nn.Linear(3, 4)\n",
    "print(test_layer.weight)\n",
    "print(test_layer.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6., 18., 36.], grad_fn=<SumBackward2>)\n",
      "tensor([[1., 1., 1.],\n",
      "        [2., 2., 2.],\n",
      "        [3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([[1,2,3],[2,3,4],[3,4,5]],dtype=torch.float32, requires_grad=True)\n",
    "y = torch.tensor([[1,1,1],[2,2,2],[3,3,3]],dtype=torch.float32)\n",
    "\n",
    "z = torch.sum(x*y, dim=1)\n",
    "print(z)\n",
    "#z.backward(torch.FloatTensor([0,0,1]), retain_graph=True)\n",
    "z.backward(torch.FloatTensor([1,1,1]), retain_graph=True)\n",
    "#z[0].backward(retain_graph=True)\n",
    "#z[1].backward(retain_graph=True)\n",
    "#z[2].backward(retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "tensor([[[[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]],\n",
      "\n",
      "         [[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]],\n",
      "\n",
      "         [[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]]],\n",
      "\n",
      "\n",
      "        [[[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]],\n",
      "\n",
      "         [[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]],\n",
      "\n",
      "         [[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]]],\n",
      "\n",
      "\n",
      "        [[[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]],\n",
      "\n",
      "         [[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]],\n",
      "\n",
      "         [[2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "          [2.5000, 2.5000, 2.5000, 2.5000]]]], dtype=torch.float64)\n",
      "tensor([120., 120., 120.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor((), dtype=torch.float64)\n",
    "y = torch.tensor((), dtype=torch.float64)\n",
    "x = x.new_ones((3,3,4,4))\n",
    "y = y.new_full((3,3,4,4), 2.5)\n",
    "print(x.dim())\n",
    "print(x*y)\n",
    "print(torch.sum(x*y, (1,2,3)))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 7, 9]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([[1,2,3]])\n",
    "y=torch.tensor([[4,5,6]])\n",
    "print(x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.218333333333334\n"
     ]
    }
   ],
   "source": [
    "time =0\n",
    "for i in range(61):\n",
    "    time += i*14.2\n",
    "print(time/60/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6)\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([[1,2,3,0,0,1,3,2]])\n",
    "y=torch.tensor([[1,2,2,1,0,1,3,2]])\n",
    "print(torch.sum(x==y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(120)\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0])\n",
    "b=torch.tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0])\n",
    "c=torch.tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1])\n",
    "d=torch.tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1])\n",
    "e=torch.tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0])\n",
    "f=torch.tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0])\n",
    "\n",
    "print(torch.sum(a==b)+torch.sum(c==d)+torch.sum(e==f))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
