{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:168: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  compared_weight_signs = torch.tensor(torch.ne(new_weight_signs, old_weight_signs).detach(), dtype=torch.float32)\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:173: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_weights = new_weight_signs*torch.tensor(torch.ge(torch.abs(self.m_accumulated),self.rho*max_weight_elem*torch.ones_like(self.m_accumulated)).detach(),dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  1  #  0.016278  #  0.716667  #\n",
      "#  2  #  0.015069  #  0.783333  #\n",
      "#  3  #  0.014245  #  0.893750  #\n",
      "#  4  #  0.013860  #  0.941667  #\n",
      "#  5  #  0.014208  #  0.877083  #\n",
      "#  6  #  0.013830  #  0.935417  #\n",
      "#  7  #  0.013860  #  0.941667  #\n",
      "#  8  #  0.013781  #  0.956250  #\n",
      "#  9  #  0.013667  #  0.958333  #\n",
      "#  10  #  0.013791  #  0.943750  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  11  #  0.013749  #  0.968750  #\n",
      "#  12  #  0.013645  #  0.968750  #\n",
      "#  13  #  0.013690  #  0.962500  #\n",
      "#  14  #  0.013619  #  0.962500  #\n",
      "#  15  #  0.013660  #  0.950000  #\n",
      "#  16  #  0.013584  #  0.968750  #\n",
      "#  17  #  0.013781  #  0.941667  #\n",
      "#  18  #  0.013726  #  0.947917  #\n",
      "#  19  #  0.014052  #  0.887500  #\n",
      "#  20  #  0.013556  #  0.960417  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  21  #  0.013544  #  0.952083  #\n",
      "#  22  #  0.013627  #  0.950000  #\n",
      "#  23  #  0.013513  #  0.956250  #\n",
      "#  24  #  0.013510  #  0.956250  #\n",
      "#  25  #  0.013541  #  0.956250  #\n",
      "#  26  #  0.013494  #  0.958333  #\n",
      "#  27  #  0.013450  #  0.970833  #\n",
      "#  28  #  0.013531  #  0.960417  #\n",
      "#  29  #  0.013435  #  0.964583  #\n",
      "#  30  #  0.013506  #  0.968750  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  31  #  0.013385  #  0.977083  #\n",
      "#  32  #  0.013483  #  0.958333  #\n",
      "#  33  #  0.013629  #  0.958333  #\n",
      "#  34  #  0.013554  #  0.954167  #\n",
      "#  35  #  0.013391  #  0.975000  #\n",
      "#  36  #  0.013501  #  0.960417  #\n",
      "#  37  #  0.013411  #  0.958333  #\n",
      "#  38  #  0.013408  #  0.975000  #\n",
      "#  39  #  0.013421  #  0.966667  #\n",
      "#  40  #  0.013556  #  0.939583  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  41  #  0.013279  #  0.985417  #\n",
      "#  42  #  0.013418  #  0.970833  #\n",
      "#  43  #  0.013484  #  0.962500  #\n",
      "#  44  #  0.013464  #  0.958333  #\n",
      "#  45  #  0.013654  #  0.937500  #\n",
      "#  46  #  0.013473  #  0.975000  #\n",
      "#  47  #  0.013612  #  0.950000  #\n",
      "#  48  #  0.013424  #  0.970833  #\n",
      "#  49  #  0.013430  #  0.979167  #\n",
      "#  50  #  0.013471  #  0.964583  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  51  #  0.013582  #  0.956250  #\n",
      "#  52  #  0.013553  #  0.950000  #\n",
      "#  53  #  0.013472  #  0.952083  #\n",
      "#  54  #  0.013560  #  0.952083  #\n",
      "#  55  #  0.013494  #  0.964583  #\n",
      "#  56  #  0.013467  #  0.966667  #\n",
      "#  57  #  0.013856  #  0.904167  #\n",
      "#  58  #  0.013533  #  0.954167  #\n",
      "#  59  #  0.013413  #  0.975000  #\n",
      "#  60  #  0.013375  #  0.977083  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  61  #  0.013521  #  0.954167  #\n",
      "#  62  #  0.013522  #  0.956250  #\n",
      "#  63  #  0.013442  #  0.964583  #\n",
      "#  64  #  0.013259  #  0.991667  #\n",
      "#  65  #  0.013379  #  0.972917  #\n",
      "#  66  #  0.013315  #  0.981250  #\n",
      "#  67  #  0.013282  #  0.983333  #\n",
      "#  68  #  0.013511  #  0.950000  #\n",
      "#  69  #  0.013418  #  0.964583  #\n",
      "#  70  #  0.013297  #  0.981250  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  71  #  0.013411  #  0.975000  #\n",
      "#  72  #  0.013526  #  0.958333  #\n",
      "#  73  #  0.013384  #  0.977083  #\n",
      "#  74  #  0.013464  #  0.979167  #\n",
      "#  75  #  0.013422  #  0.970833  #\n",
      "#  76  #  0.013456  #  0.975000  #\n",
      "#  77  #  0.013409  #  0.979167  #\n",
      "#  78  #  0.013408  #  0.975000  #\n",
      "#  79  #  0.013432  #  0.960417  #\n",
      "#  80  #  0.013357  #  0.981250  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  81  #  0.013410  #  0.962500  #\n",
      "#  82  #  0.013374  #  0.972917  #\n",
      "#  83  #  0.013561  #  0.956250  #\n",
      "#  84  #  0.013376  #  0.975000  #\n",
      "#  85  #  0.013574  #  0.956250  #\n",
      "#  86  #  0.013448  #  0.968750  #\n",
      "#  87  #  0.013399  #  0.975000  #\n",
      "#  88  #  0.013481  #  0.968750  #\n",
      "#  89  #  0.013492  #  0.966667  #\n",
      "#  90  #  0.013329  #  0.983333  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  91  #  0.013461  #  0.968750  #\n",
      "#  92  #  0.013485  #  0.962500  #\n",
      "#  93  #  0.013394  #  0.970833  #\n",
      "#  94  #  0.013426  #  0.972917  #\n",
      "#  95  #  0.013407  #  0.970833  #\n",
      "#  96  #  0.013447  #  0.968750  #\n",
      "#  97  #  0.013399  #  0.983333  #\n",
      "#  98  #  0.013509  #  0.964583  #\n",
      "#  99  #  0.013440  #  0.970833  #\n",
      "#  100  #  0.013440  #  0.968750  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  101  #  0.013442  #  0.977083  #\n",
      "#  102  #  0.013497  #  0.962500  #\n",
      "#  103  #  0.013464  #  0.962500  #\n",
      "#  104  #  0.013417  #  0.979167  #\n",
      "#  105  #  0.013471  #  0.966667  #\n",
      "#  106  #  0.013515  #  0.962500  #\n",
      "#  107  #  0.013420  #  0.977083  #\n",
      "#  108  #  0.013400  #  0.977083  #\n",
      "#  109  #  0.013547  #  0.962500  #\n",
      "#  110  #  0.013441  #  0.970833  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  111  #  0.013436  #  0.968750  #\n",
      "#  112  #  0.013590  #  0.954167  #\n",
      "#  113  #  0.013529  #  0.954167  #\n",
      "#  114  #  0.013349  #  0.981250  #\n",
      "#  115  #  0.013418  #  0.975000  #\n",
      "#  116  #  0.013404  #  0.983333  #\n",
      "#  117  #  0.013406  #  0.975000  #\n",
      "#  118  #  0.013524  #  0.958333  #\n",
      "#  119  #  0.013491  #  0.962500  #\n",
      "#  120  #  0.013474  #  0.972917  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  121  #  0.013481  #  0.960417  #\n",
      "#  122  #  0.013346  #  0.983333  #\n",
      "#  123  #  0.013407  #  0.983333  #\n",
      "#  124  #  0.013460  #  0.968750  #\n",
      "#  125  #  0.013414  #  0.979167  #\n",
      "#  126  #  0.013514  #  0.970833  #\n",
      "#  127  #  0.013543  #  0.958333  #\n",
      "#  128  #  0.013489  #  0.972917  #\n",
      "#  129  #  0.013421  #  0.979167  #\n",
      "#  130  #  0.013322  #  0.989583  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  131  #  0.013423  #  0.981250  #\n",
      "#  132  #  0.013377  #  0.987500  #\n",
      "#  133  #  0.013414  #  0.972917  #\n",
      "#  134  #  0.013476  #  0.964583  #\n",
      "#  135  #  0.013388  #  0.981250  #\n",
      "#  136  #  0.013405  #  0.972917  #\n",
      "#  137  #  0.013443  #  0.970833  #\n",
      "#  138  #  0.013440  #  0.968750  #\n",
      "#  139  #  0.013361  #  0.983333  #\n",
      "#  140  #  0.013383  #  0.989583  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  141  #  0.013411  #  0.972917  #\n",
      "#  142  #  0.013429  #  0.972917  #\n",
      "#  143  #  0.013451  #  0.966667  #\n",
      "#  144  #  0.013361  #  0.977083  #\n",
      "#  145  #  0.013399  #  0.979167  #\n",
      "#  146  #  0.013479  #  0.966667  #\n",
      "#  147  #  0.013482  #  0.966667  #\n",
      "#  148  #  0.013535  #  0.960417  #\n",
      "#  149  #  0.013365  #  0.977083  #\n",
      "#  150  #  0.013390  #  0.981250  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  151  #  0.013455  #  0.972917  #\n",
      "#  152  #  0.013462  #  0.964583  #\n",
      "#  153  #  0.013410  #  0.983333  #\n",
      "#  154  #  0.013382  #  0.989583  #\n",
      "#  155  #  0.013438  #  0.972917  #\n",
      "#  156  #  0.013481  #  0.966667  #\n",
      "#  157  #  0.013419  #  0.972917  #\n",
      "#  158  #  0.013352  #  0.985417  #\n",
      "#  159  #  0.013362  #  0.987500  #\n",
      "#  160  #  0.013370  #  0.985417  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  161  #  0.013388  #  0.981250  #\n",
      "#  162  #  0.013471  #  0.975000  #\n",
      "#  163  #  0.013452  #  0.968750  #\n",
      "#  164  #  0.013523  #  0.958333  #\n",
      "#  165  #  0.013405  #  0.966667  #\n",
      "#  166  #  0.013322  #  0.987500  #\n",
      "#  167  #  0.013411  #  0.979167  #\n",
      "#  168  #  0.013468  #  0.966667  #\n",
      "#  169  #  0.013455  #  0.966667  #\n",
      "#  170  #  0.013342  #  0.985417  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  171  #  0.013492  #  0.950000  #\n",
      "#  172  #  0.013368  #  0.979167  #\n",
      "#  173  #  0.013463  #  0.968750  #\n",
      "#  174  #  0.013326  #  0.979167  #\n",
      "#  175  #  0.013401  #  0.987500  #\n",
      "#  176  #  0.013470  #  0.966667  #\n",
      "#  177  #  0.013365  #  0.977083  #\n",
      "#  178  #  0.013441  #  0.970833  #\n",
      "#  179  #  0.013490  #  0.954167  #\n",
      "#  180  #  0.013441  #  0.972917  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  181  #  0.013407  #  0.972917  #\n",
      "#  182  #  0.013468  #  0.964583  #\n",
      "#  183  #  0.013450  #  0.966667  #\n",
      "#  184  #  0.013389  #  0.972917  #\n",
      "#  185  #  0.013425  #  0.968750  #\n",
      "#  186  #  0.013346  #  0.977083  #\n",
      "#  187  #  0.013460  #  0.966667  #\n",
      "#  188  #  0.013446  #  0.970833  #\n",
      "#  189  #  0.013437  #  0.968750  #\n",
      "#  190  #  0.013468  #  0.968750  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  191  #  0.013429  #  0.977083  #\n",
      "#  192  #  0.013378  #  0.977083  #\n",
      "#  193  #  0.013422  #  0.975000  #\n",
      "#  194  #  0.013487  #  0.968750  #\n",
      "#  195  #  0.013413  #  0.970833  #\n",
      "#  196  #  0.013423  #  0.964583  #\n",
      "#  197  #  0.013455  #  0.964583  #\n",
      "#  198  #  0.013414  #  0.979167  #\n",
      "#  199  #  0.013380  #  0.975000  #\n",
      "#  200  #  0.013481  #  0.970833  #\n",
      "Time elapsed:  29.856586868\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import ConvNet, FCMSANet\n",
    "import torch\n",
    "#torch.manual_seed(0)\n",
    "train_loader, test_loader = makeMoonsDataset(600,40)\n",
    "#torch.manual_seed(3)\n",
    "\n",
    "#for i in range(5,7):\n",
    "#    for j in range(97,100):\n",
    "#        print('Rho '+str(i/10)+' #############################################')\n",
    "#        print('EMA Alpha '+str(j/100)+' ###############################')\n",
    "#        net = FCNet(num_fc=4,sizes_fc=[2,4,8,16,2], bias=True, batchnorm=True, test=False)\n",
    "#        net.set_rho(i/10)\n",
    "#        net.set_ema_alpha(j/100)\n",
    "#        net.train_msa(150,train_loader)\n",
    "    \n",
    "net = FCMSANet(num_fc=5,sizes_fc=[2,8,16,32,64,2], bias=True, batchnorm=True, test=False)  \n",
    "net.set_rho(0.5)\n",
    "net.set_ema_alpha(0.99)\n",
    "net.train_msa(200,train_loader)\n",
    "\n",
    "#net = ConvNet(3, [1], [3,6], num_fc=2, sizes_fc=[100,10])\n",
    "\n",
    "#train_loader, test_loader = loadMNIST('Dataset/input/mnist_train.csv', 'Dataset/input/mnist_test.csv')\n",
    "\n",
    "#net.train_backprop(1,train_loader)\n",
    "#net.test(test_loader,10000)\n",
    "#print(train_loader.batch_size)\n",
    "\n",
    "#\n",
    "#for index, (data, label) in enumerate(train_loader):\n",
    "#    print(data)\n",
    "#    print(net.forward(data))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for index, (data, label) in enumerate(train_loader):\n",
    "#    print(data)\n",
    "#    print(net.forward(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  1  #  0.017424  #  0.541667  #\n",
      "#  2  #  0.016514  #  0.535417  #\n",
      "#  3  #  0.016395  #  0.543750  #\n",
      "#  4  #  0.016218  #  0.560417  #\n",
      "#  5  #  0.016070  #  0.581250  #\n",
      "#  6  #  0.015739  #  0.600000  #\n",
      "#  7  #  0.015331  #  0.629167  #\n",
      "#  8  #  0.014850  #  0.639583  #\n",
      "#  9  #  0.014170  #  0.672917  #\n",
      "#  10  #  0.013250  #  0.706250  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  11  #  0.012472  #  0.735417  #\n",
      "#  12  #  0.011912  #  0.758333  #\n",
      "#  13  #  0.011361  #  0.762500  #\n",
      "#  14  #  0.010865  #  0.781250  #\n",
      "#  15  #  0.010360  #  0.802083  #\n",
      "#  16  #  0.009967  #  0.802083  #\n",
      "#  17  #  0.009551  #  0.820833  #\n",
      "#  18  #  0.009112  #  0.827083  #\n",
      "#  19  #  0.008774  #  0.843750  #\n",
      "#  20  #  0.008446  #  0.852083  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  21  #  0.008167  #  0.856250  #\n",
      "#  22  #  0.007907  #  0.854167  #\n",
      "#  23  #  0.007609  #  0.860417  #\n",
      "#  24  #  0.007330  #  0.868750  #\n",
      "#  25  #  0.007133  #  0.870833  #\n",
      "#  26  #  0.006938  #  0.877083  #\n",
      "#  27  #  0.006686  #  0.887500  #\n",
      "#  28  #  0.006489  #  0.889583  #\n",
      "#  29  #  0.006282  #  0.893750  #\n",
      "#  30  #  0.006111  #  0.897917  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  31  #  0.005966  #  0.906250  #\n",
      "#  32  #  0.005785  #  0.912500  #\n",
      "#  33  #  0.005615  #  0.912500  #\n",
      "#  34  #  0.005452  #  0.918750  #\n",
      "#  35  #  0.005348  #  0.912500  #\n",
      "#  36  #  0.005172  #  0.914583  #\n",
      "#  37  #  0.005049  #  0.918750  #\n",
      "#  38  #  0.004919  #  0.920833  #\n",
      "#  39  #  0.004819  #  0.922917  #\n",
      "#  40  #  0.004658  #  0.929167  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  41  #  0.004546  #  0.933333  #\n",
      "#  42  #  0.004454  #  0.933333  #\n",
      "#  43  #  0.004342  #  0.935417  #\n",
      "#  44  #  0.004203  #  0.937500  #\n",
      "#  45  #  0.004098  #  0.943750  #\n",
      "#  46  #  0.003995  #  0.947917  #\n",
      "#  47  #  0.003899  #  0.954167  #\n",
      "#  48  #  0.003787  #  0.954167  #\n",
      "#  49  #  0.003671  #  0.960417  #\n",
      "#  50  #  0.003558  #  0.962500  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  51  #  0.003473  #  0.962500  #\n",
      "#  52  #  0.003394  #  0.962500  #\n",
      "#  53  #  0.003331  #  0.962500  #\n",
      "#  54  #  0.003232  #  0.964583  #\n",
      "#  55  #  0.003153  #  0.968750  #\n",
      "#  56  #  0.003077  #  0.968750  #\n",
      "#  57  #  0.003045  #  0.968750  #\n",
      "#  58  #  0.002978  #  0.970833  #\n",
      "#  59  #  0.002917  #  0.972917  #\n",
      "#  60  #  0.002881  #  0.968750  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  61  #  0.002841  #  0.970833  #\n",
      "#  62  #  0.002784  #  0.970833  #\n",
      "#  63  #  0.002734  #  0.975000  #\n",
      "#  64  #  0.002696  #  0.970833  #\n",
      "#  65  #  0.002656  #  0.975000  #\n",
      "#  66  #  0.002603  #  0.977083  #\n",
      "#  67  #  0.002601  #  0.975000  #\n",
      "#  68  #  0.002549  #  0.975000  #\n",
      "#  69  #  0.002526  #  0.972917  #\n",
      "#  70  #  0.002507  #  0.977083  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  71  #  0.002480  #  0.975000  #\n",
      "#  72  #  0.002431  #  0.979167  #\n",
      "#  73  #  0.002412  #  0.972917  #\n",
      "#  74  #  0.002388  #  0.977083  #\n",
      "#  75  #  0.002372  #  0.977083  #\n",
      "#  76  #  0.002353  #  0.977083  #\n",
      "#  77  #  0.002329  #  0.979167  #\n",
      "#  78  #  0.002315  #  0.979167  #\n",
      "#  79  #  0.002295  #  0.977083  #\n",
      "#  80  #  0.002289  #  0.975000  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  81  #  0.002266  #  0.977083  #\n",
      "#  82  #  0.002243  #  0.975000  #\n",
      "#  83  #  0.002226  #  0.977083  #\n",
      "#  84  #  0.002203  #  0.975000  #\n",
      "#  85  #  0.002187  #  0.977083  #\n",
      "#  86  #  0.002184  #  0.981250  #\n",
      "#  87  #  0.002168  #  0.977083  #\n",
      "#  88  #  0.002151  #  0.979167  #\n",
      "#  89  #  0.002131  #  0.977083  #\n",
      "#  90  #  0.002112  #  0.977083  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  91  #  0.002098  #  0.979167  #\n",
      "#  92  #  0.002082  #  0.977083  #\n",
      "#  93  #  0.002083  #  0.981250  #\n",
      "#  94  #  0.002067  #  0.977083  #\n",
      "#  95  #  0.002052  #  0.977083  #\n",
      "#  96  #  0.002034  #  0.981250  #\n",
      "#  97  #  0.002024  #  0.981250  #\n",
      "#  98  #  0.002035  #  0.981250  #\n",
      "#  99  #  0.002051  #  0.979167  #\n",
      "#  100  #  0.002024  #  0.979167  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  101  #  0.001977  #  0.981250  #\n",
      "#  102  #  0.001980  #  0.981250  #\n",
      "#  103  #  0.001953  #  0.979167  #\n",
      "#  104  #  0.001958  #  0.981250  #\n",
      "#  105  #  0.001946  #  0.979167  #\n",
      "#  106  #  0.001957  #  0.981250  #\n",
      "#  107  #  0.001921  #  0.981250  #\n",
      "#  108  #  0.001933  #  0.981250  #\n",
      "#  109  #  0.001911  #  0.981250  #\n",
      "#  110  #  0.001884  #  0.983333  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  111  #  0.001903  #  0.979167  #\n",
      "#  112  #  0.001870  #  0.981250  #\n",
      "#  113  #  0.001875  #  0.979167  #\n",
      "#  114  #  0.001865  #  0.981250  #\n",
      "#  115  #  0.001856  #  0.981250  #\n",
      "#  116  #  0.001846  #  0.983333  #\n",
      "#  117  #  0.001835  #  0.979167  #\n",
      "#  118  #  0.001838  #  0.983333  #\n",
      "#  119  #  0.001836  #  0.983333  #\n",
      "#  120  #  0.001828  #  0.981250  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  121  #  0.001804  #  0.983333  #\n",
      "#  122  #  0.001792  #  0.983333  #\n",
      "#  123  #  0.001810  #  0.981250  #\n",
      "#  124  #  0.001803  #  0.979167  #\n",
      "#  125  #  0.001777  #  0.981250  #\n",
      "#  126  #  0.001775  #  0.983333  #\n",
      "#  127  #  0.001768  #  0.987500  #\n",
      "#  128  #  0.001749  #  0.985417  #\n",
      "#  129  #  0.001737  #  0.985417  #\n",
      "#  130  #  0.001750  #  0.985417  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  131  #  0.001731  #  0.983333  #\n",
      "#  132  #  0.001714  #  0.985417  #\n",
      "#  133  #  0.001715  #  0.987500  #\n",
      "#  134  #  0.001707  #  0.985417  #\n",
      "#  135  #  0.001715  #  0.981250  #\n",
      "#  136  #  0.001690  #  0.985417  #\n",
      "#  137  #  0.001683  #  0.983333  #\n",
      "#  138  #  0.001673  #  0.985417  #\n",
      "#  139  #  0.001687  #  0.985417  #\n",
      "#  140  #  0.001659  #  0.985417  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  141  #  0.001647  #  0.987500  #\n",
      "#  142  #  0.001684  #  0.987500  #\n",
      "#  143  #  0.001647  #  0.985417  #\n",
      "#  144  #  0.001653  #  0.985417  #\n",
      "#  145  #  0.001642  #  0.987500  #\n",
      "#  146  #  0.001643  #  0.985417  #\n",
      "#  147  #  0.001614  #  0.985417  #\n",
      "#  148  #  0.001600  #  0.985417  #\n",
      "#  149  #  0.001622  #  0.985417  #\n",
      "#  150  #  0.001604  #  0.983333  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  151  #  0.001612  #  0.985417  #\n",
      "#  152  #  0.001598  #  0.987500  #\n",
      "#  153  #  0.001572  #  0.985417  #\n",
      "#  154  #  0.001570  #  0.987500  #\n",
      "#  155  #  0.001572  #  0.985417  #\n",
      "#  156  #  0.001571  #  0.987500  #\n",
      "#  157  #  0.001558  #  0.985417  #\n",
      "#  158  #  0.001554  #  0.985417  #\n",
      "#  159  #  0.001556  #  0.985417  #\n",
      "#  160  #  0.001565  #  0.987500  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  161  #  0.001557  #  0.987500  #\n",
      "#  162  #  0.001601  #  0.983333  #\n",
      "#  163  #  0.001531  #  0.987500  #\n",
      "#  164  #  0.001525  #  0.985417  #\n",
      "#  165  #  0.001525  #  0.985417  #\n",
      "#  166  #  0.001510  #  0.985417  #\n",
      "#  167  #  0.001497  #  0.989583  #\n",
      "#  168  #  0.001520  #  0.985417  #\n",
      "#  169  #  0.001504  #  0.987500  #\n",
      "#  170  #  0.001505  #  0.985417  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  171  #  0.001495  #  0.987500  #\n",
      "#  172  #  0.001489  #  0.985417  #\n",
      "#  173  #  0.001483  #  0.985417  #\n",
      "#  174  #  0.001485  #  0.985417  #\n",
      "#  175  #  0.001492  #  0.987500  #\n",
      "#  176  #  0.001487  #  0.987500  #\n",
      "#  177  #  0.001484  #  0.987500  #\n",
      "#  178  #  0.001445  #  0.987500  #\n",
      "#  179  #  0.001451  #  0.987500  #\n",
      "#  180  #  0.001453  #  0.987500  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  181  #  0.001448  #  0.987500  #\n",
      "#  182  #  0.001439  #  0.987500  #\n",
      "#  183  #  0.001439  #  0.987500  #\n",
      "#  184  #  0.001420  #  0.987500  #\n",
      "#  185  #  0.001449  #  0.987500  #\n",
      "#  186  #  0.001441  #  0.987500  #\n",
      "#  187  #  0.001418  #  0.987500  #\n",
      "#  188  #  0.001415  #  0.987500  #\n",
      "#  189  #  0.001407  #  0.987500  #\n",
      "#  190  #  0.001403  #  0.985417  #\n",
      "#  Epoch  #  Avg-Loss  #  Train-Acc  ###############\n",
      "#  191  #  0.001419  #  0.987500  #\n",
      "#  192  #  0.001398  #  0.987500  #\n",
      "#  193  #  0.001391  #  0.985417  #\n",
      "#  194  #  0.001386  #  0.987500  #\n",
      "#  195  #  0.001388  #  0.987500  #\n",
      "#  196  #  0.001355  #  0.985417  #\n",
      "#  197  #  0.001366  #  0.989583  #\n",
      "#  198  #  0.001395  #  0.987500  #\n",
      "#  199  #  0.001375  #  0.987500  #\n",
      "#  200  #  0.001364  #  0.985417  #\n",
      "Time elapsed:  6.89216834299998\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import ConvNet, FCMSANet, ResAntiSymNet\n",
    "import torch\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset(600,40)\n",
    "gamma = 0.1\n",
    "net = ResAntiSymNet(features=2, classes=2, num_layers=5, gamma=gamma, bias=True)\n",
    "net.train(train_loader, num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:340: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:343: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  compared_weight_signs = torch.tensor(torch.ne(new_weight_signs, old_weight_signs).detach(), dtype=torch.float32)\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:148: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_weights = new_weight_signs*torch.tensor(torch.ge(torch.abs(self.m_accumulated),self.rho*max_weight_elem*torch.ones_like(self.m_accumulated)).detach(),dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed:  10.146643088\n",
      "Seed 0   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.60604992\n",
      "Seed 1   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.699145381999998\n",
      "Seed 2   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.273395311000002\n",
      "Seed 3   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.354167834000002\n",
      "Seed 4   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.854717102000002\n",
      "Seed 5   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.360114429000006\n",
      "Seed 6   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.212407759000001\n",
      "Seed 7   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.896629595000007\n",
      "Seed 8   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.603672412999998\n",
      "Seed 9   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.097800613000004\n",
      "Seed 10   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.248547408000007\n",
      "Seed 11   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.193569678999978\n",
      "Seed 12   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.238679597000015\n",
      "Seed 13   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.202587755999986\n",
      "Seed 14   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.563680431999984\n",
      "Seed 15   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.800362409000002\n",
      "Seed 16   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.172943199000002\n",
      "Seed 17   ###   Best-Avg 0.8\n",
      "Time elapsed:  11.664164929999998\n",
      "Seed 18   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.64493668099999\n",
      "Seed 19   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.808216150000021\n",
      "Seed 20   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.073397882000023\n",
      "Seed 21   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.22652783800001\n",
      "Seed 22   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.820626478999998\n",
      "Seed 23   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.476716109999984\n",
      "Seed 24   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.439701024999977\n",
      "Seed 25   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.500485010999967\n",
      "Seed 26   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.450659146999953\n",
      "Seed 27   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.663561942000001\n",
      "Seed 28   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.897770284000046\n",
      "Seed 29   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.072501369000008\n",
      "Seed 30   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.462130296999987\n",
      "Seed 31   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.181674433000012\n",
      "Seed 32   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.720457097999997\n",
      "Seed 33   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.911982378000005\n",
      "Seed 34   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.12446210600001\n",
      "Seed 35   ###   Best-Avg 0.8\n",
      "Time elapsed:  11.751844816999949\n",
      "Seed 36   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.914494053999988\n",
      "Seed 37   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.721946830999968\n",
      "Seed 38   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.854742291999969\n",
      "Seed 39   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.348190395000017\n",
      "Seed 40   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.857938689000036\n",
      "Seed 41   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.154126195999993\n",
      "Seed 42   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.182050207999964\n",
      "Seed 43   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.263982440000007\n",
      "Seed 44   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.456949645000066\n",
      "Seed 45   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.78960939500007\n",
      "Seed 46   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.985499520000076\n",
      "Seed 47   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.699192676000052\n",
      "Seed 48   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.425626183999952\n",
      "Seed 49   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.54046722499993\n",
      "Seed 50   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.030507655000065\n",
      "Seed 51   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.125098506000086\n",
      "Seed 52   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.919794993999972\n",
      "Seed 53   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.273062202999995\n",
      "Seed 54   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.648992066000005\n",
      "Seed 55   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.420492824999997\n",
      "Seed 56   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.824312771999985\n",
      "Seed 57   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.768213887999991\n",
      "Seed 58   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.478139016\n",
      "Seed 59   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.264060061999999\n",
      "Seed 60   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.428517747\n",
      "Seed 61   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.804717488000051\n",
      "Seed 62   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.830327735999958\n",
      "Seed 63   ###   Best-Avg 0.84375\n",
      "Time elapsed:  13.149567551000018\n",
      "Seed 64   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.077774036999926\n",
      "Seed 65   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.304072090999966\n",
      "Seed 66   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.241903238999953\n",
      "Seed 67   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.745793354000057\n",
      "Seed 68   ###   Best-Avg 0.84375\n",
      "Time elapsed:  13.113821154999982\n",
      "Seed 69   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.56083924699999\n",
      "Seed 70   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.023275922000039\n",
      "Seed 71   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.34733552099999\n",
      "Seed 72   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.205158547999986\n",
      "Seed 73   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.781024666000008\n",
      "Seed 74   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.818582595000066\n",
      "Seed 75   ###   Best-Avg 0.8\n",
      "Time elapsed:  12.775833219999981\n",
      "Seed 76   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.596671489999949\n",
      "Seed 77   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.738887018000014\n",
      "Seed 78   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.079107496999995\n",
      "Seed 79   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.085618524999973\n",
      "Seed 80   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.447313674000043\n",
      "Seed 81   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.547355056000015\n",
      "Seed 82   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.684601208999993\n",
      "Seed 83   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.680478483000002\n",
      "Seed 84   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.97397130999991\n",
      "Seed 85   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.147157143999948\n",
      "Seed 86   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.294491636999965\n",
      "Seed 87   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.387910954999938\n",
      "Seed 88   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.786750217999952\n",
      "Seed 89   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.385938010000018\n",
      "Seed 90   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.857001564999791\n",
      "Seed 91   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.818907478000028\n",
      "Seed 92   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.13523568200003\n",
      "Seed 93   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.777255096999852\n",
      "Seed 94   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.35629602400013\n",
      "Seed 95   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.564049009999962\n",
      "Seed 96   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.523687422999956\n",
      "Seed 97   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.850773782000033\n",
      "Seed 98   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.6953454840002\n",
      "Seed 99   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.015339932000188\n",
      "Seed 100   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.39123638000001\n",
      "Seed 101   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.594417870000143\n",
      "Seed 102   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.783619105999833\n",
      "Seed 103   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.876350617000071\n",
      "Seed 104   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.439500544000111\n",
      "Seed 105   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.181604522000043\n",
      "Seed 106   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.076896543999965\n",
      "Seed 107   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.832593179000014\n",
      "Seed 108   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.307717772999922\n",
      "Seed 109   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.43920239099998\n",
      "Seed 110   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.498897093999858\n",
      "Seed 111   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.657795781000004\n",
      "Seed 112   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.814789377999887\n",
      "Seed 113   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.931495412999993\n",
      "Seed 114   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.288754776999895\n",
      "Seed 115   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.244204666000087\n",
      "Seed 116   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.550813107000067\n",
      "Seed 117   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.388158215999965\n",
      "Seed 118   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.750045622000016\n",
      "Seed 119   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.235819392000167\n",
      "Seed 120   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.996203699000034\n",
      "Seed 121   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.989480880999963\n",
      "Seed 122   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.056708553999897\n",
      "Seed 123   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.285523938999859\n",
      "Seed 124   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.525031678000005\n",
      "Seed 125   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.853770211999972\n",
      "Seed 126   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.874691244999894\n",
      "Seed 127   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.638378874000182\n",
      "Seed 128   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.042981213999838\n",
      "Seed 129   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.147106766999968\n",
      "Seed 130   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.252185378999911\n",
      "Seed 131   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.136590218000038\n",
      "Seed 132   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.631353791000038\n",
      "Seed 133   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.084455215999924\n",
      "Seed 134   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.278537409000137\n",
      "Seed 135   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.179119578000154\n",
      "Seed 136   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.569564312000011\n",
      "Seed 137   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.627897280999832\n",
      "Seed 138   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.784591183999964\n",
      "Seed 139   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.15707995899993\n",
      "Seed 140   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.181525357000055\n",
      "Seed 141   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.311887792000107\n",
      "Seed 142   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.489734566999914\n",
      "Seed 143   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.879751607999879\n",
      "Seed 144   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.325272771000073\n",
      "Seed 145   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.095041162999905\n",
      "Seed 146   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.463122424999938\n",
      "Seed 147   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.492982883999957\n",
      "Seed 148   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.48772615300004\n",
      "Seed 149   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.673118235000175\n",
      "Seed 150   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.028434469000103\n",
      "Seed 151   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.975339212000108\n",
      "Seed 152   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.189310215000205\n",
      "Seed 153   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.902211723999926\n",
      "Seed 154   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.252953891999823\n",
      "Seed 155   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.101709491000065\n",
      "Seed 156   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.322991391999949\n",
      "Seed 157   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.231441695000058\n",
      "Seed 158   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.062093285999936\n",
      "Seed 159   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.669411380999918\n",
      "Seed 160   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.712783790999993\n",
      "Seed 161   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.289970005000214\n",
      "Seed 162   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.054718644999866\n",
      "Seed 163   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.402621168999985\n",
      "Seed 164   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.74273369599996\n",
      "Seed 165   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.994736071000034\n",
      "Seed 166   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.28945646399984\n",
      "Seed 167   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.997000486000161\n",
      "Seed 168   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.741073809999989\n",
      "Seed 169   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.930781389999993\n",
      "Seed 170   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.058178237999982\n",
      "Seed 171   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.299073929000087\n",
      "Seed 172   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.528790453000056\n",
      "Seed 173   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.644680681000182\n",
      "Seed 174   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.82024248000016\n",
      "Seed 175   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.990435996000087\n",
      "Seed 176   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.8661229679999\n",
      "Seed 177   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.123140467999974\n",
      "Seed 178   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.590671946999919\n",
      "Seed 179   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.689152142000012\n",
      "Seed 180   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.67604732399991\n",
      "Seed 181   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.032750995000015\n",
      "Seed 182   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.450169252000023\n",
      "Seed 183   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.201859338999839\n",
      "Seed 184   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.017197729000145\n",
      "Seed 185   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.891306548999637\n",
      "Seed 186   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.39308081199988\n",
      "Seed 187   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.244145035999736\n",
      "Seed 188   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.604870163000214\n",
      "Seed 189   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.960879856999782\n",
      "Seed 190   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.77327322300016\n",
      "Seed 191   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.03098315699981\n",
      "Seed 192   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.10578955100027\n",
      "Seed 193   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.368450354000288\n",
      "Seed 194   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.467802901000141\n",
      "Seed 195   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.05645203999984\n",
      "Seed 196   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.349908370000321\n",
      "Seed 197   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.354014645000007\n",
      "Seed 198   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.494870496999738\n",
      "Seed 199   ###   Best-Avg 0.825\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import ConvNet, FCMSANet\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset()\n",
    "\n",
    "for seed in range(200):\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    net = FCMSANet(num_fc=2,sizes_fc=[2,4,2], bias=False, test=False)\n",
    "    \n",
    "    net.train_msa(60,train_loader)\n",
    "    print('Seed '+str(seed)+'   ###   Best-Avg '+str(net.best_avg))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  ###   Avg-Loss 0.016693341732025146   ###   Correct predictions 0.7354166666666667\n",
      "Epoch 2  ###   Avg-Loss 0.01539247731367747   ###   Correct predictions 0.7895833333333333\n",
      "Epoch 3  ###   Avg-Loss 0.013269580403963725   ###   Correct predictions 0.7958333333333333\n",
      "Epoch 4  ###   Avg-Loss 0.010812340180079143   ###   Correct predictions 0.8125\n",
      "Epoch 5  ###   Avg-Loss 0.009092676639556884   ###   Correct predictions 0.8375\n",
      "Epoch 6  ###   Avg-Loss 0.007869457205136618   ###   Correct predictions 0.8583333333333333\n",
      "Epoch 7  ###   Avg-Loss 0.007090519865353902   ###   Correct predictions 0.8625\n",
      "Epoch 8  ###   Avg-Loss 0.00648987740278244   ###   Correct predictions 0.8916666666666667\n",
      "Epoch 9  ###   Avg-Loss 0.006088708837827046   ###   Correct predictions 0.8979166666666667\n",
      "Epoch 10  ###   Avg-Loss 0.005733463664849599   ###   Correct predictions 0.9\n",
      "Epoch 11  ###   Avg-Loss 0.005439147353172302   ###   Correct predictions 0.9041666666666667\n",
      "Epoch 12  ###   Avg-Loss 0.005182546377182007   ###   Correct predictions 0.9125\n",
      "Epoch 13  ###   Avg-Loss 0.0049934898813565574   ###   Correct predictions 0.9104166666666667\n",
      "Epoch 14  ###   Avg-Loss 0.004753189285596212   ###   Correct predictions 0.91875\n",
      "Epoch 15  ###   Avg-Loss 0.004416432480017344   ###   Correct predictions 0.9229166666666667\n",
      "Epoch 16  ###   Avg-Loss 0.004125663638114929   ###   Correct predictions 0.93125\n",
      "Epoch 17  ###   Avg-Loss 0.003898448000351588   ###   Correct predictions 0.93125\n",
      "Epoch 18  ###   Avg-Loss 0.003615226596593857   ###   Correct predictions 0.9375\n",
      "Epoch 19  ###   Avg-Loss 0.0030299144486586253   ###   Correct predictions 0.95\n",
      "Epoch 20  ###   Avg-Loss 0.0029660664498806   ###   Correct predictions 0.95625\n",
      "Epoch 21  ###   Avg-Loss 0.00286577045917511   ###   Correct predictions 0.9479166666666666\n",
      "Epoch 22  ###   Avg-Loss 0.002026676634947459   ###   Correct predictions 0.9770833333333333\n",
      "Epoch 23  ###   Avg-Loss 0.00176669346789519   ###   Correct predictions 0.9791666666666666\n",
      "Epoch 24  ###   Avg-Loss 0.0016901899129152299   ###   Correct predictions 0.98125\n",
      "Epoch 25  ###   Avg-Loss 0.001406506821513176   ###   Correct predictions 0.9854166666666667\n",
      "Epoch 26  ###   Avg-Loss 0.0012301721920569737   ###   Correct predictions 0.9895833333333334\n",
      "Epoch 27  ###   Avg-Loss 0.0012656405568122863   ###   Correct predictions 0.9895833333333334\n",
      "Epoch 28  ###   Avg-Loss 0.0009948708117008208   ###   Correct predictions 0.9916666666666667\n",
      "Epoch 29  ###   Avg-Loss 0.0008995652198791504   ###   Correct predictions 0.99375\n",
      "Epoch 30  ###   Avg-Loss 0.0008839219808578491   ###   Correct predictions 0.99375\n",
      "Epoch 31  ###   Avg-Loss 0.0007204620788494746   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 32  ###   Avg-Loss 0.0006353583186864853   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 33  ###   Avg-Loss 0.0006963463500142097   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 34  ###   Avg-Loss 0.0006097466374437014   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 35  ###   Avg-Loss 0.0006425640856226285   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 36  ###   Avg-Loss 0.0004952810704708099   ###   Correct predictions 1.0\n",
      "Epoch 37  ###   Avg-Loss 0.00046397863576809565   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 38  ###   Avg-Loss 0.00047715206940968834   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 39  ###   Avg-Loss 0.0004436716747780641   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 40  ###   Avg-Loss 0.00044246241450309756   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 41  ###   Avg-Loss 0.0003799566999077797   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 42  ###   Avg-Loss 0.00037729634592930473   ###   Correct predictions 1.0\n",
      "Epoch 43  ###   Avg-Loss 0.00042406826590498287   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 44  ###   Avg-Loss 0.0003210838573674361   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 45  ###   Avg-Loss 0.00035742980738480884   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 46  ###   Avg-Loss 0.0003395354375243187   ###   Correct predictions 1.0\n",
      "Epoch 47  ###   Avg-Loss 0.0003530730493366718   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 48  ###   Avg-Loss 0.0003453268048663934   ###   Correct predictions 0.9958333333333333\n",
      "Epoch 49  ###   Avg-Loss 0.0003007656273742517   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 50  ###   Avg-Loss 0.0002979370454947154   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 51  ###   Avg-Loss 0.0002575600054115057   ###   Correct predictions 1.0\n",
      "Epoch 52  ###   Avg-Loss 0.0003250787034630775   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 53  ###   Avg-Loss 0.00023143263533711433   ###   Correct predictions 1.0\n",
      "Epoch 54  ###   Avg-Loss 0.0002476739386717478   ###   Correct predictions 1.0\n",
      "Epoch 55  ###   Avg-Loss 0.0002808337099850178   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 56  ###   Avg-Loss 0.00022636189435919126   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 57  ###   Avg-Loss 0.00020725494250655173   ###   Correct predictions 1.0\n",
      "Epoch 58  ###   Avg-Loss 0.0002392620158692201   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 59  ###   Avg-Loss 0.0002071976816902558   ###   Correct predictions 1.0\n",
      "Epoch 60  ###   Avg-Loss 0.00020230164130528768   ###   Correct predictions 1.0\n",
      "Epoch 61  ###   Avg-Loss 0.00018292929356296858   ###   Correct predictions 1.0\n",
      "Epoch 62  ###   Avg-Loss 0.00022179240671296914   ###   Correct predictions 1.0\n",
      "Epoch 63  ###   Avg-Loss 0.00019362818760176498   ###   Correct predictions 1.0\n",
      "Epoch 64  ###   Avg-Loss 0.00022219737681249778   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 65  ###   Avg-Loss 0.00018464084714651108   ###   Correct predictions 1.0\n",
      "Epoch 66  ###   Avg-Loss 0.00021055651207764944   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 67  ###   Avg-Loss 0.00020628821415205796   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 68  ###   Avg-Loss 0.00019506152408818405   ###   Correct predictions 1.0\n",
      "Epoch 69  ###   Avg-Loss 0.0001632713247090578   ###   Correct predictions 1.0\n",
      "Epoch 70  ###   Avg-Loss 0.00017592293831209343   ###   Correct predictions 1.0\n",
      "Epoch 71  ###   Avg-Loss 0.00016277733569343886   ###   Correct predictions 1.0\n",
      "Epoch 72  ###   Avg-Loss 0.0001429748721420765   ###   Correct predictions 1.0\n",
      "Epoch 73  ###   Avg-Loss 0.0001682426780462265   ###   Correct predictions 1.0\n",
      "Epoch 74  ###   Avg-Loss 0.00015255645848810673   ###   Correct predictions 1.0\n",
      "Epoch 75  ###   Avg-Loss 0.00013346169143915176   ###   Correct predictions 1.0\n",
      "Epoch 76  ###   Avg-Loss 0.00015274204003314177   ###   Correct predictions 1.0\n",
      "Epoch 77  ###   Avg-Loss 0.0001632209246357282   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 78  ###   Avg-Loss 0.00016444246284663677   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 79  ###   Avg-Loss 0.0001559902292986711   ###   Correct predictions 1.0\n",
      "Epoch 80  ###   Avg-Loss 0.00012979219512393076   ###   Correct predictions 1.0\n",
      "Epoch 81  ###   Avg-Loss 0.00014563739920655887   ###   Correct predictions 1.0\n",
      "Epoch 82  ###   Avg-Loss 0.00012268397646645704   ###   Correct predictions 1.0\n",
      "Epoch 83  ###   Avg-Loss 0.00014513544738292694   ###   Correct predictions 1.0\n",
      "Epoch 84  ###   Avg-Loss 0.00012508279954393705   ###   Correct predictions 1.0\n",
      "Epoch 85  ###   Avg-Loss 0.00013901602166394394   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 86  ###   Avg-Loss 0.0001333181746304035   ###   Correct predictions 1.0\n",
      "Epoch 87  ###   Avg-Loss 0.00012044358688096205   ###   Correct predictions 1.0\n",
      "Epoch 88  ###   Avg-Loss 0.00014618256439765295   ###   Correct predictions 1.0\n",
      "Epoch 89  ###   Avg-Loss 0.00012452843754241864   ###   Correct predictions 1.0\n",
      "Epoch 90  ###   Avg-Loss 0.00011009617398182551   ###   Correct predictions 1.0\n",
      "Epoch 91  ###   Avg-Loss 0.00012323612657686074   ###   Correct predictions 1.0\n",
      "Epoch 92  ###   Avg-Loss 0.00011581191793084145   ###   Correct predictions 0.9979166666666667\n",
      "Epoch 93  ###   Avg-Loss 0.0001325206986318032   ###   Correct predictions 1.0\n",
      "Epoch 94  ###   Avg-Loss 0.00010437506716698408   ###   Correct predictions 1.0\n",
      "Epoch 95  ###   Avg-Loss 0.00010432829149067401   ###   Correct predictions 1.0\n",
      "Epoch 96  ###   Avg-Loss 0.00011434933015455803   ###   Correct predictions 1.0\n",
      "Epoch 97  ###   Avg-Loss 0.00010554267403980096   ###   Correct predictions 1.0\n",
      "Epoch 98  ###   Avg-Loss 0.00013531527171532312   ###   Correct predictions 1.0\n",
      "Epoch 99  ###   Avg-Loss 9.843817291160425e-05   ###   Correct predictions 1.0\n",
      "Epoch 100  ###   Avg-Loss 9.126464525858562e-05   ###   Correct predictions 1.0\n",
      "Epoch 101  ###   Avg-Loss 8.715117195000251e-05   ###   Correct predictions 1.0\n",
      "Epoch 102  ###   Avg-Loss 9.70254031320413e-05   ###   Correct predictions 1.0\n",
      "Epoch 103  ###   Avg-Loss 0.00010043517686426639   ###   Correct predictions 1.0\n",
      "Epoch 104  ###   Avg-Loss 8.699353784322739e-05   ###   Correct predictions 1.0\n",
      "Epoch 105  ###   Avg-Loss 9.705725436409315e-05   ###   Correct predictions 1.0\n",
      "Epoch 106  ###   Avg-Loss 8.577681922664246e-05   ###   Correct predictions 1.0\n",
      "Epoch 107  ###   Avg-Loss 0.00010882464703172445   ###   Correct predictions 1.0\n",
      "Epoch 108  ###   Avg-Loss 0.00010686044115573167   ###   Correct predictions 1.0\n",
      "Epoch 109  ###   Avg-Loss 0.00010626811999827623   ###   Correct predictions 1.0\n",
      "Epoch 110  ###   Avg-Loss 0.00010530042927712203   ###   Correct predictions 1.0\n",
      "Epoch 111  ###   Avg-Loss 0.00010232297548403342   ###   Correct predictions 1.0\n",
      "Epoch 112  ###   Avg-Loss 8.531966401884954e-05   ###   Correct predictions 1.0\n",
      "Epoch 113  ###   Avg-Loss 8.699455453703801e-05   ###   Correct predictions 1.0\n",
      "Epoch 114  ###   Avg-Loss 8.856581213573615e-05   ###   Correct predictions 1.0\n",
      "Epoch 115  ###   Avg-Loss 7.67768050233523e-05   ###   Correct predictions 1.0\n",
      "Epoch 116  ###   Avg-Loss 8.174949325621129e-05   ###   Correct predictions 1.0\n",
      "Epoch 117  ###   Avg-Loss 7.995864531646172e-05   ###   Correct predictions 1.0\n",
      "Epoch 118  ###   Avg-Loss 7.204515859484672e-05   ###   Correct predictions 1.0\n",
      "Epoch 119  ###   Avg-Loss 8.846645553906758e-05   ###   Correct predictions 1.0\n",
      "Epoch 120  ###   Avg-Loss 8.003233621517817e-05   ###   Correct predictions 1.0\n",
      "Epoch 121  ###   Avg-Loss 7.330861408263444e-05   ###   Correct predictions 1.0\n",
      "Epoch 122  ###   Avg-Loss 6.652111187577248e-05   ###   Correct predictions 1.0\n",
      "Epoch 123  ###   Avg-Loss 7.577384822070599e-05   ###   Correct predictions 1.0\n",
      "Epoch 124  ###   Avg-Loss 6.504503932471076e-05   ###   Correct predictions 1.0\n",
      "Epoch 125  ###   Avg-Loss 9.361816725383202e-05   ###   Correct predictions 1.0\n",
      "Epoch 126  ###   Avg-Loss 6.556252483278513e-05   ###   Correct predictions 1.0\n",
      "Epoch 127  ###   Avg-Loss 6.442156542713443e-05   ###   Correct predictions 1.0\n",
      "Epoch 128  ###   Avg-Loss 7.758416080226501e-05   ###   Correct predictions 1.0\n",
      "Epoch 129  ###   Avg-Loss 7.533366636683544e-05   ###   Correct predictions 1.0\n",
      "Epoch 130  ###   Avg-Loss 6.837865803390741e-05   ###   Correct predictions 1.0\n",
      "Epoch 131  ###   Avg-Loss 7.118641709287961e-05   ###   Correct predictions 1.0\n",
      "Epoch 132  ###   Avg-Loss 8.623798688252766e-05   ###   Correct predictions 1.0\n",
      "Epoch 133  ###   Avg-Loss 6.864879590769608e-05   ###   Correct predictions 1.0\n",
      "Epoch 134  ###   Avg-Loss 7.340631758173307e-05   ###   Correct predictions 1.0\n",
      "Epoch 135  ###   Avg-Loss 5.6213835099091135e-05   ###   Correct predictions 1.0\n",
      "Epoch 136  ###   Avg-Loss 5.9551175218075515e-05   ###   Correct predictions 1.0\n",
      "Epoch 137  ###   Avg-Loss 7.160236903776725e-05   ###   Correct predictions 1.0\n",
      "Epoch 138  ###   Avg-Loss 6.109048845246435e-05   ###   Correct predictions 1.0\n",
      "Epoch 139  ###   Avg-Loss 7.094782777130603e-05   ###   Correct predictions 1.0\n",
      "Epoch 140  ###   Avg-Loss 7.791101622084776e-05   ###   Correct predictions 1.0\n",
      "Epoch 141  ###   Avg-Loss 6.113395793363451e-05   ###   Correct predictions 1.0\n",
      "Epoch 142  ###   Avg-Loss 6.176692356045048e-05   ###   Correct predictions 1.0\n",
      "Epoch 143  ###   Avg-Loss 7.38037284463644e-05   ###   Correct predictions 1.0\n",
      "Epoch 144  ###   Avg-Loss 6.984003509084384e-05   ###   Correct predictions 1.0\n",
      "Epoch 145  ###   Avg-Loss 6.579244509339333e-05   ###   Correct predictions 1.0\n",
      "Epoch 146  ###   Avg-Loss 5.51832839846611e-05   ###   Correct predictions 1.0\n",
      "Epoch 147  ###   Avg-Loss 5.711079575121403e-05   ###   Correct predictions 1.0\n",
      "Epoch 148  ###   Avg-Loss 5.0937927638490996e-05   ###   Correct predictions 1.0\n",
      "Epoch 149  ###   Avg-Loss 5.608038045465946e-05   ###   Correct predictions 1.0\n",
      "Epoch 150  ###   Avg-Loss 6.162119098007679e-05   ###   Correct predictions 1.0\n",
      "Epoch 151  ###   Avg-Loss 5.776884887988369e-05   ###   Correct predictions 1.0\n",
      "Epoch 152  ###   Avg-Loss 6.204020076741776e-05   ###   Correct predictions 1.0\n",
      "Epoch 153  ###   Avg-Loss 5.5288333290567e-05   ###   Correct predictions 1.0\n",
      "Epoch 154  ###   Avg-Loss 4.973360725368063e-05   ###   Correct predictions 1.0\n",
      "Epoch 155  ###   Avg-Loss 5.3552817553281785e-05   ###   Correct predictions 1.0\n",
      "Epoch 156  ###   Avg-Loss 4.901509576787551e-05   ###   Correct predictions 1.0\n",
      "Epoch 157  ###   Avg-Loss 5.7395625238617264e-05   ###   Correct predictions 1.0\n",
      "Epoch 158  ###   Avg-Loss 4.948605395232638e-05   ###   Correct predictions 1.0\n",
      "Epoch 159  ###   Avg-Loss 4.963681955511371e-05   ###   Correct predictions 1.0\n",
      "Epoch 160  ###   Avg-Loss 5.433883440370361e-05   ###   Correct predictions 1.0\n",
      "Epoch 161  ###   Avg-Loss 5.3806684445589784e-05   ###   Correct predictions 1.0\n",
      "Epoch 162  ###   Avg-Loss 4.746093569944302e-05   ###   Correct predictions 1.0\n",
      "Epoch 163  ###   Avg-Loss 7.585322794814905e-05   ###   Correct predictions 1.0\n",
      "Epoch 164  ###   Avg-Loss 4.612901248037815e-05   ###   Correct predictions 1.0\n",
      "Epoch 165  ###   Avg-Loss 4.46078289921085e-05   ###   Correct predictions 1.0\n",
      "Epoch 166  ###   Avg-Loss 4.7858059406280515e-05   ###   Correct predictions 1.0\n",
      "Epoch 167  ###   Avg-Loss 5.252285239597161e-05   ###   Correct predictions 1.0\n",
      "Epoch 168  ###   Avg-Loss 6.123439331228535e-05   ###   Correct predictions 1.0\n",
      "Epoch 169  ###   Avg-Loss 5.4478478462745744e-05   ###   Correct predictions 1.0\n",
      "Epoch 170  ###   Avg-Loss 5.318818924327691e-05   ###   Correct predictions 1.0\n",
      "Epoch 171  ###   Avg-Loss 4.756053676828742e-05   ###   Correct predictions 1.0\n",
      "Epoch 172  ###   Avg-Loss 4.761051774645845e-05   ###   Correct predictions 1.0\n",
      "Epoch 173  ###   Avg-Loss 5.6479397850732006e-05   ###   Correct predictions 1.0\n",
      "Epoch 174  ###   Avg-Loss 4.788977870096763e-05   ###   Correct predictions 1.0\n",
      "Epoch 175  ###   Avg-Loss 6.446146095792452e-05   ###   Correct predictions 1.0\n",
      "Epoch 176  ###   Avg-Loss 4.3726297250638405e-05   ###   Correct predictions 1.0\n",
      "Epoch 177  ###   Avg-Loss 5.430232267826796e-05   ###   Correct predictions 1.0\n",
      "Epoch 178  ###   Avg-Loss 5.736845002199213e-05   ###   Correct predictions 1.0\n",
      "Epoch 179  ###   Avg-Loss 5.051289529850086e-05   ###   Correct predictions 1.0\n",
      "Epoch 180  ###   Avg-Loss 5.14243069725732e-05   ###   Correct predictions 1.0\n",
      "Epoch 181  ###   Avg-Loss 3.913740317026774e-05   ###   Correct predictions 1.0\n",
      "Epoch 182  ###   Avg-Loss 5.9283819670478506e-05   ###   Correct predictions 1.0\n",
      "Epoch 183  ###   Avg-Loss 4.4025426420072714e-05   ###   Correct predictions 1.0\n",
      "Epoch 184  ###   Avg-Loss 4.923796902100245e-05   ###   Correct predictions 1.0\n",
      "Epoch 185  ###   Avg-Loss 4.7920217427114645e-05   ###   Correct predictions 1.0\n",
      "Epoch 186  ###   Avg-Loss 4.310249350965023e-05   ###   Correct predictions 1.0\n",
      "Epoch 187  ###   Avg-Loss 4.640198312699795e-05   ###   Correct predictions 1.0\n",
      "Epoch 188  ###   Avg-Loss 4.455156158655882e-05   ###   Correct predictions 1.0\n",
      "Epoch 189  ###   Avg-Loss 4.795944939057032e-05   ###   Correct predictions 1.0\n",
      "Epoch 190  ###   Avg-Loss 3.5239538798729576e-05   ###   Correct predictions 1.0\n",
      "Epoch 191  ###   Avg-Loss 4.360287372643749e-05   ###   Correct predictions 1.0\n",
      "Epoch 192  ###   Avg-Loss 4.576954136913021e-05   ###   Correct predictions 1.0\n",
      "Epoch 193  ###   Avg-Loss 3.716207187001904e-05   ###   Correct predictions 1.0\n",
      "Epoch 194  ###   Avg-Loss 4.478615010157227e-05   ###   Correct predictions 1.0\n",
      "Epoch 195  ###   Avg-Loss 4.2566253493229546e-05   ###   Correct predictions 1.0\n",
      "Epoch 196  ###   Avg-Loss 4.4678539658586185e-05   ###   Correct predictions 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197  ###   Avg-Loss 3.961453524728616e-05   ###   Correct predictions 1.0\n",
      "Epoch 198  ###   Avg-Loss 4.014573448027174e-05   ###   Correct predictions 1.0\n",
      "Epoch 199  ###   Avg-Loss 4.041891467447082e-05   ###   Correct predictions 1.0\n",
      "Epoch 200  ###   Avg-Loss 3.8891894898066916e-05   ###   Correct predictions 1.0\n",
      "Epoch 201  ###   Avg-Loss 3.913562589635452e-05   ###   Correct predictions 1.0\n",
      "Epoch 202  ###   Avg-Loss 4.7289044596254824e-05   ###   Correct predictions 1.0\n",
      "Epoch 203  ###   Avg-Loss 3.683992739145954e-05   ###   Correct predictions 1.0\n",
      "Epoch 204  ###   Avg-Loss 3.6930983575681846e-05   ###   Correct predictions 1.0\n",
      "Epoch 205  ###   Avg-Loss 3.837041634445389e-05   ###   Correct predictions 1.0\n",
      "Epoch 206  ###   Avg-Loss 3.6403711419552565e-05   ###   Correct predictions 1.0\n",
      "Epoch 207  ###   Avg-Loss 3.761501672367255e-05   ###   Correct predictions 1.0\n",
      "Epoch 208  ###   Avg-Loss 3.68371062601606e-05   ###   Correct predictions 1.0\n",
      "Epoch 209  ###   Avg-Loss 4.079292993992567e-05   ###   Correct predictions 1.0\n",
      "Epoch 210  ###   Avg-Loss 3.946068463847041e-05   ###   Correct predictions 1.0\n",
      "Epoch 211  ###   Avg-Loss 4.385570452238123e-05   ###   Correct predictions 1.0\n",
      "Epoch 212  ###   Avg-Loss 4.163978931804498e-05   ###   Correct predictions 1.0\n",
      "Epoch 213  ###   Avg-Loss 3.332675745089849e-05   ###   Correct predictions 1.0\n",
      "Epoch 214  ###   Avg-Loss 4.257323065151771e-05   ###   Correct predictions 1.0\n",
      "Epoch 215  ###   Avg-Loss 4.2936753015965226e-05   ###   Correct predictions 1.0\n",
      "Epoch 216  ###   Avg-Loss 3.636674955487251e-05   ###   Correct predictions 1.0\n",
      "Epoch 217  ###   Avg-Loss 3.94138313519458e-05   ###   Correct predictions 1.0\n",
      "Epoch 218  ###   Avg-Loss 3.3846831259628135e-05   ###   Correct predictions 1.0\n",
      "Epoch 219  ###   Avg-Loss 3.627579426392913e-05   ###   Correct predictions 1.0\n",
      "Epoch 220  ###   Avg-Loss 4.044672629485528e-05   ###   Correct predictions 1.0\n",
      "Epoch 221  ###   Avg-Loss 3.899487977226575e-05   ###   Correct predictions 1.0\n",
      "Epoch 222  ###   Avg-Loss 3.2388655624041956e-05   ###   Correct predictions 1.0\n",
      "Epoch 223  ###   Avg-Loss 3.148379425207774e-05   ###   Correct predictions 1.0\n",
      "Epoch 224  ###   Avg-Loss 4.479126849522193e-05   ###   Correct predictions 1.0\n",
      "Epoch 225  ###   Avg-Loss 3.618796666463216e-05   ###   Correct predictions 1.0\n",
      "Epoch 226  ###   Avg-Loss 3.777043893933296e-05   ###   Correct predictions 1.0\n",
      "Epoch 227  ###   Avg-Loss 3.232196516667803e-05   ###   Correct predictions 1.0\n",
      "Epoch 228  ###   Avg-Loss 3.010243138608833e-05   ###   Correct predictions 1.0\n",
      "Epoch 229  ###   Avg-Loss 3.525512292981148e-05   ###   Correct predictions 1.0\n",
      "Epoch 230  ###   Avg-Loss 3.3884646836668256e-05   ###   Correct predictions 1.0\n",
      "Epoch 231  ###   Avg-Loss 3.234480197230975e-05   ###   Correct predictions 1.0\n",
      "Epoch 232  ###   Avg-Loss 3.662161761894822e-05   ###   Correct predictions 1.0\n",
      "Epoch 233  ###   Avg-Loss 3.697317248831193e-05   ###   Correct predictions 1.0\n",
      "Epoch 234  ###   Avg-Loss 2.878650751275321e-05   ###   Correct predictions 1.0\n",
      "Epoch 235  ###   Avg-Loss 3.556391845146815e-05   ###   Correct predictions 1.0\n",
      "Epoch 236  ###   Avg-Loss 2.7765481111903984e-05   ###   Correct predictions 1.0\n",
      "Epoch 237  ###   Avg-Loss 3.6760074241707726e-05   ###   Correct predictions 1.0\n",
      "Epoch 238  ###   Avg-Loss 3.500080201774835e-05   ###   Correct predictions 1.0\n",
      "Epoch 239  ###   Avg-Loss 3.9084527331093946e-05   ###   Correct predictions 1.0\n",
      "Epoch 240  ###   Avg-Loss 3.469903022050858e-05   ###   Correct predictions 1.0\n",
      "Epoch 241  ###   Avg-Loss 3.173545701429248e-05   ###   Correct predictions 1.0\n",
      "Epoch 242  ###   Avg-Loss 3.360264624158541e-05   ###   Correct predictions 1.0\n",
      "Epoch 243  ###   Avg-Loss 3.289517092828949e-05   ###   Correct predictions 1.0\n",
      "Epoch 244  ###   Avg-Loss 2.8689632502694926e-05   ###   Correct predictions 1.0\n",
      "Epoch 245  ###   Avg-Loss 3.048427946244677e-05   ###   Correct predictions 1.0\n",
      "Epoch 246  ###   Avg-Loss 3.113332592571775e-05   ###   Correct predictions 1.0\n",
      "Epoch 247  ###   Avg-Loss 2.8724937389294308e-05   ###   Correct predictions 1.0\n",
      "Epoch 248  ###   Avg-Loss 3.149114587965111e-05   ###   Correct predictions 1.0\n",
      "Epoch 249  ###   Avg-Loss 3.239394476016362e-05   ###   Correct predictions 1.0\n",
      "Epoch 250  ###   Avg-Loss 2.6755715953186155e-05   ###   Correct predictions 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('lin1',nn.Linear(2, 16, bias=True)),\n",
    "    ('relu1',nn.ReLU()),\n",
    "    ('lin2',nn.Linear(16, 32, bias=True)),\n",
    "    ('relu2',nn.ReLU()),\n",
    "    ('lin3',nn.Linear(32, 64, bias=True)),\n",
    "    ('relu3',nn.ReLU()),\n",
    "    ('lin4',nn.Linear(64, 2, bias=True))\n",
    "])\n",
    ")\n",
    "\n",
    "output_frequency = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(250):\n",
    "    loss_sum = 0\n",
    "    correct_pred = 0\n",
    "    for index, (data, target) in enumerate(train_loader):\n",
    "        #print(index)\n",
    "        output = model.forward(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss_sum = loss_sum + loss.data\n",
    "        for i in range(len(output)):\n",
    "            _, ind = torch.max(output[i],0)\n",
    "            label = target[i]\n",
    "            \n",
    "            if ind.data == label.data:\n",
    "                correct_pred +=1\n",
    "        #if index % (10*(output_frequency)) == 0:\n",
    "        #    print(\"#  Epoch  #  Batch  #  Avg-Loss ###############\")\n",
    "        #if index % (output_frequency) == 0 and index > 0:\n",
    "        #    print(\"#  %d  #  %d  #  %f  #\" % (epoch+1, index, loss_sum/output_frequency))\n",
    "        #    loss_sum = 0\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch '+str(epoch+1)+'  ###   Avg-Loss '+str(loss_sum.item()/480)+'   ###   Correct predictions '+str(correct_pred/480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  ###   Avg-Loss 0.019352535406748455   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 2  ###   Avg-Loss 0.017910422881444295   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 3  ###   Avg-Loss 0.01750417153040568   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 4  ###   Avg-Loss 0.017391308148701986   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 5  ###   Avg-Loss 0.017361233631769817   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 6  ###   Avg-Loss 0.017345829804738363   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 7  ###   Avg-Loss 0.017331127325693765   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 8  ###   Avg-Loss 0.01733660101890564   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 9  ###   Avg-Loss 0.01734957695007324   ###   Correct predictions 0.4895833333333333\n",
      "Epoch 10  ###   Avg-Loss 0.01734344959259033   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 11  ###   Avg-Loss 0.017335693041483562   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 12  ###   Avg-Loss 0.01733403404553731   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 13  ###   Avg-Loss 0.017345335086186728   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 14  ###   Avg-Loss 0.017336952686309814   ###   Correct predictions 0.50625\n",
      "Epoch 15  ###   Avg-Loss 0.017330414056777953   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 16  ###   Avg-Loss 0.01733735203742981   ###   Correct predictions 0.49375\n",
      "Epoch 17  ###   Avg-Loss 0.017340999841690064   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 18  ###   Avg-Loss 0.01733502944310506   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 19  ###   Avg-Loss 0.017336159944534302   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 20  ###   Avg-Loss 0.01733076572418213   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 21  ###   Avg-Loss 0.017335595687230428   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 22  ###   Avg-Loss 0.017331228653589884   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 23  ###   Avg-Loss 0.017345523834228514   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 24  ###   Avg-Loss 0.017328786849975585   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 25  ###   Avg-Loss 0.017338613669077556   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 26  ###   Avg-Loss 0.017339388529459637   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 27  ###   Avg-Loss 0.017334789037704468   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 28  ###   Avg-Loss 0.017332659165064494   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 29  ###   Avg-Loss 0.017332019408543904   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 30  ###   Avg-Loss 0.017343803246816   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 31  ###   Avg-Loss 0.017337658007939658   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 32  ###   Avg-Loss 0.017327052354812623   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 33  ###   Avg-Loss 0.017350995540618898   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 34  ###   Avg-Loss 0.017341856161753336   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 35  ###   Avg-Loss 0.01733092466990153   ###   Correct predictions 0.49375\n",
      "Epoch 36  ###   Avg-Loss 0.017343801259994508   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 37  ###   Avg-Loss 0.017331286271413168   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 38  ###   Avg-Loss 0.0173449436823527   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 39  ###   Avg-Loss 0.017336622873942057   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 40  ###   Avg-Loss 0.01733430624008179   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 41  ###   Avg-Loss 0.01733665664990743   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 42  ###   Avg-Loss 0.01732741594314575   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 43  ###   Avg-Loss 0.017335259914398195   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 44  ###   Avg-Loss 0.017344200611114503   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 45  ###   Avg-Loss 0.017335255940755207   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 46  ###   Avg-Loss 0.01732981006304423   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 47  ###   Avg-Loss 0.017340336243311563   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 48  ###   Avg-Loss 0.017337270577748618   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 49  ###   Avg-Loss 0.01734388470649719   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 50  ###   Avg-Loss 0.017336700359980264   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 51  ###   Avg-Loss 0.017336754004160564   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 52  ###   Avg-Loss 0.017329061031341554   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 53  ###   Avg-Loss 0.017343024412790935   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 54  ###   Avg-Loss 0.017335404952367146   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 55  ###   Avg-Loss 0.017333406209945678   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 56  ###   Avg-Loss 0.01734092434247335   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 57  ###   Avg-Loss 0.017334075768788655   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 58  ###   Avg-Loss 0.017333996295928956   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 59  ###   Avg-Loss 0.01734001040458679   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 60  ###   Avg-Loss 0.017343183358510334   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 61  ###   Avg-Loss 0.017334266503651937   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 62  ###   Avg-Loss 0.017326943079630532   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 63  ###   Avg-Loss 0.017331435283025106   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 64  ###   Avg-Loss 0.01734351714452108   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 65  ###   Avg-Loss 0.01733464002609253   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 66  ###   Avg-Loss 0.017335947354634604   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 67  ###   Avg-Loss 0.017339940865834555   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 68  ###   Avg-Loss 0.017335885763168336   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 69  ###   Avg-Loss 0.017330139875411987   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 70  ###   Avg-Loss 0.017336839437484743   ###   Correct predictions 0.48125\n",
      "Epoch 71  ###   Avg-Loss 0.01733196576436361   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 72  ###   Avg-Loss 0.01733160416285197   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 73  ###   Avg-Loss 0.017341158787409463   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 74  ###   Avg-Loss 0.01734090844790141   ###   Correct predictions 0.4895833333333333\n",
      "Epoch 75  ###   Avg-Loss 0.01733473539352417   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 76  ###   Avg-Loss 0.01733032464981079   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 77  ###   Avg-Loss 0.01734740932782491   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 78  ###   Avg-Loss 0.017335404952367146   ###   Correct predictions 0.49375\n",
      "Epoch 79  ###   Avg-Loss 0.017347671588261924   ###   Correct predictions 0.4895833333333333\n",
      "Epoch 80  ###   Avg-Loss 0.017328637838363647   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 81  ###   Avg-Loss 0.017335180441538492   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 82  ###   Avg-Loss 0.017335520188013712   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 83  ###   Avg-Loss 0.01733883221944173   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 84  ###   Avg-Loss 0.017346135775248208   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 85  ###   Avg-Loss 0.0173315425713857   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 86  ###   Avg-Loss 0.017339418331782024   ###   Correct predictions 0.48125\n",
      "Epoch 87  ###   Avg-Loss 0.01734487811724345   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 88  ###   Avg-Loss 0.01733728249867757   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 89  ###   Avg-Loss 0.01733392079671224   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 90  ###   Avg-Loss 0.017338260014851888   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 91  ###   Avg-Loss 0.017348573605219523   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 92  ###   Avg-Loss 0.017336924870808918   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 93  ###   Avg-Loss 0.017339259386062622   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 94  ###   Avg-Loss 0.017346447706222533   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 95  ###   Avg-Loss 0.01733930706977844   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 96  ###   Avg-Loss 0.01734011769294739   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 97  ###   Avg-Loss 0.01733176310857137   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 98  ###   Avg-Loss 0.017335659265518187   ###   Correct predictions 0.48125\n",
      "Epoch 99  ###   Avg-Loss 0.017331586281458537   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 100  ###   Avg-Loss 0.017337832848230997   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 101  ###   Avg-Loss 0.017333618799845376   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 102  ###   Avg-Loss 0.017344723145167034   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 103  ###   Avg-Loss 0.0173422376314799   ###   Correct predictions 0.48541666666666666\n",
      "Epoch 104  ###   Avg-Loss 0.017340991894404092   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 105  ###   Avg-Loss 0.01734160582224528   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 106  ###   Avg-Loss 0.017327898740768434   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 107  ###   Avg-Loss 0.01732935905456543   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 108  ###   Avg-Loss 0.017331963777542113   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 109  ###   Avg-Loss 0.01732992132504781   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 110  ###   Avg-Loss 0.017332820097605388   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 111  ###   Avg-Loss 0.017328357696533202   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 112  ###   Avg-Loss 0.017340288559595744   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 113  ###   Avg-Loss 0.017335466543833413   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 114  ###   Avg-Loss 0.017333012819290162   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 115  ###   Avg-Loss 0.017334548632303874   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 116  ###   Avg-Loss 0.017332889636357627   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 117  ###   Avg-Loss 0.017335166533788044   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 118  ###   Avg-Loss 0.01733022133509318   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 119  ###   Avg-Loss 0.017332543929417927   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 120  ###   Avg-Loss 0.017333388328552246   ###   Correct predictions 0.48541666666666666\n",
      "Epoch 121  ###   Avg-Loss 0.017331175009409585   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 122  ###   Avg-Loss 0.017329037189483643   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 123  ###   Avg-Loss 0.017340620358784992   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 124  ###   Avg-Loss 0.01733303666114807   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 125  ###   Avg-Loss 0.0173516849676768   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 126  ###   Avg-Loss 0.0173428475856781   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 127  ###   Avg-Loss 0.017335009574890137   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 128  ###   Avg-Loss 0.017333370447158814   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 129  ###   Avg-Loss 0.017340290546417236   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 130  ###   Avg-Loss 0.01732499400774638   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 131  ###   Avg-Loss 0.01733763813972473   ###   Correct predictions 0.48541666666666666\n",
      "Epoch 132  ###   Avg-Loss 0.017332053184509276   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 133  ###   Avg-Loss 0.017353246609369915   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 134  ###   Avg-Loss 0.017330745855967205   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 135  ###   Avg-Loss 0.01732763648033142   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 136  ###   Avg-Loss 0.017330714066823325   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 137  ###   Avg-Loss 0.017336773872375488   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 138  ###   Avg-Loss 0.017343024412790935   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 139  ###   Avg-Loss 0.01733009417851766   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 140  ###   Avg-Loss 0.017354045311609903   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 141  ###   Avg-Loss 0.017332154512405395   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 142  ###   Avg-Loss 0.017334461212158203   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 143  ###   Avg-Loss 0.017343628406524658   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 144  ###   Avg-Loss 0.017340620358784992   ###   Correct predictions 0.46875\n",
      "Epoch 145  ###   Avg-Loss 0.01733053723971049   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 146  ###   Avg-Loss 0.017334272464116413   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 147  ###   Avg-Loss 0.017340207099914552   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 148  ###   Avg-Loss 0.01733244260152181   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 149  ###   Avg-Loss 0.017334314187367757   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 150  ###   Avg-Loss 0.0173399825890859   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 151  ###   Avg-Loss 0.017332218090693154   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 152  ###   Avg-Loss 0.017343036333719888   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 153  ###   Avg-Loss 0.01734464367230733   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 154  ###   Avg-Loss 0.017339040835698444   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 155  ###   Avg-Loss 0.01733132799466451   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 156  ###   Avg-Loss 0.0173313041528066   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 157  ###   Avg-Loss 0.017345698674519856   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 158  ###   Avg-Loss 0.01734340190887451   ###   Correct predictions 0.47708333333333336\n",
      "Epoch 159  ###   Avg-Loss 0.017333271106084187   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 160  ###   Avg-Loss 0.01733576456705729   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 161  ###   Avg-Loss 0.01733560562133789   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 162  ###   Avg-Loss 0.017329166332880657   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 163  ###   Avg-Loss 0.017341987291971842   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 164  ###   Avg-Loss 0.017338371276855467   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 165  ###   Avg-Loss 0.017340058088302614   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 166  ###   Avg-Loss 0.017344150940577188   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 167  ###   Avg-Loss 0.017344385385513306   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 168  ###   Avg-Loss 0.017346356312433878   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 169  ###   Avg-Loss 0.017333459854125977   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 170  ###   Avg-Loss 0.01733231743176778   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 171  ###   Avg-Loss 0.01732916235923767   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 172  ###   Avg-Loss 0.017331536610921225   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 173  ###   Avg-Loss 0.01733509699503581   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 174  ###   Avg-Loss 0.01732465426127116   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 175  ###   Avg-Loss 0.017342160145441692   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 176  ###   Avg-Loss 0.017330139875411987   ###   Correct predictions 0.5104166666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177  ###   Avg-Loss 0.017332039276758828   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 178  ###   Avg-Loss 0.01733199159304301   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 179  ###   Avg-Loss 0.017338985204696657   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 180  ###   Avg-Loss 0.017342575391133628   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 181  ###   Avg-Loss 0.017335110902786256   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 182  ###   Avg-Loss 0.017331095536549886   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 183  ###   Avg-Loss 0.017341347535451253   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 184  ###   Avg-Loss 0.017332384983698528   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 185  ###   Avg-Loss 0.017330108086268108   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 186  ###   Avg-Loss 0.017338589827219645   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 187  ###   Avg-Loss 0.017340189218521117   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 188  ###   Avg-Loss 0.017331349849700927   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 189  ###   Avg-Loss 0.017359145482381187   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 190  ###   Avg-Loss 0.017331101497014365   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 191  ###   Avg-Loss 0.017330982287724814   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 192  ###   Avg-Loss 0.017326368888219198   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 193  ###   Avg-Loss 0.017343854904174803   ###   Correct predictions 0.47708333333333336\n",
      "Epoch 194  ###   Avg-Loss 0.017337441444396973   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 195  ###   Avg-Loss 0.017333298921585083   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 196  ###   Avg-Loss 0.017332341273625693   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 197  ###   Avg-Loss 0.017343161503473918   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 198  ###   Avg-Loss 0.017330855131149292   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 199  ###   Avg-Loss 0.01732876698176066   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 200  ###   Avg-Loss 0.017356745402018228   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 201  ###   Avg-Loss 0.017346465587615968   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 202  ###   Avg-Loss 0.017329835891723634   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 203  ###   Avg-Loss 0.01734454035758972   ###   Correct predictions 0.4895833333333333\n",
      "Epoch 204  ###   Avg-Loss 0.01733029286066691   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 205  ###   Avg-Loss 0.01733484069506327   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 206  ###   Avg-Loss 0.017335607608159383   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 207  ###   Avg-Loss 0.017337212959925335   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 208  ###   Avg-Loss 0.017347023884455363   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 209  ###   Avg-Loss 0.017332309484481813   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 210  ###   Avg-Loss 0.01733231743176778   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 211  ###   Avg-Loss 0.01734496553738912   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 212  ###   Avg-Loss 0.0173362930615743   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 213  ###   Avg-Loss 0.017337344090143838   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 214  ###   Avg-Loss 0.017353427410125733   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 215  ###   Avg-Loss 0.017331113417943318   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 216  ###   Avg-Loss 0.01733746329943339   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 217  ###   Avg-Loss 0.017332746585210165   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 218  ###   Avg-Loss 0.01733715335528056   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 219  ###   Avg-Loss 0.017337143421173096   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 220  ###   Avg-Loss 0.0173479954401652   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 221  ###   Avg-Loss 0.01733072797457377   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 222  ###   Avg-Loss 0.01734137535095215   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 223  ###   Avg-Loss 0.017333173751831056   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 224  ###   Avg-Loss 0.017326873540878297   ###   Correct predictions 0.50625\n",
      "Epoch 225  ###   Avg-Loss 0.01732667684555054   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 226  ###   Avg-Loss 0.017343149582544962   ###   Correct predictions 0.47708333333333336\n",
      "Epoch 227  ###   Avg-Loss 0.017348565657933555   ###   Correct predictions 0.49375\n",
      "Epoch 228  ###   Avg-Loss 0.017334616184234618   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 229  ###   Avg-Loss 0.017330666383107502   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 230  ###   Avg-Loss 0.017340850830078126   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 231  ###   Avg-Loss 0.01733234922091166   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 232  ###   Avg-Loss 0.01734009782473246   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 233  ###   Avg-Loss 0.017341488599777223   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 234  ###   Avg-Loss 0.017334944009780882   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 235  ###   Avg-Loss 0.017334401607513428   ###   Correct predictions 0.50625\n",
      "Epoch 236  ###   Avg-Loss 0.01733740170796712   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 237  ###   Avg-Loss 0.017334469159444175   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 238  ###   Avg-Loss 0.01733429233233134   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 239  ###   Avg-Loss 0.017332722743352253   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 240  ###   Avg-Loss 0.01733097235361735   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 241  ###   Avg-Loss 0.017334298292795817   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 242  ###   Avg-Loss 0.017329289515813192   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 243  ###   Avg-Loss 0.01733185847600301   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 244  ###   Avg-Loss 0.017335520188013712   ###   Correct predictions 0.4979166666666667\n",
      "Epoch 245  ###   Avg-Loss 0.017344415187835693   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 246  ###   Avg-Loss 0.017336014906565347   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 247  ###   Avg-Loss 0.017329887549082438   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 248  ###   Avg-Loss 0.017341379324595133   ###   Correct predictions 0.5020833333333333\n",
      "Epoch 249  ###   Avg-Loss 0.01733956535657247   ###   Correct predictions 0.5104166666666666\n",
      "Epoch 250  ###   Avg-Loss 0.01733067234357198   ###   Correct predictions 0.5104166666666666\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "from Dataset.Dataset import makeMoonsDataset\n",
    "\n",
    "dataset_size = 600\n",
    "batch_size = 40\n",
    "train_size = dataset_size*0.8\n",
    "test_size = dataset_size*0.2\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset(dataset_size,batch_size)\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('lin1',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu1',nn.ReLU()),\n",
    "    ('lin2',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu2',nn.ReLU()),\n",
    "    ('lin3',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu3',nn.ReLU()),\n",
    "    ('lin4',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu4',nn.ReLU()),\n",
    "    ('lin5',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu5',nn.ReLU()),\n",
    "    ('lin6',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu6',nn.ReLU()),\n",
    "    ('lin7',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu7',nn.ReLU()),\n",
    "    ('lin8',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu8',nn.ReLU()),\n",
    "    ('lin9',nn.Linear(2, 2, bias=True)),\n",
    "    ('relu9',nn.ReLU()),\n",
    "    ('lin10',nn.Linear(2, 2, bias=True))\n",
    "])\n",
    ")\n",
    "\n",
    "output_frequency = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(250):\n",
    "    loss_sum = 0\n",
    "    correct_pred = 0\n",
    "    for index, (data, target) in enumerate(train_loader):\n",
    "        #print(index)\n",
    "        output = model.forward(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss_sum = loss_sum + loss.data\n",
    "        for i in range(len(output)):\n",
    "            _, ind = torch.max(output[i],0)\n",
    "            label = target[i]\n",
    "            \n",
    "            if ind.data == label.data:\n",
    "                correct_pred +=1\n",
    "        #if index % (10*(output_frequency)) == 0:\n",
    "        #    print(\"#  Epoch  #  Batch  #  Avg-Loss ###############\")\n",
    "        #if index % (output_frequency) == 0 and index > 0:\n",
    "        #    print(\"#  %d  #  %d  #  %f  #\" % (epoch+1, index, loss_sum/output_frequency))\n",
    "        #    loss_sum = 0\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch '+str(epoch+1)+'  ###   Avg-Loss '+str(loss_sum.item()/train_size)+'   ###   Correct predictions '+str(correct_pred/train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7115,  1.4547]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.6434, -2.7388]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3886,  2.0311]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.9741, -3.1041]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-0.8089,  0.1275]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3526, -2.2834]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.8463, -2.8233]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.9055, -2.8244]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-1.7085,  1.4306]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.7206,  2.3536]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 0.7751, -1.6380]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.4437,  2.1093]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.1609,  1.7324]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-0.5344,  0.0122]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3842,  2.1159]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-1.5523,  1.2432]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.5676, -2.6339]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 2.0958, -3.2221]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 0.9023, -1.6219]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.7551,  2.3714]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3445, -2.3139]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.5533,  2.1857]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3841, -2.4257]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3780,  2.0977]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 0.0020, -0.6190]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 0.1119, -0.9363]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3832, -2.3543]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.5189, -2.5425]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-1.9774,  1.4456]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.2206, -2.2302]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-0.1666, -0.6191]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.3856,  2.0124]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.4732, -2.4705]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.4295,  2.1136]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-0.6206,  0.0895]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.7165, -2.7979]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.8907, -2.8468]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.8076, -2.8020]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3121,  1.9327]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.0975,  1.7790]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "for index, (data, target) in enumerate(test_loader):\n",
    "    print(model.forward(data))\n",
    "    print(target)\n",
    "    print('####################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 0.9916666666666667\n"
     ]
    }
   ],
   "source": [
    "net.test(test_loader,120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1])\n",
      "tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
      "        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0])\n",
      "tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,\n",
      "        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1])\n",
      "tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
      "        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0])\n",
      "tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0])\n",
      "tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0])\n",
      "tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1])\n",
      "tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0])\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
      "        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1])\n",
      "tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n",
      "        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0])\n",
      "tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
      "        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0])\n",
      "[1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "from pandas import DataFrame\n",
    "import torch\n",
    "\n",
    "x1 = []\n",
    "x2 = []\n",
    "y = []\n",
    "for (data,label) in train_loader:\n",
    "    res = torch.argmax(net.forward(data), dim=1)\n",
    "    print(res)\n",
    "    for point in range(len(data)):\n",
    "        x1.append(data[point][0].item())\n",
    "        x2.append(data[point][1].item())\n",
    "        y.append(res[point].item())\n",
    "#print(x1)\n",
    "df = DataFrame(dict(x=x1, y=x2, label=y))\n",
    "print(y)\n",
    "colors = {0:'red', 1:'blue'}\n",
    "fig, ax = pyplot.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 1.0\n"
     ]
    }
   ],
   "source": [
    "correct_pred = 0\n",
    "for _, (data, label) in enumerate(test_loader):\n",
    "    prediction = net.forward(data)\n",
    "    _, ind = torch.max(prediction,1)\n",
    "    if ind.data == label.data:\n",
    "        correct_pred +=1\n",
    "print('Correct predictions: '+str(correct_pred/120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:168: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  compared_weight_signs = torch.tensor(torch.ne(new_weight_signs, old_weight_signs).detach(), dtype=torch.float32)\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:173: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_weights = new_weight_signs*torch.tensor(torch.ge(torch.abs(self.m_accumulated),self.rho*max_weight_elem*torch.ones_like(self.m_accumulated)).detach(),dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import loadMNIST\n",
    "from Networks.ResNet import ConvNet, FCMSANet\n",
    "\n",
    "#net = ConvNet(3, [1], [3,6], num_fc=2, sizes_fc=[100,10])\n",
    "net = FCMSANet(num_fc=4,sizes_fc=[784,1024,1024,1024,10], bias=False, batchnorm=True, test=False)\n",
    "train_loader, test_loader = loadMNIST('Dataset/input/mnist_train.csv', 'Dataset/input/mnist_test.csv')\n",
    "#net.train_backprop(1,train_loader)\n",
    "#net.test(test_loader,10000)\n",
    "print(train_loader.batch_size)\n",
    "\n",
    "net.train_msa(1,train_loader)\n",
    "\n",
    "#for index, (data, target) in enumerate(train_loader):\n",
    "#    if index == 1:\n",
    "#        print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.test(test_loader,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1574, -0.1913,  0.3191],\n",
      "        [ 0.4584,  0.3066, -0.3247],\n",
      "        [ 0.2495,  0.4553,  0.0500],\n",
      "        [ 0.3111, -0.3222,  0.2152]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.4820, -0.5667, -0.2615,  0.5324], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "test_layer = torch.nn.Linear(3, 4)\n",
    "print(test_layer.weight)\n",
    "print(test_layer.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6., 18., 36.], grad_fn=<SumBackward2>)\n",
      "tensor([[1., 1., 1.],\n",
      "        [2., 2., 2.],\n",
      "        [3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([[1,2,3],[2,3,4],[3,4,5]],dtype=torch.float32, requires_grad=True)\n",
    "y = torch.tensor([[1,1,1],[2,2,2],[3,3,3]],dtype=torch.float32)\n",
    "\n",
    "z = torch.sum(x*y, dim=1)\n",
    "print(z)\n",
    "#z.backward(torch.FloatTensor([0,0,1]), retain_graph=True)\n",
    "z.backward(torch.FloatTensor([1,1,1]), retain_graph=True)\n",
    "#z[0].backward(retain_graph=True)\n",
    "#z[1].backward(retain_graph=True)\n",
    "#z[2].backward(retain_graph=True)\n",
    "print(x.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
