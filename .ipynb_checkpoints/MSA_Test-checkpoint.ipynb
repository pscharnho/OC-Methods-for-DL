{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:340: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:343: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  compared_weight_signs = torch.tensor(torch.ne(new_weight_signs, old_weight_signs).detach(), dtype=torch.float32)\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:148: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_weights = new_weight_signs*torch.tensor(torch.ge(torch.abs(self.m_accumulated),self.rho*max_weight_elem*torch.ones_like(self.m_accumulated)).detach(),dtype=torch.float32)\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:166: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  compared_bias_signs = torch.tensor(torch.ne(new_bias_signs, old_bias_signs).detach(), dtype=torch.float32)\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:169: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_biases = new_bias_signs*torch.tensor(torch.ge(torch.abs(self.m2_accumulated),self.rho*max_bias_elem*torch.ones_like(self.m2_accumulated)).detach(),dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Correct predictions: 0.45625\n",
      "Epoch 2\n",
      "Correct predictions: 0.45625\n",
      "Epoch 3\n",
      "Correct predictions: 0.45625\n",
      "Epoch 4\n",
      "Correct predictions: 0.45625\n",
      "Epoch 5\n",
      "Correct predictions: 0.45625\n",
      "Epoch 6\n",
      "Correct predictions: 0.45625\n",
      "Epoch 7\n",
      "Correct predictions: 0.45625\n",
      "Epoch 8\n",
      "Correct predictions: 0.45625\n",
      "Epoch 9\n",
      "Correct predictions: 0.45625\n",
      "Epoch 10\n",
      "Correct predictions: 0.45625\n",
      "Epoch 11\n",
      "Correct predictions: 0.45625\n",
      "Epoch 12\n",
      "Correct predictions: 0.45625\n",
      "Epoch 13\n",
      "Correct predictions: 0.45625\n",
      "Epoch 14\n",
      "Correct predictions: 0.45625\n",
      "Epoch 15\n",
      "Correct predictions: 0.45625\n",
      "Epoch 16\n",
      "Correct predictions: 0.45625\n",
      "Epoch 17\n",
      "Correct predictions: 0.45625\n",
      "Epoch 18\n",
      "Correct predictions: 0.6375\n",
      "Epoch 19\n",
      "Correct predictions: 0.6375\n",
      "Epoch 20\n",
      "Correct predictions: 0.6375\n",
      "Epoch 21\n",
      "Correct predictions: 0.6375\n",
      "Epoch 22\n",
      "Correct predictions: 0.7375\n",
      "Epoch 23\n",
      "Correct predictions: 0.6375\n",
      "Epoch 24\n",
      "Correct predictions: 0.7375\n",
      "Epoch 25\n",
      "Correct predictions: 0.86875\n",
      "Epoch 26\n",
      "Correct predictions: 0.86875\n",
      "Epoch 27\n",
      "Correct predictions: 0.7125\n",
      "Epoch 28\n",
      "Correct predictions: 0.925\n",
      "Time elapsed:  9.446578060999997\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import ConvNet, FCNet\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "train_loader, test_loader = makeMoonsDataset()\n",
    "torch.manual_seed(143)\n",
    "\n",
    "#net = ConvNet(3, [1], [3,6], num_fc=2, sizes_fc=[100,10])\n",
    "net = FCNet(num_fc=4,sizes_fc=[2,4,8,8,2], bias=True, test=False)\n",
    "#train_loader, test_loader = loadMNIST('Dataset/input/mnist_train.csv', 'Dataset/input/mnist_test.csv')\n",
    "\n",
    "#net.train_backprop(1,train_loader)\n",
    "#net.test(test_loader,10000)\n",
    "print(train_loader.batch_size)\n",
    "\n",
    "#\n",
    "#for index, (data, label) in enumerate(train_loader):\n",
    "#    print(data)\n",
    "#    print(net.forward(data))\n",
    "    \n",
    "net.train_msa(28,train_loader)\n",
    "\n",
    "\n",
    "\n",
    "#for index, (data, label) in enumerate(train_loader):\n",
    "#    print(data)\n",
    "#    print(net.forward(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:340: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:343: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  compared_weight_signs = torch.tensor(torch.ne(new_weight_signs, old_weight_signs).detach(), dtype=torch.float32)\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:148: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_weights = new_weight_signs*torch.tensor(torch.ge(torch.abs(self.m_accumulated),self.rho*max_weight_elem*torch.ones_like(self.m_accumulated)).detach(),dtype=torch.float32)\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:166: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  compared_bias_signs = torch.tensor(torch.ne(new_bias_signs, old_bias_signs).detach(), dtype=torch.float32)\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:169: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_biases = new_bias_signs*torch.tensor(torch.ge(torch.abs(self.m2_accumulated),self.rho*max_bias_elem*torch.ones_like(self.m2_accumulated)).detach(),dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed:  20.575132048\n",
      "Seed 50   ###   Best-Avg 0.825\n",
      "Time elapsed:  21.965501217000003\n",
      "Seed 51   ###   Best-Avg 0.8375\n",
      "Time elapsed:  22.445882544\n",
      "Seed 52   ###   Best-Avg 0.85625\n",
      "Time elapsed:  23.524419952000002\n",
      "Seed 53   ###   Best-Avg 0.825\n",
      "Time elapsed:  25.652287161999993\n",
      "Seed 54   ###   Best-Avg 0.95\n",
      "Time elapsed:  24.76668097400001\n",
      "Seed 55   ###   Best-Avg 0.875\n",
      "Time elapsed:  21.885561978999988\n",
      "Seed 56   ###   Best-Avg 0.8625\n",
      "Time elapsed:  20.858169499000013\n",
      "Seed 57   ###   Best-Avg 0.8375\n",
      "Time elapsed:  21.27320665299999\n",
      "Seed 58   ###   Best-Avg 0.84375\n",
      "Time elapsed:  22.38183995899999\n",
      "Seed 59   ###   Best-Avg 0.81875\n",
      "Time elapsed:  22.39914512599998\n",
      "Seed 60   ###   Best-Avg 0.84375\n",
      "Time elapsed:  23.092609679999953\n",
      "Seed 61   ###   Best-Avg 0.825\n",
      "Time elapsed:  24.668828442000006\n",
      "Seed 62   ###   Best-Avg 0.925\n",
      "Time elapsed:  23.90732400600001\n",
      "Seed 63   ###   Best-Avg 0.88125\n",
      "Time elapsed:  21.226910293999993\n",
      "Seed 64   ###   Best-Avg 0.825\n",
      "Time elapsed:  22.09916388900001\n",
      "Seed 65   ###   Best-Avg 0.8125\n",
      "Time elapsed:  22.52286030800002\n",
      "Seed 66   ###   Best-Avg 0.85\n",
      "Time elapsed:  23.917439077999973\n",
      "Seed 67   ###   Best-Avg 0.8125\n",
      "Time elapsed:  24.50687215199997\n",
      "Seed 68   ###   Best-Avg 0.8125\n",
      "Time elapsed:  23.151332303999993\n",
      "Seed 69   ###   Best-Avg 0.85\n",
      "Time elapsed:  21.232584439999982\n",
      "Seed 70   ###   Best-Avg 0.90625\n",
      "Time elapsed:  22.181122337000033\n",
      "Seed 71   ###   Best-Avg 0.85625\n",
      "Time elapsed:  23.27440645899992\n",
      "Seed 72   ###   Best-Avg 0.8125\n",
      "Time elapsed:  23.61875634599994\n",
      "Seed 73   ###   Best-Avg 0.9375\n",
      "Time elapsed:  25.293840842999998\n",
      "Seed 74   ###   Best-Avg 0.81875\n",
      "Time elapsed:  22.743097648000003\n",
      "Seed 75   ###   Best-Avg 0.875\n",
      "Time elapsed:  22.072785127999964\n",
      "Seed 76   ###   Best-Avg 0.85\n",
      "Time elapsed:  23.880283656000074\n",
      "Seed 77   ###   Best-Avg 0.83125\n",
      "Time elapsed:  23.491481327999963\n",
      "Seed 78   ###   Best-Avg 0.875\n",
      "Time elapsed:  24.397996211999953\n",
      "Seed 79   ###   Best-Avg 0.81875\n",
      "Time elapsed:  22.81099873300002\n",
      "Seed 80   ###   Best-Avg 0.89375\n",
      "Time elapsed:  20.926476688000093\n",
      "Seed 81   ###   Best-Avg 0.9125\n",
      "Time elapsed:  21.257081244000005\n",
      "Seed 82   ###   Best-Avg 0.86875\n",
      "Time elapsed:  22.755032475000007\n",
      "Seed 83   ###   Best-Avg 0.825\n",
      "Time elapsed:  23.24522712299995\n",
      "Seed 84   ###   Best-Avg 0.83125\n",
      "Time elapsed:  22.023612114000002\n",
      "Seed 85   ###   Best-Avg 0.9375\n",
      "Time elapsed:  21.74782027599997\n",
      "Seed 86   ###   Best-Avg 0.91875\n",
      "Time elapsed:  23.52947928800006\n",
      "Seed 87   ###   Best-Avg 0.81875\n",
      "Time elapsed:  23.204836235000016\n",
      "Seed 88   ###   Best-Avg 0.91875\n",
      "Time elapsed:  25.210515007000026\n",
      "Seed 89   ###   Best-Avg 0.81875\n",
      "Time elapsed:  22.874206490999995\n",
      "Seed 90   ###   Best-Avg 0.9125\n",
      "Time elapsed:  22.326290085999972\n",
      "Seed 91   ###   Best-Avg 0.86875\n",
      "Time elapsed:  22.48480220500005\n",
      "Seed 92   ###   Best-Avg 0.825\n",
      "Time elapsed:  23.248691342999905\n",
      "Seed 93   ###   Best-Avg 0.80625\n",
      "Time elapsed:  23.90473933499993\n",
      "Seed 94   ###   Best-Avg 0.80625\n",
      "Time elapsed:  23.40174521799986\n",
      "Seed 95   ###   Best-Avg 0.88125\n",
      "Time elapsed:  21.406157871000005\n",
      "Seed 96   ###   Best-Avg 0.83125\n",
      "Time elapsed:  21.794302721999884\n",
      "Seed 97   ###   Best-Avg 0.825\n",
      "Time elapsed:  22.504040734\n",
      "Seed 98   ###   Best-Avg 0.81875\n",
      "Time elapsed:  23.42958081400002\n",
      "Seed 99   ###   Best-Avg 0.9375\n",
      "Time elapsed:  22.041294597999922\n",
      "Seed 100   ###   Best-Avg 0.85625\n",
      "Time elapsed:  20.945530671999904\n",
      "Seed 101   ###   Best-Avg 0.81875\n",
      "Time elapsed:  22.368418996000173\n",
      "Seed 102   ###   Best-Avg 0.83125\n",
      "Time elapsed:  22.847850348000065\n",
      "Seed 103   ###   Best-Avg 0.81875\n",
      "Time elapsed:  24.667489839999917\n",
      "Seed 104   ###   Best-Avg 0.8875\n",
      "Time elapsed:  24.121018446000107\n",
      "Seed 105   ###   Best-Avg 0.825\n",
      "Time elapsed:  21.768976231999886\n",
      "Seed 106   ###   Best-Avg 0.825\n",
      "Time elapsed:  20.913267002999874\n",
      "Seed 107   ###   Best-Avg 0.94375\n",
      "Time elapsed:  22.59963810499994\n",
      "Seed 108   ###   Best-Avg 0.88125\n",
      "Time elapsed:  22.8170275760001\n",
      "Seed 109   ###   Best-Avg 0.8375\n",
      "Time elapsed:  21.83252943599996\n",
      "Seed 110   ###   Best-Avg 0.85625\n",
      "Time elapsed:  22.58115266599998\n",
      "Seed 111   ###   Best-Avg 0.8125\n",
      "Time elapsed:  23.596687426000017\n",
      "Seed 112   ###   Best-Avg 0.825\n",
      "Time elapsed:  23.072702877999973\n",
      "Seed 113   ###   Best-Avg 0.825\n",
      "Time elapsed:  22.00472160000004\n",
      "Seed 114   ###   Best-Avg 0.90625\n",
      "Time elapsed:  21.575470297000038\n",
      "Seed 115   ###   Best-Avg 0.8125\n",
      "Time elapsed:  22.158154334999836\n",
      "Seed 116   ###   Best-Avg 0.85\n",
      "Time elapsed:  22.653328638999938\n",
      "Seed 117   ###   Best-Avg 0.8125\n",
      "Time elapsed:  23.340229217000115\n",
      "Seed 118   ###   Best-Avg 0.825\n",
      "Time elapsed:  24.154899334999982\n",
      "Seed 119   ###   Best-Avg 0.8375\n",
      "Time elapsed:  22.51527798899997\n",
      "Seed 120   ###   Best-Avg 0.85625\n",
      "Time elapsed:  22.599394441999948\n",
      "Seed 121   ###   Best-Avg 0.83125\n",
      "Time elapsed:  23.875517332000072\n",
      "Seed 122   ###   Best-Avg 0.93125\n",
      "Time elapsed:  24.573712225999998\n",
      "Seed 123   ###   Best-Avg 0.86875\n",
      "Time elapsed:  25.698377383000206\n",
      "Seed 124   ###   Best-Avg 0.825\n",
      "Time elapsed:  24.15837023899985\n",
      "Seed 125   ###   Best-Avg 0.81875\n",
      "Time elapsed:  23.224884402000043\n",
      "Seed 126   ###   Best-Avg 0.825\n",
      "Time elapsed:  23.953550967000183\n",
      "Seed 127   ###   Best-Avg 0.825\n",
      "Time elapsed:  25.414457299000105\n",
      "Seed 128   ###   Best-Avg 0.825\n",
      "Time elapsed:  24.934309354000106\n",
      "Seed 129   ###   Best-Avg 0.9125\n",
      "Time elapsed:  24.654892394999933\n",
      "Seed 130   ###   Best-Avg 0.825\n",
      "Time elapsed:  22.820277436000197\n",
      "Seed 131   ###   Best-Avg 0.91875\n",
      "Time elapsed:  23.427607867999996\n",
      "Seed 132   ###   Best-Avg 0.825\n",
      "Time elapsed:  25.22937262099981\n",
      "Seed 133   ###   Best-Avg 0.825\n",
      "Time elapsed:  23.82371903799981\n",
      "Seed 134   ###   Best-Avg 0.925\n",
      "Time elapsed:  23.477427562999992\n",
      "Seed 135   ###   Best-Avg 0.8125\n",
      "Time elapsed:  23.556699043000208\n",
      "Seed 136   ###   Best-Avg 0.825\n",
      "Time elapsed:  23.484821223999916\n",
      "Seed 137   ###   Best-Avg 0.80625\n",
      "Time elapsed:  24.334050268999817\n",
      "Seed 138   ###   Best-Avg 0.8625\n",
      "Time elapsed:  23.739139420000356\n",
      "Seed 139   ###   Best-Avg 0.8875\n",
      "Time elapsed:  22.871303105000152\n",
      "Seed 140   ###   Best-Avg 0.8625\n",
      "Time elapsed:  24.039115002000017\n",
      "Seed 141   ###   Best-Avg 0.94375\n",
      "Time elapsed:  24.48560156099984\n",
      "Seed 142   ###   Best-Avg 0.88125\n",
      "Time elapsed:  25.268112363\n",
      "Seed 143   ###   Best-Avg 0.925\n",
      "Time elapsed:  23.15973968199978\n",
      "Seed 144   ###   Best-Avg 0.8375\n",
      "Time elapsed:  23.7064511399999\n",
      "Seed 145   ###   Best-Avg 0.9\n",
      "Time elapsed:  23.555291560000114\n",
      "Seed 146   ###   Best-Avg 0.875\n",
      "Time elapsed:  23.26770265999994\n",
      "Seed 147   ###   Best-Avg 0.8125\n",
      "Time elapsed:  24.128525199999785\n",
      "Seed 148   ###   Best-Avg 0.85625\n",
      "Time elapsed:  25.635435391999636\n",
      "Seed 149   ###   Best-Avg 0.85\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import ConvNet, FCNet\n",
    "import torch\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset()\n",
    "\n",
    "for seed in range(100):\n",
    "    torch.manual_seed(seed+50)\n",
    "\n",
    "    net = FCNet(num_fc=4,sizes_fc=[2,4,8,8,2], bias=True, test=False)\n",
    "    \n",
    "    net.train_msa(60,train_loader)\n",
    "    print('Seed '+str(seed+50)+'   ###   Best-Avg '+str(net.best_avg))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  ###   Avg-Loss 0.03437475562095642   ###   Correct predictions 0.525\n",
      "Epoch 2  ###   Avg-Loss 0.03403787612915039   ###   Correct predictions 0.61875\n",
      "Epoch 3  ###   Avg-Loss 0.033597889542579654   ###   Correct predictions 0.60625\n",
      "Epoch 4  ###   Avg-Loss 0.03306081593036651   ###   Correct predictions 0.6625\n",
      "Epoch 5  ###   Avg-Loss 0.032476571202278134   ###   Correct predictions 0.6375\n",
      "Epoch 6  ###   Avg-Loss 0.03148809373378754   ###   Correct predictions 0.7\n",
      "Epoch 7  ###   Avg-Loss 0.030320319533348083   ###   Correct predictions 0.7\n",
      "Epoch 8  ###   Avg-Loss 0.029096662998199463   ###   Correct predictions 0.6875\n",
      "Epoch 9  ###   Avg-Loss 0.027504265308380127   ###   Correct predictions 0.71875\n",
      "Epoch 10  ###   Avg-Loss 0.02602536976337433   ###   Correct predictions 0.71875\n",
      "Epoch 11  ###   Avg-Loss 0.024707286059856413   ###   Correct predictions 0.75\n",
      "Epoch 12  ###   Avg-Loss 0.023344963788986206   ###   Correct predictions 0.7375\n",
      "Epoch 13  ###   Avg-Loss 0.022469085454940797   ###   Correct predictions 0.7875\n",
      "Epoch 14  ###   Avg-Loss 0.02126854658126831   ###   Correct predictions 0.7875\n",
      "Epoch 15  ###   Avg-Loss 0.020223084092140197   ###   Correct predictions 0.7875\n",
      "Epoch 16  ###   Avg-Loss 0.01908561438322067   ###   Correct predictions 0.825\n",
      "Epoch 17  ###   Avg-Loss 0.01827494353055954   ###   Correct predictions 0.825\n",
      "Epoch 18  ###   Avg-Loss 0.017472554743289948   ###   Correct predictions 0.8375\n",
      "Epoch 19  ###   Avg-Loss 0.01655877083539963   ###   Correct predictions 0.8375\n",
      "Epoch 20  ###   Avg-Loss 0.015839333832263946   ###   Correct predictions 0.85625\n",
      "Epoch 21  ###   Avg-Loss 0.01550280600786209   ###   Correct predictions 0.85\n",
      "Epoch 22  ###   Avg-Loss 0.01506531536579132   ###   Correct predictions 0.85\n",
      "Epoch 23  ###   Avg-Loss 0.014990772306919097   ###   Correct predictions 0.8625\n",
      "Epoch 24  ###   Avg-Loss 0.014799822866916657   ###   Correct predictions 0.85625\n",
      "Epoch 25  ###   Avg-Loss 0.014737404882907867   ###   Correct predictions 0.85\n",
      "Epoch 26  ###   Avg-Loss 0.015444470942020417   ###   Correct predictions 0.84375\n",
      "Epoch 27  ###   Avg-Loss 0.014612197875976562   ###   Correct predictions 0.86875\n",
      "Epoch 28  ###   Avg-Loss 0.014810052514076234   ###   Correct predictions 0.8625\n",
      "Epoch 29  ###   Avg-Loss 0.014188997447490692   ###   Correct predictions 0.8625\n",
      "Epoch 30  ###   Avg-Loss 0.014685487747192383   ###   Correct predictions 0.85625\n",
      "Epoch 31  ###   Avg-Loss 0.014190948009490967   ###   Correct predictions 0.85\n",
      "Epoch 32  ###   Avg-Loss 0.014508916437625885   ###   Correct predictions 0.85\n",
      "Epoch 33  ###   Avg-Loss 0.0138005793094635   ###   Correct predictions 0.85625\n",
      "Epoch 34  ###   Avg-Loss 0.013933828473091126   ###   Correct predictions 0.85\n",
      "Epoch 35  ###   Avg-Loss 0.014268833398818969   ###   Correct predictions 0.86875\n",
      "Epoch 36  ###   Avg-Loss 0.01403689980506897   ###   Correct predictions 0.8625\n",
      "Epoch 37  ###   Avg-Loss 0.014650769531726837   ###   Correct predictions 0.8625\n",
      "Epoch 38  ###   Avg-Loss 0.013478754460811615   ###   Correct predictions 0.85625\n",
      "Epoch 39  ###   Avg-Loss 0.013431890308856964   ###   Correct predictions 0.88125\n",
      "Epoch 40  ###   Avg-Loss 0.014199352264404297   ###   Correct predictions 0.85\n",
      "Epoch 41  ###   Avg-Loss 0.013317318260669708   ###   Correct predictions 0.85625\n",
      "Epoch 42  ###   Avg-Loss 0.013320791721343993   ###   Correct predictions 0.85625\n",
      "Epoch 43  ###   Avg-Loss 0.013442695140838623   ###   Correct predictions 0.8625\n",
      "Epoch 44  ###   Avg-Loss 0.013873571157455444   ###   Correct predictions 0.8625\n",
      "Epoch 45  ###   Avg-Loss 0.013620680570602417   ###   Correct predictions 0.85\n",
      "Epoch 46  ###   Avg-Loss 0.013160236179828644   ###   Correct predictions 0.85625\n",
      "Epoch 47  ###   Avg-Loss 0.013158756494522094   ###   Correct predictions 0.86875\n",
      "Epoch 48  ###   Avg-Loss 0.013062146306037904   ###   Correct predictions 0.84375\n",
      "Epoch 49  ###   Avg-Loss 0.013201622664928437   ###   Correct predictions 0.85625\n",
      "Epoch 50  ###   Avg-Loss 0.013338328897953033   ###   Correct predictions 0.8625\n",
      "Epoch 51  ###   Avg-Loss 0.013170592486858368   ###   Correct predictions 0.88125\n",
      "Epoch 52  ###   Avg-Loss 0.013176913559436797   ###   Correct predictions 0.85\n",
      "Epoch 53  ###   Avg-Loss 0.013779737055301666   ###   Correct predictions 0.875\n",
      "Epoch 54  ###   Avg-Loss 0.014511215686798095   ###   Correct predictions 0.88125\n",
      "Epoch 55  ###   Avg-Loss 0.014076924324035645   ###   Correct predictions 0.84375\n",
      "Epoch 56  ###   Avg-Loss 0.013449697196483612   ###   Correct predictions 0.8625\n",
      "Epoch 57  ###   Avg-Loss 0.014046886563301086   ###   Correct predictions 0.85\n",
      "Epoch 58  ###   Avg-Loss 0.013201718032360078   ###   Correct predictions 0.8625\n",
      "Epoch 59  ###   Avg-Loss 0.015341933071613311   ###   Correct predictions 0.83125\n",
      "Epoch 60  ###   Avg-Loss 0.013065189123153687   ###   Correct predictions 0.8625\n",
      "Epoch 61  ###   Avg-Loss 0.013219170272350311   ###   Correct predictions 0.875\n",
      "Epoch 62  ###   Avg-Loss 0.013396385312080383   ###   Correct predictions 0.84375\n",
      "Epoch 63  ###   Avg-Loss 0.013470773398876191   ###   Correct predictions 0.875\n",
      "Epoch 64  ###   Avg-Loss 0.013682921230793   ###   Correct predictions 0.84375\n",
      "Epoch 65  ###   Avg-Loss 0.013259419798851013   ###   Correct predictions 0.85625\n",
      "Epoch 66  ###   Avg-Loss 0.01370035856962204   ###   Correct predictions 0.8625\n",
      "Epoch 67  ###   Avg-Loss 0.013078434765338898   ###   Correct predictions 0.86875\n",
      "Epoch 68  ###   Avg-Loss 0.012513828277587891   ###   Correct predictions 0.8625\n",
      "Epoch 69  ###   Avg-Loss 0.012576594948768616   ###   Correct predictions 0.85\n",
      "Epoch 70  ###   Avg-Loss 0.012652423977851868   ###   Correct predictions 0.875\n",
      "Epoch 71  ###   Avg-Loss 0.012533575296401978   ###   Correct predictions 0.875\n",
      "Epoch 72  ###   Avg-Loss 0.011995188146829604   ###   Correct predictions 0.86875\n",
      "Epoch 73  ###   Avg-Loss 0.012221866101026536   ###   Correct predictions 0.86875\n",
      "Epoch 74  ###   Avg-Loss 0.012150802463293076   ###   Correct predictions 0.86875\n",
      "Epoch 75  ###   Avg-Loss 0.012550941109657288   ###   Correct predictions 0.8625\n",
      "Epoch 76  ###   Avg-Loss 0.01299712508916855   ###   Correct predictions 0.875\n",
      "Epoch 77  ###   Avg-Loss 0.012512941658496857   ###   Correct predictions 0.875\n",
      "Epoch 78  ###   Avg-Loss 0.011615315079689026   ###   Correct predictions 0.85\n",
      "Epoch 79  ###   Avg-Loss 0.012027522921562195   ###   Correct predictions 0.9\n",
      "Epoch 80  ###   Avg-Loss 0.011313224583864212   ###   Correct predictions 0.875\n",
      "Epoch 81  ###   Avg-Loss 0.010980334877967835   ###   Correct predictions 0.8875\n",
      "Epoch 82  ###   Avg-Loss 0.011381042748689651   ###   Correct predictions 0.86875\n",
      "Epoch 83  ###   Avg-Loss 0.010612208396196365   ###   Correct predictions 0.90625\n",
      "Epoch 84  ###   Avg-Loss 0.010340522229671478   ###   Correct predictions 0.9\n",
      "Epoch 85  ###   Avg-Loss 0.010397825390100479   ###   Correct predictions 0.88125\n",
      "Epoch 86  ###   Avg-Loss 0.010047973692417144   ###   Correct predictions 0.9\n",
      "Epoch 87  ###   Avg-Loss 0.00960089936852455   ###   Correct predictions 0.90625\n",
      "Epoch 88  ###   Avg-Loss 0.008348696678876878   ###   Correct predictions 0.9375\n",
      "Epoch 89  ###   Avg-Loss 0.008384684473276139   ###   Correct predictions 0.925\n",
      "Epoch 90  ###   Avg-Loss 0.00793372243642807   ###   Correct predictions 0.94375\n",
      "Epoch 91  ###   Avg-Loss 0.007119672000408172   ###   Correct predictions 0.9375\n",
      "Epoch 92  ###   Avg-Loss 0.006680969148874283   ###   Correct predictions 0.95\n",
      "Epoch 93  ###   Avg-Loss 0.00654827356338501   ###   Correct predictions 0.95\n",
      "Epoch 94  ###   Avg-Loss 0.006420309841632843   ###   Correct predictions 0.95\n",
      "Epoch 95  ###   Avg-Loss 0.005244392529129982   ###   Correct predictions 0.9625\n",
      "Epoch 96  ###   Avg-Loss 0.00477382205426693   ###   Correct predictions 0.96875\n",
      "Epoch 97  ###   Avg-Loss 0.005167572945356369   ###   Correct predictions 0.95625\n",
      "Epoch 98  ###   Avg-Loss 0.004002885520458221   ###   Correct predictions 0.96875\n",
      "Epoch 99  ###   Avg-Loss 0.004299940541386604   ###   Correct predictions 0.98125\n",
      "Epoch 100  ###   Avg-Loss 0.00342341847717762   ###   Correct predictions 0.98125\n",
      "Epoch 101  ###   Avg-Loss 0.0033155716955661774   ###   Correct predictions 0.975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102  ###   Avg-Loss 0.003072446584701538   ###   Correct predictions 0.98125\n",
      "Epoch 103  ###   Avg-Loss 0.0029605085030198095   ###   Correct predictions 0.9875\n",
      "Epoch 104  ###   Avg-Loss 0.0025154590606689454   ###   Correct predictions 0.99375\n",
      "Epoch 105  ###   Avg-Loss 0.002787162363529205   ###   Correct predictions 0.99375\n",
      "Epoch 106  ###   Avg-Loss 0.002531791105866432   ###   Correct predictions 0.9875\n",
      "Epoch 107  ###   Avg-Loss 0.0021460149437189102   ###   Correct predictions 0.99375\n",
      "Epoch 108  ###   Avg-Loss 0.002054961398243904   ###   Correct predictions 0.99375\n",
      "Epoch 109  ###   Avg-Loss 0.0024909861385822296   ###   Correct predictions 0.9875\n",
      "Epoch 110  ###   Avg-Loss 0.0019441859796643258   ###   Correct predictions 0.99375\n",
      "Epoch 111  ###   Avg-Loss 0.0017635172232985496   ###   Correct predictions 0.99375\n",
      "Epoch 112  ###   Avg-Loss 0.0019753661006689073   ###   Correct predictions 0.9875\n",
      "Epoch 113  ###   Avg-Loss 0.0016155904158949852   ###   Correct predictions 0.99375\n",
      "Epoch 114  ###   Avg-Loss 0.0018413538113236428   ###   Correct predictions 0.99375\n",
      "Epoch 115  ###   Avg-Loss 0.0016367867588996886   ###   Correct predictions 0.99375\n",
      "Epoch 116  ###   Avg-Loss 0.0014754066243767739   ###   Correct predictions 0.99375\n",
      "Epoch 117  ###   Avg-Loss 0.00150737501680851   ###   Correct predictions 0.99375\n",
      "Epoch 118  ###   Avg-Loss 0.0013377727940678597   ###   Correct predictions 0.99375\n",
      "Epoch 119  ###   Avg-Loss 0.0014603572897613048   ###   Correct predictions 0.99375\n",
      "Epoch 120  ###   Avg-Loss 0.0012958093546330928   ###   Correct predictions 0.99375\n",
      "Epoch 121  ###   Avg-Loss 0.0012244334444403648   ###   Correct predictions 0.99375\n",
      "Epoch 122  ###   Avg-Loss 0.0011688649654388427   ###   Correct predictions 1.0\n",
      "Epoch 123  ###   Avg-Loss 0.0010741700418293477   ###   Correct predictions 1.0\n",
      "Epoch 124  ###   Avg-Loss 0.0011224309913814069   ###   Correct predictions 0.99375\n",
      "Epoch 125  ###   Avg-Loss 0.001119146402925253   ###   Correct predictions 1.0\n",
      "Epoch 126  ###   Avg-Loss 0.0011391081847250462   ###   Correct predictions 0.99375\n",
      "Epoch 127  ###   Avg-Loss 0.0010315742343664168   ###   Correct predictions 1.0\n",
      "Epoch 128  ###   Avg-Loss 0.0008185401558876038   ###   Correct predictions 1.0\n",
      "Epoch 129  ###   Avg-Loss 0.001008896715939045   ###   Correct predictions 0.99375\n",
      "Epoch 130  ###   Avg-Loss 0.0007932381704449654   ###   Correct predictions 1.0\n",
      "Epoch 131  ###   Avg-Loss 0.0008593238890171051   ###   Correct predictions 0.99375\n",
      "Epoch 132  ###   Avg-Loss 0.0008853483945131302   ###   Correct predictions 1.0\n",
      "Epoch 133  ###   Avg-Loss 0.000796498078852892   ###   Correct predictions 0.99375\n",
      "Epoch 134  ###   Avg-Loss 0.0006980234757065773   ###   Correct predictions 1.0\n",
      "Epoch 135  ###   Avg-Loss 0.0008030121214687824   ###   Correct predictions 0.99375\n",
      "Epoch 136  ###   Avg-Loss 0.0006811345927417278   ###   Correct predictions 1.0\n",
      "Epoch 137  ###   Avg-Loss 0.0007383035030215978   ###   Correct predictions 0.99375\n",
      "Epoch 138  ###   Avg-Loss 0.0007504169829189777   ###   Correct predictions 1.0\n",
      "Epoch 139  ###   Avg-Loss 0.0006550038233399391   ###   Correct predictions 1.0\n",
      "Epoch 140  ###   Avg-Loss 0.0006720185279846192   ###   Correct predictions 1.0\n",
      "Epoch 141  ###   Avg-Loss 0.0007017632480710745   ###   Correct predictions 1.0\n",
      "Epoch 142  ###   Avg-Loss 0.0006072117481380701   ###   Correct predictions 1.0\n",
      "Epoch 143  ###   Avg-Loss 0.0005939907394349575   ###   Correct predictions 1.0\n",
      "Epoch 144  ###   Avg-Loss 0.0005999019835144281   ###   Correct predictions 1.0\n",
      "Epoch 145  ###   Avg-Loss 0.0006430903449654579   ###   Correct predictions 1.0\n",
      "Epoch 146  ###   Avg-Loss 0.0006323282141238451   ###   Correct predictions 1.0\n",
      "Epoch 147  ###   Avg-Loss 0.0005223453044891357   ###   Correct predictions 1.0\n",
      "Epoch 148  ###   Avg-Loss 0.0005990971811115741   ###   Correct predictions 1.0\n",
      "Epoch 149  ###   Avg-Loss 0.0005174027755856514   ###   Correct predictions 1.0\n",
      "Epoch 150  ###   Avg-Loss 0.0005918710492551326   ###   Correct predictions 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('lin1',nn.Linear(2, 4, bias=True)),\n",
    "    ('relu1',nn.ReLU()),\n",
    "    ('lin2',nn.Linear(4, 8, bias=True)),\n",
    "    ('relu2',nn.ReLU()),\n",
    "    ('lin3',nn.Linear(8, 8, bias=True)),\n",
    "    ('relu3',nn.ReLU()),\n",
    "    ('lin4',nn.Linear(8, 2, bias=True))\n",
    "])\n",
    ")\n",
    "\n",
    "output_frequency = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(150):\n",
    "    loss_sum = 0\n",
    "    correct_pred = 0\n",
    "    for index, (data, target) in enumerate(train_loader):\n",
    "        #print(index)\n",
    "        output = model.forward(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss_sum = loss_sum + loss.data\n",
    "        for i in range(len(output)):\n",
    "            _, ind = torch.max(output[i],0)\n",
    "            label = target[i]\n",
    "            \n",
    "            if ind.data == label.data:\n",
    "                correct_pred +=1\n",
    "        #if index % (10*(output_frequency)) == 0:\n",
    "        #    print(\"#  Epoch  #  Batch  #  Avg-Loss ###############\")\n",
    "        #if index % (output_frequency) == 0 and index > 0:\n",
    "        #    print(\"#  %d  #  %d  #  %f  #\" % (epoch+1, index, loss_sum/output_frequency))\n",
    "        #    loss_sum = 0\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch '+str(epoch+1)+'  ###   Avg-Loss '+str(loss_sum.item()/160)+'   ###   Correct predictions '+str(correct_pred/160))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7115,  1.4547]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.6434, -2.7388]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3886,  2.0311]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.9741, -3.1041]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-0.8089,  0.1275]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3526, -2.2834]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.8463, -2.8233]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.9055, -2.8244]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-1.7085,  1.4306]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.7206,  2.3536]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 0.7751, -1.6380]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.4437,  2.1093]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.1609,  1.7324]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-0.5344,  0.0122]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3842,  2.1159]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-1.5523,  1.2432]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.5676, -2.6339]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 2.0958, -3.2221]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 0.9023, -1.6219]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.7551,  2.3714]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3445, -2.3139]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.5533,  2.1857]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3841, -2.4257]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3780,  2.0977]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 0.0020, -0.6190]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 0.1119, -0.9363]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3832, -2.3543]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.5189, -2.5425]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-1.9774,  1.4456]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.2206, -2.2302]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-0.1666, -0.6191]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.3856,  2.0124]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.4732, -2.4705]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.4295,  2.1136]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-0.6206,  0.0895]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.7165, -2.7979]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.8907, -2.8468]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.8076, -2.8020]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3121,  1.9327]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.0975,  1.7790]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "for index, (data, target) in enumerate(test_loader):\n",
    "    print(model.forward(data))\n",
    "    print(target)\n",
    "    print('####################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:340: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:343: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 0.9\n"
     ]
    }
   ],
   "source": [
    "net.test(test_loader,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:340: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:343: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "from pandas import DataFrame\n",
    "import torch\n",
    "\n",
    "x1 = []\n",
    "x2 = []\n",
    "y = []\n",
    "for (data,label) in train_loader:\n",
    "    for point in data:\n",
    "        x1.append(point[0].item())\n",
    "        x2.append(point[1].item())\n",
    "        y.append(torch.argmax(net.forward(point)).item())\n",
    "#print(x1)\n",
    "df = DataFrame(dict(x=x1, y=x2, label=y))\n",
    "print(y)\n",
    "colors = {0:'red', 1:'blue'}\n",
    "fig, ax = pyplot.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 0.875\n"
     ]
    }
   ],
   "source": [
    "correct_pred = 0\n",
    "for _, (data, label) in enumerate(test_loader):\n",
    "    prediction = model.forward(data)\n",
    "    _, ind = torch.max(prediction,1)\n",
    "    if ind.data == label.data:\n",
    "        correct_pred +=1\n",
    "print('Correct predictions: '+str(correct_pred/40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################\n",
      "tensor([-0.7453,  0.6668])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:340: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:343: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.5517,  3.5517], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([ 1.4441, -0.3960])\n",
      "tensor([ 3.8032, -3.8032], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([-0.7237,  0.6901])\n",
      "tensor([-3.5532,  3.5532], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([0.4723, 0.8815])\n",
      "tensor([-1.5511,  1.5511], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([0.9284, 0.3717])\n",
      "tensor([ 2.0221, -2.0221], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([ 0.1587, -0.0406])\n",
      "tensor([ 0.7871, -0.7871], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([1.9754, 0.2797])\n",
      "tensor([ 3.7395, -3.7395], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([0.0246, 0.2797])\n",
      "tensor([-0.9989,  0.9989], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([0.0476, 0.9989])\n",
      "tensor([-2.9615,  2.9615], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([ 1.8053, -0.0929])\n",
      "tensor([ 3.8243, -3.8243], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([ 1.6056, -0.2958])\n",
      "tensor([ 3.8254, -3.8254], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([0.1736, 0.9848])\n",
      "tensor([-2.6809,  2.6809], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([ 0.5000, -0.3660])\n",
      "tensor([ 2.7974, -2.7974], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([0.9819, 0.1893])\n",
      "tensor([ 2.6397, -2.6397], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([ 0.8264, -0.4848])\n",
      "tensor([ 3.4583, -3.4583], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([-0.9029,  0.4298])\n",
      "tensor([-3.4797,  3.4797], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([ 1.8580, -0.0137])\n",
      "tensor([ 3.8150, -3.8150], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([0.7453, 0.6668])\n",
      "tensor([ 0.3133, -0.3133], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([0.2048, 0.9788])\n",
      "tensor([-2.5970,  2.5970], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([0.1262, 0.0138])\n",
      "tensor([ 0.4475, -0.4475], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "for index, (data, label) in enumerate(train_loader):\n",
    "    if index == 2:\n",
    "        for i in range(len(data)):\n",
    "            print('######################')\n",
    "            print(data[i])\n",
    "            print(net.forward(data[i]))\n",
    "            print(label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FC0', 'FC1', 'FC2', 'FC3']\n",
      "MSALinearLayer(\n",
      "  (linear): Linear(in_features=2, out_features=4, bias=False)\n",
      ")\n",
      "ReluLayer()\n",
      "MSALinearLayer(\n",
      "  (linear): Linear(in_features=4, out_features=2, bias=False)\n",
      ")\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'FC3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-1a0203474ed2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'FC1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'FC2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'FC3'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'FC3'"
     ]
    }
   ],
   "source": [
    "print(net.fc_keys)\n",
    "print(net.layers_dict['FC0'])\n",
    "print(net.layers_dict['FC1'])\n",
    "print(net.layers_dict['FC2'])\n",
    "print(net.layers_dict['FC3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:340: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:343: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  compared_signs = torch.tensor(torch.ne(new_signs, old_signs).detach(), dtype=torch.float32)\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:149: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_weights = new_signs*torch.tensor(torch.ge(torch.abs(self.m_accumulated),self.rho*max_elem*torch.ones_like(self.m_accumulated)).detach(),dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import loadMNIST\n",
    "from Networks.ResNet import ConvNet, FCNet\n",
    "\n",
    "#net = ConvNet(3, [1], [3,6], num_fc=2, sizes_fc=[100,10])\n",
    "net = FCNet(num_fc=4,sizes_fc=[784,2048,2048,2048,10])\n",
    "train_loader, test_loader = loadMNIST('Dataset/input/mnist_train.csv', 'Dataset/input/mnist_test.csv')\n",
    "#net.train_backprop(1,train_loader)\n",
    "#net.test(test_loader,10000)\n",
    "print(train_loader.batch_size)\n",
    "\n",
    "net.train_msa(1,train_loader)\n",
    "\n",
    "#for index, (data, target) in enumerate(train_loader):\n",
    "#    if index == 1:\n",
    "#        print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 4, 1])\n",
      "tensor([[-0.5047, -0.7449,  0.6850,  0.4106, -0.2220],\n",
      "        [-1.2125, -2.6010, -1.0037,  0.0987,  1.0532],\n",
      "        [-0.4957, -0.2819, -2.5197,  0.0198, -0.2908]])\n",
      "tensor(1.4569)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "batch_size = 1\n",
    "a = torch.LongTensor(batch_size).random_(5)\n",
    "b = torch.randn(batch_size, 5)\n",
    "print(a)\n",
    "print(b)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(b, a)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "index 10 is out of bounds for dim with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-f8db13b9caa0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'FC4'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: index 10 is out of bounds for dim with size 10"
     ]
    }
   ],
   "source": [
    "test = net.layers_dict['FC4'].linear.weight\n",
    "print(test[(test == 0).nonzero()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.test(test_loader,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1574, -0.1913,  0.3191],\n",
      "        [ 0.4584,  0.3066, -0.3247],\n",
      "        [ 0.2495,  0.4553,  0.0500],\n",
      "        [ 0.3111, -0.3222,  0.2152]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.4820, -0.5667, -0.2615,  0.5324], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "test_layer = torch.nn.Linear(3, 4)\n",
    "print(test_layer.weight)\n",
    "print(test_layer.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lin1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-ce97ca40c88a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlin1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'lin1' is not defined"
     ]
    }
   ],
   "source": [
    "print(model[lin1].parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.1500, 3.1500, 3.1500, 3.1500, 3.1500, 3.1500, 3.1500, 3.1500, 3.1500,\n",
      "        3.1500, 3.1500, 3.1500, 3.1500, 3.1500, 3.1500, 3.1500, 3.1500, 3.1500,\n",
      "        3.1500, 3.1500, 3.1500, 3.1500, 3.1500, 3.1500, 3.1500, 3.1500, 3.1500,\n",
      "        3.1500, 3.1500, 3.1500, 3.1500, 4.3300, 3.6900, 4.8500, 4.8100, 4.6700,\n",
      "        4.6100], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "print(net.avg_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
