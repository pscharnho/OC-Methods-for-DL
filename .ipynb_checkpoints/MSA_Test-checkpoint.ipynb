{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:164: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  compared_weight_signs = torch.tensor(torch.ne(new_weight_signs, old_weight_signs).detach(), dtype=torch.float32)\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:169: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_weights = new_weight_signs*torch.tensor(torch.ge(torch.abs(self.m_accumulated),self.rho*max_weight_elem*torch.ones_like(self.m_accumulated)).detach(),dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ##############\n",
      "Correct predictions: 0.6625\n",
      "Epoch 2 ##############\n",
      "Correct predictions: 0.7791666666666667\n",
      "Epoch 3 ##############\n",
      "Correct predictions: 0.81875\n",
      "Epoch 4 ##############\n",
      "Correct predictions: 0.8083333333333333\n",
      "Epoch 5 ##############\n",
      "Correct predictions: 0.7875\n",
      "Epoch 6 ##############\n",
      "Correct predictions: 0.8020833333333334\n",
      "Epoch 7 ##############\n",
      "Correct predictions: 0.8125\n",
      "Epoch 8 ##############\n",
      "Correct predictions: 0.4895833333333333\n",
      "Epoch 9 ##############\n",
      "Correct predictions: 0.81875\n",
      "Epoch 10 ##############\n",
      "Correct predictions: 0.8166666666666667\n",
      "Epoch 11 ##############\n",
      "Correct predictions: 0.54375\n",
      "Epoch 12 ##############\n",
      "Correct predictions: 0.7979166666666667\n",
      "Epoch 13 ##############\n",
      "Correct predictions: 0.7916666666666666\n",
      "Epoch 14 ##############\n",
      "Correct predictions: 0.5833333333333334\n",
      "Epoch 15 ##############\n",
      "Correct predictions: 0.59375\n",
      "Epoch 16 ##############\n",
      "Correct predictions: 0.8208333333333333\n",
      "Epoch 17 ##############\n",
      "Correct predictions: 0.8229166666666666\n",
      "Epoch 18 ##############\n",
      "Correct predictions: 0.7770833333333333\n",
      "Epoch 19 ##############\n",
      "Correct predictions: 0.59375\n",
      "Epoch 20 ##############\n",
      "Correct predictions: 0.7791666666666667\n",
      "Epoch 21 ##############\n",
      "Correct predictions: 0.8\n",
      "Epoch 22 ##############\n",
      "Correct predictions: 0.8145833333333333\n",
      "Epoch 23 ##############\n",
      "Correct predictions: 0.5083333333333333\n",
      "Epoch 24 ##############\n",
      "Correct predictions: 0.7604166666666666\n",
      "Epoch 25 ##############\n",
      "Correct predictions: 0.5875\n",
      "Epoch 26 ##############\n",
      "Correct predictions: 0.8145833333333333\n",
      "Epoch 27 ##############\n",
      "Correct predictions: 0.9041666666666667\n",
      "Epoch 28 ##############\n",
      "Correct predictions: 0.875\n",
      "Epoch 29 ##############\n",
      "Correct predictions: 0.8729166666666667\n",
      "Epoch 30 ##############\n",
      "Correct predictions: 0.8604166666666667\n",
      "Epoch 31 ##############\n",
      "Correct predictions: 0.8729166666666667\n",
      "Epoch 32 ##############\n",
      "Correct predictions: 0.85625\n",
      "Epoch 33 ##############\n",
      "Correct predictions: 0.8458333333333333\n",
      "Epoch 34 ##############\n",
      "Correct predictions: 0.8729166666666667\n",
      "Epoch 35 ##############\n",
      "Correct predictions: 0.875\n",
      "Epoch 36 ##############\n",
      "Correct predictions: 0.9\n",
      "Epoch 37 ##############\n",
      "Correct predictions: 0.9145833333333333\n",
      "Epoch 38 ##############\n",
      "Correct predictions: 0.8416666666666667\n",
      "Epoch 39 ##############\n",
      "Correct predictions: 0.85\n",
      "Epoch 40 ##############\n",
      "Correct predictions: 0.9145833333333333\n",
      "Epoch 41 ##############\n",
      "Correct predictions: 0.9145833333333333\n",
      "Epoch 42 ##############\n",
      "Correct predictions: 0.9125\n",
      "Epoch 43 ##############\n",
      "Correct predictions: 0.9125\n",
      "Epoch 44 ##############\n",
      "Correct predictions: 0.9104166666666667\n",
      "Epoch 45 ##############\n",
      "Correct predictions: 0.9125\n",
      "Epoch 46 ##############\n",
      "Correct predictions: 0.9125\n",
      "Epoch 47 ##############\n",
      "Correct predictions: 0.9145833333333333\n",
      "Epoch 48 ##############\n",
      "Correct predictions: 0.9125\n",
      "Epoch 49 ##############\n",
      "Correct predictions: 0.9083333333333333\n",
      "Epoch 50 ##############\n",
      "Correct predictions: 0.9125\n",
      "Epoch 51 ##############\n",
      "Correct predictions: 0.8458333333333333\n",
      "Epoch 52 ##############\n",
      "Correct predictions: 0.8520833333333333\n",
      "Epoch 53 ##############\n",
      "Correct predictions: 0.86875\n",
      "Epoch 54 ##############\n",
      "Correct predictions: 0.88125\n",
      "Epoch 55 ##############\n",
      "Correct predictions: 0.89375\n",
      "Epoch 56 ##############\n",
      "Correct predictions: 0.8666666666666667\n",
      "Epoch 57 ##############\n",
      "Correct predictions: 0.8854166666666666\n",
      "Epoch 58 ##############\n",
      "Correct predictions: 0.88125\n",
      "Epoch 59 ##############\n",
      "Correct predictions: 0.9145833333333333\n",
      "Epoch 60 ##############\n",
      "Correct predictions: 0.88125\n",
      "Time elapsed:  48.930104379\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import ConvNet, FCNet\n",
    "import torch\n",
    "#torch.manual_seed(0)\n",
    "train_loader, test_loader = makeMoonsDataset(600,40)\n",
    "torch.manual_seed(3)\n",
    "\n",
    "#for i in range(5,7):\n",
    "#    for j in range(97,100):\n",
    "#        print('Rho '+str(i/10)+' #############################################')\n",
    "#        print('EMA Alpha '+str(j/100)+' ###############################')\n",
    "#        net = FCNet(num_fc=4,sizes_fc=[2,4,8,16,2], bias=True, batchnorm=True, test=False)\n",
    "#        net.set_rho(i/10)\n",
    "#        net.set_ema_alpha(j/100)\n",
    "#        net.train_msa(150,train_loader)\n",
    "    \n",
    "net = FCNet(num_fc=4,sizes_fc=[2,4,8,16,2], bias=False, batchnorm=True, test=False)  \n",
    "net.set_rho(0.6)\n",
    "net.set_ema_alpha(0.99)\n",
    "net.train_msa(60,train_loader)\n",
    "\n",
    "#net = ConvNet(3, [1], [3,6], num_fc=2, sizes_fc=[100,10])\n",
    "\n",
    "#train_loader, test_loader = loadMNIST('Dataset/input/mnist_train.csv', 'Dataset/input/mnist_test.csv')\n",
    "\n",
    "#net.train_backprop(1,train_loader)\n",
    "#net.test(test_loader,10000)\n",
    "#print(train_loader.batch_size)\n",
    "\n",
    "#\n",
    "#for index, (data, label) in enumerate(train_loader):\n",
    "#    print(data)\n",
    "#    print(net.forward(data))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for index, (data, label) in enumerate(train_loader):\n",
    "#    print(data)\n",
    "#    print(net.forward(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:340: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:343: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  compared_weight_signs = torch.tensor(torch.ne(new_weight_signs, old_weight_signs).detach(), dtype=torch.float32)\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:148: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_weights = new_weight_signs*torch.tensor(torch.ge(torch.abs(self.m_accumulated),self.rho*max_weight_elem*torch.ones_like(self.m_accumulated)).detach(),dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed:  10.146643088\n",
      "Seed 0   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.60604992\n",
      "Seed 1   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.699145381999998\n",
      "Seed 2   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.273395311000002\n",
      "Seed 3   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.354167834000002\n",
      "Seed 4   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.854717102000002\n",
      "Seed 5   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.360114429000006\n",
      "Seed 6   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.212407759000001\n",
      "Seed 7   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.896629595000007\n",
      "Seed 8   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.603672412999998\n",
      "Seed 9   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.097800613000004\n",
      "Seed 10   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.248547408000007\n",
      "Seed 11   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.193569678999978\n",
      "Seed 12   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.238679597000015\n",
      "Seed 13   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.202587755999986\n",
      "Seed 14   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.563680431999984\n",
      "Seed 15   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.800362409000002\n",
      "Seed 16   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.172943199000002\n",
      "Seed 17   ###   Best-Avg 0.8\n",
      "Time elapsed:  11.664164929999998\n",
      "Seed 18   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.64493668099999\n",
      "Seed 19   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.808216150000021\n",
      "Seed 20   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.073397882000023\n",
      "Seed 21   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.22652783800001\n",
      "Seed 22   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.820626478999998\n",
      "Seed 23   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.476716109999984\n",
      "Seed 24   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.439701024999977\n",
      "Seed 25   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.500485010999967\n",
      "Seed 26   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.450659146999953\n",
      "Seed 27   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.663561942000001\n",
      "Seed 28   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.897770284000046\n",
      "Seed 29   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.072501369000008\n",
      "Seed 30   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.462130296999987\n",
      "Seed 31   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.181674433000012\n",
      "Seed 32   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.720457097999997\n",
      "Seed 33   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.911982378000005\n",
      "Seed 34   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.12446210600001\n",
      "Seed 35   ###   Best-Avg 0.8\n",
      "Time elapsed:  11.751844816999949\n",
      "Seed 36   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.914494053999988\n",
      "Seed 37   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.721946830999968\n",
      "Seed 38   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.854742291999969\n",
      "Seed 39   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.348190395000017\n",
      "Seed 40   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.857938689000036\n",
      "Seed 41   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.154126195999993\n",
      "Seed 42   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.182050207999964\n",
      "Seed 43   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.263982440000007\n",
      "Seed 44   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.456949645000066\n",
      "Seed 45   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.78960939500007\n",
      "Seed 46   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.985499520000076\n",
      "Seed 47   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.699192676000052\n",
      "Seed 48   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.425626183999952\n",
      "Seed 49   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.54046722499993\n",
      "Seed 50   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.030507655000065\n",
      "Seed 51   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.125098506000086\n",
      "Seed 52   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.919794993999972\n",
      "Seed 53   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.273062202999995\n",
      "Seed 54   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.648992066000005\n",
      "Seed 55   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.420492824999997\n",
      "Seed 56   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.824312771999985\n",
      "Seed 57   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.768213887999991\n",
      "Seed 58   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.478139016\n",
      "Seed 59   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.264060061999999\n",
      "Seed 60   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.428517747\n",
      "Seed 61   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.804717488000051\n",
      "Seed 62   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.830327735999958\n",
      "Seed 63   ###   Best-Avg 0.84375\n",
      "Time elapsed:  13.149567551000018\n",
      "Seed 64   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.077774036999926\n",
      "Seed 65   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.304072090999966\n",
      "Seed 66   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.241903238999953\n",
      "Seed 67   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.745793354000057\n",
      "Seed 68   ###   Best-Avg 0.84375\n",
      "Time elapsed:  13.113821154999982\n",
      "Seed 69   ###   Best-Avg 0.825\n",
      "Time elapsed:  12.56083924699999\n",
      "Seed 70   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.023275922000039\n",
      "Seed 71   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.34733552099999\n",
      "Seed 72   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.205158547999986\n",
      "Seed 73   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.781024666000008\n",
      "Seed 74   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.818582595000066\n",
      "Seed 75   ###   Best-Avg 0.8\n",
      "Time elapsed:  12.775833219999981\n",
      "Seed 76   ###   Best-Avg 0.84375\n",
      "Time elapsed:  12.596671489999949\n",
      "Seed 77   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.738887018000014\n",
      "Seed 78   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.079107496999995\n",
      "Seed 79   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.085618524999973\n",
      "Seed 80   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.447313674000043\n",
      "Seed 81   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.547355056000015\n",
      "Seed 82   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.684601208999993\n",
      "Seed 83   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.680478483000002\n",
      "Seed 84   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.97397130999991\n",
      "Seed 85   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.147157143999948\n",
      "Seed 86   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.294491636999965\n",
      "Seed 87   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.387910954999938\n",
      "Seed 88   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.786750217999952\n",
      "Seed 89   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.385938010000018\n",
      "Seed 90   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.857001564999791\n",
      "Seed 91   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.818907478000028\n",
      "Seed 92   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.13523568200003\n",
      "Seed 93   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.777255096999852\n",
      "Seed 94   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.35629602400013\n",
      "Seed 95   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.564049009999962\n",
      "Seed 96   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.523687422999956\n",
      "Seed 97   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.850773782000033\n",
      "Seed 98   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.6953454840002\n",
      "Seed 99   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.015339932000188\n",
      "Seed 100   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.39123638000001\n",
      "Seed 101   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.594417870000143\n",
      "Seed 102   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.783619105999833\n",
      "Seed 103   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.876350617000071\n",
      "Seed 104   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.439500544000111\n",
      "Seed 105   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.181604522000043\n",
      "Seed 106   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.076896543999965\n",
      "Seed 107   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.832593179000014\n",
      "Seed 108   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.307717772999922\n",
      "Seed 109   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.43920239099998\n",
      "Seed 110   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.498897093999858\n",
      "Seed 111   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.657795781000004\n",
      "Seed 112   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.814789377999887\n",
      "Seed 113   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.931495412999993\n",
      "Seed 114   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.288754776999895\n",
      "Seed 115   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.244204666000087\n",
      "Seed 116   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.550813107000067\n",
      "Seed 117   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.388158215999965\n",
      "Seed 118   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.750045622000016\n",
      "Seed 119   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.235819392000167\n",
      "Seed 120   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.996203699000034\n",
      "Seed 121   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.989480880999963\n",
      "Seed 122   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.056708553999897\n",
      "Seed 123   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.285523938999859\n",
      "Seed 124   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.525031678000005\n",
      "Seed 125   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.853770211999972\n",
      "Seed 126   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.874691244999894\n",
      "Seed 127   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.638378874000182\n",
      "Seed 128   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.042981213999838\n",
      "Seed 129   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.147106766999968\n",
      "Seed 130   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.252185378999911\n",
      "Seed 131   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.136590218000038\n",
      "Seed 132   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.631353791000038\n",
      "Seed 133   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.084455215999924\n",
      "Seed 134   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.278537409000137\n",
      "Seed 135   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.179119578000154\n",
      "Seed 136   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.569564312000011\n",
      "Seed 137   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.627897280999832\n",
      "Seed 138   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.784591183999964\n",
      "Seed 139   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.15707995899993\n",
      "Seed 140   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.181525357000055\n",
      "Seed 141   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.311887792000107\n",
      "Seed 142   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.489734566999914\n",
      "Seed 143   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.879751607999879\n",
      "Seed 144   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.325272771000073\n",
      "Seed 145   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.095041162999905\n",
      "Seed 146   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.463122424999938\n",
      "Seed 147   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.492982883999957\n",
      "Seed 148   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.48772615300004\n",
      "Seed 149   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.673118235000175\n",
      "Seed 150   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.028434469000103\n",
      "Seed 151   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.975339212000108\n",
      "Seed 152   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.189310215000205\n",
      "Seed 153   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.902211723999926\n",
      "Seed 154   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.252953891999823\n",
      "Seed 155   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.101709491000065\n",
      "Seed 156   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.322991391999949\n",
      "Seed 157   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.231441695000058\n",
      "Seed 158   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.062093285999936\n",
      "Seed 159   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.669411380999918\n",
      "Seed 160   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.712783790999993\n",
      "Seed 161   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.289970005000214\n",
      "Seed 162   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.054718644999866\n",
      "Seed 163   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.402621168999985\n",
      "Seed 164   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.74273369599996\n",
      "Seed 165   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.994736071000034\n",
      "Seed 166   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.28945646399984\n",
      "Seed 167   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.997000486000161\n",
      "Seed 168   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.741073809999989\n",
      "Seed 169   ###   Best-Avg 0.825\n",
      "Time elapsed:  9.930781389999993\n",
      "Seed 170   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.058178237999982\n",
      "Seed 171   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.299073929000087\n",
      "Seed 172   ###   Best-Avg 0.8\n",
      "Time elapsed:  10.528790453000056\n",
      "Seed 173   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.644680681000182\n",
      "Seed 174   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.82024248000016\n",
      "Seed 175   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.990435996000087\n",
      "Seed 176   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.8661229679999\n",
      "Seed 177   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.123140467999974\n",
      "Seed 178   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.590671946999919\n",
      "Seed 179   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.689152142000012\n",
      "Seed 180   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.67604732399991\n",
      "Seed 181   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.032750995000015\n",
      "Seed 182   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.450169252000023\n",
      "Seed 183   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.201859338999839\n",
      "Seed 184   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.017197729000145\n",
      "Seed 185   ###   Best-Avg 0.84375\n",
      "Time elapsed:  9.891306548999637\n",
      "Seed 186   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.39308081199988\n",
      "Seed 187   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.244145035999736\n",
      "Seed 188   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.604870163000214\n",
      "Seed 189   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.960879856999782\n",
      "Seed 190   ###   Best-Avg 0.825\n",
      "Time elapsed:  10.77327322300016\n",
      "Seed 191   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.03098315699981\n",
      "Seed 192   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.10578955100027\n",
      "Seed 193   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.368450354000288\n",
      "Seed 194   ###   Best-Avg 0.84375\n",
      "Time elapsed:  11.467802901000141\n",
      "Seed 195   ###   Best-Avg 0.825\n",
      "Time elapsed:  11.05645203999984\n",
      "Seed 196   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.349908370000321\n",
      "Seed 197   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.354014645000007\n",
      "Seed 198   ###   Best-Avg 0.84375\n",
      "Time elapsed:  10.494870496999738\n",
      "Seed 199   ###   Best-Avg 0.825\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import makeMoonsDataset\n",
    "from Networks.ResNet import ConvNet, FCNet\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "train_loader, test_loader = makeMoonsDataset()\n",
    "\n",
    "for seed in range(200):\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    net = FCNet(num_fc=2,sizes_fc=[2,4,2], bias=False, test=False)\n",
    "    \n",
    "    net.train_msa(60,train_loader)\n",
    "    print('Seed '+str(seed)+'   ###   Best-Avg '+str(net.best_avg))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  ###   Avg-Loss 0.006972106099128723   ###   Correct predictions 0.5125\n",
      "Epoch 2  ###   Avg-Loss 0.006884024739265442   ###   Correct predictions 0.64\n",
      "Epoch 3  ###   Avg-Loss 0.006815706491470337   ###   Correct predictions 0.8075\n",
      "Epoch 4  ###   Avg-Loss 0.00672156810760498   ###   Correct predictions 0.7875\n",
      "Epoch 5  ###   Avg-Loss 0.006602643728256225   ###   Correct predictions 0.81875\n",
      "Epoch 6  ###   Avg-Loss 0.006437674760818481   ###   Correct predictions 0.78125\n",
      "Epoch 7  ###   Avg-Loss 0.006203002333641052   ###   Correct predictions 0.8075\n",
      "Epoch 8  ###   Avg-Loss 0.005890456438064576   ###   Correct predictions 0.80625\n",
      "Epoch 9  ###   Avg-Loss 0.005492082238197327   ###   Correct predictions 0.81125\n",
      "Epoch 10  ###   Avg-Loss 0.005035045146942138   ###   Correct predictions 0.815\n",
      "Epoch 11  ###   Avg-Loss 0.004574441313743591   ###   Correct predictions 0.8225\n",
      "Epoch 12  ###   Avg-Loss 0.004161638021469116   ###   Correct predictions 0.8275\n",
      "Epoch 13  ###   Avg-Loss 0.003833962678909302   ###   Correct predictions 0.835\n",
      "Epoch 14  ###   Avg-Loss 0.003560939431190491   ###   Correct predictions 0.8425\n",
      "Epoch 15  ###   Avg-Loss 0.003347531259059906   ###   Correct predictions 0.85\n",
      "Epoch 16  ###   Avg-Loss 0.0031882518529891967   ###   Correct predictions 0.85625\n",
      "Epoch 17  ###   Avg-Loss 0.003036620020866394   ###   Correct predictions 0.86125\n",
      "Epoch 18  ###   Avg-Loss 0.0029288554191589356   ###   Correct predictions 0.86625\n",
      "Epoch 19  ###   Avg-Loss 0.0028459420800209046   ###   Correct predictions 0.87125\n",
      "Epoch 20  ###   Avg-Loss 0.0027791747450828554   ###   Correct predictions 0.87875\n",
      "Epoch 21  ###   Avg-Loss 0.0027196919918060303   ###   Correct predictions 0.875\n",
      "Epoch 22  ###   Avg-Loss 0.002649485468864441   ###   Correct predictions 0.8775\n",
      "Epoch 23  ###   Avg-Loss 0.002617426514625549   ###   Correct predictions 0.88\n",
      "Epoch 24  ###   Avg-Loss 0.002624366581439972   ###   Correct predictions 0.88\n",
      "Epoch 25  ###   Avg-Loss 0.0025755509734153746   ###   Correct predictions 0.875\n",
      "Epoch 26  ###   Avg-Loss 0.002591806352138519   ###   Correct predictions 0.875\n",
      "Epoch 27  ###   Avg-Loss 0.0025393998622894286   ###   Correct predictions 0.8775\n",
      "Epoch 28  ###   Avg-Loss 0.0025472497940063478   ###   Correct predictions 0.88125\n",
      "Epoch 29  ###   Avg-Loss 0.002594262659549713   ###   Correct predictions 0.88\n",
      "Epoch 30  ###   Avg-Loss 0.0025022655725479128   ###   Correct predictions 0.88125\n",
      "Epoch 31  ###   Avg-Loss 0.0024907472729682923   ###   Correct predictions 0.88\n",
      "Epoch 32  ###   Avg-Loss 0.002492455095052719   ###   Correct predictions 0.87625\n",
      "Epoch 33  ###   Avg-Loss 0.0024940899014472963   ###   Correct predictions 0.885\n",
      "Epoch 34  ###   Avg-Loss 0.002469843327999115   ###   Correct predictions 0.8825\n",
      "Epoch 35  ###   Avg-Loss 0.0024411413073539734   ###   Correct predictions 0.885\n",
      "Epoch 36  ###   Avg-Loss 0.0024338716268539427   ###   Correct predictions 0.8875\n",
      "Epoch 37  ###   Avg-Loss 0.0024314931035041808   ###   Correct predictions 0.88625\n",
      "Epoch 38  ###   Avg-Loss 0.0025060570240020754   ###   Correct predictions 0.8775\n",
      "Epoch 39  ###   Avg-Loss 0.0025447937846183777   ###   Correct predictions 0.88375\n",
      "Epoch 40  ###   Avg-Loss 0.0024182854592800142   ###   Correct predictions 0.8825\n",
      "Epoch 41  ###   Avg-Loss 0.002450411766767502   ###   Correct predictions 0.87625\n",
      "Epoch 42  ###   Avg-Loss 0.0026019251346588133   ###   Correct predictions 0.8775\n",
      "Epoch 43  ###   Avg-Loss 0.0024240769445896147   ###   Correct predictions 0.885\n",
      "Epoch 44  ###   Avg-Loss 0.0023758623003959658   ###   Correct predictions 0.88625\n",
      "Epoch 45  ###   Avg-Loss 0.0023497729003429412   ###   Correct predictions 0.89875\n",
      "Epoch 46  ###   Avg-Loss 0.002412199079990387   ###   Correct predictions 0.88375\n",
      "Epoch 47  ###   Avg-Loss 0.0023585808277130126   ###   Correct predictions 0.8875\n",
      "Epoch 48  ###   Avg-Loss 0.0023639774322509765   ###   Correct predictions 0.89125\n",
      "Epoch 49  ###   Avg-Loss 0.0023049387335777284   ###   Correct predictions 0.8875\n",
      "Epoch 50  ###   Avg-Loss 0.0023973146080970764   ###   Correct predictions 0.8875\n",
      "Epoch 51  ###   Avg-Loss 0.002304441034793854   ###   Correct predictions 0.8875\n",
      "Epoch 52  ###   Avg-Loss 0.0023454967141151428   ###   Correct predictions 0.8875\n",
      "Epoch 53  ###   Avg-Loss 0.0023232349753379822   ###   Correct predictions 0.8925\n",
      "Epoch 54  ###   Avg-Loss 0.002257949560880661   ###   Correct predictions 0.89625\n",
      "Epoch 55  ###   Avg-Loss 0.0023503759503364563   ###   Correct predictions 0.88875\n",
      "Epoch 56  ###   Avg-Loss 0.0023565545678138735   ###   Correct predictions 0.88375\n",
      "Epoch 57  ###   Avg-Loss 0.0022511546313762664   ###   Correct predictions 0.89125\n",
      "Epoch 58  ###   Avg-Loss 0.002321882396936417   ###   Correct predictions 0.89125\n",
      "Epoch 59  ###   Avg-Loss 0.002391671687364578   ###   Correct predictions 0.88875\n",
      "Epoch 60  ###   Avg-Loss 0.002248111665248871   ###   Correct predictions 0.89125\n",
      "Epoch 61  ###   Avg-Loss 0.002222311496734619   ###   Correct predictions 0.89\n",
      "Epoch 62  ###   Avg-Loss 0.0022854414582252503   ###   Correct predictions 0.89625\n",
      "Epoch 63  ###   Avg-Loss 0.0022377817332744597   ###   Correct predictions 0.885\n",
      "Epoch 64  ###   Avg-Loss 0.0022107312083244324   ###   Correct predictions 0.89375\n",
      "Epoch 65  ###   Avg-Loss 0.0022558075189590453   ###   Correct predictions 0.89625\n",
      "Epoch 66  ###   Avg-Loss 0.0022417186200618744   ###   Correct predictions 0.89375\n",
      "Epoch 67  ###   Avg-Loss 0.0021777452528476716   ###   Correct predictions 0.89625\n",
      "Epoch 68  ###   Avg-Loss 0.002168337553739548   ###   Correct predictions 0.8925\n",
      "Epoch 69  ###   Avg-Loss 0.0022282488644123077   ###   Correct predictions 0.8975\n",
      "Epoch 70  ###   Avg-Loss 0.0023211857676506045   ###   Correct predictions 0.88625\n",
      "Epoch 71  ###   Avg-Loss 0.0022627274692058564   ###   Correct predictions 0.89125\n",
      "Epoch 72  ###   Avg-Loss 0.002182486653327942   ###   Correct predictions 0.8925\n",
      "Epoch 73  ###   Avg-Loss 0.0021395877003669737   ###   Correct predictions 0.89375\n",
      "Epoch 74  ###   Avg-Loss 0.0021964783966541292   ###   Correct predictions 0.89125\n",
      "Epoch 75  ###   Avg-Loss 0.002174032628536224   ###   Correct predictions 0.8975\n",
      "Epoch 76  ###   Avg-Loss 0.0021467871963977815   ###   Correct predictions 0.8925\n",
      "Epoch 77  ###   Avg-Loss 0.0021711555123329162   ###   Correct predictions 0.89625\n",
      "Epoch 78  ###   Avg-Loss 0.002161819040775299   ###   Correct predictions 0.89375\n",
      "Epoch 79  ###   Avg-Loss 0.002142372727394104   ###   Correct predictions 0.89875\n",
      "Epoch 80  ###   Avg-Loss 0.0021935708820819853   ###   Correct predictions 0.89375\n",
      "Epoch 81  ###   Avg-Loss 0.002289169430732727   ###   Correct predictions 0.89375\n",
      "Epoch 82  ###   Avg-Loss 0.00214740589261055   ###   Correct predictions 0.89875\n",
      "Epoch 83  ###   Avg-Loss 0.002204541563987732   ###   Correct predictions 0.90125\n",
      "Epoch 84  ###   Avg-Loss 0.0021494406461715697   ###   Correct predictions 0.89625\n",
      "Epoch 85  ###   Avg-Loss 0.0022098098695278166   ###   Correct predictions 0.89625\n",
      "Epoch 86  ###   Avg-Loss 0.0021729624271392823   ###   Correct predictions 0.8975\n",
      "Epoch 87  ###   Avg-Loss 0.0021450120210647584   ###   Correct predictions 0.90125\n",
      "Epoch 88  ###   Avg-Loss 0.0021200229227542875   ###   Correct predictions 0.89875\n",
      "Epoch 89  ###   Avg-Loss 0.002118009626865387   ###   Correct predictions 0.8975\n",
      "Epoch 90  ###   Avg-Loss 0.002136733680963516   ###   Correct predictions 0.89875\n",
      "Epoch 91  ###   Avg-Loss 0.0021567654609680174   ###   Correct predictions 0.8975\n",
      "Epoch 92  ###   Avg-Loss 0.002105867862701416   ###   Correct predictions 0.90125\n",
      "Epoch 93  ###   Avg-Loss 0.0021249252557754518   ###   Correct predictions 0.89875\n",
      "Epoch 94  ###   Avg-Loss 0.002201254963874817   ###   Correct predictions 0.89875\n",
      "Epoch 95  ###   Avg-Loss 0.002204790711402893   ###   Correct predictions 0.8925\n",
      "Epoch 96  ###   Avg-Loss 0.0021366432309150698   ###   Correct predictions 0.90375\n",
      "Epoch 97  ###   Avg-Loss 0.0020975126326084137   ###   Correct predictions 0.89875\n",
      "Epoch 98  ###   Avg-Loss 0.002266747057437897   ###   Correct predictions 0.8925\n",
      "Epoch 99  ###   Avg-Loss 0.002124752551317215   ###   Correct predictions 0.89875\n",
      "Epoch 100  ###   Avg-Loss 0.00208818718791008   ###   Correct predictions 0.89875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101  ###   Avg-Loss 0.0021341311931610107   ###   Correct predictions 0.8975\n",
      "Epoch 102  ###   Avg-Loss 0.0021500085294246674   ###   Correct predictions 0.90625\n",
      "Epoch 103  ###   Avg-Loss 0.0021351224184036257   ###   Correct predictions 0.89125\n",
      "Epoch 104  ###   Avg-Loss 0.002126927077770233   ###   Correct predictions 0.89625\n",
      "Epoch 105  ###   Avg-Loss 0.0021133683621883392   ###   Correct predictions 0.90125\n",
      "Epoch 106  ###   Avg-Loss 0.0021343450248241425   ###   Correct predictions 0.89875\n",
      "Epoch 107  ###   Avg-Loss 0.002157096415758133   ###   Correct predictions 0.89875\n",
      "Epoch 108  ###   Avg-Loss 0.0020963265001773834   ###   Correct predictions 0.90375\n",
      "Epoch 109  ###   Avg-Loss 0.0021307580173015594   ###   Correct predictions 0.89875\n",
      "Epoch 110  ###   Avg-Loss 0.0021491865813732147   ###   Correct predictions 0.8925\n",
      "Epoch 111  ###   Avg-Loss 0.0021473711729049685   ###   Correct predictions 0.89375\n",
      "Epoch 112  ###   Avg-Loss 0.002167387455701828   ###   Correct predictions 0.89125\n",
      "Epoch 113  ###   Avg-Loss 0.0021715067327022553   ###   Correct predictions 0.89125\n",
      "Epoch 114  ###   Avg-Loss 0.002129904478788376   ###   Correct predictions 0.9025\n",
      "Epoch 115  ###   Avg-Loss 0.0021559429168701173   ###   Correct predictions 0.89\n",
      "Epoch 116  ###   Avg-Loss 0.0021476863324642183   ###   Correct predictions 0.90125\n",
      "Epoch 117  ###   Avg-Loss 0.0022298774123191835   ###   Correct predictions 0.8925\n",
      "Epoch 118  ###   Avg-Loss 0.0021476173400878907   ###   Correct predictions 0.8925\n",
      "Epoch 119  ###   Avg-Loss 0.0021120616793632507   ###   Correct predictions 0.8975\n",
      "Epoch 120  ###   Avg-Loss 0.002086097002029419   ###   Correct predictions 0.8975\n",
      "Epoch 121  ###   Avg-Loss 0.0021203896403312683   ###   Correct predictions 0.89375\n",
      "Epoch 122  ###   Avg-Loss 0.002081574648618698   ###   Correct predictions 0.90125\n",
      "Epoch 123  ###   Avg-Loss 0.002098546475172043   ###   Correct predictions 0.89625\n",
      "Epoch 124  ###   Avg-Loss 0.0020746497809886934   ###   Correct predictions 0.90875\n",
      "Epoch 125  ###   Avg-Loss 0.0021681295335292815   ###   Correct predictions 0.89125\n",
      "Epoch 126  ###   Avg-Loss 0.0021261833608150482   ###   Correct predictions 0.90625\n",
      "Epoch 127  ###   Avg-Loss 0.0020812276005744935   ###   Correct predictions 0.89875\n",
      "Epoch 128  ###   Avg-Loss 0.0021395283937454225   ###   Correct predictions 0.89625\n",
      "Epoch 129  ###   Avg-Loss 0.0022570984065532684   ###   Correct predictions 0.89\n",
      "Epoch 130  ###   Avg-Loss 0.0021125401556491854   ###   Correct predictions 0.90125\n",
      "Epoch 131  ###   Avg-Loss 0.0021096372604370115   ###   Correct predictions 0.89625\n",
      "Epoch 132  ###   Avg-Loss 0.0021685321629047395   ###   Correct predictions 0.89625\n",
      "Epoch 133  ###   Avg-Loss 0.002167028188705444   ###   Correct predictions 0.8975\n",
      "Epoch 134  ###   Avg-Loss 0.002117723822593689   ###   Correct predictions 0.8975\n",
      "Epoch 135  ###   Avg-Loss 0.0021560899913311005   ###   Correct predictions 0.89375\n",
      "Epoch 136  ###   Avg-Loss 0.00217378169298172   ###   Correct predictions 0.89875\n",
      "Epoch 137  ###   Avg-Loss 0.002112492173910141   ###   Correct predictions 0.89375\n",
      "Epoch 138  ###   Avg-Loss 0.0021403497457504273   ###   Correct predictions 0.89625\n",
      "Epoch 139  ###   Avg-Loss 0.0021941222250461577   ###   Correct predictions 0.8975\n",
      "Epoch 140  ###   Avg-Loss 0.002133398652076721   ###   Correct predictions 0.89125\n",
      "Epoch 141  ###   Avg-Loss 0.0021412031352519988   ###   Correct predictions 0.8975\n",
      "Epoch 142  ###   Avg-Loss 0.0021418288350105287   ###   Correct predictions 0.89\n",
      "Epoch 143  ###   Avg-Loss 0.002090528905391693   ###   Correct predictions 0.9\n",
      "Epoch 144  ###   Avg-Loss 0.0020878922939300535   ###   Correct predictions 0.9\n",
      "Epoch 145  ###   Avg-Loss 0.002126625329256058   ###   Correct predictions 0.9025\n",
      "Epoch 146  ###   Avg-Loss 0.0021703025698661805   ###   Correct predictions 0.89875\n",
      "Epoch 147  ###   Avg-Loss 0.0021028600633144377   ###   Correct predictions 0.90125\n",
      "Epoch 148  ###   Avg-Loss 0.0021504926681518554   ###   Correct predictions 0.89875\n",
      "Epoch 149  ###   Avg-Loss 0.0021622103452682497   ###   Correct predictions 0.89875\n",
      "Epoch 150  ###   Avg-Loss 0.0020923557877540588   ###   Correct predictions 0.89625\n",
      "Epoch 151  ###   Avg-Loss 0.002095424234867096   ###   Correct predictions 0.9\n",
      "Epoch 152  ###   Avg-Loss 0.0021267318725585937   ###   Correct predictions 0.89625\n",
      "Epoch 153  ###   Avg-Loss 0.0020793309807777406   ###   Correct predictions 0.9\n",
      "Epoch 154  ###   Avg-Loss 0.0020644918084144594   ###   Correct predictions 0.8975\n",
      "Epoch 155  ###   Avg-Loss 0.0020956143736839293   ###   Correct predictions 0.8975\n",
      "Epoch 156  ###   Avg-Loss 0.0020987506210803984   ###   Correct predictions 0.89375\n",
      "Epoch 157  ###   Avg-Loss 0.002146490514278412   ###   Correct predictions 0.9\n",
      "Epoch 158  ###   Avg-Loss 0.0020957326889038085   ###   Correct predictions 0.9\n",
      "Epoch 159  ###   Avg-Loss 0.002110455632209778   ###   Correct predictions 0.8975\n",
      "Epoch 160  ###   Avg-Loss 0.002185174524784088   ###   Correct predictions 0.89125\n",
      "Epoch 161  ###   Avg-Loss 0.0020939233899116517   ###   Correct predictions 0.89875\n",
      "Epoch 162  ###   Avg-Loss 0.00215307280421257   ###   Correct predictions 0.89125\n",
      "Epoch 163  ###   Avg-Loss 0.0021246403455734255   ###   Correct predictions 0.89625\n",
      "Epoch 164  ###   Avg-Loss 0.0021406733989715577   ###   Correct predictions 0.89625\n",
      "Epoch 165  ###   Avg-Loss 0.0021972601115703583   ###   Correct predictions 0.9\n",
      "Epoch 166  ###   Avg-Loss 0.0020952421426773072   ###   Correct predictions 0.89625\n",
      "Epoch 167  ###   Avg-Loss 0.0020894958078861237   ###   Correct predictions 0.89375\n",
      "Epoch 168  ###   Avg-Loss 0.0020859694480895997   ###   Correct predictions 0.89625\n",
      "Epoch 169  ###   Avg-Loss 0.002123457044363022   ###   Correct predictions 0.9\n",
      "Epoch 170  ###   Avg-Loss 0.002098346948623657   ###   Correct predictions 0.89875\n",
      "Epoch 171  ###   Avg-Loss 0.0021686474978923798   ###   Correct predictions 0.89125\n",
      "Epoch 172  ###   Avg-Loss 0.0020766380429267884   ###   Correct predictions 0.9025\n",
      "Epoch 173  ###   Avg-Loss 0.0020893582701683043   ###   Correct predictions 0.89625\n",
      "Epoch 174  ###   Avg-Loss 0.0020893758535385134   ###   Correct predictions 0.905\n",
      "Epoch 175  ###   Avg-Loss 0.0021876700222492216   ###   Correct predictions 0.90125\n",
      "Epoch 176  ###   Avg-Loss 0.0020727656781673433   ###   Correct predictions 0.89375\n",
      "Epoch 177  ###   Avg-Loss 0.002087211012840271   ###   Correct predictions 0.90375\n",
      "Epoch 178  ###   Avg-Loss 0.002143702358007431   ###   Correct predictions 0.89375\n",
      "Epoch 179  ###   Avg-Loss 0.002098761796951294   ###   Correct predictions 0.8975\n",
      "Epoch 180  ###   Avg-Loss 0.0021268966794013976   ###   Correct predictions 0.90125\n",
      "Epoch 181  ###   Avg-Loss 0.0022495988011360167   ###   Correct predictions 0.89\n",
      "Epoch 182  ###   Avg-Loss 0.0020958711206912994   ###   Correct predictions 0.89875\n",
      "Epoch 183  ###   Avg-Loss 0.0021196284890174864   ###   Correct predictions 0.8975\n",
      "Epoch 184  ###   Avg-Loss 0.0021685026586055756   ###   Correct predictions 0.89875\n",
      "Epoch 185  ###   Avg-Loss 0.0021614480018615724   ###   Correct predictions 0.9025\n",
      "Epoch 186  ###   Avg-Loss 0.0020828977227210998   ###   Correct predictions 0.8975\n",
      "Epoch 187  ###   Avg-Loss 0.002146569937467575   ###   Correct predictions 0.89875\n",
      "Epoch 188  ###   Avg-Loss 0.002130833715200424   ###   Correct predictions 0.905\n",
      "Epoch 189  ###   Avg-Loss 0.002144412696361542   ###   Correct predictions 0.90125\n",
      "Epoch 190  ###   Avg-Loss 0.002131014317274094   ###   Correct predictions 0.895\n",
      "Epoch 191  ###   Avg-Loss 0.0021802690625190736   ###   Correct predictions 0.9\n",
      "Epoch 192  ###   Avg-Loss 0.002145068049430847   ###   Correct predictions 0.8975\n",
      "Epoch 193  ###   Avg-Loss 0.0021085891127586363   ###   Correct predictions 0.8975\n",
      "Epoch 194  ###   Avg-Loss 0.002115703523159027   ###   Correct predictions 0.89375\n",
      "Epoch 195  ###   Avg-Loss 0.002082614004611969   ###   Correct predictions 0.8975\n",
      "Epoch 196  ###   Avg-Loss 0.0021343673765659332   ###   Correct predictions 0.88875\n",
      "Epoch 197  ###   Avg-Loss 0.002094375342130661   ###   Correct predictions 0.9025\n",
      "Epoch 198  ###   Avg-Loss 0.002094741463661194   ###   Correct predictions 0.89875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199  ###   Avg-Loss 0.0021512162685394286   ###   Correct predictions 0.89375\n",
      "Epoch 200  ###   Avg-Loss 0.00208706796169281   ###   Correct predictions 0.89875\n",
      "Epoch 201  ###   Avg-Loss 0.002100135236978531   ###   Correct predictions 0.90125\n",
      "Epoch 202  ###   Avg-Loss 0.002144135534763336   ###   Correct predictions 0.89125\n",
      "Epoch 203  ###   Avg-Loss 0.0020716144144535065   ###   Correct predictions 0.90125\n",
      "Epoch 204  ###   Avg-Loss 0.002132171988487244   ###   Correct predictions 0.89875\n",
      "Epoch 205  ###   Avg-Loss 0.0021002674102783203   ###   Correct predictions 0.895\n",
      "Epoch 206  ###   Avg-Loss 0.0020911028981208802   ###   Correct predictions 0.895\n",
      "Epoch 207  ###   Avg-Loss 0.0020838662981987   ###   Correct predictions 0.90625\n",
      "Epoch 208  ###   Avg-Loss 0.0021056993305683135   ###   Correct predictions 0.905\n",
      "Epoch 209  ###   Avg-Loss 0.0021229469776153564   ###   Correct predictions 0.89625\n",
      "Epoch 210  ###   Avg-Loss 0.0020624643564224245   ###   Correct predictions 0.90125\n",
      "Epoch 211  ###   Avg-Loss 0.0020648226141929626   ###   Correct predictions 0.9\n",
      "Epoch 212  ###   Avg-Loss 0.002145279794931412   ###   Correct predictions 0.89875\n",
      "Epoch 213  ###   Avg-Loss 0.0020835797488689425   ###   Correct predictions 0.8925\n",
      "Epoch 214  ###   Avg-Loss 0.0021529135107994078   ###   Correct predictions 0.89875\n",
      "Epoch 215  ###   Avg-Loss 0.0021812224388122557   ###   Correct predictions 0.8975\n",
      "Epoch 216  ###   Avg-Loss 0.0020714248716831207   ###   Correct predictions 0.89625\n",
      "Epoch 217  ###   Avg-Loss 0.002128612697124481   ###   Correct predictions 0.9\n",
      "Epoch 218  ###   Avg-Loss 0.0021317698061466216   ###   Correct predictions 0.905\n",
      "Epoch 219  ###   Avg-Loss 0.0020909613370895387   ###   Correct predictions 0.9\n",
      "Epoch 220  ###   Avg-Loss 0.0021028324961662294   ###   Correct predictions 0.89625\n",
      "Epoch 221  ###   Avg-Loss 0.002093113511800766   ###   Correct predictions 0.8975\n",
      "Epoch 222  ###   Avg-Loss 0.002074725925922394   ###   Correct predictions 0.89375\n",
      "Epoch 223  ###   Avg-Loss 0.002126944214105606   ###   Correct predictions 0.895\n",
      "Epoch 224  ###   Avg-Loss 0.0020962639153003695   ###   Correct predictions 0.895\n",
      "Epoch 225  ###   Avg-Loss 0.0020684316754341126   ###   Correct predictions 0.89375\n",
      "Epoch 226  ###   Avg-Loss 0.0020523492991924284   ###   Correct predictions 0.9\n",
      "Epoch 227  ###   Avg-Loss 0.0020868098735809327   ###   Correct predictions 0.89625\n",
      "Epoch 228  ###   Avg-Loss 0.0021401473879814146   ###   Correct predictions 0.89625\n",
      "Epoch 229  ###   Avg-Loss 0.0020922765135765074   ###   Correct predictions 0.90125\n",
      "Epoch 230  ###   Avg-Loss 0.002123728394508362   ###   Correct predictions 0.90375\n",
      "Epoch 231  ###   Avg-Loss 0.0020917418599128725   ###   Correct predictions 0.89875\n",
      "Epoch 232  ###   Avg-Loss 0.0020763929188251495   ###   Correct predictions 0.8975\n",
      "Epoch 233  ###   Avg-Loss 0.002099200040102005   ###   Correct predictions 0.8975\n",
      "Epoch 234  ###   Avg-Loss 0.002119156867265701   ###   Correct predictions 0.89125\n",
      "Epoch 235  ###   Avg-Loss 0.002094581127166748   ###   Correct predictions 0.9025\n",
      "Epoch 236  ###   Avg-Loss 0.002112274616956711   ###   Correct predictions 0.9025\n",
      "Epoch 237  ###   Avg-Loss 0.002125059813261032   ###   Correct predictions 0.9\n",
      "Epoch 238  ###   Avg-Loss 0.0020462696254253386   ###   Correct predictions 0.90125\n",
      "Epoch 239  ###   Avg-Loss 0.0022120854258537294   ###   Correct predictions 0.89375\n",
      "Epoch 240  ###   Avg-Loss 0.0021627524495124815   ###   Correct predictions 0.89\n",
      "Epoch 241  ###   Avg-Loss 0.0020735353231430055   ###   Correct predictions 0.89875\n",
      "Epoch 242  ###   Avg-Loss 0.002082364410161972   ###   Correct predictions 0.89625\n",
      "Epoch 243  ###   Avg-Loss 0.002074003666639328   ###   Correct predictions 0.905\n",
      "Epoch 244  ###   Avg-Loss 0.0020876920223236086   ###   Correct predictions 0.9\n",
      "Epoch 245  ###   Avg-Loss 0.002080937772989273   ###   Correct predictions 0.895\n",
      "Epoch 246  ###   Avg-Loss 0.0020526759326457977   ###   Correct predictions 0.89375\n",
      "Epoch 247  ###   Avg-Loss 0.002084536850452423   ###   Correct predictions 0.89375\n",
      "Epoch 248  ###   Avg-Loss 0.0020781992375850677   ###   Correct predictions 0.90375\n",
      "Epoch 249  ###   Avg-Loss 0.002160542011260986   ###   Correct predictions 0.89875\n",
      "Epoch 250  ###   Avg-Loss 0.0020603817701339722   ###   Correct predictions 0.9025\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('lin1',nn.Linear(2, 4, bias=True)),\n",
    "    ('relu1',nn.ReLU()),\n",
    "    ('lin2',nn.Linear(4, 8, bias=True)),\n",
    "    ('relu2',nn.ReLU()),\n",
    "    ('lin3',nn.Linear(8, 16, bias=True)),\n",
    "    ('relu3',nn.ReLU()),\n",
    "    ('lin4',nn.Linear(16, 2, bias=True))\n",
    "])\n",
    ")\n",
    "\n",
    "output_frequency = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(250):\n",
    "    loss_sum = 0\n",
    "    correct_pred = 0\n",
    "    for index, (data, target) in enumerate(train_loader):\n",
    "        #print(index)\n",
    "        output = model.forward(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss_sum = loss_sum + loss.data\n",
    "        for i in range(len(output)):\n",
    "            _, ind = torch.max(output[i],0)\n",
    "            label = target[i]\n",
    "            \n",
    "            if ind.data == label.data:\n",
    "                correct_pred +=1\n",
    "        #if index % (10*(output_frequency)) == 0:\n",
    "        #    print(\"#  Epoch  #  Batch  #  Avg-Loss ###############\")\n",
    "        #if index % (output_frequency) == 0 and index > 0:\n",
    "        #    print(\"#  %d  #  %d  #  %f  #\" % (epoch+1, index, loss_sum/output_frequency))\n",
    "        #    loss_sum = 0\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch '+str(epoch+1)+'  ###   Avg-Loss '+str(loss_sum.item()/800)+'   ###   Correct predictions '+str(correct_pred/800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7115,  1.4547]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.6434, -2.7388]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3886,  2.0311]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.9741, -3.1041]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-0.8089,  0.1275]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3526, -2.2834]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.8463, -2.8233]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.9055, -2.8244]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-1.7085,  1.4306]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.7206,  2.3536]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 0.7751, -1.6380]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.4437,  2.1093]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.1609,  1.7324]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-0.5344,  0.0122]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3842,  2.1159]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-1.5523,  1.2432]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.5676, -2.6339]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 2.0958, -3.2221]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 0.9023, -1.6219]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.7551,  2.3714]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3445, -2.3139]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.5533,  2.1857]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3841, -2.4257]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3780,  2.0977]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 0.0020, -0.6190]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 0.1119, -0.9363]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.3832, -2.3543]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.5189, -2.5425]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-1.9774,  1.4456]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.2206, -2.2302]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-0.1666, -0.6191]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.3856,  2.0124]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[ 1.4732, -2.4705]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.4295,  2.1136]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-0.6206,  0.0895]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.7165, -2.7979]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.8907, -2.8468]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[ 1.8076, -2.8020]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "####################\n",
      "tensor([[-2.3121,  1.9327]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n",
      "tensor([[-2.0975,  1.7790]], grad_fn=<AddmmBackward>)\n",
      "tensor([1])\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "for index, (data, target) in enumerate(test_loader):\n",
    "    print(model.forward(data))\n",
    "    print(target)\n",
    "    print('####################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 0.8583333333333333\n"
     ]
    }
   ],
   "source": [
    "net.test(test_loader,120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
      "        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1])\n",
      "tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,\n",
      "        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0])\n",
      "tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n",
      "        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,\n",
      "        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0])\n",
      "tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
      "        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0])\n",
      "tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
      "        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0])\n",
      "tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1])\n",
      "tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n",
      "        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1])\n",
      "tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0])\n",
      "tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0])\n",
      "tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
      "        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1])\n",
      "[0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "from pandas import DataFrame\n",
    "import torch\n",
    "\n",
    "x1 = []\n",
    "x2 = []\n",
    "y = []\n",
    "for (data,label) in train_loader:\n",
    "    res = torch.argmax(net.forward(data), dim=1)\n",
    "    print(res)\n",
    "    for point in range(len(data)):\n",
    "        x1.append(data[point][0].item())\n",
    "        x2.append(data[point][1].item())\n",
    "        y.append(res[point].item())\n",
    "#print(x1)\n",
    "df = DataFrame(dict(x=x1, y=x2, label=y))\n",
    "print(y)\n",
    "colors = {0:'red', 1:'blue'}\n",
    "fig, ax = pyplot.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 0.875\n"
     ]
    }
   ],
   "source": [
    "correct_pred = 0\n",
    "for _, (data, label) in enumerate(test_loader):\n",
    "    prediction = model.forward(data)\n",
    "    _, ind = torch.max(prediction,1)\n",
    "    if ind.data == label.data:\n",
    "        correct_pred +=1\n",
    "print('Correct predictions: '+str(correct_pred/40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################\n",
      "tensor([-0.7453,  0.6668])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:340: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Networks\\ResNet.py:343: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_dict.update({x_key:torch.tensor(x, requires_grad=True)})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.5517,  3.5517], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([ 1.4441, -0.3960])\n",
      "tensor([ 3.8032, -3.8032], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([-0.7237,  0.6901])\n",
      "tensor([-3.5532,  3.5532], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([0.4723, 0.8815])\n",
      "tensor([-1.5511,  1.5511], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([0.9284, 0.3717])\n",
      "tensor([ 2.0221, -2.0221], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([ 0.1587, -0.0406])\n",
      "tensor([ 0.7871, -0.7871], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([1.9754, 0.2797])\n",
      "tensor([ 3.7395, -3.7395], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([0.0246, 0.2797])\n",
      "tensor([-0.9989,  0.9989], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([0.0476, 0.9989])\n",
      "tensor([-2.9615,  2.9615], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([ 1.8053, -0.0929])\n",
      "tensor([ 3.8243, -3.8243], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([ 1.6056, -0.2958])\n",
      "tensor([ 3.8254, -3.8254], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([0.1736, 0.9848])\n",
      "tensor([-2.6809,  2.6809], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([ 0.5000, -0.3660])\n",
      "tensor([ 2.7974, -2.7974], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([0.9819, 0.1893])\n",
      "tensor([ 2.6397, -2.6397], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([ 0.8264, -0.4848])\n",
      "tensor([ 3.4583, -3.4583], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([-0.9029,  0.4298])\n",
      "tensor([-3.4797,  3.4797], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([ 1.8580, -0.0137])\n",
      "tensor([ 3.8150, -3.8150], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n",
      "######################\n",
      "tensor([0.7453, 0.6668])\n",
      "tensor([ 0.3133, -0.3133], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([0.2048, 0.9788])\n",
      "tensor([-2.5970,  2.5970], grad_fn=<SqueezeBackward3>)\n",
      "tensor(0)\n",
      "######################\n",
      "tensor([0.1262, 0.0138])\n",
      "tensor([ 0.4475, -0.4475], grad_fn=<SqueezeBackward3>)\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "for index, (data, label) in enumerate(train_loader):\n",
    "    if index == 2:\n",
    "        for i in range(len(data)):\n",
    "            print('######################')\n",
    "            print(data[i])\n",
    "            print(net.forward(data[i]))\n",
    "            print(label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FC0', 'FC1', 'FC2', 'FC3']\n",
      "MSALinearLayer(\n",
      "  (linear): Linear(in_features=2, out_features=4, bias=False)\n",
      ")\n",
      "ReluLayer()\n",
      "MSALinearLayer(\n",
      "  (linear): Linear(in_features=4, out_features=2, bias=False)\n",
      ")\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'FC3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-1a0203474ed2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'FC1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'FC2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'FC3'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'FC3'"
     ]
    }
   ],
   "source": [
    "print(net.fc_keys)\n",
    "print(net.layers_dict['FC0'])\n",
    "print(net.layers_dict['FC1'])\n",
    "print(net.layers_dict['FC2'])\n",
    "print(net.layers_dict['FC3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:164: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  compared_weight_signs = torch.tensor(torch.ne(new_weight_signs, old_weight_signs).detach(), dtype=torch.float32)\n",
      "C:\\Users\\pscha\\Documents\\GitHub\\OC-Methods-for-DL\\Layers\\Layers.py:169: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_weights = new_weight_signs*torch.tensor(torch.ge(torch.abs(self.m_accumulated),self.rho*max_weight_elem*torch.ones_like(self.m_accumulated)).detach(),dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "from Dataset.Dataset import loadMNIST\n",
    "from Networks.ResNet import ConvNet, FCNet\n",
    "\n",
    "#net = ConvNet(3, [1], [3,6], num_fc=2, sizes_fc=[100,10])\n",
    "net = FCNet(num_fc=4,sizes_fc=[784,2048,2048,2048,10], bias=False)\n",
    "train_loader, test_loader = loadMNIST('Dataset/input/mnist_train.csv', 'Dataset/input/mnist_test.csv')\n",
    "#net.train_backprop(1,train_loader)\n",
    "#net.test(test_loader,10000)\n",
    "print(train_loader.batch_size)\n",
    "\n",
    "net.train_msa(1,train_loader)\n",
    "\n",
    "#for index, (data, target) in enumerate(train_loader):\n",
    "#    if index == 1:\n",
    "#        print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 4, 1])\n",
      "tensor([[-0.5047, -0.7449,  0.6850,  0.4106, -0.2220],\n",
      "        [-1.2125, -2.6010, -1.0037,  0.0987,  1.0532],\n",
      "        [-0.4957, -0.2819, -2.5197,  0.0198, -0.2908]])\n",
      "tensor(1.4569)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "batch_size = 1\n",
    "a = torch.LongTensor(batch_size).random_(5)\n",
    "b = torch.randn(batch_size, 5)\n",
    "print(a)\n",
    "print(b)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(b, a)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "index 10 is out of bounds for dim with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-f8db13b9caa0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'FC4'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: index 10 is out of bounds for dim with size 10"
     ]
    }
   ],
   "source": [
    "test = net.layers_dict['FC4'].linear.weight\n",
    "print(test[(test == 0).nonzero()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.test(test_loader,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1574, -0.1913,  0.3191],\n",
      "        [ 0.4584,  0.3066, -0.3247],\n",
      "        [ 0.2495,  0.4553,  0.0500],\n",
      "        [ 0.3111, -0.3222,  0.2152]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.4820, -0.5667, -0.2615,  0.5324], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "test_layer = torch.nn.Linear(3, 4)\n",
    "print(test_layer.weight)\n",
    "print(test_layer.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lin1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-ce97ca40c88a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlin1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'lin1' is not defined"
     ]
    }
   ],
   "source": [
    "print(model[lin1].parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-673cdcf4d885>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavg_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "print(net.avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6., 18., 36.], grad_fn=<SumBackward2>)\n",
      "tensor([[1., 1., 1.],\n",
      "        [2., 2., 2.],\n",
      "        [3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([[1,2,3],[2,3,4],[3,4,5]],dtype=torch.float32, requires_grad=True)\n",
    "y = torch.tensor([[1,1,1],[2,2,2],[3,3,3]],dtype=torch.float32)\n",
    "\n",
    "z = torch.sum(x*y, dim=1)\n",
    "print(z)\n",
    "#z.backward(torch.FloatTensor([0,0,1]), retain_graph=True)\n",
    "#z.backward(torch.FloatTensor([0,1,0]), retain_graph=True)\n",
    "z[0].backward(retain_graph=True)\n",
    "z[1].backward(retain_graph=True)\n",
    "z[2].backward(retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3490, 0.1698, 0.0949])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "x = torch.tensor([[-1,1,2],[1,-2,-1],[-1,2,-1]], dtype=torch.float32)\n",
    "y = torch.tensor([2,0,1])\n",
    "\n",
    "print(criterion(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(3%3 == 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
